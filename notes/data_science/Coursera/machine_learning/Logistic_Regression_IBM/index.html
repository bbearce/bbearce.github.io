<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>5. Logistic Regression - Ben's Code Journal</title>
    <link href="../../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../../css/highlight.css">
    <link href="../../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Logistic Regression", url: "#_top", children: [
              {title: "Intro to Logistic Regression", url: "#intro-to-logistic-regression" },
              {title: "Applications", url: "#applications" },
              {title: "Logistic Regression vs Linear Regression", url: "#logistic-regression-vs-linear-regression" },
              {title: "Logistic Regression Training", url: "#logistic-regression-training" },
              {title: "Logistic Regression Code Example", url: "#logistic-regression-code-example" },
          ]},
        ];

    </script>
    <script src="../../../../../js/base.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-link">
        6. Support Vector Machine
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Regression_Trees_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Regression_Trees_IBM/" class="btn btn-xs btn-link">
        4. Regression Trees
      </a>
    </div>
    
  </div>

    

    <h1 id="logistic-regression">Logistic Regression</h1>
<h2 id="intro-to-logistic-regression">Intro to Logistic Regression</h2>
<h3 id="overview">Overview</h3>
<p>Logistic regression is a statistical and machine learning technique for classifying records of a datastr based on the values of the input fields.</p>
<p>Let's say we have a telecommunication dataset that we'd like to analyze in order to understand which customers might leave us next month. This is historical customer data where each row represents one customer. Imagine that you're an analyst at this company and you have to find out who is leaving and why? You'll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group.</p>
<p>In logistic regression, we use one or more independent variables such as tenure, age, and income to predict an outcome, such as churn, which we call the dependent variable representing whether or not customers will stop using the service.  </p>
<p>Logistic regression is analogous to linear regression but tries to predict a categorical or discrete target field instead of a numeric one. In linear regression, we might try to predict a continuous value of variables such as the price of a house, blood pressure of a patient, or fuel consumption of a car. But in logistic regression, we predict a variable which is binary such as yes/no, true/false, successful or not successful, pregnant/not pregnant, and so on, all of which can be coded as zero or one.</p>
<p>In logistic regression independent variables should be continuous. If categorical, they should be dummy or indicator coded. This means we have to transform them to some continuous value.</p>
<p>Please note that logistic regression can be used for both binary classification and multi-class classification.</p>
<h2 id="applications">Applications</h2>
<ul>
<li>To predict the probability of a person having a heart attack within a specified time period</li>
<li>Predict the chance of mortality in an injured patient</li>
<li>Predict whether a patient has a given disease such as diabetes</li>
<li>Predict the likelihood of a customer purchasing a product or halting a subscription as we've done in our churn example</li>
<li>Probability of failure of a given process, system or product</li>
<li>Predict the likelihood of a homeowner defaulting on a mortgage</li>
</ul>
<blockquote>
<p>Notice that in all these examples not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.</p>
</blockquote>
<ul>
<li>one<ul>
<li>inner 1</li>
<li>inner 2</li>
</ul>
</li>
<li>two</li>
</ul>
<p>The question is, when should we use logistic regression?  </p>
<ol>
<li>If your data is binary:  <ul>
<li>0/1, YES/NO, True/False  </li>
</ul>
</li>
<li>If you need probabalistic results:  </li>
<li>When you need a linear decision boundary:  <ul>
<li>The decision boundary of a logistic regression is a line or a plane or a hyper plane  </li>
<li>A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.  </li>
<li>For example, if we have just two features and are not applying any polynomial processing we can obtain an inequality like <span class="arithmatex">\(\theta_0 + \theta_1x_1 + \theta_2x2 &gt; 0\)</span>, which is a half-plane easily plottable.  </li>
</ul>
</li>
<li>You need to understand the impact of a feature.</li>
<li>You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.</li>
<li>That is, after finding the optimum parameters, a feature X with the weight <span class="arithmatex">\(\theta_1\)</span> close to 0 has a smaller effect on the prediction than features with large absolute values of <span class="arithmatex">\(\theta_1\)</span>.</li>
</ol>
<p><img alt="logistic_regression_1.jpg" src="../Images/logistic_regression/logistic_regression_1.jpg" /></p>
<ul>
<li>X is our dataset in the space of real numbers of m by n:  <span class="arithmatex">\(X\epsilon \mathbb{R}^{m \times n}\)</span>  </li>
<li>Y is the class that we want to predict: <span class="arithmatex">\(y\epsilon\{0,1\}\)</span>, which can be either 0 or 1  </li>
<li>Ideally, a logistic regression model, so-called Y hat, can predict that the class of the customer is one, given its features X: <span class="arithmatex">\(\hat{y} = P(y=1|x)\)</span><br />
 It can also be shown quite easily that the probability of a customer being in class zero can be calculated as one minus the probability that the class of the customer is one: <span class="arithmatex">\(P(y=0|x)= 1 - P(y=1|x)\)</span></li>
</ul>
<h2 id="logistic-regression-vs-linear-regression">Logistic Regression vs Linear Regression</h2>
<ul>
<li>
<p>We will learn the difference between linear regression and logistic regression.</p>
</li>
<li>
<p>We go over linear regression and see why it cannot be used properly for some binary classification problems.</p>
</li>
<li>
<p>We also look at the sigmoid function, which is the main part 
of logistic regression. </p>
</li>
</ul>
<p>Recall we want to predict the class of each customer <span class="arithmatex">\(\hat{y} = P(y=1|x)\)</span> and also the probability of each sample belonging to a class.</p>
<blockquote>
<p>y is the label's vector, also called <strong>actual values</strong>, that we would like to predict, and <span class="arithmatex">\(\hat{y}\)</span> is the vector of the predicted values by our model.</p>
</blockquote>
<h3 id="lets-try-linear-regression">Let's Try Linear Regression</h3>
<p>Let's use age to predict chrun (categorical value). We need to plot churn versus age which will have two horizontal areas of points for churn=0 and churn=1.</p>
<p><img alt="logistic_regression_2" src="../Images/logistic_regression/logistic_regression_2.jpg" /></p>
<ul>
<li>Prediction can be represented as <span class="arithmatex">\(\hat{y}=\theta_0 + \theta_1x_1\)</span></li>
<li>Where <span class="arithmatex">\(\theta^T=[\theta_0,\theta_1,...,\theta_n]\)</span></li>
<li>Formally the line is <span class="arithmatex">\(\theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\)</span></li>
<li>X (feature set) can be represented as: <span class="arithmatex">\(X = \left[ \begin{array}{c} 1 \\ x_1 \\ x_2 \\ \end{array} \right]\)</span></li>
</ul>
<p>Seen here we can estimate the regression line and predict churn with <span class="arithmatex">\(\theta^TX\)</span>. Let's try with 13 for <span class="arithmatex">\(x_1\)</span>.</p>
<p><img alt="logistic_regression_3.jpg" src="../Images/logistic_regression/logistic_regression_3.jpg" /></p>
<p>Then set a threshold of 0.5 for determining 0 or 1. Finally <span class="arithmatex">\(\hat{y}\)</span> can be represented as a piecewise function to force it to be categorical. In this example <span class="arithmatex">\(P_1=[13]\)</span> is Class 0. There is one problem here. What is the probability that this customer belongs to class zero?</p>
<p>As you can see, it's not the best model to solve this problem. Also, there are some other issues which verify that linear regression is not the proper method for classification problems. So, as mentioned, if we use the regression line to calculate the class of a point, it always returns a number such as three or negative two, and so on. Then, we should use a threshold, for example, 0.5, to assign that point to either class of zero or one. This threshold works as a step function that outputs zero or one regardless of how big or small, positive or negative the input is. So, using the threshold, we can find the class of a record. Notice that in the step function, no matter how big the value is, as long as it's greater than 0.5, it simply equals one and vice versa. Regardless of how small the value y is, the output would be zero if it is less than 0.5. In other words, there is no difference between a customer who has a value of one or 1,000. The outcome would be one. Instead of having this step function, wouldn't it be nice if we had a smoother line, one that would project these values between zero and one? </p>
<p>Let's use the <strong>Sigmoid Function</strong>.
<img alt="logistic_regression_4.jpg" src="../Images/logistic_regression/logistic_regression_4.jpg" /></p>
<p>It gives a non-step function approach that gives us the probabilty that a point belongs to a certain class instead of the value of y directly.</p>
<p>Instead of calculating the value of Theta transpose x directly, it returns the probability that a Theta transpose x is very big or very small. It always returns a value between 0 and 1, depending on how large the Theta transpose x actually is. </p>
<p>Now, our model is <span class="arithmatex">\(\hat{y}=\sigma(\theta^TX)\)</span>, which represents the probability that the output is 1 given x.</p>
<h4 id="sigmoid-function">Sigmoid Function</h4>
<p><span class="arithmatex">\(\sigma(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}\)</span></p>
<p>When <span class="arithmatex">\(\theta^T\)</span> goes up <span class="arithmatex">\(\sigma(\theta^TX)\)</span> tends towards 1 and when small it tends to 0. This is the probability not the class itself.</p>
<h5 id="what-is-the-output-of-our-model">What is the output of our model?</h5>
<ul>
<li><span class="arithmatex">\(P(Y=1|X)\)</span> - probability of Y=1 given X  </li>
<li><span class="arithmatex">\(P(Y=0|X) = 1-P(Y=1|X)\)</span> - probability of Y=0 given X</li>
</ul>
<p>Ex:
* <span class="arithmatex">\(P(Churn=1|income,age)\)</span> = 0.8
* <span class="arithmatex">\(P(Churn=0|income,age)\)</span> = 1- 0.8 = 0.2</p>
<p>Now we can train our model to set its parameter values in such a way that our model is a good estimate of the probablity of Y given X:  <span class="arithmatex">\(\sigma(\theta^TX)\)</span> -&gt; <span class="arithmatex">\(P(y=1|x)\)</span>.</p>
<ul>
<li><span class="arithmatex">\(\sigma\)</span> is model.</li>
<li><span class="arithmatex">\(P(y=1|x)\)</span> and <span class="arithmatex">\(P(y=0|x)\)</span> are actual probabilities. </li>
</ul>
<h5 id="how-to-train-this-model">How to train this model?</h5>
<p><span class="arithmatex">\(\sigma(\theta^TX)\)</span> -&gt; <span class="arithmatex">\(P(y=1|x)\)</span></p>
<ol>
<li>Initialize <span class="arithmatex">\(\theta\)</span> with random values: <span class="arithmatex">\(\theta\)</span> = [-1, 2]</li>
<li>Calculate model output <span class="arithmatex">\(\hat{y} = \sigma(\theta^TX)\)</span> for a sample customer.<br />
<span class="arithmatex">\(\hat{y} = \sigma([-1,2]] x [2,5])\)</span> = 0.7 - "2 and 5 are age and income"</li>
<li>Compare the output of <span class="arithmatex">\(\hat{y}\)</span> with the actual output of custom, y, and record it as error. Example: <span class="arithmatex">\(Error = 1 - 0.7 = 0.3\)</span>. 1 Here was Churn and 0.7 was the probability of Chrun from model</li>
<li>Calculate the error for all customers. Add up these errors. Total error is the cost of your model and is calculated by your models cost function. The cost function, by the way, basically represents how to calculate the error of the model which is the difference between the actual and the models predicted values. You want low cost (preferrably 0).</li>
<li>Cost is high right now. We need to change to get lower cost. We change <span class="arithmatex">\(\theta\)</span> in such a way to hopefully reduce the total cost. </li>
<li>Go back to step 2. and iterate to get a new cost. We do this until the cost is low enough.</li>
</ol>
<p>One of the most popular ways is gradient descent. Also, there are various ways to stop iterations, but essentially you stop training by calculating the accuracy of your model and stop it when it's satisfactory. </p>
<h2 id="logistic-regression-training">Logistic Regression Training</h2>
<p>Objective - <span class="arithmatex">\(\sigma(\theta^TX)\)</span> -&gt; <span class="arithmatex">\(P(y=1|x)\)</span></p>
<p>Let's look at the cost function.</p>
<p>Cost(<span class="arithmatex">\(\hat{y}, y\)</span>) = <span class="arithmatex">\(\sigma(\theta^TX) - y\)</span></p>
<p>Usually the square of this equation is used because of the possibility of the negative result and for the sake of simplicity, half of this value is considered as the cost function through the derivative process.</p>
<p>Cost(<span class="arithmatex">\(\hat{y}, y\)</span>) = <span class="arithmatex">\(\frac{(\sigma(\theta^TX) - y)^2}{2}\)</span></p>
<blockquote>
<p>Note this is for a particular sample.</p>
</blockquote>
<p>If we want for all samples we need a new equation:<br />
<span class="arithmatex">\(J(\theta) = \frac{1}{m} \sum_{i-1}^mCost(\hat{y},y)\)</span></p>
<blockquote>
<p>This confused me for a second as <span class="arithmatex">\(\theta\)</span> refers to the models parameters. It's the cost across all samples divided by the number of samples. This is also called <strong>mean squared error</strong>. The reason it's a function of <span class="arithmatex">\(\theta\)</span> is because all this "cost" we are measuring is for a specific set of <span class="arithmatex">\(\theta\)</span> parameters.</p>
</blockquote>
<p>Now, how do we find or set the best weights or parameters that minimize this cost function? The answer is, we should calculate the minimum point of this cost function and it will show us the best parameters for our model.</p>
<p>Although we can find the minimum point of a function using the derivative of a function, there's not an easy way to find the global minimum point for such an equation. Given this complexity, describing how to reach the global minimum for this equation is outside the scope of this video.</p>
<p>So, what is the solution? Well we should find another cost function instead, one which has the same behavior but is easier to find its minimum point. Let's plot the desirable cost function for our model. </p>
<p>Let's plot the desirable cost function for our model. Recall that our model is y hat. Our actual value is y which equals zero or one, and our model tries to estimate it as we want to find a simple cost function for our model.</p>
<p>This means our model is best if it estimates y equals one. In this case, we need a cost function that returns zero if the outcome of our model is one, which is the same as the actual label. And the cost should keep increasing as the outcome of our model gets farther from one. And cost should be very large if the outcome of our model is close to zero. </p>
<p>We can see that a function <span class="arithmatex">\(-log(\hat{y})\)</span> looks similar to how we want to model this:</p>
<p><img alt="logistic_regression_5.jpg" src="../Images/logistic_regression/logistic_regression_5.jpg" /></p>
<p>Recall the derivative of the cost function was hard to calculate. So instead we define as:</p>
<div class="arithmatex">\[
Cost(\hat{y},y) = 
\begin{cases}
  -log(\hat{y}) &amp; \text{if } y = 1 \\
  -log(1-\hat{y}) &amp; \text{if } y = 0
\end{cases}
\]</div>
<p>and redefine <span class="arithmatex">\(J(\theta)\)</span> as:<br />
<span class="arithmatex">\(J(\theta) = \frac{1}{m} \sum_{i-1}^m y^i log(\hat{y}^i) + (1-y^i) log(1-\hat{y}^i)\)</span></p>
<h3 id="gradient-descent-to-minimize-the-cost">Gradient Descent To Minimize the Cost</h3>
<p>Let's assume <span class="arithmatex">\(\hat{y} = \sigma(\theta_1x_i + \theta_2x_2)\)</span>  </p>
<p>and</p>
<p><span class="arithmatex">\(J(\theta) = \frac{1}{m} \sum_{i-1}^m y^i log(\hat{y}^i) + (1-y^i) log(1-\hat{y}^i)\)</span></p>
<p>Imagine that it makes a bowl as shown:
<img alt="logistic_regression_6.jpg" src="../Images/logistic_regression/logistic_regression_6.jpg" /></p>
<p>We need to find the minimum of this cost function. We change the parameters of <span class="arithmatex">\((\Delta\theta_1,\Delta\theta_2)\)</span>. We keep taking steps. First while slope is large we take larger steps. As slope gets smaller we take smaller steps.</p>
<h4 id="how-do-we-calculate-slope">How do we calculate slope?</h4>
<p><span class="arithmatex">\(\frac{\delta J}{\delta \theta_1}\)</span> = <span class="arithmatex">\(-\frac{1}{m}\sum_i^m(y^i-\hat{y})x_1^i\)</span></p>
<blockquote>
<p>It's technically some math proof we should look up if we are interested. If the derivative of J is positive with respect to <span class="arithmatex">\(\theta_1\)</span> then J increases as <span class="arithmatex">\(\theta1\)</span> increases. This tells us to move in the opposite direction. We can take smaller steps as the slop decreases.</p>
</blockquote>
<p>That is just for <span class="arithmatex">\(\theta_1\)</span>. We can represent the vector of all the slopes as the gradient vector:
<span class="arithmatex">\(\nabla J = 
\left[ 
\begin{array}{c} 
\frac{\delta J}{\delta \theta_1} \\ 
\frac{\delta J}{\delta \theta_2} \\ 
\frac{\delta J}{\delta \theta_3} \\ 
... \\ 
\frac{\delta J}{\delta \theta_k} \\ 
\end{array} 
\right]\)</span></p>
<p>We can use this vector to change or update all parameters. We take the previous values of the parametes and subtract the error derivative. This results in the new parameters for <span class="arithmatex">\(\theta\)</span> that we know decrease the cost.</p>
<p><span class="arithmatex">\(New\theta = old\theta - \nabla J\)</span></p>
<blockquote>
<p>Note: Units of gradient vector are <span class="arithmatex">\(\delta loss/ \delta \theta\)</span> so that it matches <span class="arithmatex">\(\theta\)</span>'s units. We can change all weights a little bit to move towards lower loss. </p>
</blockquote>
<p>Last we add a learning rate <span class="arithmatex">\(\mu\)</span> (dimensionless number | hyperparameter) to change the step we take towards lowering the cost.</p>
<p><span class="arithmatex">\(New\theta = old\theta - \mu \nabla J\)</span></p>
<h2 id="logistic-regression-code-example">Logistic Regression Code Example</h2>
<pre><code class="language-bash"># Setup Environment
cd ~/Desktop; rm -r temp; # To remove
cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;
</code></pre>
<p>A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.</p>
<p>The dataset includes information about:
*   Customers who left within the last month – the column is called Churn
*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges
*   Demographic info about customers – gender, age range, and if they have partners and dependents
Download the data:</p>
<pre><code class="language-bash">wget &quot;https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv&quot;
</code></pre>
<pre><code class="language-python">import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import jaccard_score
from sklearn.metrics import classification_report
from sklearn.metrics import log_loss
import itertools

churn_df = pd.read_csv(&quot;ChurnData.csv&quot;)
churn_df.head()
# Let's select some features for the modeling.
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]
churn_df['churn'] = churn_df['churn'].astype('int')
churn_df.head()
</code></pre>
<blockquote>
<p>Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm:</p>
</blockquote>
<pre><code class="language-python"># Define X and Y for the dataset
X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])
X[0:5]
y = np.asarray(churn_df['churn'])
y [0:5]
# Normalize the dataset
X = preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
# Train\Test
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)
</code></pre>
<p>Let's build our model using <strong>LogisticRegression</strong> from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers. You can find extensive information about the pros and cons of these optimizers if you search it in the internet.</p>
<p>The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem of machine learning models.
<strong>C</strong> parameter indicates <strong>inverse of regularization strength</strong> which must be a positive float. Smaller values specify stronger regularization.
Now let's fit our model with train set:</p>
<pre><code class="language-python">LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
LR
# Predict on test set
yhat = LR.predict(X_test)
yhat
</code></pre>
<p><strong>predict_proba</strong>  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X):</p>
<pre><code class="language-python">yhat_prob = LR.predict_proba(X_test)
yhat_prob
# Evaluation (Jaccard Index)
jaccard_score(y_test, yhat,pos_label=0)
# Confusion Matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    &quot;&quot;&quot;
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    &quot;&quot;&quot;
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print(&quot;Normalized confusion matrix&quot;)
    else:
        print('Confusion matrix, without normalization')
    print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment=&quot;center&quot;,
                 color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

print(confusion_matrix(y_test, yhat, labels=[1,0]))
# Plot Confusion Matrix
# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])
np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')
plt.show()
# Print Classification Report
print (classification_report(y_test, yhat))
log_loss(y_test, yhat_prob)
</code></pre>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-link">
        6. Support Vector Machine
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Regression_Trees_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Regression_Trees_IBM/" class="btn btn-xs btn-link">
        4. Regression Trees
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>