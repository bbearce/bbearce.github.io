<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Cluster - Ben's Code Journal</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Welcome To The Martinos Center", url: "#_top", children: [
              {title: "Loggin in", url: "#loggin-in" },
              {title: "Home Directories and Data", url: "#home-directories-and-data" },
              {title: "Anaconda", url: "#anaconda" },
              {title: "Submitting Jobs", url: "#submitting-jobs" },
              {title: "Singularity", url: "#singularity" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../MKDocs/Quick%20Notes/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../MKDocs/Quick%20Notes/" class="btn btn-xs btn-link">
        MKDocs
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../machine_learning/Coursera/Logistic_Regression_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../machine_learning/Coursera/Logistic_Regression_IBM/" class="btn btn-xs btn-link">
        5. Logistic Regression
      </a>
    </div>
    
  </div>

    

    <h1 id="welcome-to-the-martinos-center">Welcome To The Martinos Center</h1>
<p><a href="https://www.nmr.mgh.harvard.edu/">Official Website</a></p>
<p><img alt="martinos_homepage" src="../martinos_homepage.jpg" /></p>
<p>Locate the <a href="https://www.nmr.mgh.harvard.edu/intranet">Intranet</a> in the top right on the homepage. This hosts all things Martinos related and should be your first stop for information.</p>
<p>Other userful links within the intranet:  </p>
<ul>
<li><a href="https://www.nmr.mgh.harvard.edu/intranet/computer">Compute Resources</a>  </li>
<li><a href="https://www.nmr.mgh.harvard.edu/faq">Frequently Asked Questions</a>  </li>
<li><a href="https://www.nmr.mgh.harvard.edu/martinos/userInfo/computer/index.php">Martinos User Information (sometimes more granular about technical things)</a>  </li>
</ul>
<p>Pre-requisites:   </p>
<ul>
<li>Partners ID  </li>
<li><a href="https://www.nmr.mgh.harvard.edu/intranet/computer/request-account">Martinos Account</a>  </li>
<li>MLSC Access  </li>
</ul>
<h2 id="loggin-in">Loggin in</h2>
<p>Test access by sshing into <code>mlsc.nmr.mgh.harvard.edu</code> while on the Partners <a href="https://rc.partners.org/it-services/remote-access">VPN (scroll down a little)</a>.</p>
<blockquote>
<p>You should be able to do this in the CMD in Windows or your terminal in MacOS\Linux.</p>
</blockquote>
<p>Ex:</p>
<pre><code class="language-bash">bbearce@pop-os:~$ ssh bb927@mlsc.nmr.mgh.harvard.edu

 /$$      /$$ /$$        /$$$$$$   /$$$$$$
| $$$    /$$$| $$       /$$__  $$ /$$__  $$
| $$$$  /$$$$| $$      | $$  \__/| $$  \__/
| $$ $$/$$ $$| $$      |  $$$$$$ | $$
| $$  $$$| $$| $$       \____  $$| $$
| $$\  $ | $$| $$       /$$  \ $$| $$    $$
| $$ \/  | $$| $$$$$$$$|  $$$$$$/|  $$$$$$/
|__/     |__/|________/ \______/  \______/

Please cite the Massachusetts Life Sciences Center 
on your posters and publications for any data analyzed 
on this cluster.

&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;

Last login: Wed Sep 22 16:06:16 2021 from 10.251.11.74
mlsc-login[0]:~$
</code></pre>
<h2 id="home-directories-and-data">Home Directories and Data</h2>
<h3 id="home-directories">Home Directories</h3>
<p>We have user accounts that are synced between all martinos machines and they all live on a drive managed by help desk. In order to keep space concerns down with 100s of users, they limit our space allowance to the point where it is unusable for more than configuratoin or tiny tests.</p>
<p>Therefore everything (99% of everything) is stored on hard drives or network storage.</p>
<p>Because of this the most important step is to symlink some important directories to change where the data stored for various programs (docker, singularity, python) will show up ultimately (not in ~/).</p>
<blockquote>
<p>Notice how <code>pwd</code> prints something like <strong>/homes/#/\&lt;username></strong> rather than <strong>/home/\&lt;username></strong>. This is because we all live in <strong>/homes</strong> which is special directory (drive) managed by <a href="help@nmr.mgh.harvard.edu">Help Desk</a>.</p>
</blockquote>
<p>Ex:</p>
<pre><code class="language-bash">mlsc-login[0]:~$ pwd
/homes/3/bb927
</code></pre>
<h3 id="drives-and-network-storage">Drives and Network Storage</h3>
<p><a href="https://www.nmr.mgh.harvard.edu/martinos/userInfo/computer/diskSpace.php">Martinos Official Notes</a></p>
<ul>
<li>Main Storage: /cluster/qtim - 20TB # Standard cluster storage.</li>
<li>New Storage: /vast/qtim - 2TB # Ultra Fast and was bought with the cluster's grant money.</li>
</ul>
<h4 id="drives">Drives</h4>
<p>There are no drives on the cluster. We do have our own Desktops and Servers in the QTIM lab, however that is not the focus of this tutorial. </p>
<blockquote>
<p>We will need to dip into this a little in the Singularity section at the end of this tutorial as we need a drive to build Singularity images.</p>
</blockquote>
<h4 id="network-storage">Network Storage</h4>
<p>As the name implies /cluster/qtim is our lab's (QTIM) storage for use with the cluster. Everyone has a user folder as demonstrated here:</p>
<pre><code class="language-bash">mlsc-login[0]:~$ ls /cluster/qtim
ID-qtim  machine_backups  users

mlsc-login[0]:~$ ls -la /cluster/qtim/users
total 154
drwxrwsr-x. 20 bb927    qtim 20 Sep 21 16:26 .
drwxrws---.  4 root     qtim  5 May 23 10:05 ..
drwxrwsr-x. 13 ai347    qtim 28 Aug  2 15:32 ai347
drwxrwsr-x. 22 apv12    qtim 41 Sep  5 16:46 apv12
drwxrwsr-x.  2 aza24    qtim  3 Jun  4 12:47 aza24
drwxrwsr-x. 15 bb927    qtim 24 Aug  5 18:14 bb927
drwxrwsr-x.  4 cl48     qtim  4 May 11 14:14 cl48
drwxrwsr-x.  3 cni1     qtim  3 Sep 21 17:07 cni1
drwxrwsr-x.  5 cpb28    qtim  5 Mar  4  2021 cpb28
drwxrwsr-x.  2 dd86     qtim  2 Apr 28 21:50 dd86
drwxrwsr-x.  5 gc660    qtim  6 Sep 22 14:41 gc660
drwxrwsr-x.  3 ij063    qtim  3 Aug 11 12:52 ij063
drwxrwsr-x.  9 jn85     qtim  9 Jul 21 13:26 jn85
drwxrwsr-x.  3 kalpathy qtim  3 Aug  4  2020 kalpathy
drwxrwsr-x.  7 nt771    qtim 52 Sep 20 08:20 sakshi
drwxrwsr-x. 10 sf172    qtim 10 May 28 14:20 sf172
drwxrwsr-x. 15 si74     qtim 20 Sep 11 06:38 si74
drwxrwsr-x.  2 bb927    qtim  4 Sep 21 17:31 slurm_jobs
drwxrwsr-x. 27 va914    qtim 35 Aug  2 13:12 va914
drwxr-sr-x. 35 bb927    qtim 43 Sep 10  2019 yifen
</code></pre>
<p>Go ahead and make yourself a directory here.</p>
<blockquote>
<p>Note we do not have <strong>sudo</strong> on these machines and everyone owns their own <strong>folder</strong> and are in the <strong>qtim</strong> group. These are denoted in columns <strong>3</strong> and <strong>4</strong>. We can technically add data to each others folders in their default state so becareful.</p>
</blockquote>
<p>Ex:</p>
<pre><code>&lt;permissions&gt;. &lt;hard-links&gt; &lt;user&gt;    &lt;group&gt; &lt;size&gt; &lt;Month  Day Time&gt; &lt;item&gt;
</code></pre>
<blockquote>
<p>This is the official place to do work, store data and code. </p>
</blockquote>
<p>This location is available you from any Martinos machine. It is 20TB large but we have a tendancy to fill up data fast. I'm in the process of clearing some out so don't worry, but there is enough room for this demo.</p>
<h2 id="anaconda">Anaconda</h2>
<p>We will need a python source for part of this demo. The Martinos center has setup an Anaconda distribution we can use thathas scientific modules as part of it. </p>
<p><a href="https://freesurfer.net/fswiki/DevelopersGuide/NMRCenterPython/UsersGuide">Instructions for setup</a></p>
<p>For now run this:</p>
<pre><code class="language-bash">export PATH=/usr/pubsw/packages/python/anaconda3.7/bin:${PATH}
</code></pre>
<p>Now you can use python:</p>
<pre><code class="language-bash">mlsc-login[0]:bb927$ python
Python 3.7.3 (default, Mar 27 2019, 22:11:17) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; 
</code></pre>
<p>and list conda packages available:</p>
<pre><code class="language-bash">mlsc-login[0]:bb927$ conda list
# packages in environment at /usr/pubsw/packages/python/anaconda3-2019.03:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  
_libgcc_mutex             0.1                        main  
_r-mutex                  1.0.0               anacondar_1  
_tflow_select             2.1.0                       gpu  
absl-py                   0.7.1                    py37_0  
alabaster                 0.7.12                   py37_0  
...
...
</code></pre>
<blockquote>
<p>Additional instructions show you how to set your system path and properly install new packages.</p>
</blockquote>
<h2 id="submitting-jobs">Submitting Jobs</h2>
<p>The Martinos Center Help Desk has created official instructions for the Cluster's use located at <a href="https://it.martinos.org/mlsc-cluster/">https://it.martinos.org/mlsc-cluster/</a>.</p>
<p>It uses <a href="https://slurm.schedmd.com/documentation.html">SLURM</a> to manage work loads.</p>
<p>The Martinos cluster docs above give the details but the most important piece is the resources available:</p>
<table>
<thead>
<tr>
<th>Description</th>
<th>#Nodes</th>
<th>#Cores</th>
<th>RAM</th>
<th>#GPU</th>
<th>Partition</th>
<th>Scratch</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dell R440 Server</td>
<td>30</td>
<td>24</td>
<td>384GB</td>
<td>none</td>
<td>basic</td>
<td>1.5TB</td>
</tr>
<tr>
<td>RTX 6000 GPU Server</td>
<td>2</td>
<td>32</td>
<td>1.5TB</td>
<td>8</td>
<td>rtx6000</td>
<td>7.0TB</td>
</tr>
<tr>
<td>RTX 8000 GPU Server</td>
<td>5</td>
<td>32</td>
<td>1.5TB</td>
<td>4 have 10, 1 has 4</td>
<td>rtx8000</td>
<td>7.0TB</td>
</tr>
<tr>
<td>NVIDIA DGX A100</td>
<td>4</td>
<td>64</td>
<td>1.0TB</td>
<td>8</td>
<td>dgx-a100</td>
<td>14TB</td>
</tr>
</tbody>
</table>
<ul>
<li>R440 Server - CPU compute cluster</li>
<li>RTX6000 - 24GB GPU RAM</li>
<li>RTX8000 - 48GB GPU RAM</li>
<li>A100 - 40GB GPU RAM (Newest, fastest and almost largest)</li>
</ul>
<blockquote>
<p>Namely the <strong>Partition column</strong>. This is the resource you will select later. The 1st is a CPU and the last 3 are incrementally larger/faster GPUs.</p>
</blockquote>
<p>You should already be signed in. Change directory to the qtim cluster network storage now (be sure to have made your own directory with <code>mkdir &lt;username&gt;</code> where \&lt;username> is your Partners ID and it should be located at /cluster/qtim/users/. Also this directory is one level higher than your user directory):</p>
<p>Ex:</p>
<pre><code class="language-bash">mlsc-login[0]:~$ cd /cluster/qtim/users
</code></pre>
<p>Notice a non-user directory called <strong>slurm_jobs</strong></p>
<pre><code class="language-bash">mlsc-login[0]:users$ ls slurm_jobs
README.md  sample_job.py
</code></pre>
<p>Here you will find a README.md file for using the cluster. It follows from the notes in the link above, but I find it handy. In it you will find two main ways to use the job scheduler SLURM.</p>
<p>They are:</p>
<pre><code class="language-bash">$ jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py
</code></pre>
<p>and </p>
<pre><code class="language-bash">srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash
</code></pre>
<p>Let's break the flags down:</p>
<pre><code>Required argument flags:
  -A &lt;account&gt;     Fairshare/billing account to use (qtim)
  -m &lt;memlimit&gt;    Max memory your job will use before killed (RAM).
  -p &lt;partition&gt;   Partition to submit the job to (machine type)
  -t &lt;timelimit&gt;   Max real time job will use before killed (DD-HH:MM:SS)
</code></pre>
<p>Other optional flags:</p>
<pre><code>  -c &lt;numcores&gt;       Number of cores per node required (default 1)
  -G &lt;numgpus&gt;        Number of GPUS required (default 0)
  -M &lt;mailtype&gt;       Send mail events. Comma separated list of:
                        BEGIN, END, FAIL, REQUEUE, ALL (previous four),
                        TIME_LIMIT_90, TIME_LIMIT_80, TIME_LIMIT_50,
                        ARRAY_TASKS, NONE (default)
  --mail-user &lt;user&gt;  Send mail to &lt;user&gt; (default is current user)

</code></pre>
<p>The more readable versions are:</p>
<pre><code class="language-bash">$ jobsubmit -p rtx6000 \
            -A qtim \
            -m 128G \
            -t 0-00:5:00 \
            -c 3 \
            -G 1 \
            -M ALL \
            --mail-user bbearce@mgh.harvard.edu \
            python sample_job.py
</code></pre>
<pre><code class="language-bash">$ srun -p rtx6000 \
       -A qtim \
       --mem 128G \
       -t 0-00:5:00 \
       -c 3 \
       -G 1 \
       --mail-user bbearce@mgh.harvard.edu \
       --pty /bin/bash
</code></pre>
<p><code>jobsubmit</code> allows for running of a script file while <code>srun</code> allows interactice mode.</p>
<h3 id="script-execution">Script Execution</h3>
<p>Let's try to submit a script (sample_job.py), but first let's open it:</p>
<p>/cluster/qtim/users/slurm_jobs/sample_job.py</p>
<pre><code class="language-python">import torch

if __name__ == &quot;__main__&quot;:
    gpu_status = torch.cuda.is_available()
    with open(&quot;/cluster/qtim/users/bb927/slurm_jobs/output.txt&quot;, &quot;w&quot;) as f:
        f.write(&quot;Did this work? GPUs available: {}&quot;.format(gpu_status));
</code></pre>
<p>This when run, will put a file called <strong>output.txt</strong> in <strong>/cluster/qtim/users/bb927/slurm_jobs</strong>. However you might not have made <strong>slurm_jobs</strong> in your user directory yet so go ahead and create it and also copy <strong>sample_job.py</strong> to your <strong>slurm_jobs</strong> directory.
Ex:</p>
<pre><code class="language-bash">mlsc-login[0]:bb927$ mkdir /cluster/qtim/users/bb927/slurm_jobs

mlsc-login[0]:bb927$ cp ../slurm_jobs/sample_job.py ./slurm_jobs/
</code></pre>
<p>Next change your copy of <strong>sample_job.py</strong> to edit the user folder to your own:<br />
from:</p>
<pre><code class="language-python">with open(&quot;/cluster/qtim/users/bb927/slurm_jobs/output.txt&quot;, &quot;w&quot;) as f:
</code></pre>
<p>to:</p>
<pre><code class="language-python">with open(&quot;/cluster/qtim/users/&lt;user directory you already made made&gt;/slurm_jobs/output.txt&quot;, &quot;w&quot;) as f:
</code></pre>
<p>Now we should be able to execute this. Change directories into your personal <strong>slurm_jobs</strong> folder and run this command substituting your <strong>--mail-user</strong> (email address):</p>
<pre><code class="language-bash">jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py
</code></pre>
<p>You will get an email and eventually <strong>output.txt</strong> will be in your <strong>slurm_jobs</strong> directory:</p>
<pre><code class="language-bash">mlsc-login[0]:slurm_jobs$ ls
README.md  sample_job.py

mlsc-login[0]:slurm_jobs$ ls
output.txt  README.md  sample_job.py
</code></pre>
<p>Look inside <strong>output.txt</strong>:</p>
<pre><code class="language-bash">mlsc-login[0]:slurm_jobs$ cat output.txt 
Did this work? GPUs available: True
</code></pre>
<p>That is how you submit a script.</p>
<h3 id="interactive-mode">Interactive Mode</h3>
<p>Maybe you haven't written a script yet or any pipeline. You can reserve access with <code>srun</code> and <code>--pty /bin/bash</code>.</p>
<pre><code class="language-bash">srun -p rtx6000 \
       -A qtim \
       --mem 4G \
       -t 0-00:1:00 \
       -c 3 \
       -G 1 \
       --mail-user bbearce@mgh.harvard.edu \
       --pty /bin/bash

[bb927@rtx-01 slurm_jobs]$
</code></pre>
<p>Notice how your terminal changes to indicate that you are connected to another server:</p>
<pre><code class="language-bash">[bb927@rtx-01 slurm_jobs]$ hostname
rtx-01.nmr.mgh.harvard.edu
</code></pre>
<p>This is the normal hostname:</p>
<pre><code class="language-bash">mlsc-login[0]:slurm_jobs$ hostname
mlsc-login.nmr.mgh.harvard.edu
</code></pre>
<blockquote>
<p>If you can't get a job it's because the "fair share" system implemented to make sure everyone has fair access is not letting you as all those resources are spoken for. Usually the smaller resources are more in demand so for demo purposes we can choose large compute and little time to ensure we get something.</p>
</blockquote>
<p>Ex:</p>
<pre><code class="language-bash">rtx-01[0]:slurm_jobs$ srun -p rtx6000 \
&gt;        -A qtim \
&gt;        --mem 4G \
&gt;        -t 0-00:1:00 \
&gt;        -c 3 \
&gt;        -G 1 \
&gt;        --mail-user bbearce@mgh.harvard.edu \
&gt;        --pty /bin/bash
srun: Job 565212 step creation temporarily disabled, retrying (Requested nodes are busy)
</code></pre>
<p>Try an RTX 8000 then:</p>
<pre><code class="language-bash">srun -p rtx8000 \
       -A qtim \
       --mem 4G \
       -t 0-00:1:00 \
       -c 3 \
       -G 1 \
       --mail-user bbearce@mgh.harvard.edu \
       --pty /bin/bash
rtx-06[0]:slurm_jobs$
</code></pre>
<p>And if all else fails, try an A100 then:</p>
<pre><code class="language-bash">srun -p dgx-a100 \
       -A qtim \
       --mem 4G \
       -t 0-00:1:00 \
       -c 3 \
       -G 1 \
       --mail-user bbearce@mgh.harvard.edu \
       --pty /bin/bash
A100-04[0]:slurm_jobs$
</code></pre>
<p>Once in a machine you can look around but know it's a different server. Also notice how <strong>/cluster/qtim</strong> is still available. This is key as our network storage is always available on every node (server) in the cluster. </p>
<pre><code class="language-bash"> rtx-01[0]:slurm_jobs$ ls /cluster/qtim
ID-qtim  machine_backups  users
</code></pre>
<p>I'm also in the starting directory I ran the script from and I can also run the previous script from here too:</p>
<pre><code class="language-bash">rtx-01[0]:slurm_jobs$ pwd
/cluster/qtim/users/bb927/slurm_jobs
rtx-01[0]:slurm_jobs$ python sample_job.py 
rtx-01[0]:slurm_jobs$ ls
output.txt  sample_job.py
</code></pre>
<h2 id="singularity">Singularity</h2>
<p><a href="https://www.nmr.mgh.harvard.edu/martinos/userInfo/computer/docker.php">Martinos Singularity Instructions</a></p>
<p>In a nut shell this is what needs to be executed:</p>
<h3 id="create-singularity-folder-in-your-cluster-user-space">Create <strong>singularity</strong> folder in your cluster user space</h3>
<blockquote>
<p>(make sure ~/.singularity does not already exist first)</p>
</blockquote>
<pre><code class="language-bash">mlsc-login[0]:~$ cd /cluster/qtim/users/bb927
mlsc-login[0]:~$ mkdir singularity
mlsc-login[0]:~$ ln -s /cluster/qtim/users/bb927/singularity ~/.singularity
</code></pre>
<p>You can now put things in either ~/.singularity or /cluster/qtim/users/bb927/singularity and they will be in /cluster/qtim/users/bb927/singularity.</p>
<h3 id="set-some-environment-variables">Set some environment variables</h3>
<pre><code class="language-bash">mkdir ~/.singularity/tmp
export SINGULARITY_TMPDIR=~/.singularity/tmp
mkdir ~/.singularity/cache
export SINGULARITY_CACHEDIR=~/.singularity/cache
</code></pre>
<blockquote>
<p>(do this or singularity will fill up workstation OS disk at /tmp)
   (in fact put this setting in your .bashrc or .cshrc)</p>
</blockquote>
<p>Test by listing home directory:</p>
<p>Ex.</p>
<pre><code class="language-bash">mlsc-login[0]:bb927$ ls -la ~
...
lrwxrwxrwx.   1 bb927 bb927     37 Sep 24 01:38 .singularity -&gt; /cluster/qtim/users/bb927/singularity
...
</code></pre>
<h3 id="using-premade-containers">Using Premade Containers</h3>
<blockquote>
<p>You might need an account on <a href="https://hub.docker.com/">DockerHUB</a></p>
</blockquote>
<h4 id="build">Build</h4>
<p>We will be bootstraping our containers from <a href="https://hub.docker.com/">DockerHUB</a>.</p>
<p>Ex:</p>
<pre><code>mlsc-login[0]:bb927$ singularity build ubuntu.simg docker://ubuntu:latest
WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process
INFO:    Starting build...
...
</code></pre>
<p>When done view it:</p>
<pre><code class="language-bash">mlsc-login[0]:bb927$ ls
ubuntu.simg
</code></pre>
<h4 id="shell-into-container">Shell Into Container</h4>
<p>Shell into it with <code>singularity shell</code>:</p>
<pre><code class="language-bash">mlsc-login[0]:test$ singularity shell ubuntu.simg 
Singularity&gt; 
</code></pre>
<p>You can execute code and look around.</p>
<blockquote>
<p>Must know:</p>
</blockquote>
<p>[1] Images are read-only by default...more on that later.</p>
<p>[2] You are not root inside your image, but rather your username on linux (Ex: bb927).</p>
<p>[3] Remember Singularity wants to auto --bind mount these 3 directories in the background whether you say to or not:
* /home/$USER
* /tmp
* $PWD</p>
<p>[4] For editable images (sandboxes) you must create your mount points ahead of time in the sandbox. (More shortly)</p>
<p>Let's talk about [3] real quick. If you do an <code>ls</code> it looks like we are in our home directory. Inside this container Singularity is making <strong>/homes/3/bb927</strong> point to <strong>/homes/3/bb927</strong> on the mlsc. This means anything you write here will be available if you exit the container.</p>
<pre><code class="language-bash">Singularity&gt; ls
Desktop    Downloads  Pictures  Templates   Untitled1.ipynb  jupyter  matlab_crash_dump.224407-1  untitled.ipynb
Documents  Music      Public    Untitled.ipynb  Videos       matlab   test.txt
Singularity&gt; pwd
/homes/3/bb927
</code></pre>
<h4 id="editable-containers">Editable Containers</h4>
<p>By default these are read only, which means you can't install software in them and besides mounts, you can't write to them. So you will need to make an editable version.</p>
<pre><code class="language-bash">mlsc-login[0]:test$ singularity build --sandbox s_ubuntu ubuntu.simg
WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process
INFO:    Starting build...
</code></pre>
<p>View it:</p>
<pre><code class="language-bash">mlsc-login[0]:test$ ls
s_ubuntu  ubuntu.simg
</code></pre>
<blockquote>
<p>Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub (SingularityHUB - A docker like repository of container sources) or dockerhub images as well.</p>
</blockquote>
<p>Sandboxes are just directories so you can see inside with <code>ls</code>:</p>
<pre><code class="language-bash">mlsc-login[0]:test$ ls s_ubuntu/
bin   dev          etc   lib    lib64   media  opt   root  sbin         srv  tmp  var
boot  environment  home  lib32  libx32  mnt    proc  run   singularity  sys  usr
</code></pre>
<p>To shell into sandboxs and make changes we need to add the <code>--writable</code>, <code>--fakeroot</code> and <code>--no-home</code> flag.</p>
<pre><code class="language-bash">mlsc-login[0]:test$ singularity shell --writable --no-home s_ubuntu/
WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container
Singularity&gt; 
</code></pre>
<p>Let's make a change:</p>
<pre><code class="language-bash">Singularity&gt; cd /

Singularity&gt; ls /
bin   dev      etc   lib    lib64   media  opt   root  sbin     srv  tmp  var
boot  environment  home  lib32  libx32  mnt    proc  run   singularity  sys  usr

Singularity&gt; touch deleteme.txt

Singularity&gt; ls | grep deleteme.txt
deleteme.txt

Singularity&gt; ls /
bin   deleteme.txt  environment...
...
</code></pre>
<p>Now let's build this back into a real container (type <code>exit</code> and hit Enter to leave continer shell):</p>
<pre><code>mlsc-login[0]:test$ singularity build s_ubuntu_modified.simg s_ubuntu
WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process
INFO:    Starting build...
</code></pre>
<p><code>ls</code>:</p>
<pre><code class="language-bash">mlsc-login[0]:test$ ls
s_ubuntu  s_ubuntu_modified.simg  ubuntu.simg
</code></pre>
<p>Shell into new Singularity Image:</p>
<pre><code class="language-bash">mlsc-login[0]:test$ singularity shell s_ubuntu_modified.simg 
Singularity&gt; ls /
bin   deleteme.txt  environment ...
...
</code></pre>
<p><strong>deleteme.txt</strong> is in the new image.</p>
<p>This exercise is to show you how to make new Singularity images after you make edits to them like installing new python packages, installing other software and adding files.</p>
<h4 id="other-useful-flags">Other Useful Flags</h4>
<p><code>-B</code> or <code>--bind</code> for mounting volumes.
<code>--nv</code> gives you access to a GPU if on a machine that has one (must be done by submitting a job, as the "mlsc")</p>
<h3 id="jupyter">Jupyter</h3>
<blockquote>
<p>Note from Martinos help Desk: If you run jupyter, you will not be able to access the port it is on (<code>8888</code>) remotely (such as your local machine at home over VPN) due to firewall on the mlsc. But port range <code>5900-5999</code> are open for VNC so if you force jupyter to use port <code>5999</code> instead of its default you can use that.</p>
</blockquote>
<p>Locate file <strong>/cluster/qtim/users/slurm_jobs/datascience-notebook_latest.sif</strong>. I recommend changing directory to /cluster/qtim/users/slurm_jobs/.</p>
<p>While in /cluster/qtim/users/slurm_jobs/:<br />
[1] Start job:</p>
<pre><code class="language-bash">srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash 
</code></pre>
<p>[2] Forward via ssh in your local machine:</p>
<pre><code class="language-bash">bbearce@pop-os:~$ ssh -N -f -L localhost:5999:localhost:5999 bb927@rtx-01.nmr.mgh.harvard.edu
</code></pre>
<p>[3] Shell into Singularity Image <strong>datascience-notebook_latest.sif</strong> and start jupyter notebook server:</p>
<pre><code class="language-bash">rtx-01[0]:slurm_jobs$ pwd
/cluster/qtim/users/slurm_jobs

rtx-01[0]:slurm_jobs$ singularity shell --nv -B /cluster/qtim/users/slurm_jobs:/new_dir datascience-notebook_latest.sif
Singularity&gt;

Singularity&gt; ls /new_dir/
datascience-notebook_latest.sif  README.md  sample_job.py
Singularity&gt; cd /new_dir/
Singularity&gt; jupyter notebook --no-browser --port=5999
/opt/conda/lib/python3.8/site-packages/jupyter_server/transutils.py:13: FutureWarning: The alias `_()` will be deprecated. Use `_i18n()` instead.
  warnings.warn(warn_msg, FutureWarning)
[W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2021-09-24 02:34:58.067 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab
[I 2021-09-24 02:34:58.067 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab
[I 02:34:58.070 NotebookApp] Serving notebooks from local directory: /new_dir
[I 02:34:58.070 NotebookApp] Jupyter Notebook 6.3.0 is running at:
[I 02:34:58.070 NotebookApp] http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4
[I 02:34:58.070 NotebookApp]  or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4
[I 02:34:58.070 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 02:34:58.081 NotebookApp] 

    To access the notebook, open this file in a browser:
        file:///homes/3/bb927/.local/share/jupyter/runtime/nbserver-3373125-open.html
    Or copy and paste one of these URLs:
        http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4
     or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4

</code></pre>
<p><img alt="singularity_jupyter_mlsc" src="../singularity_jupyter_mlsc.jpg" /></p>
<blockquote>
<p>Now because of the ssh tunnel we setup in step [2] we can reach this with "localhost" or "127.0.0.1" as hostnames. Note we can also use the machine name "rtx-01.nmr.mgh.harvard.edu"</p>
</blockquote>
<p>We can test the <code>--nv</code>flag as well by seeing if <code>nvidia-smi</code> works:
<img alt="singularity_jupyter_mlsc_nvidia-smi" src="../singularity_jupyter_mlsc_nvidia-smi.jpg" /></p>
<p>Finally if you don't ssh tunnel then you have to use the hostname:
<img alt="singularity_jupyter_mlsc_nvidia-smi-no-ssh-tunnel" src="../singularity_jupyter_mlsc_nvidia-smi-no-ssh-tunnel.jpg" /></p>
<blockquote>
<p>Find host name with <code>hostname</code> once your job gives you a resource:</p>
</blockquote>
<pre><code class="language-bash">rtx-01[0]:slurm_jobs$ hostname
rtx-01.nmr.mgh.harvard.edu
</code></pre>
<h3 id="building-your-own-sinularity-images">Building Your Own Sinularity Images</h3>
<p><a href="https://bbearce.github.io/#notes/singularity/demo_for_qtim/">Original Demo</a></p>
<p>We can't build on the /cluster/qtim network storage so if we want to use recipe files, we need to build images on hard drives and for that we need QTIM servers. We can touch on this another time.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../MKDocs/Quick%20Notes/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../MKDocs/Quick%20Notes/" class="btn btn-xs btn-link">
        MKDocs
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../machine_learning/Coursera/Logistic_Regression_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../machine_learning/Coursera/Logistic_Regression_IBM/" class="btn btn-xs btn-link">
        5. Logistic Regression
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>