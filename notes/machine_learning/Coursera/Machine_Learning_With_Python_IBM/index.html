<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Intro - Ben's Code Journal</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Machine Learning With Python", url: "#_top", children: [
              {title: "Supervised and Unsupervised Learning", url: "#supervised-and-unsupervised-learning" },
              {title: "Regression", url: "#regression_1" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../javascripts/mathjax.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-link">
        Cluster
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../layouts/holy_grail/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../layouts/holy_grail/" class="btn btn-xs btn-link">
        Holy Grail
      </a>
    </div>
    
  </div>

    

    <h1 id="machine-learning-with-python">Machine Learning With Python</h1>
<p>Source: <a href="https://www.coursera.org/learn/machine-learning-with-python">Machine_Learning_With_Python_IBM</a></p>
<h2 id="supervised-and-unsupervised-learning">Supervised and Unsupervised Learning</h2>
<h3 id="supervised">Supervised</h3>
<p>Overall this is labeled data. Data from a csv is labeled. However be careful. <em>Supervised</em> learning is predicting a column for which you already have data but for a new observation or row that you haven't seen yet.</p>
<p>There are two types of supervised techniques.</p>
<h4 id="classification">Classification</h4>
<p>Predict discrete class label or category</p>
<p>So with a csv like:</p>
<pre><code class="language-bash">animal_id|eye_size|num_legs|weight|animal
...
</code></pre>
<p>We could predict the <em>animal</em> col as we have that label.</p>
<h4 id="regression">Regression</h4>
<p>Predict continuous value. So with a csv like:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<p>If you have many observations, you could attempt to predict <em>co2_emissions</em>  for a new observation</p>
<h3 id="unsupervised">Unsupervised</h3>
<p>Overall this is unlabeled data. This could still be from the same csv of data but you would be predicting a new column you don't know about yet. So with a csv like:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<h4 id="dimension-reduction">Dimension Reduction</h4>
<p>Feature selection and dimension reduction reduces redundant features to make the clasification easier.</p>
<h4 id="density-estimation">Density Estimation</h4>
<p>Simple concept to explore the data to find structue within it.</p>
<h4 id="market-basket-analysis">Market Basket Analysis</h4>
<p>If you buy a certain group of items, you are likely to buy another group of items.</p>
<h4 id="clustering">Clustering</h4>
<p>Grouping data points and objects that are somehow similar.</p>
<ul>
<li>Discover structure</li>
<li>Summarization</li>
<li>Anomaly detection</li>
</ul>
<p>Summary supervised learning has labeled data and unsupervised does not.</p>
<h2 id="regression_1">Regression</h2>
<p>Take this dataset:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<p>Can we predict the co2 emissions of the car given all features but co2?</p>
<p><em>Dependent</em> (y) variables are the goal we study. The the dependent variables have to be continuous.</p>
<p><em>Independent</em> (x) variables are the causes of the dependent variable states. Independent variables can be continuous or categorical.</p>
<h3 id="simple-regression">Simple Regression</h3>
<p>One independent variable is used to predict a dependent variable. </p>
<h3 id="multiple-regression">Multiple Regression</h3>
<p>Multiple independent variables are used to predict a dependent variable. </p>
<h3 id="applications-of-regression">Applications of Regression</h3>
<ul>
<li>Sales forecasting</li>
<li>Satisfaction analysis</li>
<li>Price estimation</li>
<li>Employment income prediction</li>
</ul>
<h3 id="regression-algorithms">Regression Algorithms</h3>
<ul>
<li>Ordinal regression</li>
<li>Poisson regression</li>
<li>Fast forest quantile regression</li>
<li>Linear, Polynomial, Lasson, Stepwise, Ridge regression</li>
<li>Bayesian linear regression</li>
<li>Neural network regression</li>
<li>Decision forest regression</li>
<li>Boosted decision tree regression</li>
<li>KNN (K-nearest neighbors)</li>
</ul>
<h3 id="linear-regression">Linear Regression</h3>
<p>Return to this this dataset:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<blockquote>
<p>Remember dependent variables have to be continuous</p>
</blockquote>
<p><span class="arithmatex">\(\hat{y} = \theta_0 + \theta_1 x_1\)</span></p>
<p>A residual error is the difference between y which is the data itself (x,y) from the csv and y(hat), the regression line evaluated at x (x, <span class="arithmatex">\(\hat{y}\)</span>).</p>
<p>The mean of all residual errors shows how poorly the estimation approximates the best regression. This is called the Mean Squared Error:</p>
<p><span class="arithmatex">\(MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2\)</span></p>
<p>It can be shown that <span class="arithmatex">\(\theta_0\)</span> and <span class="arithmatex">\(\theta_1\)</span> are:</p>
<p><span class="arithmatex">\(\theta_1 = \frac{\sum_{i=1}^{S} (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^{S} (x_i - \bar{x})^2}\)</span></p>
<p><span class="arithmatex">\(\theta_0 = \bar{y} - \theta_1 \bar{x}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\bar{x}\)</span> is the mean of all x values</li>
<li><span class="arithmatex">\(\bar{y}\)</span> is the mean of all y values</li>
</ul>
<p>Let's use code:</p>
<pre><code class="language-python">import pandas as pd

engine_size = [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7]
cylinders = [4,4,4,6,6,6,6,6,6]
fuel_consumption_comb = [8.5,9.6,5.9,11.1,10.6,10.0,10.1,11.1,11.6]
co2_emissions = [196,221,136,255,244,230,232,255,267]

engines = pd.DataFrame({'engine_size':engine_size,
                        'cylinders':cylinders,
                        'fuel_consumption_comb':fuel_consumption_comb,
                        'co2_emissions':co2_emissions})

engines

theta_0 = 100
theta_1 = 30
x1 = 4

y = theta_0 + theta_1 * 4
y
</code></pre>
<p>Pros of regressions:
* Very fast
* No parameter tuning
* East to understand, and highly interpretable</p>
<h3 id="model-evaluation-approaches">Model Evaluation Approaches</h3>
<ul>
<li>Train and Test on the Same Dataset</li>
<li>Train/Test Split</li>
</ul>
<h4 id="train-and-test-on-the-same-dataset">Train and Test on the Same Dataset</h4>
<p>Identify a testing subset of all data having observations for a given <em>dependent</em> variable. We train on all the data then predict on the testing subset and compare predictions of the <em>dependent</em> variable from the test set to the observations of the test set for the same variable.</p>
<ul>
<li>Training set is <em>all</em> data.</li>
<li>Testing set is <em>subset</em> of training.</li>
</ul>
<p>Let's evaluate the error (<span class="arithmatex">\(y_i\)</span> being the actual observation and <span class="arithmatex">\(\hat{y}_j\)</span> being the prediction):</p>
<p>Error = <span class="arithmatex">\(\frac{1}{n}\sum_{j=1}^{n}|y_i - \hat{y}_j|\)</span></p>
<p>It's the <em>average</em> error across all values.</p>
<p>This approach has a high <em>training accuracy</em> but a low <em>out-of-sample</em> accuracy. High training accuracy isn't necessarily a good thing. It results in overfitting. It's important that our models have a high, out-of-sample accuracy.</p>
<blockquote>
<p>The quiz made a point that I want to record. If a model is overly trained to the dataset, it may capture noise and produce a non-generalized model.</p>
</blockquote>
<h4 id="traintest-split">Train/Test Split</h4>
<p>Let's select a train/test split. </p>
<ul>
<li>Training set is <em>subset</em> of data.</li>
<li>Testing set is also a <em>subset</em> of data but with no overlap with the training set.</li>
<li>Training and Test data are <em>mutually exclusive</em>, which is a statistical term describing two or more events that cannot happen simultaneously.</li>
</ul>
<p>This is more realistic for real world problems.</p>
<p>One problem with this approach is the model is highly dependent on which datasets the data is trained and tested.</p>
<h4 id="k-fold-cross-validation">K-Fold Cross-Validation</h4>
<p>Another evaluation model called <strong>K-fold cross-validation</strong> can resolve most of these issues. How do you fix a high variation that results from a dataset dependency? Well you average it.</p>
<p>Example:
Take a dataset and make a train\test split where 75% of the data is training and 25% is test. If we have <strong>K equals four folds</strong>, then we make 4 train/test splits. In the <strong>first</strong> fold for example, we use the <strong>first</strong> 25 percent of the dataset for testing and the rest for training. The model is built using the training set and is evaluated using the test set. Then, in the next round or in the <strong>second</strong> fold, the <strong>second</strong> 25 percent of the dataset is used for testing and the rest for training the model. Again, the accuracy of the model is calculated. We continue for all folds. Finally, the result of all four evaluations are averaged. That is, the accuracy of each fold is then averaged, keeping in mind that each fold is distinct, <strong>where no training data in one fold is used in another</strong>. K-fold cross-validation in its simplest form performs multiple train/test splits, using the same dataset where each split is different. Then, the result is average to produce a more consistent out-of-sample accuracy.</p>
<blockquote>
<p>However, going in depth with K-fold cross-validation model is out of the scope for this course.</p>
</blockquote>
<h3 id="evaluation-metrics-in-regression-models">Evaluation Metrics in Regression Models</h3>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-link">
        Cluster
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../layouts/holy_grail/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../layouts/holy_grail/" class="btn btn-xs btn-link">
        Holy Grail
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>