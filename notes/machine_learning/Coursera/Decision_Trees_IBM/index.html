<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Decision Trees - Ben's Code Journal</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Decision Trees", url: "#_top", children: [
              {title: "What Is A Decision Tree", url: "#what-is-a-decision-tree" },
              {title: "Building Decision Trees", url: "#building-decision-trees" },
              {title: "Entropy", url: "#entropy" },
              {title: "Information Gain", url: "#information-gain" },
              {title: "Decision Tree Example", url: "#decision-tree-example" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../javascripts/mathjax.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-link">
        Cluster
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Classification_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Classification_IBM/" class="btn btn-xs btn-link">
        Classification
      </a>
    </div>
    
  </div>

    

    <h1 id="decision-trees">Decision Trees</h1>
<p>Source: <a href="https://www.coursera.org/learn/machine-learning-with-python">Machine_Learning_With_Python_IBM</a></p>
<h2 id="what-is-a-decision-tree">What Is A Decision Tree</h2>
<p>Image a csv with patients and demographics and the target is the drug that each patient responded to. Part of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness.</p>
<table>
<thead>
<tr>
<th>patient_id</th>
<th>age</th>
<th>sex</th>
<th>BP</th>
<th>cholesterol</th>
<th>drug</th>
</tr>
</thead>
<tbody>
<tr>
<td>p1</td>
<td>Young</td>
<td>F</td>
<td>High</td>
<td>Normal</td>
<td>Drug A</td>
</tr>
<tr>
<td>p2</td>
<td>Young</td>
<td>F</td>
<td>High</td>
<td>High</td>
<td>Drug A</td>
</tr>
<tr>
<td>p3</td>
<td>Middle-age</td>
<td>F</td>
<td>High</td>
<td>Normal</td>
<td>Drug B</td>
</tr>
<tr>
<td>p4</td>
<td>Senior</td>
<td>F</td>
<td>Young</td>
<td>Normal</td>
<td>Drug B</td>
</tr>
</tbody>
</table>
<p><img alt="Basic Decision Tree" src="../Images/decision_trees/basic_decision_tree.jpg" /></p>
<blockquote>
<p>Note "B" is drug B and "A" is drug A.</p>
</blockquote>
<ol>
<li>If Middle-age recommend drug B</li>
<li>If either Young or Senior proceed to that branch and ask another relevant question...and so on</li>
</ol>
<h2 id="building-decision-trees">Building Decision Trees</h2>
<p>Refer to above dataset again. Use recursive partitioning to classify the data. We need to determine which attribute is the best or more predictive to split data based on the feature. Let's choose Cholesterol.</p>
<ul>
<li>
<p>If patient has high cholesterol we can't say with high confidence that drugB might be suitable. Same for normal.<br />
<img alt="Attribute Choice" src="../Images/decision_trees/attribute_choice.jpg" /></p>
</li>
<li>
<p>Let's try sex. It is seems much better but not perfect.<br />
<img alt="Sex Attribute" src="../Images/decision_trees/sex_attribute.jpg" /></p>
</li>
<li>
<p>Let's got a step further<br />
<img alt="Impurity Entropy" src="../Images/decision_trees/impurity_entropy.jpg" /></p>
</li>
<li>
<p>For the male patient branch, we again test other attributes to split the sub-tree. We test cholesterol again here, as you can see it results in even more pure leaves. So we can easily make a decision here. For example, if a patient is male and his cholesterol is high, we can certainly prescribe drug A, but if it is normal, we can prescribe drug B with high confidence.</p>
</li>
<li>
<p>As you might notice, the choice of attribute to split data is very important and it is all about purity of the leaves after the split. A node in the tree is considered pure if in 100 percent of the cases, the nodes fall into a specific category of the target field.</p>
</li>
</ul>
<h2 id="entropy">Entropy</h2>
<p>Impurity of nodes is calculated by entropy of data in the node. What is entropy? Entropy is the amount of information disorder or the amount of randomness in the data. The entropy in the node depends on how much random data is in that node and is calculated for each node. In decision trees, we're looking for trees that have the smallest entropy in their nodes.</p>
<ul>
<li>{"DrugA": 0, "DrugB": 8 } -&gt; Entropy is 0</li>
<li>{"DrugA": 1, "DrugB": 7 } -&gt; Entropy is Low</li>
<li>{"DrugA": 3, "DrugB": 5 } -&gt; Entropy is High</li>
<li>{"DrugA": 4, "DrugB": 4 } -&gt; Entropy is 1</li>
</ul>
<p>The lower the entropy , the less uniform the distribution.</p>
<p><span class="arithmatex">\(Entropy = -p(A)\log_2(p(A)) - p(B)\log_2(p(B))\)</span></p>
<blockquote>
<p><span class="arithmatex">\(p(A)\)</span> is the probability of drug A and is also x on the graph below</p>
</blockquote>
<p>For reference: <span class="arithmatex">\(y(x)=log_2(x)\)</span></p>
<pre><code class="language-bash">  |
  |
  |----0.1----0.2----0.3----0.4----0.5----0.6----0.7----0.8----0.9---- . ----
  |                                                .                      
-1|                                 .                   
  |                         .                           
  |                   .                                
-2|               .                                    
  |           .                                         
  |         .                                          
-3|       .                                             
</code></pre>
<ul>
<li>Calculate entropy before splitting it<br />
<img alt="Entropy 1" src="../Images/decision_trees/entropy_1.jpg" /></li>
<li>After split<br />
<img alt="Entropy 2" src="../Images/decision_trees/entropy_2.jpg" /></li>
<li>Another node, which has the least entropy? The answer is the tree with the higher information gain after splitting.<br />
<img alt="Entropy 3" src="../Images/decision_trees/entropy_3.jpg" /></li>
</ul>
<h2 id="information-gain">Information Gain</h2>
<p>Information gain is the information that can increase the level of certainty after splitting. It is the entropy of a tree before the split minus the weighted entropy after the split by an attribute. We can think of information gain and entropy as opposites.</p>
<p>As entropy or the amount of randomness decreases, the information gain or amount of certainty increases and vice versa.</p>
<p><span class="arithmatex">\(Information Gain = (EntropyBeforeSplit) - (WeightedEntropyAfterSplit)\)</span>  </p>
<p>Ex:<br />
<img alt="Entropy 4" src="../Images/decision_trees/entropy_4.jpg" /></p>
<blockquote>
<p>Weights come from each node. For each decision, count total observations of that decision over total observations from both decisions</p>
</blockquote>
<p>What we learn is that we should choose sex first when choosing between sex and cholesterol. How do we choose the next branch? Well, as you can guess, we should repeat the process for each branch and test each of the other attributes to continue to reach the most pure leaves. </p>
<h2 id="decision-tree-example">Decision Tree Example</h2>
<p><a href="../JupyterNotebooks/classification_tree_svm.ipynb">classification_tree_svm.ipynb</a>
We will be exploring a decision tree and a support vector machine (SVM) to recognize fraudulent credit card transactions. You will use the trained model to assess if a credit card transaction is legitimate or not.</p>
<p>After completing this lab you will be able to:
* Perform basic data preprocessing in Python
* Model a classification task using the Scikit-Learn and Snap ML Python APIs
* Train Suppport Vector Machine and Decision Tree models using Scikit-Learn and Snap ML
* Run inference and assess the quality of the trained models</p>
<p>A transaction belongs to the positive class (1) if it is a fraud, otherwise it belongs to the negative class (0). You have access to transactions that occured over a certain period of time. The majority of the transactions are normally legitimate and only a small fraction are non-legitimate. Thus, typically you have access to a dataset that is highly unbalanced. This is also the case of the current dataset: only 492 transactions out of 284,807 are fraudulent (the positive class - the frauds - accounts for 0.172% of all transactions).</p>
<p>To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset.</p>
<pre><code class="language-bash"># Setup Environment
cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;
# cd ~/Desktop; rm -r temp; # To remove
</code></pre>
<pre><code class="language-python">import opendatasets as od

# download the dataset (this is a Kaggle dataset)
# during download you will be required to input your Kaggle username and password
od.download(&quot;https://www.kaggle.com/mlg-ulb/creditcardfraud&quot;)
</code></pre>
<pre><code class="language-python"># Import the libraries we need to use in this lab
from __future__ import print_function
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import roc_auc_score
import time
import warnings
warnings.filterwarnings('ignore')
# read the input data
raw_data = pd.read_csv('creditcardfraud/creditcard.csv')
print(&quot;There are &quot; + str(len(raw_data)) + &quot; observations in the credit card fraud dataset.&quot;)
print(&quot;There are &quot; + str(len(raw_data.columns)) + &quot; variables in the dataset.&quot;)
# display the first rows in the dataset
raw_data.head()
# In practice, a financial institution may have access to a much larger dataset of transactions. To simulate such a case, we will inflate the original one 10 times.
n_replicas = 10
# inflate the original dataset
big_raw_data = pd.DataFrame(np.repeat(raw_data.values, n_replicas, axis=0), columns=raw_data.columns)
print(&quot;There are &quot; + str(len(big_raw_data)) + &quot; observations in the inflated credit card fraud dataset.&quot;)
print(&quot;There are &quot; + str(len(big_raw_data.columns)) + &quot; variables in the dataset.&quot;)
# display first rows in the new dataset
big_raw_data.head()
# 'Class' is the target variable (1-fraud 0-otherwise)
# get the set of distinct classes
labels = big_raw_data.Class.unique()
# get the count of each class
sizes = big_raw_data.Class.value_counts().values
# plot the class value counts
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, autopct='%1.3f%%')
ax.set_title('Target Variable Value Counts')
plt.show()
</code></pre>
<p><img alt="lab_class_value_counts" src="../Images/decision_trees/lab_class_value_counts.jpg" /></p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../../Martinos/cluster/cluster/" class="btn btn-xs btn-link">
        Cluster
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Classification_IBM/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Classification_IBM/" class="btn btn-xs btn-link">
        Classification
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>