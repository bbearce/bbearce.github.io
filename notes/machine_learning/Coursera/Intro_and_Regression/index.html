<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>1. Intro and Regression - Ben's Code Journal</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Machine Learning With Python", url: "#_top", children: [
              {title: "Supervised and Unsupervised Learning", url: "#supervised-and-unsupervised-learning" },
              {title: "Regression", url: "#regression_1" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Classification_IBM/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Classification_IBM/" class="btn btn-xs btn-link">
        2. Classification
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../layouts/holy_grail/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../layouts/holy_grail/" class="btn btn-xs btn-link">
        Holy Grail
      </a>
    </div>
    
  </div>

    

    <h1 id="machine-learning-with-python">Machine Learning With Python</h1>
<p>Source: <a href="https://www.coursera.org/learn/machine-learning-with-python">Machine_Learning_With_Python_IBM</a></p>
<h2 id="supervised-and-unsupervised-learning">Supervised and Unsupervised Learning</h2>
<h3 id="supervised">Supervised</h3>
<p>Overall this is labeled data. Data from a csv is labeled. However be careful. <em>Supervised</em> learning is predicting a column for which you already have data but for a new observation or row that you haven't seen yet.</p>
<p>There are two types of supervised techniques.</p>
<h4 id="classification">Classification</h4>
<p>Predict discrete class label or category</p>
<p>So with a csv like:</p>
<pre><code class="language-bash">animal_id|eye_size|num_legs|weight|animal
...
</code></pre>
<p>We could predict the <em>animal</em> col as we have that label.</p>
<h4 id="regression">Regression</h4>
<p>Predict continuous value. So with a csv like:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<p>If you have many observations, you could attempt to predict <em>co2_emissions</em>  for a new observation</p>
<h3 id="unsupervised">Unsupervised</h3>
<p>Overall this is unlabeled data. This could still be from the same csv of data but you would be predicting a new column you don't know about yet. So with a csv like:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<h4 id="dimension-reduction">Dimension Reduction</h4>
<p>Feature selection and dimension reduction reduces redundant features to make the clasification easier.</p>
<h4 id="density-estimation">Density Estimation</h4>
<p>Simple concept to explore the data to find structue within it.</p>
<h4 id="market-basket-analysis">Market Basket Analysis</h4>
<p>If you buy a certain group of items, you are likely to buy another group of items.</p>
<h4 id="clustering">Clustering</h4>
<p>Grouping data points and objects that are somehow similar.</p>
<ul>
<li>Discover structure</li>
<li>Summarization</li>
<li>Anomaly detection</li>
</ul>
<p>Summary supervised learning has labeled data and unsupervised does not.</p>
<h2 id="regression_1">Regression</h2>
<p>Take this dataset:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<p>Can we predict the co2 emissions of the car given all features but co2?</p>
<p><em>Dependent</em> (y) variables are the goal we study. The the dependent variables have to be continuous.</p>
<p><em>Independent</em> (x) variables are the causes of the dependent variable states. Independent variables can be continuous or categorical.</p>
<h3 id="simple-regression">Simple Regression</h3>
<p>One independent variable is used to predict a dependent variable. </p>
<h3 id="multiple-regression">Multiple Regression</h3>
<p>Multiple independent variables are used to predict a dependent variable. </p>
<h3 id="applications-of-regression">Applications of Regression</h3>
<ul>
<li>Sales forecasting</li>
<li>Satisfaction analysis</li>
<li>Price estimation</li>
<li>Employment income prediction</li>
</ul>
<h3 id="regression-algorithms">Regression Algorithms</h3>
<ul>
<li>Ordinal regression</li>
<li>Poisson regression</li>
<li>Fast forest quantile regression</li>
<li>Linear, Polynomial, Lasson, Stepwise, Ridge regression</li>
<li>Bayesian linear regression</li>
<li>Neural network regression</li>
<li>Decision forest regression</li>
<li>Boosted decision tree regression</li>
<li>KNN (K-nearest neighbors)</li>
</ul>
<h3 id="simple-linear-regression">Simple Linear Regression</h3>
<p>Return to this this dataset:</p>
<pre><code class="language-bash">car_id|engine_size|cylinders|fuel_consumption|co2_emissions
...
</code></pre>
<blockquote>
<p>Remember dependent variables have to be continuous</p>
</blockquote>
<p><span class="arithmatex">\(\hat{y} = \theta_0 + \theta_1 x_1\)</span></p>
<p>A residual error is the difference between y which is the data itself (x,y) from the csv and y(hat), the regression line evaluated at x (x, <span class="arithmatex">\(\hat{y}\)</span>).</p>
<p>The mean of all residual errors shows how poorly the estimation approximates the best regression. This is called the Mean Squared Error:</p>
<p><span class="arithmatex">\(MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2\)</span></p>
<p>It can be shown that <span class="arithmatex">\(\theta_0\)</span> and <span class="arithmatex">\(\theta_1\)</span> are:</p>
<p><span class="arithmatex">\(\theta_1 = \frac{\sum_{i=1}^{S} (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^{S} (x_i - \bar{x})^2}\)</span></p>
<p><span class="arithmatex">\(\theta_0 = \bar{y} - \theta_1 \bar{x}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\bar{x}\)</span> is the mean of all x values</li>
<li><span class="arithmatex">\(\bar{y}\)</span> is the mean of all y values</li>
</ul>
<p>Let's use code:</p>
<pre><code class="language-python">import pandas as pd

engine_size = [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7]
cylinders = [4,4,4,6,6,6,6,6,6]
fuel_consumption_comb = [8.5,9.6,5.9,11.1,10.6,10.0,10.1,11.1,11.6]
co2_emissions = [196,221,136,255,244,230,232,255,267]

engines = pd.DataFrame({'engine_size':engine_size,
                        'cylinders':cylinders,
                        'fuel_consumption_comb':fuel_consumption_comb,
                        'co2_emissions':co2_emissions})

engines

theta_0 = 100
theta_1 = 30
x1 = 4

y = theta_0 + theta_1 * 4
y
</code></pre>
<p>Pros of regressions:
* Very fast
* No parameter tuning
* East to understand, and highly interpretable</p>
<h3 id="model-evaluation-approaches">Model Evaluation Approaches</h3>
<ul>
<li>Train and Test on the Same Dataset</li>
<li>Train/Test Split</li>
</ul>
<h4 id="train-and-test-on-the-same-dataset">Train and Test on the Same Dataset</h4>
<p>Identify a testing subset of all data having observations for a given <em>dependent</em> variable. We train on all the data then predict on the testing subset and compare predictions of the <em>dependent</em> variable from the test set to the observations of the test set for the same variable.</p>
<ul>
<li>Training set is <em>all</em> data.</li>
<li>Testing set is <em>subset</em> of training.</li>
</ul>
<p>Let's evaluate the error (<span class="arithmatex">\(y_i\)</span> being the actual observation and <span class="arithmatex">\(\hat{y}_j\)</span> being the prediction):</p>
<p>Error = <span class="arithmatex">\(\frac{1}{n}\sum_{j=1}^{n}|y_i - \hat{y}_j|\)</span></p>
<p>Above is the <em>mean absolute error</em> across all values.</p>
<p>This approach has a high <em>training accuracy</em> but a low <em>out-of-sample</em> accuracy. High training accuracy isn't necessarily a good thing. It results in overfitting. It's important that our models have a high, out-of-sample accuracy.</p>
<blockquote>
<p>The quiz made a point that I want to record. If a model is overly trained to the dataset, it may capture noise and produce a non-generalized model.</p>
</blockquote>
<h4 id="traintest-split">Train/Test Split</h4>
<p>Let's select a train/test split. </p>
<ul>
<li>Training set is <em>subset</em> of data.</li>
<li>Testing set is also a <em>subset</em> of data but with no overlap with the training set.</li>
<li>Training and Test data are <em>mutually exclusive</em>, which is a statistical term describing two or more events that cannot happen simultaneously.</li>
</ul>
<p>This is more realistic for real world problems.</p>
<p>One problem with this approach is the model is highly dependent on which datasets the data is trained and tested.</p>
<h4 id="k-fold-cross-validation">K-Fold Cross-Validation</h4>
<p>Another evaluation model called <strong>K-fold cross-validation</strong> can resolve most of these issues. How do you fix a high variation that results from a dataset dependency? Well you average it.</p>
<p>Example:
Take a dataset and make a train\test split where 75% of the data is training and 25% is test. If we have <strong>K equals four folds</strong>, then we make 4 train/test splits. In the <strong>first</strong> fold for example, we use the <strong>first</strong> 25 percent of the dataset for testing and the rest for training. The model is built using the training set and is evaluated using the test set. Then, in the next round or in the <strong>second</strong> fold, the <strong>second</strong> 25 percent of the dataset is used for testing and the rest for training the model. Again, the accuracy of the model is calculated. We continue for all folds. Finally, the result of all four evaluations are averaged. That is, the accuracy of each fold is then averaged, keeping in mind that each fold is distinct, <strong>where no training data in one fold is used in another</strong>. K-fold cross-validation in its simplest form performs multiple train/test splits, using the same dataset where each split is different. Then, the result is average to produce a more consistent out-of-sample accuracy.</p>
<blockquote>
<p>However, going in depth with K-fold cross-validation model is out of the scope for this course.</p>
</blockquote>
<h3 id="evaluation-metrics-in-regression-models">Evaluation Metrics in Regression Models</h3>
<ul>
<li><strong>M</strong>ean <strong>A</strong>bsolute <strong>E</strong>rror - Average error.<br />
<span class="arithmatex">\(MAE =\frac{1}{n}\sum_{j=1}^{n}|y_j-\hat{y}_j|\)</span></li>
<li><strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror - Focus is geared towards large errors. It accentuates worse errors. Also a normalized residual sum of square (which is the same equation without dividing by <em>n</em>).
<span class="arithmatex">\(MSE = \frac{1}{n}\sum_{j=1}^{n}(y_j-\hat{y}_j)^2\)</span></li>
<li><strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror - Square root of MSE. It is interpretable in the same units as response vector or Y units.<br />
<span class="arithmatex">\(RMSE = \sqrt{\frac{1}{n}\sum_{j=1}^{n}(y_j-\hat{y}_j)^2}\)</span></li>
<li><strong>R</strong>elative <strong>A</strong>bsolute <strong>E</strong>rror - Takes the MAE and normalizes it.<br />
<span class="arithmatex">\(RAE = \frac{\sum_{j=1}^{n}|y_j-\hat{y}_j|}{\sum_{j=1}^{n}|y_j-\bar{y}_j|}\)</span></li>
<li><strong>R</strong>elative <strong>S</strong>quared <strong>E</strong>rror - Similar to RAE, but is widely adopted by the data science community as it is used for calculating <em>R-squared</em>. <em>R-squared</em> is not an error per say but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the <em>R-squared</em>, the better the model fits your data.<br />
<span class="arithmatex">\(RSE = \frac{\sum_{j=1}^{n}(y_j-\hat{y}_j)^2}{\sum_{j=1}^{n}(y_j-\bar{y}_j)^2}\)</span></li>
</ul>
<p><span class="arithmatex">\(R^2 = 1 - RSE\)</span></p>
<p>Error is difference betwee data points and the trend generated by the algorithm.</p>
<h3 id="lab-simple-linear-regression">Lab: Simple Linear Regression</h3>
<p><a href="../JupyterNotebooks/ML0101EN-Reg-Simple-Linear-Regression-Co2.ipynb">ML0101EN-Reg-Simple-Linear-Regression-Co2.ipynb</a></p>
<h4 id="sklearn-intro">Sklearn Intro</h4>
<pre><code class="language-bash"># Setup Environment
cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;
wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv
# cd ~/Desktop; rm -r temp; # To remove
</code></pre>
<pre><code class="language-python">import pandas as pd, numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt
df = pd.read_csv(&quot;FuelConsumption.csv&quot;)
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
# summarize the data
df.describe()
# matplotlib
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
plt.xlabel(&quot;Engine size&quot;)
plt.ylabel(&quot;Emission&quot;)
plt.show()
# train\test
msk = np.random.rand(len(df)) &lt; 0.8
train = cdf[msk]
test = cdf[~msk]
# train data distribution
plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
plt.xlabel(&quot;Engine size&quot;)
plt.ylabel(&quot;Emission&quot;)
plt.show()
# modeling with sklearn
regr = linear_model.LinearRegression()
train_x = np.asanyarray(train[['ENGINESIZE']])
train_y = np.asanyarray(train[['CO2EMISSIONS']])
regr.fit(train_x, train_y)
# The coefficients
print ('Coefficients: (theta1)', regr.coef_)
print ('Intercept: (theta0)',regr.intercept_)
# fit that regression!
plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')
plt.xlabel(&quot;Engine size&quot;)
plt.ylabel(&quot;Emission&quot;)
plt.show()
# MSE calculations
test_x = np.asanyarray(test[['ENGINESIZE']])
test_y = np.asanyarray(test[['CO2EMISSIONS']])
test_y_ = regr.predict(test_x)

print(&quot;Mean absolute error: %.2f&quot; % np.mean(np.absolute(test_y_ - test_y)))
print(&quot;Residual sum of squares (MSE): %.2f&quot; % np.mean((test_y_ - test_y) ** 2))
print(&quot;R2-score: %.2f&quot; % r2_score(test_y , test_y_) )
</code></pre>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<p><a href="../JupyterNotebooks/ML0101EN-Reg-Mulitple-Linear-Regression-Co2.ipynb">ML0101EN-Reg-Mulitple-Linear-Regression-Co2.ipynb</a>
Review:
* Simple Linear Regression - predict co2 emission vs engine ize
* Multiple Linear Regression - predict co2 emission vs engine size and cyclinders</p>
<h4 id="applications">Applications:</h4>
<ul>
<li>
<p>Independent variables effectiveness on prediction. Does revision time, test anxiety, lecture attendance and gender have any effect on the exam performance of students.</p>
</li>
<li>
<p>Predicting impacts of change. How mch does blood pressure go up (or down) for every unit increase (or decease) in the BMI index of patient.</p>
</li>
</ul>
<p>Ex:</p>
<p><span class="arithmatex">\(Co2 = \theta_0 + \theta_1EngineSize + \theta2Cylinders + ...\)</span></p>
<p>Multiple Linear Regression:<br />
<span class="arithmatex">\(\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n\)</span></p>
<p>Vector Form (more concise):<br />
<span class="arithmatex">\(\hat{y} = \theta^TX\)</span> "Theta Transpose X, it's really just missing <span class="arithmatex">\(\theta_0\)</span> from above expanded form. That is <em>bias</em>."</p>
<p><span class="arithmatex">\(\theta^T = [\theta^0,\theta^1,\theta^2,...]\)</span></p>
<p><span class="arithmatex">\(X = \left[ \begin{array}{c} 1 \\ x_1 \\ x_2 \\ \end{array} \right]\)</span></p>
<h4 id="estimating-multiple-linear-regression-parameters">Estimating Multiple Linear Regression Parameters</h4>
<p>How to estimate <span class="arithmatex">\(\theta^T\)</span>?
* Ordinary Least Squares
  * Linear algebra operations
  * Takes a long time for large datasets (10K+rows)
* An optimized algorithm
  * Gradient Descent
  * Proper approach if you have a very large dataset.</p>
<p>Questions to consider:<br />
* How to determine whether to use simple or multiple linear regression?
* How many independent variables should you use?
  * Too many causes overfit. It's too specific.
* Should the indendent variable be continuous?
* What are the linear relationships between the dependent variable and the independent variables?</p>
<h4 id="sklearn-multiple-lenear-regression">Sklearn Multiple Lenear Regression</h4>
<pre><code class="language-bash"># Setup Environment
cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;
wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv
# cd ~/Desktop; rm -r temp; # To remove
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
import pandas as pd
import pylab as pl
import numpy as np
from sklearn import linear_model
# train\teset
df = pd.read_csv(&quot;FuelConsumption.csv&quot;)
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
msk = np.random.rand(len(df)) &lt; 0.8
train = cdf[msk]
test = cdf[~msk]
# multiple regression model
regr = linear_model.LinearRegression()
x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
y = np.asanyarray(train[['CO2EMISSIONS']])
regr.fit (x, y)
# The coefficients
print ('Coefficients: ', regr.coef_)
</code></pre>
<p>As mentioned before, <strong>Coefficient</strong> and <strong>Intercept</strong>  are the parameters of the fitted line. 
Given that it is a multiple linear regression model with 3 parameters and that the parameters are the intercept and coefficients of the hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain <strong>Ordinary Least Squares</strong> method to solve this problem.</p>
<p>It tries to minimize the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output (<span class="arithmatex">\(\hat{y}\)</span>) over all samples in the dataset.</p>
<p>OLS can find the best parameters using of the following methods:
* Solving the model parameters analytically using closed-form equations
* Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)</p>
<p>Prediction:</p>
<pre><code class="language-python">y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
y = np.asanyarray(test[['CO2EMISSIONS']])
print(&quot;Mean Squared Error (MSE) : %.2f&quot;
      % np.mean((y_hat - y) ** 2))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % regr.score(x, y))
</code></pre>
<p><strong>Explained variance regression score (<span class="arithmatex">\(R^2\)</span>):</strong><br />
Let <span class="arithmatex">\(\hat{y}\)</span> be the estimated target output, y the corresponding (correct) target output, and Var be the Variance (the square of the standard deviation). Then the explained variance is estimated as follows:</p>
<p><span class="arithmatex">\(\texttt{explainedVariance}(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}\)</span><br />
The best possible score is 1.0, the lower values are worse.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Classification_IBM/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Classification_IBM/" class="btn btn-xs btn-link">
        2. Classification
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../../layouts/holy_grail/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../../layouts/holy_grail/" class="btn btn-xs btn-link">
        Holy Grail
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>