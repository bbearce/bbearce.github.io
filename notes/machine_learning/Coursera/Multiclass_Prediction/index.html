<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>7. Multiclass Prediction - Ben's Code Journal</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Multiclass Prediction", url: "#_top", children: [
              {title: "SoftMax Regression, One-vs-All \u0026amp; One-vs-One for Multi-class Classification", url: "#softmax-regression-one-vs-all-one-vs-one-for-multi-class-classification" },
              {title: "SoftMax Regression", url: "#softmax-regression" },
              {title: "One-vs-All", url: "#one-vs-all" },
              {title: "Multi-class Classification", url: "#multi-class-classification" },
              {title: "One-vs-One Classification", url: "#one-vs-one-classification" },
              {title: "Lab", url: "#lab" },
              {title: "SVM", url: "#svm" },
              {title: "One vs All", url: "#one-vs-all_1" },
              {title: "One vs One", url: "#one-vs-one" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Clustering/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Clustering/" class="btn btn-xs btn-link">
        8. Clustering
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Support_Vector_Machine/" class="btn btn-xs btn-link">
        6. Support Vector Machine
      </a>
    </div>
    
  </div>

    

    <h1 id="multiclass-prediction">Multiclass Prediction</h1>
<h2 id="softmax-regression-one-vs-all-one-vs-one-for-multi-class-classification">SoftMax Regression, One-vs-All &amp; One-vs-One for Multi-class Classification</h2>
<p>Unlike classification trees and nearest neighbors, the concept of Multi-class classification for linear classifiers is not as straightforward. We can convert logistic regression to Multi-class classification using multinomial logistic regression or SoftMax regression; this is a generalization of logistic regression. SoftMax regression will not work for Support Vector Machines (SVM); One vs. All (One-vs-Rest) and One vs One are two other multi-class classification techniques that can convert most two-class classifiers to a multi-class classifier.</p>
<h2 id="softmax-regression">SoftMax Regression</h2>
<p>SoftMax regression is similar to logistic regression, the SoftMax function converts the actual distances i.e. dot products of <span class="arithmatex">\(x\)</span> with each of the parameters <span class="arithmatex">\(\theta_i\)</span> for <span class="arithmatex">\(K\)</span> classes in the range from 0 to <span class="arithmatex">\(K-1\)</span>. This is converted to probabilities using the following formula.</p>
<p><span class="arithmatex">\(softmax(x,i) = \frac{e^{-\theta_i^Tx}}{\sum_{j=1}^{K}e^{-\theta_j^Tx}}\)</span></p>
<p>The training procedure is almost identical to logistic regression using cross-entropy, but the prediction is different. Consider the three-class example where:</p>
<p><span class="arithmatex">\(y\)</span> <span class="arithmatex">\(\epsilon\)</span> {<span class="arithmatex">\(0,1,2\)</span>} i.e. <span class="arithmatex">\(y\)</span> can equal 0,1,2.  We would like to classify <span class="arithmatex">\(x\)</span>. We can use the SoftMax function to generate a probability of how likely the sample belongs to each class. We then  make a <strong>prediction</strong> using the argmax function:</p>
<p><span class="arithmatex">\(\hat{y} = argmax_i(softmax(x,i))\)</span></p>
<p>Let’s do an example, consider sample <span class="arithmatex">\(x_i\)</span> , we will start by creating a table where each column will be the <span class="arithmatex">\(i−th\)</span> values of the SoftMax function. The index of each column is the same as the class. </p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>probability of <span class="arithmatex">\(\hat{y} = 0\)</span></td>
<td>probability of <span class="arithmatex">\(\hat{y} = 1\)</span></td>
<td>probability of <span class="arithmatex">\(\hat{y} = 2\)</span></td>
</tr>
<tr>
<td>softmax(<span class="arithmatex">\(x_1\)</span>,0)</td>
<td>softmax(<span class="arithmatex">\(x_1\)</span> ,1)</td>
<td>softmax(<span class="arithmatex">\(x_1\)</span> ,2)</td>
</tr>
<tr>
<td>i=0</td>
<td>i=1</td>
<td>i=2</td>
</tr>
</tbody>
</table>
<p>Sample output probabilities (notice they add to 1):
|                               |                               |                               |
|-------------------------------|-------------------------------|-------------------------------|
| 0.97                          | 0.02                          | 0.01                          |
| i = 0                         | i = 1                         | i = 2                         |</p>
<p>We can represent the probability as a vector [0.97, 0.02, 0.01]. To get the class we simply apply the <em>argmax</em> function, this returns the index of the largest value.</p>
<p><span class="arithmatex">\(\hat{y} = argmax_i([0.97, 0.02, 0.01])\)</span></p>
<h3 id="geometric-inerpretation">Geometric Inerpretation</h3>
<p>Each <span class="arithmatex">\(\theta_i^Tx\)</span> is the equation of a hyperplane, we plot the intersection of the three hyperplanes with 0 (I don't know what this means -BB-) in fig 1 as colored lines, in addition, we can overlay several training samples. We also shade the regions where the value of 
<span class="arithmatex">\(\theta_i^Tx\)</span> is largest, this also corresponds to the largest probability. This color corresponds to where a sample 
<span class="arithmatex">\(x\)</span> would be classified.  For example if the input is in the blue region, the sample would be classified <span class="arithmatex">\(\hat{y}=0\)</span>, If the input is in the red region it would be classified as <span class="arithmatex">\(\hat{y}=1\)</span>, and in the yellow region <span class="arithmatex">\(\hat{y}=2\)</span>. We will use this convention going forward. </p>
<p><img alt="softmax.jpg" src="../Images/multiclass_prediction/softmax.jpg" /></p>
<p>One problem with SoftMax regression with cross-entropy is it cannot be used for SVM and other types of two-class classifiers.  </p>
<h2 id="one-vs-all">One-vs-All</h2>
<p>For one-vs-All classification, if we have <span class="arithmatex">\(K\)</span> classes, we use <span class="arithmatex">\(K\)</span> two-class classifier models, the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class. We will call this "dummy" class. For each classifier, we split the data into two classes. We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a  classification, we can use majority vote or use the classifier with the highest probability, disregarding the probabilities generated for the dummy class. </p>
<h2 id="multi-class-classification">Multi-class Classification</h2>
<p>Although classifiers such as logistic regression and SVM class values are {0,1} and {−1,1} respectively, we will use arbitrary class values. Consider the following samples colored according to class y=0 for blue, y=1 for red, and y=2 for yellow:</p>
<p><img alt="one_v_all_1.jpg" src="../Images/multiclass_prediction/one_v_all_1.jpg" /></p>
<p>For each class we take the class samples we would like to classify, and the rest will be labeled as a “dummy” class. For example, to build a classifier for the blue class we simply assign all other labels that are not in the blue class to the Dummy class, we then train the classifier accordingly. The result is shown in fig 3 where the classifier predicts blue 
<span class="arithmatex">\(\hat{y} = 0\)</span> and in the purple region where we have our “dummy class" <span class="arithmatex">\(\hat{y} = dummy\)</span>.</p>
<p><img alt="one_v_all_2.jpg" src="../Images/multiclass_prediction/one_v_all_2.jpg" /></p>
<p>We repeat the process for each class as shown in Fig 4, the actual class is shown with the same color and the corresponding dummy class is shown in purple. The color of the space is the actual classifier predictions shown in the same manner as above.</p>
<p><img alt="one_v_all_3.jpg" src="../Images/multiclass_prediction/one_v_all_3.jpg" /></p>
<p>One issue with one vs all is the ambiguous regions as shown in Fig 5 in purple. In these regions you may get multiple classes for example <span class="arithmatex">\(\hat{y}_0=0\)</span> and <span class="arithmatex">\(\hat{y}_1=1\)</span> or all the outputs will equal ”dummy.”</p>
<p><img alt="one_v_all_4.jpg" src="../Images/multiclass_prediction/one_v_all_4.jpg" /></p>
<p>There are several ways to reduce this ambiguous region, you can use the output based on the output of the linear function this is called the fusion rule. We can also use the probability of a sample belonging to the actual class as shown in Fig 6, where we select the class with the largest probability in this case <span class="arithmatex">\(\hat{y}_0=0\)</span>; we disregard the dummy values. These probabilities are scores, as the probabilities are between the dummy class and the actual class not between classes. Just a note packages like Scikit-learn can output probabilities for SVM. </p>
<p><img alt="one_v_all_5.jpg" src="../Images/multiclass_prediction/one_v_all_5.jpg" /></p>
<blockquote>
<p>I think this chart is misleading as it makes it see like the model is confident that its prediction is either <span class="arithmatex">\(\hat{y}_0\)</span>, <span class="arithmatex">\(\hat{y}_1\)</span> or <span class="arithmatex">\(\hat{y}_2\)</span>. In reality I think the likely scenarios is one where your data was always a "dummy" and you get likelihoods that look like:</p>
</blockquote>
<ul>
<li><span class="arithmatex">\(P(\hat{y}_0 = 0 | x) = 0.45\)</span></li>
<li><span class="arithmatex">\(P(\hat{y}_1 = 0 | x) = 0.39\)</span></li>
<li><span class="arithmatex">\(P(\hat{y}_2 = 0 | x) = 0.28\)</span></li>
</ul>
<p>Not confident anywhere but at least mostly confident in <span class="arithmatex">\(\hat{y}_0\)</span>.</p>
<h2 id="one-vs-one-classification">One-vs-One Classification</h2>
<p>In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1, and 2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2, and a final classifier on samples of class 1 and class 2. Fig 7 is an example of class 0 vs class 1, where we drop training samples  of class 2.  Using the same convention as above where the color of the training samples are based on their class. The separating plane of the classifier is in black.  The color represents the output of the classifier for that particular point in space.  </p>
<p><img alt="one_v_all_6.jpg" src="../Images/multiclass_prediction/one_v_all_6.jpg" /></p>
<p>We repeat the process for each pair of classes, in Fig 8. For <span class="arithmatex">\(K\)</span> classes, we have to train <span class="arithmatex">\(K \frac{K−1}{2}\)</span>classifiers. So if <span class="arithmatex">\(K=3\)</span>, we have <span class="arithmatex">\((3×2)/2=3\)</span> classes.</p>
<p><img alt="one_v_all_7.jpg" src="../Images/multiclass_prediction/one_v_all_7.jpg" /></p>
<p>To perform Classification on a sample, we perform a majority vote where we select the class with the most predictions.  This is shown in Fig  9 where the black point represents a new sample and the output of each classifier is shown in the table. In this case, the final output is one as selected by two of the three classifiers. There is also an ambiguous region but it’s smaller, we can also use similar schemes as in One vs all like the fusion rule or using the probability. Check out the labs for more.  </p>
<p><img alt="one_v_all_8.jpg" src="../Images/multiclass_prediction/one_v_all_8.jpg" /></p>
<h2 id="lab">Lab</h2>
<pre><code class="language-bash"># Setup Environment
cd ~/Desktop; rm -r temp; # To remove
cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;
</code></pre>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
import pandas as pd

plot_colors = &quot;ryb&quot;
plot_step = 0.02
# This functions Plots different decision boundary 
def decision_boundary (X,y,model,iris, two=None):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                         np.arange(y_min, y_max, plot_step))
    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu)
    if two:
        cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu)
        for i, color in zip(np.unique(y), plot_colors):

            idx = np.where( y== i)
            plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, s=15)
        plt.show()
    else:
        set_={0,1,2}
        print(set_)
        for i, color in zip(range(3), plot_colors):
            idx = np.where( y== i)
            if np.any(idx):
                set_.remove(i)
                plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)
        for  i in set_:
            idx = np.where( iris.target== i)
            plt.scatter(X[idx, 0], X[idx, 1], marker='x',color='black')
        plt.show()

# This function will plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number.
def plot_probability_array(X,probability_array):
    plot_array=np.zeros((X.shape[0],30))
    col_start=0
    ones=np.ones((X.shape[0],30))
    for class_,col_end in enumerate([10,20,30]):
        plot_array[:,col_start:col_end]= np.repeat(probability_array[:,class_].reshape(-1,1), 10,axis=1)
        col_start=col_end
    plt.imshow(plot_array)
    plt.xticks([])
    plt.ylabel(&quot;samples&quot;)
    plt.xlabel(&quot;probability of 3 classes&quot;)
    plt.colorbar()
    plt.show()

# In ths lab we will use the  iris dataset, it consists of 3 different types of irises’ (Setosa y=0, Versicolour y=1, and Virginica y=2) or targets and petal and sepal length and width (data), stored in a 150x4 numpy.ndarray.
pair=[1, 3]
iris = datasets.load_iris()
X = iris.data[:, pair]
y = iris.target
np.unique(y)
# plot
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)
plt.xlabel(&quot;sepal width (cm)&quot;)
plt.ylabel(&quot;petal width&quot;)
plt.show()
</code></pre>
<p>SoftMax regression is similar to logistic regression, the softmax function converts the actual distances i.e. dot products of <span class="arithmatex">\(x\)</span> with each of the parameters <span class="arithmatex">\(\theta_i\)</span> for the <span class="arithmatex">\(K\)</span> classes. This is converted to probabilities using the following:</p>
<p><span class="arithmatex">\(softmax(x,i) = \frac{e^{ \theta_i^T \bf x}}{\sum_{j=1}^K e^{\theta_j^T x}}\)</span></p>
<pre><code class="language-python"># Softmax Regression
# sklearn does this automatically, but we can verify the prediction step, we fit the model:
lr = LogisticRegression(random_state=0).fit(X, y)
# Generate probability uing predict_proba
probability=lr.predict_proba(X)
# We can plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number.
plot_probability_array(X,probability)
# First sample
probability[0,:]
# We see it sums to 1
probability[0,:].sum()
# Apply argmax for first prediction
np.argmax(probability[0,:]) # 0
# We can apply to each sample
softmax_prediction=np.argmax(probability,axis=1)
softmax_prediction
# we took lr.predict_proba, applied argmax across all probabilities of all classes to get prediction
# can have lr do all this with lr.predict. Here we test they are the same:
yhat =lr.predict(X)
accuracy_score(yhat,softmax_prediction) # 1
</code></pre>
<p>We can't use Softmax regression for SVMs let explore two methods of Multi-class Classification. that we can apply to SVM.</p>
<p>Sklean performs Multi-class Classification automatically, we can apply the method and calculate the accuracy. Train a SVM classifier with the <code>kernel</code> set to <code>linear</code>, <code>gamma</code> set to <code>0.5</code>, and the <code>probability</code> paramter set to <code>True</code>, then train the model using the <code>X</code> and <code>y</code> data.</p>
<h2 id="svm">SVM</h2>
<pre><code class="language-python"># linear kernel
linear_svm = SVC(kernel='linear', gamma=0.5, probability=True)
linear_svm.fit(X, y) 
yhat = linear_svm.predict(X)
print (classification_report(y, yhat))
# f1_score
f1_score(y, yhat, average='weighted') 
# notice accuracy_score
accuracy_score(y, yhat)
# plot decision boundary
decision_boundary (X,y,linear_svm,iris)
</code></pre>
<h2 id="one-vs-all_1">One vs All</h2>
<p>For one-vs-All classification, if we have K classes, we use K  two-class classifier models—the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class we will call this "dummy" class. For each classifier, we split the data into two classes.  We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a  classification, we use the classifier with the highest probability, disregarding the dummy class.</p>
<pre><code class="language-python">#dummy class
dummy_class=y.max()+1
#list used for classifiers 
my_models=[]
#iterate through each class
for class_ in np.unique(y):
    #select the index of our  class
    select=(y==class_)
    temp_y=np.zeros(y.shape)
    #class, we are trying to classify 
    temp_y[y==class_]=class_
    #set other samples  to a dummy class 
    temp_y[y!=class_]=dummy_class
    #Train model and add to list 
    model=SVC(kernel='linear', gamma=.5, probability=True)    
    my_models.append(model.fit(X,temp_y))
    #plot decision boundary 
    decision_boundary (X,temp_y,model,iris)

</code></pre>
<p><img alt="one_v_all_9.jpg" src="../Images/multiclass_prediction/one_v_all_9.jpg" /></p>
<p>For each sample we calculate the  probability of belonging to each class, not including the dummy class.</p>
<pre><code class="language-python">probability_array=np.zeros((X.shape[0],3))
for j,model in enumerate(my_models):
    real_class=np.where(np.array(model.classes_)!=3)[0]
    probability_array[:,j]=model.predict_proba(X)[:,real_class][:,0]

# first sample
probability_array[0,:]
# As each is the probability of belonging to the actual class and not the dummy class is does not sum to one. 
probability_array[0,:].sum()
# We can plot the probability of belonging to the class. The row number is the sample number.
plot_probability_array(X,probability_array)
# We can apply the argmax function to each sample to find the class 
one_vs_all=np.argmax(probability_array,axis=1)
one_vs_all
# we can calculate the accuracy 
accuracy_score(y,one_vs_all)
# We see the accuracy is less than the one obtained by sklearn, and this is because for SVM sklearn uses one vs one; let's verify it by comparing the outputs. 
accuracy_score(one_vs_all,yhat)
# we see that the output are different, now lets implement one vs one 
</code></pre>
<h2 id="one-vs-one">One vs One</h2>
<p>In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1,2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2 and a final classifier on samples of class 1 and class 2.</p>
<p>For <span class="arithmatex">\(K\)</span> classes, we have to train  <span class="arithmatex">\(K(K-1)/2\)</span>  classifiers. So if <span class="arithmatex">\(K=3\)</span>, we have <span class="arithmatex">\((3x2)/2=3\)</span> classes.</p>
<p>To perform classification on a sample, we perform a majority vote and select the class with the most predictions. </p>
<pre><code class="language-python"># list each class
classes_=set(np.unique(y))
classes_
# determine the number of classifiers:
K=len(classes_)
K*(K-1)/2
# We then train a two-class classifier on each pair of classes. We plot the different training points for each of the two classes 
pairs=[]
left_overs=classes_.copy()
#list used for classifiers 
my_models=[]
#iterate through each class
for class_ in classes_:
    #remove class we have seen before 
    left_overs.remove(class_)
    #the second class in the pair
    for second_class in left_overs:
        pairs.append(str(class_)+' and '+str(second_class))
        print(&quot;class {} vs class {} &quot;.format(class_,second_class) )
        temp_y=np.zeros(y.shape)
        #find classes in pair 
        select=np.logical_or(y==class_ , y==second_class)
        #train model 
        model=SVC(kernel='linear', gamma=.5, probability=True)  
        model.fit(X[select,:],y[select])
        my_models.append(model)
        #Plot decision boundary for each pair and corresponding Training samples. 
        decision_boundary (X[select,:],y[select],model,iris,two=True)

# pairs 
pairs # ['0 and 1', '0 and 2', '1 and 2']
</code></pre>
<p><img alt="one_v_all_10.jpg" src="../Images/multiclass_prediction/one_v_all_10.jpg" /></p>
<pre><code class="language-python"># As we can see, our data is left-skewed, containing greater number of '5' star reviews. 
# (This means the blue regions have more data points)
# Here, we are ploting the disribution of ext length
pairs
majority_vote_array=np.zeros((X.shape[0],3))
majority_vote_dict={}
for j,(model,pair) in enumerate(zip(my_models,pairs)):
    majority_vote_dict[pair]=model.predict(X)
    majority_vote_array[:,j]=model.predict(X)

pd.DataFrame(majority_vote_dict).head(10)
pd.DataFrame(majority_vote_dict).iloc[60:70]

# To perform classification on a sample, we perform a majority vote i.e. select the class with the most predictions. We repeat the process for each sample. 
one_vs_one=np.array([np.bincount(sample.astype(int)).argmax() for sample  in majority_vote_array]) 
one_vs_one
# We calculate the accuracy:
accuracy_score(y,one_vs_one)
# we compare it to sklearn , it's the same! 
accuracy_score(yhat,one_vs_one)
</code></pre>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../Clustering/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../Clustering/" class="btn btn-xs btn-link">
        8. Clustering
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../Support_Vector_Machine/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../Support_Vector_Machine/" class="btn btn-xs btn-link">
        6. Support Vector Machine
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>