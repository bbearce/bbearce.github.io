{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Ben's Code Journal May it aid you in your coding adventures if a google search lands you here. aws_code-journal","title":"Home"},{"location":"#welcome-to-bens-code-journal","text":"May it aid you in your coding adventures if a google search lands you here. aws_code-journal","title":"Welcome to Ben's Code Journal"},{"location":"notes/MKDocs/Quick%20Notes/","text":"Quick Notes Deploy the server\\site Be sure you are in ./code-journal/ or ./<project root> code-journal |-docs |-site |-mkdocs.yml Benjamins-MBP-2:code-journal bbearce$ pwd /Users/bbearce/Documents/Code/code-journal Use this code to serve the developer site $ mkdocs serve Use this code to build the site $ mkdocs build To push to github do this: cd ../bbearce.github.io/ mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master Detailed Github Notes github deploy Note for the venv pip freeze lists these packages: Click==7.0 Jinja2==2.10.1 livereload==2.6.1 Markdown==3.1.1 MarkupSafe==1.1.1 mkdocs==1.0.4 mkdocs-rtd-dropdown==1.0.2 mkdocs-windmill-dark==0.2.0 PyYAML==5.1.2 six==1.12.0 tornado==6.0.3 I am pretty sure I only installed mkdocs and mkdocs-windmill-dark","title":"MKDocs"},{"location":"notes/MKDocs/Quick%20Notes/#quick-notes","text":"","title":"Quick Notes"},{"location":"notes/MKDocs/Quick%20Notes/#deploy-the-serversite","text":"Be sure you are in ./code-journal/ or ./<project root> code-journal |-docs |-site |-mkdocs.yml Benjamins-MBP-2:code-journal bbearce$ pwd /Users/bbearce/Documents/Code/code-journal Use this code to serve the developer site $ mkdocs serve Use this code to build the site $ mkdocs build To push to github do this: cd ../bbearce.github.io/ mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master","title":"Deploy the server\\site"},{"location":"notes/MKDocs/Quick%20Notes/#detailed-github-notes","text":"github deploy","title":"Detailed Github Notes"},{"location":"notes/MKDocs/Quick%20Notes/#note-for-the-venv","text":"pip freeze lists these packages: Click==7.0 Jinja2==2.10.1 livereload==2.6.1 Markdown==3.1.1 MarkupSafe==1.1.1 mkdocs==1.0.4 mkdocs-rtd-dropdown==1.0.2 mkdocs-windmill-dark==0.2.0 PyYAML==5.1.2 six==1.12.0 tornado==6.0.3 I am pretty sure I only installed mkdocs and mkdocs-windmill-dark","title":"Note for the venv"},{"location":"notes/Martinos/cluster/cluster/","text":"Welcome To The Martinos Center Official Website Locate the Intranet in the top right on the homepage. This hosts all things Martinos related and should be your first stop for information. Other userful links within the intranet: Compute Resources Frequently Asked Questions Martinos User Information (sometimes more granular about technical things) Pre-requisites: Partners ID Martinos Account MLSC Access Loggin in Test access by sshing into mlsc.nmr.mgh.harvard.edu while on the Partners VPN (scroll down a little) . You should be able to do this in the CMD in Windows or your terminal in MacOS\\Linux. Ex: bbearce@pop-os:~$ ssh bb927@mlsc.nmr.mgh.harvard.edu /$$ /$$ /$$ /$$$$$$ /$$$$$$ | $$$ /$$$| $$ /$$__ $$ /$$__ $$ | $$$$ /$$$$| $$ | $$ \\__/| $$ \\__/ | $$ $$/$$ $$| $$ | $$$$$$ | $$ | $$ $$$| $$| $$ \\____ $$| $$ | $$\\ $ | $$| $$ /$$ \\ $$| $$ $$ | $$ \\/ | $$| $$$$$$$$| $$$$$$/| $$$$$$/ |__/ |__/|________/ \\______/ \\______/ Please cite the Massachusetts Life Sciences Center on your posters and publications for any data analyzed on this cluster. <><><><><><><><><><><><><><><><><><><><><><> Last login: Wed Sep 22 16:06:16 2021 from 10.251.11.74 mlsc-login[0]:~$ Home Directories and Data Home Directories We have user accounts that are synced between all martinos machines and they all live on a drive managed by help desk. In order to keep space concerns down with 100s of users, they limit our space allowance to the point where it is unusable for more than configuratoin or tiny tests. Therefore everything (99% of everything) is stored on hard drives or network storage. Because of this the most important step is to symlink some important directories to change where the data stored for various programs (docker, singularity, python) will show up ultimately (not in ~/). Notice how pwd prints something like /homes/#/\\<username> rather than /home/\\<username> . This is because we all live in /homes which is special directory (drive) managed by Help Desk . Ex: mlsc-login[0]:~$ pwd /homes/3/bb927 Drives and Network Storage Martinos Official Notes Main Storage: /cluster/qtim - 20TB # Standard cluster storage. New Storage: /vast/qtim - 2TB # Ultra Fast and was bought with the cluster's grant money. Drives There are no drives on the cluster. We do have our own Desktops and Servers in the QTIM lab, however that is not the focus of this tutorial. We will need to dip into this a little in the Singularity section at the end of this tutorial as we need a drive to build Singularity images. Network Storage As the name implies /cluster/qtim is our lab's (QTIM) storage for use with the cluster. Everyone has a user folder as demonstrated here: mlsc-login[0]:~$ ls /cluster/qtim ID-qtim machine_backups users mlsc-login[0]:~$ ls -la /cluster/qtim/users total 154 drwxrwsr-x. 20 bb927 qtim 20 Sep 21 16:26 . drwxrws---. 4 root qtim 5 May 23 10:05 .. drwxrwsr-x. 13 ai347 qtim 28 Aug 2 15:32 ai347 drwxrwsr-x. 22 apv12 qtim 41 Sep 5 16:46 apv12 drwxrwsr-x. 2 aza24 qtim 3 Jun 4 12:47 aza24 drwxrwsr-x. 15 bb927 qtim 24 Aug 5 18:14 bb927 drwxrwsr-x. 4 cl48 qtim 4 May 11 14:14 cl48 drwxrwsr-x. 3 cni1 qtim 3 Sep 21 17:07 cni1 drwxrwsr-x. 5 cpb28 qtim 5 Mar 4 2021 cpb28 drwxrwsr-x. 2 dd86 qtim 2 Apr 28 21:50 dd86 drwxrwsr-x. 5 gc660 qtim 6 Sep 22 14:41 gc660 drwxrwsr-x. 3 ij063 qtim 3 Aug 11 12:52 ij063 drwxrwsr-x. 9 jn85 qtim 9 Jul 21 13:26 jn85 drwxrwsr-x. 3 kalpathy qtim 3 Aug 4 2020 kalpathy drwxrwsr-x. 7 nt771 qtim 52 Sep 20 08:20 sakshi drwxrwsr-x. 10 sf172 qtim 10 May 28 14:20 sf172 drwxrwsr-x. 15 si74 qtim 20 Sep 11 06:38 si74 drwxrwsr-x. 2 bb927 qtim 4 Sep 21 17:31 slurm_jobs drwxrwsr-x. 27 va914 qtim 35 Aug 2 13:12 va914 drwxr-sr-x. 35 bb927 qtim 43 Sep 10 2019 yifen Go ahead and make yourself a directory here. Note we do not have sudo on these machines and everyone owns their own folder and are in the qtim group. These are denoted in columns 3 and 4 . We can technically add data to each others folders in their default state so becareful. Ex: <permissions>. <hard-links> <user> <group> <size> <Month Day Time> <item> This is the official place to do work, store data and code. This location is available you from any Martinos machine. It is 20TB large but we have a tendancy to fill up data fast. I'm in the process of clearing some out so don't worry, but there is enough room for this demo. Anaconda We will need a python source for part of this demo. The Martinos center has setup an Anaconda distribution we can use thathas scientific modules as part of it. Instructions for setup For now run this: export PATH=/usr/pubsw/packages/python/anaconda3.7/bin:${PATH} Now you can use python: mlsc-login[0]:bb927$ python Python 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> and list conda packages available: mlsc-login[0]:bb927$ conda list # packages in environment at /usr/pubsw/packages/python/anaconda3-2019.03: # # Name Version Build Channel _ipyw_jlab_nb_ext_conf 0.1.0 py37_0 _libgcc_mutex 0.1 main _r-mutex 1.0.0 anacondar_1 _tflow_select 2.1.0 gpu absl-py 0.7.1 py37_0 alabaster 0.7.12 py37_0 ... ... Additional instructions show you how to set your system path and properly install new packages. Submitting Jobs The Martinos Center Help Desk has created official instructions for the Cluster's use located at https://it.martinos.org/mlsc-cluster/ . It uses SLURM to manage work loads. The Martinos cluster docs above give the details but the most important piece is the resources available: Description #Nodes #Cores RAM #GPU Partition Scratch Dell R440 Server 30 24 384GB none basic 1.5TB RTX 6000 GPU Server 2 32 1.5TB 8 rtx6000 7.0TB RTX 8000 GPU Server 5 32 1.5TB 4 have 10, 1 has 4 rtx8000 7.0TB NVIDIA DGX A100 4 64 1.0TB 8 dgx-a100 14TB R440 Server - CPU compute cluster RTX6000 - 24GB GPU RAM RTX8000 - 48GB GPU RAM A100 - 40GB GPU RAM (Newest, fastest and almost largest) Namely the Partition column . This is the resource you will select later. The 1st is a CPU and the last 3 are incrementally larger/faster GPUs. You should already be signed in. Change directory to the qtim cluster network storage now (be sure to have made your own directory with mkdir <username> where \\<username> is your Partners ID and it should be located at /cluster/qtim/users/. Also this directory is one level higher than your user directory): Ex: mlsc-login[0]:~$ cd /cluster/qtim/users Notice a non-user directory called slurm_jobs mlsc-login[0]:users$ ls slurm_jobs README.md sample_job.py Here you will find a README.md file for using the cluster. It follows from the notes in the link above, but I find it handy. In it you will find two main ways to use the job scheduler SLURM. They are: $ jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py and srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash Let's break the flags down: Required argument flags: -A <account> Fairshare/billing account to use (qtim) -m <memlimit> Max memory your job will use before killed (RAM). -p <partition> Partition to submit the job to (machine type) -t <timelimit> Max real time job will use before killed (DD-HH:MM:SS) Other optional flags: -c <numcores> Number of cores per node required (default 1) -G <numgpus> Number of GPUS required (default 0) -M <mailtype> Send mail events. Comma separated list of: BEGIN, END, FAIL, REQUEUE, ALL (previous four), TIME_LIMIT_90, TIME_LIMIT_80, TIME_LIMIT_50, ARRAY_TASKS, NONE (default) --mail-user <user> Send mail to <user> (default is current user) The more readable versions are: $ jobsubmit -p rtx6000 \\ -A qtim \\ -m 128G \\ -t 0-00:5:00 \\ -c 3 \\ -G 1 \\ -M ALL \\ --mail-user bbearce@mgh.harvard.edu \\ python sample_job.py $ srun -p rtx6000 \\ -A qtim \\ --mem 128G \\ -t 0-00:5:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash jobsubmit allows for running of a script file while srun allows interactice mode. Script Execution Let's try to submit a script (sample_job.py), but first let's open it: /cluster/qtim/users/slurm_jobs/sample_job.py import torch if __name__ == \"__main__\": gpu_status = torch.cuda.is_available() with open(\"/cluster/qtim/users/bb927/slurm_jobs/output.txt\", \"w\") as f: f.write(\"Did this work? GPUs available: {}\".format(gpu_status)); This when run, will put a file called output.txt in /cluster/qtim/users/bb927/slurm_jobs . However you might not have made slurm_jobs in your user directory yet so go ahead and create it and also copy sample_job.py to your slurm_jobs directory. Ex: mlsc-login[0]:bb927$ mkdir /cluster/qtim/users/bb927/slurm_jobs mlsc-login[0]:bb927$ cp ../slurm_jobs/sample_job.py ./slurm_jobs/ Next change your copy of sample_job.py to edit the user folder to your own: from: with open(\"/cluster/qtim/users/bb927/slurm_jobs/output.txt\", \"w\") as f: to: with open(\"/cluster/qtim/users/<user directory you already made made>/slurm_jobs/output.txt\", \"w\") as f: Now we should be able to execute this. Change directories into your personal slurm_jobs folder and run this command substituting your --mail-user (email address): jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py You will get an email and eventually output.txt will be in your slurm_jobs directory: mlsc-login[0]:slurm_jobs$ ls README.md sample_job.py mlsc-login[0]:slurm_jobs$ ls output.txt README.md sample_job.py Look inside output.txt : mlsc-login[0]:slurm_jobs$ cat output.txt Did this work? GPUs available: True That is how you submit a script. Interactive Mode Maybe you haven't written a script yet or any pipeline. You can reserve access with srun and --pty /bin/bash . srun -p rtx6000 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash [bb927@rtx-01 slurm_jobs]$ Notice how your terminal changes to indicate that you are connected to another server: [bb927@rtx-01 slurm_jobs]$ hostname rtx-01.nmr.mgh.harvard.edu This is the normal hostname: mlsc-login[0]:slurm_jobs$ hostname mlsc-login.nmr.mgh.harvard.edu If you can't get a job it's because the \"fair share\" system implemented to make sure everyone has fair access is not letting you as all those resources are spoken for. Usually the smaller resources are more in demand so for demo purposes we can choose large compute and little time to ensure we get something. Ex: rtx-01[0]:slurm_jobs$ srun -p rtx6000 \\ > -A qtim \\ > --mem 4G \\ > -t 0-00:1:00 \\ > -c 3 \\ > -G 1 \\ > --mail-user bbearce@mgh.harvard.edu \\ > --pty /bin/bash srun: Job 565212 step creation temporarily disabled, retrying (Requested nodes are busy) Try an RTX 8000 then: srun -p rtx8000 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash rtx-06[0]:slurm_jobs$ And if all else fails, try an A100 then: srun -p dgx-a100 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash A100-04[0]:slurm_jobs$ Once in a machine you can look around but know it's a different server. Also notice how /cluster/qtim is still available. This is key as our network storage is always available on every node (server) in the cluster. rtx-01[0]:slurm_jobs$ ls /cluster/qtim ID-qtim machine_backups users I'm also in the starting directory I ran the script from and I can also run the previous script from here too: rtx-01[0]:slurm_jobs$ pwd /cluster/qtim/users/bb927/slurm_jobs rtx-01[0]:slurm_jobs$ python sample_job.py rtx-01[0]:slurm_jobs$ ls output.txt sample_job.py Singularity Martinos Singularity Instructions In a nut shell this is what needs to be executed: Create singularity folder in your cluster user space (make sure ~/.singularity does not already exist first) mlsc-login[0]:~$ cd /cluster/qtim/users/bb927 mlsc-login[0]:~$ mkdir singularity mlsc-login[0]:~$ ln -s /cluster/qtim/users/bb927/singularity ~/.singularity You can now put things in either ~/.singularity or /cluster/qtim/users/bb927/singularity and they will be in /cluster/qtim/users/bb927/singularity. Set some environment variables mkdir ~/.singularity/tmp export SINGULARITY_TMPDIR=~/.singularity/tmp mkdir ~/.singularity/cache export SINGULARITY_CACHEDIR=~/.singularity/cache (do this or singularity will fill up workstation OS disk at /tmp) (in fact put this setting in your .bashrc or .cshrc) Test by listing home directory: Ex. mlsc-login[0]:bb927$ ls -la ~ ... lrwxrwxrwx. 1 bb927 bb927 37 Sep 24 01:38 .singularity -> /cluster/qtim/users/bb927/singularity ... Using Premade Containers You might need an account on DockerHUB Build We will be bootstraping our containers from DockerHUB . Ex: mlsc-login[0]:bb927$ singularity build ubuntu.simg docker://ubuntu:latest WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... ... When done view it: mlsc-login[0]:bb927$ ls ubuntu.simg Shell Into Container Shell into it with singularity shell : mlsc-login[0]:test$ singularity shell ubuntu.simg Singularity> You can execute code and look around. Must know: [1] Images are read-only by default...more on that later. [2] You are not root inside your image, but rather your username on linux (Ex: bb927). [3] Remember Singularity wants to auto --bind mount these 3 directories in the background whether you say to or not: * /home/$USER * /tmp * $PWD [4] For editable images (sandboxes) you must create your mount points ahead of time in the sandbox. (More shortly) Let's talk about [3] real quick. If you do an ls it looks like we are in our home directory. Inside this container Singularity is making /homes/3/bb927 point to /homes/3/bb927 on the mlsc. This means anything you write here will be available if you exit the container. Singularity> ls Desktop Downloads Pictures Templates Untitled1.ipynb jupyter matlab_crash_dump.224407-1 untitled.ipynb Documents Music Public Untitled.ipynb Videos matlab test.txt Singularity> pwd /homes/3/bb927 Editable Containers By default these are read only, which means you can't install software in them and besides mounts, you can't write to them. So you will need to make an editable version. mlsc-login[0]:test$ singularity build --sandbox s_ubuntu ubuntu.simg WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... View it: mlsc-login[0]:test$ ls s_ubuntu ubuntu.simg Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub (SingularityHUB - A docker like repository of container sources) or dockerhub images as well. Sandboxes are just directories so you can see inside with ls : mlsc-login[0]:test$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr To shell into sandboxs and make changes we need to add the --writable , --fakeroot and --no-home flag. mlsc-login[0]:test$ singularity shell --writable --no-home s_ubuntu/ WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> Let's make a change: Singularity> cd / Singularity> ls / bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr Singularity> touch deleteme.txt Singularity> ls | grep deleteme.txt deleteme.txt Singularity> ls / bin deleteme.txt environment... ... Now let's build this back into a real container (type exit and hit Enter to leave continer shell): mlsc-login[0]:test$ singularity build s_ubuntu_modified.simg s_ubuntu WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... ls : mlsc-login[0]:test$ ls s_ubuntu s_ubuntu_modified.simg ubuntu.simg Shell into new Singularity Image: mlsc-login[0]:test$ singularity shell s_ubuntu_modified.simg Singularity> ls / bin deleteme.txt environment ... ... deleteme.txt is in the new image. This exercise is to show you how to make new Singularity images after you make edits to them like installing new python packages, installing other software and adding files. Other Useful Flags -B or --bind for mounting volumes. --nv gives you access to a GPU if on a machine that has one (must be done by submitting a job, as the \"mlsc\") Jupyter Note from Martinos help Desk: If you run jupyter, you will not be able to access the port it is on ( 8888 ) remotely (such as your local machine at home over VPN) due to firewall on the mlsc. But port range 5900-5999 are open for VNC so if you force jupyter to use port 5999 instead of its default you can use that. Locate file /cluster/qtim/users/slurm_jobs/datascience-notebook_latest.sif . I recommend changing directory to /cluster/qtim/users/slurm_jobs/. While in /cluster/qtim/users/slurm_jobs/: [1] Start job: srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash [2] Forward via ssh in your local machine: bbearce@pop-os:~$ ssh -N -f -L localhost:5999:localhost:5999 bb927@rtx-01.nmr.mgh.harvard.edu [3] Shell into Singularity Image datascience-notebook_latest.sif and start jupyter notebook server: rtx-01[0]:slurm_jobs$ pwd /cluster/qtim/users/slurm_jobs rtx-01[0]:slurm_jobs$ singularity shell --nv -B /cluster/qtim/users/slurm_jobs:/new_dir datascience-notebook_latest.sif Singularity> Singularity> ls /new_dir/ datascience-notebook_latest.sif README.md sample_job.py Singularity> cd /new_dir/ Singularity> jupyter notebook --no-browser --port=5999 /opt/conda/lib/python3.8/site-packages/jupyter_server/transutils.py:13: FutureWarning: The alias `_()` will be deprecated. Use `_i18n()` instead. warnings.warn(warn_msg, FutureWarning) [W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [I 2021-09-24 02:34:58.067 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [I 2021-09-24 02:34:58.067 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 02:34:58.070 NotebookApp] Serving notebooks from local directory: /new_dir [I 02:34:58.070 NotebookApp] Jupyter Notebook 6.3.0 is running at: [I 02:34:58.070 NotebookApp] http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 [I 02:34:58.070 NotebookApp] or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 [I 02:34:58.070 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 02:34:58.081 NotebookApp] To access the notebook, open this file in a browser: file:///homes/3/bb927/.local/share/jupyter/runtime/nbserver-3373125-open.html Or copy and paste one of these URLs: http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 Now because of the ssh tunnel we setup in step [2] we can reach this with \"localhost\" or \"127.0.0.1\" as hostnames. Note we can also use the machine name \"rtx-01.nmr.mgh.harvard.edu\" We can test the --nv flag as well by seeing if nvidia-smi works: Finally if you don't ssh tunnel then you have to use the hostname: Find host name with hostname once your job gives you a resource: rtx-01[0]:slurm_jobs$ hostname rtx-01.nmr.mgh.harvard.edu Building Your Own Sinularity Images Original Demo We can't build on the /cluster/qtim network storage so if we want to use recipe files, we need to build images on hard drives and for that we need QTIM servers. We can touch on this another time.","title":"Cluster"},{"location":"notes/Martinos/cluster/cluster/#welcome-to-the-martinos-center","text":"Official Website Locate the Intranet in the top right on the homepage. This hosts all things Martinos related and should be your first stop for information. Other userful links within the intranet: Compute Resources Frequently Asked Questions Martinos User Information (sometimes more granular about technical things) Pre-requisites: Partners ID Martinos Account MLSC Access","title":"Welcome To The Martinos Center"},{"location":"notes/Martinos/cluster/cluster/#loggin-in","text":"Test access by sshing into mlsc.nmr.mgh.harvard.edu while on the Partners VPN (scroll down a little) . You should be able to do this in the CMD in Windows or your terminal in MacOS\\Linux. Ex: bbearce@pop-os:~$ ssh bb927@mlsc.nmr.mgh.harvard.edu /$$ /$$ /$$ /$$$$$$ /$$$$$$ | $$$ /$$$| $$ /$$__ $$ /$$__ $$ | $$$$ /$$$$| $$ | $$ \\__/| $$ \\__/ | $$ $$/$$ $$| $$ | $$$$$$ | $$ | $$ $$$| $$| $$ \\____ $$| $$ | $$\\ $ | $$| $$ /$$ \\ $$| $$ $$ | $$ \\/ | $$| $$$$$$$$| $$$$$$/| $$$$$$/ |__/ |__/|________/ \\______/ \\______/ Please cite the Massachusetts Life Sciences Center on your posters and publications for any data analyzed on this cluster. <><><><><><><><><><><><><><><><><><><><><><> Last login: Wed Sep 22 16:06:16 2021 from 10.251.11.74 mlsc-login[0]:~$","title":"Loggin in"},{"location":"notes/Martinos/cluster/cluster/#home-directories-and-data","text":"","title":"Home Directories and Data"},{"location":"notes/Martinos/cluster/cluster/#home-directories","text":"We have user accounts that are synced between all martinos machines and they all live on a drive managed by help desk. In order to keep space concerns down with 100s of users, they limit our space allowance to the point where it is unusable for more than configuratoin or tiny tests. Therefore everything (99% of everything) is stored on hard drives or network storage. Because of this the most important step is to symlink some important directories to change where the data stored for various programs (docker, singularity, python) will show up ultimately (not in ~/). Notice how pwd prints something like /homes/#/\\<username> rather than /home/\\<username> . This is because we all live in /homes which is special directory (drive) managed by Help Desk . Ex: mlsc-login[0]:~$ pwd /homes/3/bb927","title":"Home Directories"},{"location":"notes/Martinos/cluster/cluster/#drives-and-network-storage","text":"Martinos Official Notes Main Storage: /cluster/qtim - 20TB # Standard cluster storage. New Storage: /vast/qtim - 2TB # Ultra Fast and was bought with the cluster's grant money.","title":"Drives and Network Storage"},{"location":"notes/Martinos/cluster/cluster/#drives","text":"There are no drives on the cluster. We do have our own Desktops and Servers in the QTIM lab, however that is not the focus of this tutorial. We will need to dip into this a little in the Singularity section at the end of this tutorial as we need a drive to build Singularity images.","title":"Drives"},{"location":"notes/Martinos/cluster/cluster/#network-storage","text":"As the name implies /cluster/qtim is our lab's (QTIM) storage for use with the cluster. Everyone has a user folder as demonstrated here: mlsc-login[0]:~$ ls /cluster/qtim ID-qtim machine_backups users mlsc-login[0]:~$ ls -la /cluster/qtim/users total 154 drwxrwsr-x. 20 bb927 qtim 20 Sep 21 16:26 . drwxrws---. 4 root qtim 5 May 23 10:05 .. drwxrwsr-x. 13 ai347 qtim 28 Aug 2 15:32 ai347 drwxrwsr-x. 22 apv12 qtim 41 Sep 5 16:46 apv12 drwxrwsr-x. 2 aza24 qtim 3 Jun 4 12:47 aza24 drwxrwsr-x. 15 bb927 qtim 24 Aug 5 18:14 bb927 drwxrwsr-x. 4 cl48 qtim 4 May 11 14:14 cl48 drwxrwsr-x. 3 cni1 qtim 3 Sep 21 17:07 cni1 drwxrwsr-x. 5 cpb28 qtim 5 Mar 4 2021 cpb28 drwxrwsr-x. 2 dd86 qtim 2 Apr 28 21:50 dd86 drwxrwsr-x. 5 gc660 qtim 6 Sep 22 14:41 gc660 drwxrwsr-x. 3 ij063 qtim 3 Aug 11 12:52 ij063 drwxrwsr-x. 9 jn85 qtim 9 Jul 21 13:26 jn85 drwxrwsr-x. 3 kalpathy qtim 3 Aug 4 2020 kalpathy drwxrwsr-x. 7 nt771 qtim 52 Sep 20 08:20 sakshi drwxrwsr-x. 10 sf172 qtim 10 May 28 14:20 sf172 drwxrwsr-x. 15 si74 qtim 20 Sep 11 06:38 si74 drwxrwsr-x. 2 bb927 qtim 4 Sep 21 17:31 slurm_jobs drwxrwsr-x. 27 va914 qtim 35 Aug 2 13:12 va914 drwxr-sr-x. 35 bb927 qtim 43 Sep 10 2019 yifen Go ahead and make yourself a directory here. Note we do not have sudo on these machines and everyone owns their own folder and are in the qtim group. These are denoted in columns 3 and 4 . We can technically add data to each others folders in their default state so becareful. Ex: <permissions>. <hard-links> <user> <group> <size> <Month Day Time> <item> This is the official place to do work, store data and code. This location is available you from any Martinos machine. It is 20TB large but we have a tendancy to fill up data fast. I'm in the process of clearing some out so don't worry, but there is enough room for this demo.","title":"Network Storage"},{"location":"notes/Martinos/cluster/cluster/#anaconda","text":"We will need a python source for part of this demo. The Martinos center has setup an Anaconda distribution we can use thathas scientific modules as part of it. Instructions for setup For now run this: export PATH=/usr/pubsw/packages/python/anaconda3.7/bin:${PATH} Now you can use python: mlsc-login[0]:bb927$ python Python 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> and list conda packages available: mlsc-login[0]:bb927$ conda list # packages in environment at /usr/pubsw/packages/python/anaconda3-2019.03: # # Name Version Build Channel _ipyw_jlab_nb_ext_conf 0.1.0 py37_0 _libgcc_mutex 0.1 main _r-mutex 1.0.0 anacondar_1 _tflow_select 2.1.0 gpu absl-py 0.7.1 py37_0 alabaster 0.7.12 py37_0 ... ... Additional instructions show you how to set your system path and properly install new packages.","title":"Anaconda"},{"location":"notes/Martinos/cluster/cluster/#submitting-jobs","text":"The Martinos Center Help Desk has created official instructions for the Cluster's use located at https://it.martinos.org/mlsc-cluster/ . It uses SLURM to manage work loads. The Martinos cluster docs above give the details but the most important piece is the resources available: Description #Nodes #Cores RAM #GPU Partition Scratch Dell R440 Server 30 24 384GB none basic 1.5TB RTX 6000 GPU Server 2 32 1.5TB 8 rtx6000 7.0TB RTX 8000 GPU Server 5 32 1.5TB 4 have 10, 1 has 4 rtx8000 7.0TB NVIDIA DGX A100 4 64 1.0TB 8 dgx-a100 14TB R440 Server - CPU compute cluster RTX6000 - 24GB GPU RAM RTX8000 - 48GB GPU RAM A100 - 40GB GPU RAM (Newest, fastest and almost largest) Namely the Partition column . This is the resource you will select later. The 1st is a CPU and the last 3 are incrementally larger/faster GPUs. You should already be signed in. Change directory to the qtim cluster network storage now (be sure to have made your own directory with mkdir <username> where \\<username> is your Partners ID and it should be located at /cluster/qtim/users/. Also this directory is one level higher than your user directory): Ex: mlsc-login[0]:~$ cd /cluster/qtim/users Notice a non-user directory called slurm_jobs mlsc-login[0]:users$ ls slurm_jobs README.md sample_job.py Here you will find a README.md file for using the cluster. It follows from the notes in the link above, but I find it handy. In it you will find two main ways to use the job scheduler SLURM. They are: $ jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py and srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash Let's break the flags down: Required argument flags: -A <account> Fairshare/billing account to use (qtim) -m <memlimit> Max memory your job will use before killed (RAM). -p <partition> Partition to submit the job to (machine type) -t <timelimit> Max real time job will use before killed (DD-HH:MM:SS) Other optional flags: -c <numcores> Number of cores per node required (default 1) -G <numgpus> Number of GPUS required (default 0) -M <mailtype> Send mail events. Comma separated list of: BEGIN, END, FAIL, REQUEUE, ALL (previous four), TIME_LIMIT_90, TIME_LIMIT_80, TIME_LIMIT_50, ARRAY_TASKS, NONE (default) --mail-user <user> Send mail to <user> (default is current user) The more readable versions are: $ jobsubmit -p rtx6000 \\ -A qtim \\ -m 128G \\ -t 0-00:5:00 \\ -c 3 \\ -G 1 \\ -M ALL \\ --mail-user bbearce@mgh.harvard.edu \\ python sample_job.py $ srun -p rtx6000 \\ -A qtim \\ --mem 128G \\ -t 0-00:5:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash jobsubmit allows for running of a script file while srun allows interactice mode.","title":"Submitting Jobs"},{"location":"notes/Martinos/cluster/cluster/#script-execution","text":"Let's try to submit a script (sample_job.py), but first let's open it: /cluster/qtim/users/slurm_jobs/sample_job.py import torch if __name__ == \"__main__\": gpu_status = torch.cuda.is_available() with open(\"/cluster/qtim/users/bb927/slurm_jobs/output.txt\", \"w\") as f: f.write(\"Did this work? GPUs available: {}\".format(gpu_status)); This when run, will put a file called output.txt in /cluster/qtim/users/bb927/slurm_jobs . However you might not have made slurm_jobs in your user directory yet so go ahead and create it and also copy sample_job.py to your slurm_jobs directory. Ex: mlsc-login[0]:bb927$ mkdir /cluster/qtim/users/bb927/slurm_jobs mlsc-login[0]:bb927$ cp ../slurm_jobs/sample_job.py ./slurm_jobs/ Next change your copy of sample_job.py to edit the user folder to your own: from: with open(\"/cluster/qtim/users/bb927/slurm_jobs/output.txt\", \"w\") as f: to: with open(\"/cluster/qtim/users/<user directory you already made made>/slurm_jobs/output.txt\", \"w\") as f: Now we should be able to execute this. Change directories into your personal slurm_jobs folder and run this command substituting your --mail-user (email address): jobsubmit -p rtx6000 -A qtim -m 128G -t 0-00:5:00 -c 3 -G 1 -M ALL --mail-user bbearce@mgh.harvard.edu python sample_job.py You will get an email and eventually output.txt will be in your slurm_jobs directory: mlsc-login[0]:slurm_jobs$ ls README.md sample_job.py mlsc-login[0]:slurm_jobs$ ls output.txt README.md sample_job.py Look inside output.txt : mlsc-login[0]:slurm_jobs$ cat output.txt Did this work? GPUs available: True That is how you submit a script.","title":"Script Execution"},{"location":"notes/Martinos/cluster/cluster/#interactive-mode","text":"Maybe you haven't written a script yet or any pipeline. You can reserve access with srun and --pty /bin/bash . srun -p rtx6000 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash [bb927@rtx-01 slurm_jobs]$ Notice how your terminal changes to indicate that you are connected to another server: [bb927@rtx-01 slurm_jobs]$ hostname rtx-01.nmr.mgh.harvard.edu This is the normal hostname: mlsc-login[0]:slurm_jobs$ hostname mlsc-login.nmr.mgh.harvard.edu If you can't get a job it's because the \"fair share\" system implemented to make sure everyone has fair access is not letting you as all those resources are spoken for. Usually the smaller resources are more in demand so for demo purposes we can choose large compute and little time to ensure we get something. Ex: rtx-01[0]:slurm_jobs$ srun -p rtx6000 \\ > -A qtim \\ > --mem 4G \\ > -t 0-00:1:00 \\ > -c 3 \\ > -G 1 \\ > --mail-user bbearce@mgh.harvard.edu \\ > --pty /bin/bash srun: Job 565212 step creation temporarily disabled, retrying (Requested nodes are busy) Try an RTX 8000 then: srun -p rtx8000 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash rtx-06[0]:slurm_jobs$ And if all else fails, try an A100 then: srun -p dgx-a100 \\ -A qtim \\ --mem 4G \\ -t 0-00:1:00 \\ -c 3 \\ -G 1 \\ --mail-user bbearce@mgh.harvard.edu \\ --pty /bin/bash A100-04[0]:slurm_jobs$ Once in a machine you can look around but know it's a different server. Also notice how /cluster/qtim is still available. This is key as our network storage is always available on every node (server) in the cluster. rtx-01[0]:slurm_jobs$ ls /cluster/qtim ID-qtim machine_backups users I'm also in the starting directory I ran the script from and I can also run the previous script from here too: rtx-01[0]:slurm_jobs$ pwd /cluster/qtim/users/bb927/slurm_jobs rtx-01[0]:slurm_jobs$ python sample_job.py rtx-01[0]:slurm_jobs$ ls output.txt sample_job.py","title":"Interactive Mode"},{"location":"notes/Martinos/cluster/cluster/#singularity","text":"Martinos Singularity Instructions In a nut shell this is what needs to be executed:","title":"Singularity"},{"location":"notes/Martinos/cluster/cluster/#create-singularity-folder-in-your-cluster-user-space","text":"(make sure ~/.singularity does not already exist first) mlsc-login[0]:~$ cd /cluster/qtim/users/bb927 mlsc-login[0]:~$ mkdir singularity mlsc-login[0]:~$ ln -s /cluster/qtim/users/bb927/singularity ~/.singularity You can now put things in either ~/.singularity or /cluster/qtim/users/bb927/singularity and they will be in /cluster/qtim/users/bb927/singularity.","title":"Create singularity folder in your cluster user space"},{"location":"notes/Martinos/cluster/cluster/#set-some-environment-variables","text":"mkdir ~/.singularity/tmp export SINGULARITY_TMPDIR=~/.singularity/tmp mkdir ~/.singularity/cache export SINGULARITY_CACHEDIR=~/.singularity/cache (do this or singularity will fill up workstation OS disk at /tmp) (in fact put this setting in your .bashrc or .cshrc) Test by listing home directory: Ex. mlsc-login[0]:bb927$ ls -la ~ ... lrwxrwxrwx. 1 bb927 bb927 37 Sep 24 01:38 .singularity -> /cluster/qtim/users/bb927/singularity ...","title":"Set some environment variables"},{"location":"notes/Martinos/cluster/cluster/#using-premade-containers","text":"You might need an account on DockerHUB","title":"Using Premade Containers"},{"location":"notes/Martinos/cluster/cluster/#build","text":"We will be bootstraping our containers from DockerHUB . Ex: mlsc-login[0]:bb927$ singularity build ubuntu.simg docker://ubuntu:latest WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... ... When done view it: mlsc-login[0]:bb927$ ls ubuntu.simg","title":"Build"},{"location":"notes/Martinos/cluster/cluster/#shell-into-container","text":"Shell into it with singularity shell : mlsc-login[0]:test$ singularity shell ubuntu.simg Singularity> You can execute code and look around. Must know: [1] Images are read-only by default...more on that later. [2] You are not root inside your image, but rather your username on linux (Ex: bb927). [3] Remember Singularity wants to auto --bind mount these 3 directories in the background whether you say to or not: * /home/$USER * /tmp * $PWD [4] For editable images (sandboxes) you must create your mount points ahead of time in the sandbox. (More shortly) Let's talk about [3] real quick. If you do an ls it looks like we are in our home directory. Inside this container Singularity is making /homes/3/bb927 point to /homes/3/bb927 on the mlsc. This means anything you write here will be available if you exit the container. Singularity> ls Desktop Downloads Pictures Templates Untitled1.ipynb jupyter matlab_crash_dump.224407-1 untitled.ipynb Documents Music Public Untitled.ipynb Videos matlab test.txt Singularity> pwd /homes/3/bb927","title":"Shell Into Container"},{"location":"notes/Martinos/cluster/cluster/#editable-containers","text":"By default these are read only, which means you can't install software in them and besides mounts, you can't write to them. So you will need to make an editable version. mlsc-login[0]:test$ singularity build --sandbox s_ubuntu ubuntu.simg WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... View it: mlsc-login[0]:test$ ls s_ubuntu ubuntu.simg Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub (SingularityHUB - A docker like repository of container sources) or dockerhub images as well. Sandboxes are just directories so you can see inside with ls : mlsc-login[0]:test$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr To shell into sandboxs and make changes we need to add the --writable , --fakeroot and --no-home flag. mlsc-login[0]:test$ singularity shell --writable --no-home s_ubuntu/ WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> Let's make a change: Singularity> cd / Singularity> ls / bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr Singularity> touch deleteme.txt Singularity> ls | grep deleteme.txt deleteme.txt Singularity> ls / bin deleteme.txt environment... ... Now let's build this back into a real container (type exit and hit Enter to leave continer shell): mlsc-login[0]:test$ singularity build s_ubuntu_modified.simg s_ubuntu WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... ls : mlsc-login[0]:test$ ls s_ubuntu s_ubuntu_modified.simg ubuntu.simg Shell into new Singularity Image: mlsc-login[0]:test$ singularity shell s_ubuntu_modified.simg Singularity> ls / bin deleteme.txt environment ... ... deleteme.txt is in the new image. This exercise is to show you how to make new Singularity images after you make edits to them like installing new python packages, installing other software and adding files.","title":"Editable Containers"},{"location":"notes/Martinos/cluster/cluster/#other-useful-flags","text":"-B or --bind for mounting volumes. --nv gives you access to a GPU if on a machine that has one (must be done by submitting a job, as the \"mlsc\")","title":"Other Useful Flags"},{"location":"notes/Martinos/cluster/cluster/#jupyter","text":"Note from Martinos help Desk: If you run jupyter, you will not be able to access the port it is on ( 8888 ) remotely (such as your local machine at home over VPN) due to firewall on the mlsc. But port range 5900-5999 are open for VNC so if you force jupyter to use port 5999 instead of its default you can use that. Locate file /cluster/qtim/users/slurm_jobs/datascience-notebook_latest.sif . I recommend changing directory to /cluster/qtim/users/slurm_jobs/. While in /cluster/qtim/users/slurm_jobs/: [1] Start job: srun -p rtx6000 -A qtim --mem 4G -t 0-00:1:00 -c 3 -G 1 --mail-user bbearce@mgh.harvard.edu --pty /bin/bash [2] Forward via ssh in your local machine: bbearce@pop-os:~$ ssh -N -f -L localhost:5999:localhost:5999 bb927@rtx-01.nmr.mgh.harvard.edu [3] Shell into Singularity Image datascience-notebook_latest.sif and start jupyter notebook server: rtx-01[0]:slurm_jobs$ pwd /cluster/qtim/users/slurm_jobs rtx-01[0]:slurm_jobs$ singularity shell --nv -B /cluster/qtim/users/slurm_jobs:/new_dir datascience-notebook_latest.sif Singularity> Singularity> ls /new_dir/ datascience-notebook_latest.sif README.md sample_job.py Singularity> cd /new_dir/ Singularity> jupyter notebook --no-browser --port=5999 /opt/conda/lib/python3.8/site-packages/jupyter_server/transutils.py:13: FutureWarning: The alias `_()` will be deprecated. Use `_i18n()` instead. warnings.warn(warn_msg, FutureWarning) [W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [W 2021-09-24 02:34:58.061 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. [I 2021-09-24 02:34:58.067 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [I 2021-09-24 02:34:58.067 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 02:34:58.070 NotebookApp] Serving notebooks from local directory: /new_dir [I 02:34:58.070 NotebookApp] Jupyter Notebook 6.3.0 is running at: [I 02:34:58.070 NotebookApp] http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 [I 02:34:58.070 NotebookApp] or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 [I 02:34:58.070 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 02:34:58.081 NotebookApp] To access the notebook, open this file in a browser: file:///homes/3/bb927/.local/share/jupyter/runtime/nbserver-3373125-open.html Or copy and paste one of these URLs: http://rtx-01.nmr.mgh.harvard.edu:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 or http://127.0.0.1:5999/?token=86c02c9b7be4ddcb5aa1540698f28d58308562223de4c7a4 Now because of the ssh tunnel we setup in step [2] we can reach this with \"localhost\" or \"127.0.0.1\" as hostnames. Note we can also use the machine name \"rtx-01.nmr.mgh.harvard.edu\" We can test the --nv flag as well by seeing if nvidia-smi works: Finally if you don't ssh tunnel then you have to use the hostname: Find host name with hostname once your job gives you a resource: rtx-01[0]:slurm_jobs$ hostname rtx-01.nmr.mgh.harvard.edu","title":"Jupyter"},{"location":"notes/Martinos/cluster/cluster/#building-your-own-sinularity-images","text":"Original Demo We can't build on the /cluster/qtim network storage so if we want to use recipe files, we need to build images on hard drives and for that we need QTIM servers. We can touch on this another time.","title":"Building Your Own Sinularity Images"},{"location":"notes/algorithms/python/alphabet_rangoli/","text":"Alphabet Rangoli An alphabet diamond design in a square You are given an integer, N. Your task is to print an alphabet rangoli of size N. (Rangoli is a form of Indian folk art based on creation of patterns.) Different sizes of alphabet rangoli are shown below: #size 3 -> 9 ----c---- --c-b-c-- c-b-a-b-c --c-b-c-- ----c---- #size 5 -> 17 --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- #size 10 -> 37 ------------------j------------------ ----------------j-i-j---------------- --------------j-i-h-i-j-------------- ------------j-i-h-g-h-i-j------------ ----------j-i-h-g-f-g-h-i-j---------- --------j-i-h-g-f-e-f-g-h-i-j-------- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- j-i-h-g-f-e-d-c-b-a-b-c-d-e-f-g-h-i-j --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ --------j-i-h-g-f-e-f-g-h-i-j-------- ----------j-i-h-g-f-g-h-i-j---------- ------------j-i-h-g-h-i-j------------ --------------j-i-h-i-j-------------- ----------------j-i-j---------------- ------------------j------------------ The center of the rangoli has the first alphabet letter a, and the boundary has the Nth alphabet letter (in alphabetical order). Input Format Only one line of input containing N, the size of the rangoli. Constraints 0< N< 27 Output Format Print the alphabet rangoli in the format explained above. Sample Input 5 Sample Output --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- The code: import string def print_rangoli(size): # your code goes here N = size width = 4*(N-1)+1 letters=string.ascii_letters[0:26] # triangle up for i in range(N-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) # center s = '{}'.format(letters[0]) for n in range(N-1): s = '{}-'.format(letters[n+1]) + s + '-{}'.format(letters[n+1]) print(s); del(s) # triangle down for i in range(N-2,-1,-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) if __name__ == '__main__': n = int(input()) print_rangoli(n)","title":"Alphabet Rangoli"},{"location":"notes/algorithms/python/alphabet_rangoli/#alphabet-rangoli","text":"An alphabet diamond design in a square You are given an integer, N. Your task is to print an alphabet rangoli of size N. (Rangoli is a form of Indian folk art based on creation of patterns.) Different sizes of alphabet rangoli are shown below: #size 3 -> 9 ----c---- --c-b-c-- c-b-a-b-c --c-b-c-- ----c---- #size 5 -> 17 --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- #size 10 -> 37 ------------------j------------------ ----------------j-i-j---------------- --------------j-i-h-i-j-------------- ------------j-i-h-g-h-i-j------------ ----------j-i-h-g-f-g-h-i-j---------- --------j-i-h-g-f-e-f-g-h-i-j-------- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- j-i-h-g-f-e-d-c-b-a-b-c-d-e-f-g-h-i-j --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ --------j-i-h-g-f-e-f-g-h-i-j-------- ----------j-i-h-g-f-g-h-i-j---------- ------------j-i-h-g-h-i-j------------ --------------j-i-h-i-j-------------- ----------------j-i-j---------------- ------------------j------------------ The center of the rangoli has the first alphabet letter a, and the boundary has the Nth alphabet letter (in alphabetical order). Input Format Only one line of input containing N, the size of the rangoli. Constraints 0< N< 27 Output Format Print the alphabet rangoli in the format explained above. Sample Input 5 Sample Output --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- The code: import string def print_rangoli(size): # your code goes here N = size width = 4*(N-1)+1 letters=string.ascii_letters[0:26] # triangle up for i in range(N-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) # center s = '{}'.format(letters[0]) for n in range(N-1): s = '{}-'.format(letters[n+1]) + s + '-{}'.format(letters[n+1]) print(s); del(s) # triangle down for i in range(N-2,-1,-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) if __name__ == '__main__': n = int(input()) print_rangoli(n)","title":"Alphabet Rangoli"},{"location":"notes/algorithms/python/averages/","text":"Averages Find the average of a student given a list of students You have a record of N students. Each record contains the student's name, and their percent marks in Maths, Physics and Chemistry. The marks can be floating values. The user enters some integer N followed by the names and marks for N students. You are required to save the record in a dictionary data type. The user then enters a student's name. Output the average percentage marks obtained by that student, correct to two decimal places. Input Format: The first line contains the integer N, the number of students. The next N lines contains the name and marks obtained by that student separated by a space. The final line contains the name of a particular student previously listed. Constraints: 2<=N<=10 0<=Marks<=100 Output Format: Print one line: The average of the marks obtained by the particular student correct to 2 decimal places. Sample Input 3 Krishna 67 68 69 Arjun 70 98 63 Malika 52 56 60 Malika Sample Output 56.00 Implementation: if __name__ == '__main__': # By accepting input as an int we can loop to grab all other inputs n = int(input()) # Initialize marks dict student_marks = {} for _ in range(n): # Nice trick to grab first value as name and the rest in a list called line name, *line = input().split() scores = list(map(float, line)) # Fill the student marks dict student_marks[name] = scores # Whose score do we want? query_name = input() selected_avg = round(sum(student_marks[query_name])/3,2) print(\"{0:.2f}\".format(selected_avg))","title":"Averages"},{"location":"notes/algorithms/python/averages/#averages","text":"Find the average of a student given a list of students You have a record of N students. Each record contains the student's name, and their percent marks in Maths, Physics and Chemistry. The marks can be floating values. The user enters some integer N followed by the names and marks for N students. You are required to save the record in a dictionary data type. The user then enters a student's name. Output the average percentage marks obtained by that student, correct to two decimal places. Input Format: The first line contains the integer N, the number of students. The next N lines contains the name and marks obtained by that student separated by a space. The final line contains the name of a particular student previously listed. Constraints: 2<=N<=10 0<=Marks<=100 Output Format: Print one line: The average of the marks obtained by the particular student correct to 2 decimal places. Sample Input 3 Krishna 67 68 69 Arjun 70 98 63 Malika 52 56 60 Malika Sample Output 56.00 Implementation: if __name__ == '__main__': # By accepting input as an int we can loop to grab all other inputs n = int(input()) # Initialize marks dict student_marks = {} for _ in range(n): # Nice trick to grab first value as name and the rest in a list called line name, *line = input().split() scores = list(map(float, line)) # Fill the student marks dict student_marks[name] = scores # Whose score do we want? query_name = input() selected_avg = round(sum(student_marks[query_name])/3,2) print(\"{0:.2f}\".format(selected_avg))","title":"Averages"},{"location":"notes/algorithms/python/binary_search/","text":"Binary Search See quick sort first as the array you are searching needs to be sorted. import os, pdb, math def b_s_iterative(arr, elem): start = 0 end = len(arr) - 1 mid = math.floor((start + end)/2) iter_count = 0 while (arr[mid] != elem and start <= end): iter_count += 1; print(iter_count) print(\"{}-{}-{}\".format(start, mid, end)) # pdb.set_trace() if elem < arr[mid]: end = mid - 1 else: start = mid + 1 mid = math.floor((start + end)/2) return mid if arr[mid] == elem else -1, iter_count Below is some practice evaluation code I was using. What I was doing was making Big O notation sample data to test how many iterations it takes to search different size data. First I do orders of 10 and then orders of 2. Technically Big O notation uses log with base 2, so the second set of data is more useful for plotting; though they both work. We should see O(log(n)) on the y-axis and n on the x-axis showing that the time complexity is log(n) iterations to search an array given n elements. big_O = [] # 10s for i in [1,10,100,1000,10000,100000,1000000,10000000]: # pdb.set_trace() if i == 0: big_O.append((0, 1)) else: array = range(i) index, count = b_s_iterative(array, 6) # pdb.set_trace() big_O.append((i, count)) big_O = [] # 2s for i in [2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072,262144,524288,1048576]: # pdb.set_trace() if i == 0: big_O.append((0, 1)) else: array = range(i) index, count = b_s_iterative(array, 6) # pdb.set_trace() big_O.append((i, count))","title":"Binary Search"},{"location":"notes/algorithms/python/binary_search/#binary-search","text":"See quick sort first as the array you are searching needs to be sorted. import os, pdb, math def b_s_iterative(arr, elem): start = 0 end = len(arr) - 1 mid = math.floor((start + end)/2) iter_count = 0 while (arr[mid] != elem and start <= end): iter_count += 1; print(iter_count) print(\"{}-{}-{}\".format(start, mid, end)) # pdb.set_trace() if elem < arr[mid]: end = mid - 1 else: start = mid + 1 mid = math.floor((start + end)/2) return mid if arr[mid] == elem else -1, iter_count Below is some practice evaluation code I was using. What I was doing was making Big O notation sample data to test how many iterations it takes to search different size data. First I do orders of 10 and then orders of 2. Technically Big O notation uses log with base 2, so the second set of data is more useful for plotting; though they both work. We should see O(log(n)) on the y-axis and n on the x-axis showing that the time complexity is log(n) iterations to search an array given n elements. big_O = [] # 10s for i in [1,10,100,1000,10000,100000,1000000,10000000]: # pdb.set_trace() if i == 0: big_O.append((0, 1)) else: array = range(i) index, count = b_s_iterative(array, 6) # pdb.set_trace() big_O.append((i, count)) big_O = [] # 2s for i in [2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072,262144,524288,1048576]: # pdb.set_trace() if i == 0: big_O.append((0, 1)) else: array = range(i) index, count = b_s_iterative(array, 6) # pdb.set_trace() big_O.append((i, count))","title":"Binary Search"},{"location":"notes/algorithms/python/breadth_first_search/","text":"Breadth First Search Solve a maze! In the breadth first search we start by imagining a grid. grid = [\"..........\", \"...#...##.\", \"..##...#..\", \".....###..\", \"......*...\"] Also define a key. wall, clear, goal = \"#\", \".\", \"*\" The constraints are that you need to provide a function that takes a grid and a starting point and finds the shortest path to the goal. Provide the starting point as a tuple (0,0) . Here is a basic example: import collections, pdb def bfs(grid, start): queue = collections.deque([[start]]) seen = set([start]) while queue: # pdb.set_trace() path = queue.popleft() x, y = path[-1] if grid[y][x] == goal: return path for x2, y2 in ((x+1,y), (x-1,y), (x,y+1), (x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and grid[y2][x2] != wall and (x2, y2) not in seen: queue.append(path + [(x2, y2)]) seen.add((x2, y2)) # pdb.set_trace(); # For when you get stuck I have another version without the collections package to see a base python implementation. lot = [[1,1,1,1,1,1,1], [1,1,0,1,1,1,1], [0,1,1,0,0,0,1], [0,0,1,1,1,1,9],] def bfs(grid, start): # queue=[start] width = len(lot[0]) height = len(lot) queue = [[start]] seen = set([start]) loop_limit = 3000 count=0 while queue and count < loop_limit: path = queue.pop(0) x, y = path[-1] if lot[y][x] == 9: return (path,len(path)) for x2, y2 in ((x+1,y),(x-1,y),(x,y+1),(x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and lot[y][x] != 0 and (x2,y2) not in seen: queue.append(path + [(x2,y2)]) seen.add((x2,y2)) count=count+1 #print(count) #print(path) return [\"we didn't get to finish\",path] shortest_path = bfs(lot, (0,0)) print(shortest_path) This prints out the following: ([(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (6, 1), (6, 2), (6, 3)], 10) A list that contains the path traversed and the the number of points in the traversed path. Finally there is a graph based approach: # Python3 Program to print BFS traversal # from a given source vertex. BFS(int s) # traverses vertices reachable from s. from collections import defaultdict import pdb # This class represents a directed graph # using adjacency list representation class Graph: # Constructor def __init__(self): # default dictionary to store graph self.graph = defaultdict(list) # function to add an edge to graph def addEdge(self,u,v): self.graph[u].append(v) # Function to print a BFS of graph def BFS(self, s): # Mark all the vertices as not visited visited = [False] * (len(self.graph)) # Create a queue for BFS queue = [] # Mark the source node as # visited and enqueue it queue.append(s) visited[s] = True while queue: pdb.set_trace() # Dequeue a vertex from # queue and print it s = queue.pop(0) print (s, end = \" \") # Get all adjacent vertices of the # dequeued vertex s. If a adjacent # has not been visited, then mark it # visited and enqueue it for i in self.graph[s]: if visited[i] == False: queue.append(i) visited[i] = True # Driver code # Create a graph given in # the above diagram g = Graph() g.addEdge(0, 1) g.addEdge(0, 2) g.addEdge(1, 2) g.addEdge(2, 0) g.addEdge(2, 3) g.addEdge(3, 3) print (\"Following is Breadth First Traversal\" \" (starting from vertex 2)\") g.BFS(2) # This code is contributed by Neelam Yadav","title":"Breadth First Search"},{"location":"notes/algorithms/python/breadth_first_search/#breadth-first-search","text":"Solve a maze! In the breadth first search we start by imagining a grid. grid = [\"..........\", \"...#...##.\", \"..##...#..\", \".....###..\", \"......*...\"] Also define a key. wall, clear, goal = \"#\", \".\", \"*\" The constraints are that you need to provide a function that takes a grid and a starting point and finds the shortest path to the goal. Provide the starting point as a tuple (0,0) . Here is a basic example: import collections, pdb def bfs(grid, start): queue = collections.deque([[start]]) seen = set([start]) while queue: # pdb.set_trace() path = queue.popleft() x, y = path[-1] if grid[y][x] == goal: return path for x2, y2 in ((x+1,y), (x-1,y), (x,y+1), (x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and grid[y2][x2] != wall and (x2, y2) not in seen: queue.append(path + [(x2, y2)]) seen.add((x2, y2)) # pdb.set_trace(); # For when you get stuck I have another version without the collections package to see a base python implementation. lot = [[1,1,1,1,1,1,1], [1,1,0,1,1,1,1], [0,1,1,0,0,0,1], [0,0,1,1,1,1,9],] def bfs(grid, start): # queue=[start] width = len(lot[0]) height = len(lot) queue = [[start]] seen = set([start]) loop_limit = 3000 count=0 while queue and count < loop_limit: path = queue.pop(0) x, y = path[-1] if lot[y][x] == 9: return (path,len(path)) for x2, y2 in ((x+1,y),(x-1,y),(x,y+1),(x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and lot[y][x] != 0 and (x2,y2) not in seen: queue.append(path + [(x2,y2)]) seen.add((x2,y2)) count=count+1 #print(count) #print(path) return [\"we didn't get to finish\",path] shortest_path = bfs(lot, (0,0)) print(shortest_path) This prints out the following: ([(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (6, 1), (6, 2), (6, 3)], 10) A list that contains the path traversed and the the number of points in the traversed path. Finally there is a graph based approach: # Python3 Program to print BFS traversal # from a given source vertex. BFS(int s) # traverses vertices reachable from s. from collections import defaultdict import pdb # This class represents a directed graph # using adjacency list representation class Graph: # Constructor def __init__(self): # default dictionary to store graph self.graph = defaultdict(list) # function to add an edge to graph def addEdge(self,u,v): self.graph[u].append(v) # Function to print a BFS of graph def BFS(self, s): # Mark all the vertices as not visited visited = [False] * (len(self.graph)) # Create a queue for BFS queue = [] # Mark the source node as # visited and enqueue it queue.append(s) visited[s] = True while queue: pdb.set_trace() # Dequeue a vertex from # queue and print it s = queue.pop(0) print (s, end = \" \") # Get all adjacent vertices of the # dequeued vertex s. If a adjacent # has not been visited, then mark it # visited and enqueue it for i in self.graph[s]: if visited[i] == False: queue.append(i) visited[i] = True # Driver code # Create a graph given in # the above diagram g = Graph() g.addEdge(0, 1) g.addEdge(0, 2) g.addEdge(1, 2) g.addEdge(2, 0) g.addEdge(2, 3) g.addEdge(3, 3) print (\"Following is Breadth First Traversal\" \" (starting from vertex 2)\") g.BFS(2) # This code is contributed by Neelam Yadav","title":"Breadth First Search"},{"location":"notes/algorithms/python/class_quiz/","text":"Class Quiz! Consider the following code, what will it output? class A(object): def go(self): print(\"go A go!\") def stop(self): print(\"stop A stop!\") def pause(self): raise Exception(\"Not Implemented\") class B(A): def go(self): super(B, self).go() print(\"go B go!\") class C(A): def go(self): super(C, self).go() print(\"go C go!\") def stop(self): super(C, self).stop() print(\"stop C stop!\") class D(B,C): def go(self): super(D, self).go() print(\"go D go!\") def stop(self): super(D, self).stop() print(\"stop D stop!\") def pause(self): print(\"wait D wait!\") class E(B,C): pass a = A() b = B() c = C() d = D() e = E() # specify output from here onwards a.go() # go A go! b.go() # go A go! # go B go! c.go() # go A go! # go C go! d.go() # go A go! # go C go! # go B go! # go D go! e.go() # go A go! # go C go! # go B go! a.stop() # stop A stop! b.stop() # stop A stop! c.stop() # stop A stop! # stop C stop! d.stop() # stop A stop! # stop C stop! # stop D stop! e.stop() # stop A stop! a.pause() # ... Exception: Not Implemented b.pause() # ... Exception: Not Implemented c.pause() # ... Exception: Not Implemented d.pause() # wait D wait! e.pause() # ...Exception: Not Implemented","title":"Class Quiz"},{"location":"notes/algorithms/python/class_quiz/#class-quiz","text":"Consider the following code, what will it output? class A(object): def go(self): print(\"go A go!\") def stop(self): print(\"stop A stop!\") def pause(self): raise Exception(\"Not Implemented\") class B(A): def go(self): super(B, self).go() print(\"go B go!\") class C(A): def go(self): super(C, self).go() print(\"go C go!\") def stop(self): super(C, self).stop() print(\"stop C stop!\") class D(B,C): def go(self): super(D, self).go() print(\"go D go!\") def stop(self): super(D, self).stop() print(\"stop D stop!\") def pause(self): print(\"wait D wait!\") class E(B,C): pass a = A() b = B() c = C() d = D() e = E() # specify output from here onwards a.go() # go A go! b.go() # go A go! # go B go! c.go() # go A go! # go C go! d.go() # go A go! # go C go! # go B go! # go D go! e.go() # go A go! # go C go! # go B go! a.stop() # stop A stop! b.stop() # stop A stop! c.stop() # stop A stop! # stop C stop! d.stop() # stop A stop! # stop C stop! # stop D stop! e.stop() # stop A stop! a.pause() # ... Exception: Not Implemented b.pause() # ... Exception: Not Implemented c.pause() # ... Exception: Not Implemented d.pause() # wait D wait! e.pause() # ...Exception: Not Implemented","title":"Class Quiz!"},{"location":"notes/algorithms/python/collections_counter/","text":"Collections Counter Your task is to compute how much money Raghu earned. Raghu is a shoe shop owner. His shop has X number of shoes.He has a list containing the size of each shoe he has in his shop. There are N number of customers who are willing to pay x_i amount of money only if they get the shoe of their desired size. Your task is to compute how much money Raghu earned. Input Format The first line contains X, the number of shoes. The second line contains the space separated list of all the shoe sizes in the shop. The third line contains N, the number of customers. The next N lines contain the space separated values of the shoe_size desired by the customer and x_i, the price of the shoe. Enter your code here. Read input from STDIN. Print output to STDOUT Sample Input: 10 --> Number of Shoes 2 3 4 5 6 8 7 6 5 18 --> all shoe sizes 6 -- > Number of Customers 6 55 6 45 6 55 4 40 18 60 10 50 S ample Output: 200 Explanation Customer 1: Purchased size 6 shoe for $55. Customer 2: Purchased size 6 shoe for $45. Customer 3: Size 6 no longer available, so no purchase. Customer 4: Purchased size 4 shoe for $40. Customer 5: Purchased size 18 shoe for $60. Customer 6: Size 10 not available, so no purchase. Total money earned = $200 from collections import Counter if __name__ == \"__main__\": num_of_shoes = int(input()) all_shoe_sizes = Counter(map(int, input().split())) num_customers = int(input()) total_earned = 0 for n in range(num_customers): size, price = map(int, input().split()) if all_shoe_sizes[size] != 0: total_earned = total_earned + price all_shoe_sizes[size] -= 1 print(total_earned)","title":"Collections Counter"},{"location":"notes/algorithms/python/collections_counter/#collections-counter","text":"Your task is to compute how much money Raghu earned. Raghu is a shoe shop owner. His shop has X number of shoes.He has a list containing the size of each shoe he has in his shop. There are N number of customers who are willing to pay x_i amount of money only if they get the shoe of their desired size. Your task is to compute how much money Raghu earned. Input Format The first line contains X, the number of shoes. The second line contains the space separated list of all the shoe sizes in the shop. The third line contains N, the number of customers. The next N lines contain the space separated values of the shoe_size desired by the customer and x_i, the price of the shoe. Enter your code here. Read input from STDIN. Print output to STDOUT Sample Input: 10 --> Number of Shoes 2 3 4 5 6 8 7 6 5 18 --> all shoe sizes 6 -- > Number of Customers 6 55 6 45 6 55 4 40 18 60 10 50 S ample Output: 200 Explanation Customer 1: Purchased size 6 shoe for $55. Customer 2: Purchased size 6 shoe for $45. Customer 3: Size 6 no longer available, so no purchase. Customer 4: Purchased size 4 shoe for $40. Customer 5: Purchased size 18 shoe for $60. Customer 6: Size 10 not available, so no purchase. Total money earned = $200 from collections import Counter if __name__ == \"__main__\": num_of_shoes = int(input()) all_shoe_sizes = Counter(map(int, input().split())) num_customers = int(input()) total_earned = 0 for n in range(num_customers): size, price = map(int, input().split()) if all_shoe_sizes[size] != 0: total_earned = total_earned + price all_shoe_sizes[size] -= 1 print(total_earned)","title":"Collections Counter"},{"location":"notes/algorithms/python/crossing_rects/","text":"Crossing Rects Find the total overlapping area without double counting. Problem: Imagine a grid with rectangles on it. Find the total overlapping area that does not repeat. In other words if a grid square is overlapped by multiple rectangles, it will only contribute 1 unit of area to the total overlapping area. Example 1: __________________________________> (x) |_| | | A (0,0), (1,1) |___| | B (0,0), (2,2) |_____| C (0,0), (3,3) | | \\/ (y) The answer is: 4 Example 2: Key: *(x,y), +(w, l) * - Top Left Corner + - Width and Length A (0,0), (9,3) _________________________________> (x) | __|____ B (6,1), (8,4) | | | _| |_____|__|__|_| C (13,2), (1,3) | | | | | |_____|_| \\/ (y) The answer is: 9 Here's the anser: def overlapping_area(rects=[ [(0,0),(1,1)],[(0,0),(2,2)] ]): \"\"\" This function takes a list of lists where each inner list is a rectangle. Each list will contain two tuples representing the top left corner of each rectangle and it's x-width and y-length. # Example: rect = [(x,y),(w,l)] \"\"\" def get_coordinates(rect=[(0,0), (0,0)]): \"\"\" Takes a list of tuples defining the top left corner coordinates of each square and the x-width, y-length and return all coordinates of rectangle. Below is the easy to see to setup the for loop, that way seeingwhat the comprehension is doing is much easier. This generates all x,y coordinates of a rect. coor = [] for i in range(rect[1][0]): for j in range(rect[1][1]): coor.append((rect[0][0]+i,rect[0][1]+j)) \"\"\" # List comprehension (list comprehensions are ordered backwards): coor = [(rect[0][0]+i,rect[0][1]+j) for j in range(rect[1][1]) for i in range(rect[1][0])] return coor # Get all rects coordinates rects_coors = [get_coordinates(r) for r in rects] area = 0 # Number of rectangles n = len(rects) seen = set() while n > 1: # Find overlap coors_in_common = set(rects_coors[n-1]) & set(rects_coors[n-2]) # Subtract out any previous overlap squares new_coors = coors_in_common - seen # Add fresh squares to area area += len(new_coors) # Mark seen area seen = set(rects_coors[n-1]) & set(rects_coors[n-2]) n -= 1 print(\"area of overlap is: {}\".format(area)) # Default overlapping_area() >>> area of overlap is: 1 # Trial 1 rects = [[(0,0), (1,1)], [(0,0), (2,2)], [(0,0), (3,3)]] overlapping_area(rects) >>> area of overlap is: 1 # Trial 2 rects = [[(0,0), (9,3)], [(6,1), (8,4)], [(13,2),(1,3)]] overlapping_area(rects) >>> area of overlap is: 9","title":"Crossing Rects"},{"location":"notes/algorithms/python/crossing_rects/#crossing-rects","text":"Find the total overlapping area without double counting. Problem: Imagine a grid with rectangles on it. Find the total overlapping area that does not repeat. In other words if a grid square is overlapped by multiple rectangles, it will only contribute 1 unit of area to the total overlapping area. Example 1: __________________________________> (x) |_| | | A (0,0), (1,1) |___| | B (0,0), (2,2) |_____| C (0,0), (3,3) | | \\/ (y) The answer is: 4 Example 2: Key: *(x,y), +(w, l) * - Top Left Corner + - Width and Length A (0,0), (9,3) _________________________________> (x) | __|____ B (6,1), (8,4) | | | _| |_____|__|__|_| C (13,2), (1,3) | | | | | |_____|_| \\/ (y) The answer is: 9 Here's the anser: def overlapping_area(rects=[ [(0,0),(1,1)],[(0,0),(2,2)] ]): \"\"\" This function takes a list of lists where each inner list is a rectangle. Each list will contain two tuples representing the top left corner of each rectangle and it's x-width and y-length. # Example: rect = [(x,y),(w,l)] \"\"\" def get_coordinates(rect=[(0,0), (0,0)]): \"\"\" Takes a list of tuples defining the top left corner coordinates of each square and the x-width, y-length and return all coordinates of rectangle. Below is the easy to see to setup the for loop, that way seeingwhat the comprehension is doing is much easier. This generates all x,y coordinates of a rect. coor = [] for i in range(rect[1][0]): for j in range(rect[1][1]): coor.append((rect[0][0]+i,rect[0][1]+j)) \"\"\" # List comprehension (list comprehensions are ordered backwards): coor = [(rect[0][0]+i,rect[0][1]+j) for j in range(rect[1][1]) for i in range(rect[1][0])] return coor # Get all rects coordinates rects_coors = [get_coordinates(r) for r in rects] area = 0 # Number of rectangles n = len(rects) seen = set() while n > 1: # Find overlap coors_in_common = set(rects_coors[n-1]) & set(rects_coors[n-2]) # Subtract out any previous overlap squares new_coors = coors_in_common - seen # Add fresh squares to area area += len(new_coors) # Mark seen area seen = set(rects_coors[n-1]) & set(rects_coors[n-2]) n -= 1 print(\"area of overlap is: {}\".format(area)) # Default overlapping_area() >>> area of overlap is: 1 # Trial 1 rects = [[(0,0), (1,1)], [(0,0), (2,2)], [(0,0), (3,3)]] overlapping_area(rects) >>> area of overlap is: 1 # Trial 2 rects = [[(0,0), (9,3)], [(6,1), (8,4)], [(13,2),(1,3)]] overlapping_area(rects) >>> area of overlap is: 9","title":"Crossing Rects"},{"location":"notes/algorithms/python/csv_loader/","text":"CSV Loader Packages needed: os pandas sqlite3 numpy This sets of scripts works as 2 independent situations: [1] Bulk folder load The function in this script looks inside a folder and using all *.csv begins a process of concatenating all csvs together and removing duplpicates on each merge. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' # Death example for function (Could require lots of Ram and DB grows in size...poorly coded to scale, but good for initializing a DB.) def create_table_from_folder_csvs(folder=None, demographic_table=False): # Could freeze 8 GB RAM machines as this lazily reads all csvs into RAM # HOWEVER IF YOU HAVE A MONSTER MACHINE USE IT...IT'S ABOUT 50+ TIMES FASTER TO USE THIS FUNCTION IF YOU CAN # Recommended: Use only a coulple files to define scheme and then use load csvs to load 1 at a time. conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) if not isinstance(folder, str): return(\"Not a valid folder\") files = [i for i in os.listdir(folder) if i.find('.csv') != -1] print(files) # base frame for all but demographics data data = pd.DataFrame() # base frame for demographics data (2 table types) demographics = pd.DataFrame() socialhistory = pd.DataFrame() for file in files: csv = pd.read_csv(os.path.join(folder, file)) # demographic tables have 2 schemes if demographic_table == True: if file.find('Demographics') != -1: demographics = pd.concat([demographics, csv], axis=0) print(demographics.shape) demographics = demographics[~demographics.duplicated(keep='first')] elif file.find('SocialHistory') != -1: socialhistory = pd.concat([socialhistory, csv], axis=0) print(socialhistory.shape) socialhistory = socialhistory[~socialhistory.duplicated(keep='first')] else: data = pd.concat([data, csv], axis=0) print(data.shape) data = data[~data.duplicated(keep='first')] frames = {'data':data, 'demographics':demographics, 'socialhistory':socialhistory} for key in frames.keys(): frame = frames[key] if frame.shape[0] != 0: # pdb.set_trace() if key == \"demographics\": frame.to_sql(f\"{folder}_demographics\", conn, if_exists=\"replace\", index=False) elif key == \"socialhistory\": frame.to_sql(f\"{folder}_socialhistory\", conn, if_exists=\"replace\", index=False) else: frame.to_sql(f\"{folder}\", conn, if_exists=\"replace\", index=False) create_table_from_folder_csvs(death_folder) create_table_from_folder_csvs(intubation_folder) create_table_from_folder_csvs(demographics_folder, demographic_table=True) create_table_from_folder_csvs(covid_test_results_folder) create_table_from_folder_csvs(chest_imaging_reports_folder) [2] Load a new csv This script could load every csv but it much much slower. Use this once a bulk database is made andd you want to add a new csv. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' def load_csv(folder=None, file=None, demographic_table=False): # Proper input check if not isinstance(folder, str): return(\"Not a valid folder\\\\table name\") if not isinstance(file, str): return(\"Not a valid file\") # DB Connection conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) c = conn.cursor() # Utility def check_if_table_exists_in_db(table): c.execute(f''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='{table}' ''') if c.fetchone()[0]==1 : print('Table exists.') return(True) else: print('Table does not exist.') return(False) ### table name table = folder if demographic_table: if file.find('Demographics') != -1: table = folder+'_demographic' elif file.find('SocialHistory') != -1: table = folder+'_socialhistory' # Load Data csv = pd.read_csv(os.path.join(folder, file)) # Create Table if it doesn't exist if not check_if_table_exists_in_db(table): print(\"creating table\") schema = '' for col in csv.columns: type_key = {'object':'TEXT', 'int64':'INTEGER', 'float64':'FLOAT'} schema += f\"'{col}' {type_key[str(csv.dtypes[col])]}, \" schema = schema[0:-2] c.execute(f\"\"\" create table '{table}' ({schema})\"\"\") def check_row_against_table(df_row, table=table ,conn=conn): # Here I had to pass in conn as it wasn't able to be seen otherwise. # This is because this is apart of the \"apply\" function in pandas (probably some sort of scoping issue) # See \"csv.apply(check_row_against_table, axis=1)\" below this function definition print(df_row) # Optional and makes things slower, but it's a good visual where_filter = '' insert_statements = '' for col in df_row.keys(): if pd.isnull(df_row[col]) == True: # Build element for queries escaping single quotes in text string for sqlite where_filter += f\"{col} is null and \" insert_statements += f\"null, \" else: value = str(df_row[col]).replace('\\'','\\'\\'') # escape quotes for use in sqlite where_filter += f\"{col} = '{value}' and \" insert_statements += f\"'{value}', \" # pdb.set_trace() # Trim commas where_filter = where_filter[0:-5] # trimming \" and \" insert_statements = insert_statements[0:-2] # trimming \", \" # Execute search for current record query = f'''select * from {table} where {where_filter};''' c.execute(query) # c.execute(\"select * from data_Death where MRN = 'not a chance'\") # Use this for debugging try: # Record is a duplicate if it doesn't throw exception original_record = c.fetchone()[0] except: # Record is brand new # pdb.set_trace() c.execute(f\"insert into {table} values ({insert_statements});\") # commit the changes to db conn.commit() # Loop over every row in csv and call above function csv.apply(check_row_against_table, axis=1) #close the connection conn.close() # #xample executions: # data_Death load_csv(folder=death_folder, file='Mortality_20200414110000.csv') load_csv(folder=death_folder, file='Mortality_20200423110000.csv') load_csv(folder=death_folder, file='Mortality_20200417110000.csv') load_csv(folder=death_folder, file='Mortality_20200422110000.csv') load_csv(folder=death_folder, file='Mortality_20200420090000.csv') load_csv(folder=death_folder, file='Mortality_20200421110000.csv') load_csv(folder=death_folder, file='Mortality_20200424080000.csv') load_csv(folder=death_folder, file='Mortality_20200418090000.csv') load_csv(folder=death_folder, file='Mortality_20200419090000.csv') load_csv(folder=death_folder, file='Mortality_20200416090000.csv') load_csv(folder=death_folder, file='Mortality_20200415090000.csv') # data_Intubation load_csv(folder=intubation_folder, file='Intubation_20200421090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200418180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200420180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200416100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200423110000.csv') load_csv(folder=intubation_folder, file='Intubation_20200422090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200415100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200424080000.csv') load_csv(folder=intubation_folder, file='Intubation_20200414160000.csv') load_csv(folder=intubation_folder, file='Intubation_20200417100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200419180000.csv') # data_COVID_TestResults load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200421090000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200423110000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200415180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200418180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200416180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200422090000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200417180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200420180000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200424080000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200419180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200414160000.csv') # data_chestImagingReports load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200421090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200417100000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200414110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200422090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200419180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200424080000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200423110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200416090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200418160000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200420180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200415110000.csv') # data_demographics load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200422090000.csv', demographic_table=True)","title":"CSV Loader"},{"location":"notes/algorithms/python/csv_loader/#csv-loader","text":"Packages needed: os pandas sqlite3 numpy This sets of scripts works as 2 independent situations:","title":"CSV Loader"},{"location":"notes/algorithms/python/csv_loader/#1-bulk-folder-load","text":"The function in this script looks inside a folder and using all *.csv begins a process of concatenating all csvs together and removing duplpicates on each merge. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' # Death example for function (Could require lots of Ram and DB grows in size...poorly coded to scale, but good for initializing a DB.) def create_table_from_folder_csvs(folder=None, demographic_table=False): # Could freeze 8 GB RAM machines as this lazily reads all csvs into RAM # HOWEVER IF YOU HAVE A MONSTER MACHINE USE IT...IT'S ABOUT 50+ TIMES FASTER TO USE THIS FUNCTION IF YOU CAN # Recommended: Use only a coulple files to define scheme and then use load csvs to load 1 at a time. conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) if not isinstance(folder, str): return(\"Not a valid folder\") files = [i for i in os.listdir(folder) if i.find('.csv') != -1] print(files) # base frame for all but demographics data data = pd.DataFrame() # base frame for demographics data (2 table types) demographics = pd.DataFrame() socialhistory = pd.DataFrame() for file in files: csv = pd.read_csv(os.path.join(folder, file)) # demographic tables have 2 schemes if demographic_table == True: if file.find('Demographics') != -1: demographics = pd.concat([demographics, csv], axis=0) print(demographics.shape) demographics = demographics[~demographics.duplicated(keep='first')] elif file.find('SocialHistory') != -1: socialhistory = pd.concat([socialhistory, csv], axis=0) print(socialhistory.shape) socialhistory = socialhistory[~socialhistory.duplicated(keep='first')] else: data = pd.concat([data, csv], axis=0) print(data.shape) data = data[~data.duplicated(keep='first')] frames = {'data':data, 'demographics':demographics, 'socialhistory':socialhistory} for key in frames.keys(): frame = frames[key] if frame.shape[0] != 0: # pdb.set_trace() if key == \"demographics\": frame.to_sql(f\"{folder}_demographics\", conn, if_exists=\"replace\", index=False) elif key == \"socialhistory\": frame.to_sql(f\"{folder}_socialhistory\", conn, if_exists=\"replace\", index=False) else: frame.to_sql(f\"{folder}\", conn, if_exists=\"replace\", index=False) create_table_from_folder_csvs(death_folder) create_table_from_folder_csvs(intubation_folder) create_table_from_folder_csvs(demographics_folder, demographic_table=True) create_table_from_folder_csvs(covid_test_results_folder) create_table_from_folder_csvs(chest_imaging_reports_folder)","title":"[1] Bulk folder load"},{"location":"notes/algorithms/python/csv_loader/#2-load-a-new-csv","text":"This script could load every csv but it much much slower. Use this once a bulk database is made andd you want to add a new csv. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' def load_csv(folder=None, file=None, demographic_table=False): # Proper input check if not isinstance(folder, str): return(\"Not a valid folder\\\\table name\") if not isinstance(file, str): return(\"Not a valid file\") # DB Connection conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) c = conn.cursor() # Utility def check_if_table_exists_in_db(table): c.execute(f''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='{table}' ''') if c.fetchone()[0]==1 : print('Table exists.') return(True) else: print('Table does not exist.') return(False) ### table name table = folder if demographic_table: if file.find('Demographics') != -1: table = folder+'_demographic' elif file.find('SocialHistory') != -1: table = folder+'_socialhistory' # Load Data csv = pd.read_csv(os.path.join(folder, file)) # Create Table if it doesn't exist if not check_if_table_exists_in_db(table): print(\"creating table\") schema = '' for col in csv.columns: type_key = {'object':'TEXT', 'int64':'INTEGER', 'float64':'FLOAT'} schema += f\"'{col}' {type_key[str(csv.dtypes[col])]}, \" schema = schema[0:-2] c.execute(f\"\"\" create table '{table}' ({schema})\"\"\") def check_row_against_table(df_row, table=table ,conn=conn): # Here I had to pass in conn as it wasn't able to be seen otherwise. # This is because this is apart of the \"apply\" function in pandas (probably some sort of scoping issue) # See \"csv.apply(check_row_against_table, axis=1)\" below this function definition print(df_row) # Optional and makes things slower, but it's a good visual where_filter = '' insert_statements = '' for col in df_row.keys(): if pd.isnull(df_row[col]) == True: # Build element for queries escaping single quotes in text string for sqlite where_filter += f\"{col} is null and \" insert_statements += f\"null, \" else: value = str(df_row[col]).replace('\\'','\\'\\'') # escape quotes for use in sqlite where_filter += f\"{col} = '{value}' and \" insert_statements += f\"'{value}', \" # pdb.set_trace() # Trim commas where_filter = where_filter[0:-5] # trimming \" and \" insert_statements = insert_statements[0:-2] # trimming \", \" # Execute search for current record query = f'''select * from {table} where {where_filter};''' c.execute(query) # c.execute(\"select * from data_Death where MRN = 'not a chance'\") # Use this for debugging try: # Record is a duplicate if it doesn't throw exception original_record = c.fetchone()[0] except: # Record is brand new # pdb.set_trace() c.execute(f\"insert into {table} values ({insert_statements});\") # commit the changes to db conn.commit() # Loop over every row in csv and call above function csv.apply(check_row_against_table, axis=1) #close the connection conn.close() # #xample executions: # data_Death load_csv(folder=death_folder, file='Mortality_20200414110000.csv') load_csv(folder=death_folder, file='Mortality_20200423110000.csv') load_csv(folder=death_folder, file='Mortality_20200417110000.csv') load_csv(folder=death_folder, file='Mortality_20200422110000.csv') load_csv(folder=death_folder, file='Mortality_20200420090000.csv') load_csv(folder=death_folder, file='Mortality_20200421110000.csv') load_csv(folder=death_folder, file='Mortality_20200424080000.csv') load_csv(folder=death_folder, file='Mortality_20200418090000.csv') load_csv(folder=death_folder, file='Mortality_20200419090000.csv') load_csv(folder=death_folder, file='Mortality_20200416090000.csv') load_csv(folder=death_folder, file='Mortality_20200415090000.csv') # data_Intubation load_csv(folder=intubation_folder, file='Intubation_20200421090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200418180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200420180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200416100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200423110000.csv') load_csv(folder=intubation_folder, file='Intubation_20200422090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200415100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200424080000.csv') load_csv(folder=intubation_folder, file='Intubation_20200414160000.csv') load_csv(folder=intubation_folder, file='Intubation_20200417100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200419180000.csv') # data_COVID_TestResults load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200421090000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200423110000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200415180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200418180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200416180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200422090000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200417180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200420180000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200424080000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200419180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200414160000.csv') # data_chestImagingReports load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200421090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200417100000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200414110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200422090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200419180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200424080000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200423110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200416090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200418160000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200420180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200415110000.csv') # data_demographics load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200422090000.csv', demographic_table=True)","title":"[2] Load a new csv"},{"location":"notes/algorithms/python/default_dict/","text":"Default Dict Print the indices of each occurrence of m in group A. In this challenge, you will be given integers, n and m. There are m words, which might repeat, in word group A. There are m words belonging to word group B. For each m words, check whether the word has appeared in group A or not. Print the indices of each occurrence of m in group A.If it does not appear, print -1. Sample Input: 5 2 a a b a b a b Sample Output: 1 2 4 3 5 Code: from collections import defaultdict if __name__ == '__main__': n, m = map(int,input().split()) A = []; A = [input() for i in range(n)] B = []; B = [input() for i in range(m)] B_dict = defaultdict(list) for wordB in B: for A_indice in range(len(A)): if wordB == A[A_indice]: B_dict[wordB].append(A_indice+1) if B_dict[wordB] == []: B_dict[wordB].append(-1) for word in B_dict: print(\" \".join([str(i) for i in B_dict[word]]))","title":"Default Dict"},{"location":"notes/algorithms/python/default_dict/#default-dict","text":"Print the indices of each occurrence of m in group A. In this challenge, you will be given integers, n and m. There are m words, which might repeat, in word group A. There are m words belonging to word group B. For each m words, check whether the word has appeared in group A or not. Print the indices of each occurrence of m in group A.If it does not appear, print -1. Sample Input: 5 2 a a b a b a b Sample Output: 1 2 4 3 5 Code: from collections import defaultdict if __name__ == '__main__': n, m = map(int,input().split()) A = []; A = [input() for i in range(n)] B = []; B = [input() for i in range(m)] B_dict = defaultdict(list) for wordB in B: for A_indice in range(len(A)): if wordB == A[A_indice]: B_dict[wordB].append(A_indice+1) if B_dict[wordB] == []: B_dict[wordB].append(-1) for word in B_dict: print(\" \".join([str(i) for i in B_dict[word]]))","title":"Default Dict"},{"location":"notes/algorithms/python/directory_compare/","text":"Directory and File Compare Source stackoverflow List out files appended to their paths: import os def listfiles(path): files = [] for dirName, subdirList, fileList in os.walk(path): dir = dirName.replace(path, '') for fname in fileList: files.append(os.path.join(dir, fname)) return files x = listfiles('D:\\\\xfiles') y = listfiles('D:\\\\yfiles') You can use list comprehensions to compare, but it's slow compared to sets : q = [filename for filename in x if filename not in y] Use sets in python to make compares: files_only_in_x = set(x) - set(y) files_only_in_y = set(y) - set(x) files_only_in_either = set(x) ^ set(y) files_in_both = set(x) & set(y) all_files = set(x) | set(y)","title":"Folder\\File Compare"},{"location":"notes/algorithms/python/directory_compare/#directory-and-file-compare","text":"Source stackoverflow","title":"Directory and File Compare"},{"location":"notes/algorithms/python/directory_compare/#list-out-files-appended-to-their-paths","text":"import os def listfiles(path): files = [] for dirName, subdirList, fileList in os.walk(path): dir = dirName.replace(path, '') for fname in fileList: files.append(os.path.join(dir, fname)) return files x = listfiles('D:\\\\xfiles') y = listfiles('D:\\\\yfiles')","title":"List out files appended to their paths:"},{"location":"notes/algorithms/python/directory_compare/#you-can-use-list-comprehensions-to-compare-but-its-slow-compared-to-sets","text":"q = [filename for filename in x if filename not in y]","title":"You can use list comprehensions to compare, but it's slow compared to sets:"},{"location":"notes/algorithms/python/directory_compare/#use-sets-in-python-to-make-compares","text":"files_only_in_x = set(x) - set(y) files_only_in_y = set(y) - set(x) files_only_in_either = set(x) ^ set(y) files_in_both = set(x) & set(y) all_files = set(x) | set(y)","title":"Use sets in python to make compares:"},{"location":"notes/algorithms/python/door_mat/","text":"Door Mat Make a doormat sized NxM, and with 'Welcome' in the middle. Mr. Vincent works in a door mat manufacturing company. One day, he designed a new door mat with the following specifications: Mat size must be NxM. (N is an odd natural number, and M is 3 times N.) The design should have 'WELCOME' written in the center. The design pattern should only use these characters: | . - Sample Designs Size: 7 x 21 ---------.|.--------- ------.|..|..|.------ ---.|..|..|..|..|.--- -------WELCOME------- ---.|..|..|..|..|.--- ------.|..|..|.------ ---------.|.--------- Size: 11 x 33 ---------------.|.--------------- ------------.|..|..|.------------ ---------.|..|..|..|..|.--------- ------.|..|..|..|..|..|..|.------ ---.|..|..|..|..|..|..|..|..|.--- -------------WELCOME------------- ---.|..|..|..|..|..|..|..|..|.--- ------.|..|..|..|..|..|..|.------ ---------.|..|..|..|..|.--------- ------------.|..|..|.------------ ---------------.|.--------------- Input Format A single line containing the space separated values of N and M. Constraints: 5<N<101 15<M<303 Example Sample Input: 9 27 Sample Output: ------------.|.------------ ---------.|..|..|.--------- ------.|..|..|..|..|.------ ---.|..|..|..|..|..|..|.--- ----------WELCOME---------- ---.|..|..|..|..|..|..|.--- ------.|..|..|..|..|.------ ---------.|..|..|.--------- ------------.|.------------ if __name__ == '__main__': N, M = map(int,input().split()) for i in range(int((N-1)/2)): print(((2*i+1)*\".|.\").center(M,'-')) print('WELCOME'.center(M,'-')) for i in range(int((N-1)/2)): print((((N-2)-2*i)*\".|.\").center(M,'-'))","title":"Door Mat"},{"location":"notes/algorithms/python/door_mat/#door-mat","text":"Make a doormat sized NxM, and with 'Welcome' in the middle. Mr. Vincent works in a door mat manufacturing company. One day, he designed a new door mat with the following specifications: Mat size must be NxM. (N is an odd natural number, and M is 3 times N.) The design should have 'WELCOME' written in the center. The design pattern should only use these characters: | . - Sample Designs Size: 7 x 21 ---------.|.--------- ------.|..|..|.------ ---.|..|..|..|..|.--- -------WELCOME------- ---.|..|..|..|..|.--- ------.|..|..|.------ ---------.|.--------- Size: 11 x 33 ---------------.|.--------------- ------------.|..|..|.------------ ---------.|..|..|..|..|.--------- ------.|..|..|..|..|..|..|.------ ---.|..|..|..|..|..|..|..|..|.--- -------------WELCOME------------- ---.|..|..|..|..|..|..|..|..|.--- ------.|..|..|..|..|..|..|.------ ---------.|..|..|..|..|.--------- ------------.|..|..|.------------ ---------------.|.--------------- Input Format A single line containing the space separated values of N and M. Constraints: 5<N<101 15<M<303 Example Sample Input: 9 27 Sample Output: ------------.|.------------ ---------.|..|..|.--------- ------.|..|..|..|..|.------ ---.|..|..|..|..|..|..|.--- ----------WELCOME---------- ---.|..|..|..|..|..|..|.--- ------.|..|..|..|..|.------ ---------.|..|..|.--------- ------------.|.------------ if __name__ == '__main__': N, M = map(int,input().split()) for i in range(int((N-1)/2)): print(((2*i+1)*\".|.\").center(M,'-')) print('WELCOME'.center(M,'-')) for i in range(int((N-1)/2)): print((((N-2)-2*i)*\".|.\").center(M,'-'))","title":"Door Mat"},{"location":"notes/algorithms/python/find_a_string/","text":"Find a string Find all occurrences of a substring in a string In this challenge, the user enters a string and a substring. You have to print the number of times that the substring occurs in the given string. String traversal will take place from left to right, not from right to left. NOTE: String letters are case-sensitive. Input Format: The first line of input contains the original string. The next line contains the substring. Constraints: 1 <= len(string) <= 200 Each character in the string is an ascii character. Output Format: Output the integer number indicating the total number of occurrences of the substring in the original string. Sample Input: ABCDCDC CDC Sample Output: 2 Concept def count_substring(string, sub_string): index = 0 counts = 0 while index < len(string): index = string.find(sub_string, index) if index == -1: break counts += 1 index += 1 return counts if __name__ == '__main__': string = input().strip() sub_string = input().strip() count = count_substring(string, sub_string) print(count)","title":"Find a String"},{"location":"notes/algorithms/python/find_a_string/#find-a-string","text":"Find all occurrences of a substring in a string In this challenge, the user enters a string and a substring. You have to print the number of times that the substring occurs in the given string. String traversal will take place from left to right, not from right to left. NOTE: String letters are case-sensitive. Input Format: The first line of input contains the original string. The next line contains the substring. Constraints: 1 <= len(string) <= 200 Each character in the string is an ascii character. Output Format: Output the integer number indicating the total number of occurrences of the substring in the original string. Sample Input: ABCDCDC CDC Sample Output: 2 Concept def count_substring(string, sub_string): index = 0 counts = 0 while index < len(string): index = string.find(sub_string, index) if index == -1: break counts += 1 index += 1 return counts if __name__ == '__main__': string = input().strip() sub_string = input().strip() count = count_substring(string, sub_string) print(count)","title":"Find a string"},{"location":"notes/algorithms/python/insertion_sort/","text":"Insertion Sort Source: geeksforgeeks Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands. The array is virtually split into a sorted and an unsorted part. Values from the unsorted part are picked and placed at the correct position in the sorted part. Algorithm To sort an array of size n in ascending order: 1: Iterate from arr[1] to arr[n] over the array. 2: Compare the current element (key) to its predecessor. 3: If the key element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. # Python program for implementation of Insertion Sort # Function to do insertion sort def insertionSort(arr): # Traverse through 1 to len(arr) for i in range(1, len(arr)): # pdb.set_trace() key = arr[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >= 0 and key < arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key print(arr) # Driver code to test above arr = [12, 11, 13, 5, 6] insertionSort(arr) for i in range(len(arr)): print (\"% d\" % arr[i]) # This code is contributed by Mohit Kumra","title":"Insertion Sort"},{"location":"notes/algorithms/python/insertion_sort/#insertion-sort","text":"Source: geeksforgeeks Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands. The array is virtually split into a sorted and an unsorted part. Values from the unsorted part are picked and placed at the correct position in the sorted part. Algorithm To sort an array of size n in ascending order: 1: Iterate from arr[1] to arr[n] over the array. 2: Compare the current element (key) to its predecessor. 3: If the key element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. # Python program for implementation of Insertion Sort # Function to do insertion sort def insertionSort(arr): # Traverse through 1 to len(arr) for i in range(1, len(arr)): # pdb.set_trace() key = arr[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >= 0 and key < arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key print(arr) # Driver code to test above arr = [12, 11, 13, 5, 6] insertionSort(arr) for i in range(len(arr)): print (\"% d\" % arr[i]) # This code is contributed by Mohit Kumra","title":"Insertion Sort"},{"location":"notes/algorithms/python/merge_sort/","text":"Merge Sort Source: geeksforgeeks Related algorithm Quick Sort Difference Between Quicksort and Merge Sort Definition Quicksort is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. In contrast, merge sort is an efficient, general purpose, comparison-based sorting algorithms. Thus, this is the fundamental difference between quicksort and merge sort. Functionality Aboveall, the functionality is the main difference between quicksort and merge sort. Quicksort sorts the elements by comparing each element with the pivot while merge sort divides the array into two subarrays (n/2) again and again until one element is left. Application Also, while quicksort is suitable for small arrays, merge sort works for any type of array. Speed Another difference between quicksort and merge sort is that the quicksort works faster for small data sets while merge sort works in consistent speed for all datasets. Space Requirement Moreover, the space requirement is also an important difference between quicksort and merge sort. Quicksort requires minimum space compared to merge sort. Efficiency Furthermore, quicksort is not efficient for large arrays, but merge sort is more efficient than quicksort. Hence, this is another difference between quicksort and merge sort. Conclusion In summary, the main difference between quicksort and merge sort is that the quicksort sorts the elements by comparing each element with an element called a pivot while the merge sort divides the array into two subarrays again and again until one element is left. Recursive Merge Sort: def merge(left, right): print(f\"left: {left}; right: {right}\") if not len(left) or not len(right): return left or right pdb.set_trace() result = [] i, j = 0, 0 while (len(result) < len(left) + len(right)): if left[i] < right[j]: result.append(left[i]) print(f\"result l < r: {result}\") i+= 1 else: result.append(right[j]) print(f\"result r > l: {result}\") j+= 1 if i == len(left) or j == len(right): result.extend(left[i:] or right[j:]) break print(f\"result: {result}\") return result def mergesort(list): if len(list) < 2: print(f\"returning list: {list}\") return list middle = int(len(list)/2) pdb.set_trace() print(f\"middle: {int(len(list)/2)}\") print(f\"left: {list[:middle]}\") print(f\"right: {list[middle:]}\") left = mergesort(list[:middle]) right = mergesort(list[middle:]) return merge(left, right) seq = [12, 11, 13, 5, 6, 7] print(\"Given array is\") print(seq); print(\"\\n\") print(\"Sorted array is\") print(mergesort(seq)) # Code Contributed by Mohit Gupta_OMG Iterative Merge Sort: # Iterative Merge sort (Bottom Up) # Iterative mergesort function to # sort arr[0...n-1] # perform bottom up merge def mergeSort(a): # start with least partition size of 2^0 = 1 width = 1 n = len(a) # subarray size grows by powers of 2 # since growth of loop condition is exponential, # time consumed is logarithmic (log2n) while (width < n): # always start from leftmost l=0; while (l < n): r = min(l+(width*2-1), n-1) m = min(l+width-1,n-1) # final merge should consider # unmerged sublist if input arr # size is not power of 2 print(f\"merge({a}, {l}, {m}, {r})\") merge(a, l, m, r) l += width*2 # Increasing sub array size by powers of 2 width *= 2 return a # Merge Function def merge(a, l, m, r): n1 = m - l + 1 n2 = r - m L = [0] * n1 R = [0] * n2 for i in range(0, n1): L[i] = a[l + i] for i in range(0, n2): R[i] = a[m + i + 1] print(f\"n1: {n1} n2: {n2} L: {L} R: {R}\") i, j, k = 0, 0, l while i < n1 and j < n2: if L[i] <= R[j]: a[k] = L[i] i += 1 else: a[k] = R[j] j += 1 k += 1 while i < n1: a[k] = L[i] i += 1 k += 1 while j < n2: a[k] = R[j] j += 1 k += 1 # Driver code a = [-74,48,-20,2,10,-84,-5,-9,11,-24,-91,2,-71,64,63,80,28,-30,-58,-11,-44,-87,-22,54,-74,-10,-55,-28,-46,29,10,50,-72,34,26,25,8,51,13,30,35,-8,50,65,-6,16,-2,21,-78,35,-13,14,23,-3,26,-90,86,25,-56,91,-13,92,-25,37,57,-20,-69,98,95,45,47,29,86,-28,73,-44,-46,65,-84,-96,-24,-12,72,-68,93,57,92,52,-45,-2,85,-63,56,55,12,-85,77,-39] a = [-1,5,12,11,13,5,6,-2,0,7] print(\"Given array is \") print(a) mergeSort(a) print(\"Sorted array is \") print(a) # Contributed by Madhur Chhangani [RCOEM] # corrected and improved by @mahee96 # perform bottom up merge def mergeSort(a): # start with least partition size of 2^0 = 1 width = 1 n = len(a) # subarray size grows by powers of 2 # since growth of loop condition is exponential, # time consumed is logarithmic (log2n) while (width < n): # always start from leftmost l=0; while (l < n): r = min(l+(width*2-1), n-1) m = min(l+width-1,n-1) # final merge should consider # unmerged sublist if input arr # size is not power of 2 merge(a, l, m, r) l += width*2 # Increasing sub array size by powers of 2 width *= 2 return a # Merge Function def merge(a, l, m, r): print(f\"{a}, {l}, {m}, {r})\") n1 = m - l + 1 n2 = r - m L = [0] * n1 R = [0] * n2 for i in range(0, n1): L[i] = a[l + i] for i in range(0, n2): R[i] = a[m + i + 1] i, j, k = 0, 0, l while i < n1 and j < n2: if L[i] <= R[j]: a[k] = L[i] i += 1 else: a[k] = R[j] j += 1 k += 1 while i < n1: a[k] = L[i] i += 1 k += 1 while j < n2: a[k] = R[j] j += 1 k += 1 # Driver code a = [-74,48,-20,2,10,-84,-5,-9,11,-24,-91,2,-71,64,63,80,28,-30,-58,-11,-44,-87,-22,54,-74,-10,-55,-28,-46,29,10,50,-72,34,26,25,8,51,13,30,35,-8,50,65,-6,16,-2,21,-78,35,-13,14,23,-3,26,-90,86,25,-56,91,-13,92,-25,37,57,-20,-69,98,95,45,47,29,86,-28,73,-44,-46,65,-84,-96,-24,-12,72,-68,93,57,92,52,-45,-2,85,-63,56,55,12,-85,77,-39] print(\"Given array is \") print(a) mergeSort(a) print(\"Sorted array is \") print(a)","title":"Merge Sort"},{"location":"notes/algorithms/python/merge_sort/#merge-sort","text":"Source: geeksforgeeks Related algorithm Quick Sort","title":"Merge Sort"},{"location":"notes/algorithms/python/merge_sort/#difference-between-quicksort-and-merge-sort","text":"","title":"Difference Between Quicksort and Merge Sort"},{"location":"notes/algorithms/python/merge_sort/#definition","text":"Quicksort is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. In contrast, merge sort is an efficient, general purpose, comparison-based sorting algorithms. Thus, this is the fundamental difference between quicksort and merge sort.","title":"Definition"},{"location":"notes/algorithms/python/merge_sort/#functionality","text":"Aboveall, the functionality is the main difference between quicksort and merge sort. Quicksort sorts the elements by comparing each element with the pivot while merge sort divides the array into two subarrays (n/2) again and again until one element is left.","title":"Functionality"},{"location":"notes/algorithms/python/merge_sort/#application","text":"Also, while quicksort is suitable for small arrays, merge sort works for any type of array.","title":"Application"},{"location":"notes/algorithms/python/merge_sort/#speed","text":"Another difference between quicksort and merge sort is that the quicksort works faster for small data sets while merge sort works in consistent speed for all datasets.","title":"Speed"},{"location":"notes/algorithms/python/merge_sort/#space-requirement","text":"Moreover, the space requirement is also an important difference between quicksort and merge sort. Quicksort requires minimum space compared to merge sort.","title":"Space Requirement"},{"location":"notes/algorithms/python/merge_sort/#efficiency","text":"Furthermore, quicksort is not efficient for large arrays, but merge sort is more efficient than quicksort. Hence, this is another difference between quicksort and merge sort.","title":"Efficiency"},{"location":"notes/algorithms/python/merge_sort/#conclusion","text":"In summary, the main difference between quicksort and merge sort is that the quicksort sorts the elements by comparing each element with an element called a pivot while the merge sort divides the array into two subarrays again and again until one element is left.","title":"Conclusion"},{"location":"notes/algorithms/python/merge_sort/#recursive-merge-sort","text":"def merge(left, right): print(f\"left: {left}; right: {right}\") if not len(left) or not len(right): return left or right pdb.set_trace() result = [] i, j = 0, 0 while (len(result) < len(left) + len(right)): if left[i] < right[j]: result.append(left[i]) print(f\"result l < r: {result}\") i+= 1 else: result.append(right[j]) print(f\"result r > l: {result}\") j+= 1 if i == len(left) or j == len(right): result.extend(left[i:] or right[j:]) break print(f\"result: {result}\") return result def mergesort(list): if len(list) < 2: print(f\"returning list: {list}\") return list middle = int(len(list)/2) pdb.set_trace() print(f\"middle: {int(len(list)/2)}\") print(f\"left: {list[:middle]}\") print(f\"right: {list[middle:]}\") left = mergesort(list[:middle]) right = mergesort(list[middle:]) return merge(left, right) seq = [12, 11, 13, 5, 6, 7] print(\"Given array is\") print(seq); print(\"\\n\") print(\"Sorted array is\") print(mergesort(seq)) # Code Contributed by Mohit Gupta_OMG","title":"Recursive Merge Sort:"},{"location":"notes/algorithms/python/merge_sort/#iterative-merge-sort","text":"# Iterative Merge sort (Bottom Up) # Iterative mergesort function to # sort arr[0...n-1] # perform bottom up merge def mergeSort(a): # start with least partition size of 2^0 = 1 width = 1 n = len(a) # subarray size grows by powers of 2 # since growth of loop condition is exponential, # time consumed is logarithmic (log2n) while (width < n): # always start from leftmost l=0; while (l < n): r = min(l+(width*2-1), n-1) m = min(l+width-1,n-1) # final merge should consider # unmerged sublist if input arr # size is not power of 2 print(f\"merge({a}, {l}, {m}, {r})\") merge(a, l, m, r) l += width*2 # Increasing sub array size by powers of 2 width *= 2 return a # Merge Function def merge(a, l, m, r): n1 = m - l + 1 n2 = r - m L = [0] * n1 R = [0] * n2 for i in range(0, n1): L[i] = a[l + i] for i in range(0, n2): R[i] = a[m + i + 1] print(f\"n1: {n1} n2: {n2} L: {L} R: {R}\") i, j, k = 0, 0, l while i < n1 and j < n2: if L[i] <= R[j]: a[k] = L[i] i += 1 else: a[k] = R[j] j += 1 k += 1 while i < n1: a[k] = L[i] i += 1 k += 1 while j < n2: a[k] = R[j] j += 1 k += 1 # Driver code a = [-74,48,-20,2,10,-84,-5,-9,11,-24,-91,2,-71,64,63,80,28,-30,-58,-11,-44,-87,-22,54,-74,-10,-55,-28,-46,29,10,50,-72,34,26,25,8,51,13,30,35,-8,50,65,-6,16,-2,21,-78,35,-13,14,23,-3,26,-90,86,25,-56,91,-13,92,-25,37,57,-20,-69,98,95,45,47,29,86,-28,73,-44,-46,65,-84,-96,-24,-12,72,-68,93,57,92,52,-45,-2,85,-63,56,55,12,-85,77,-39] a = [-1,5,12,11,13,5,6,-2,0,7] print(\"Given array is \") print(a) mergeSort(a) print(\"Sorted array is \") print(a) # Contributed by Madhur Chhangani [RCOEM] # corrected and improved by @mahee96 # perform bottom up merge def mergeSort(a): # start with least partition size of 2^0 = 1 width = 1 n = len(a) # subarray size grows by powers of 2 # since growth of loop condition is exponential, # time consumed is logarithmic (log2n) while (width < n): # always start from leftmost l=0; while (l < n): r = min(l+(width*2-1), n-1) m = min(l+width-1,n-1) # final merge should consider # unmerged sublist if input arr # size is not power of 2 merge(a, l, m, r) l += width*2 # Increasing sub array size by powers of 2 width *= 2 return a # Merge Function def merge(a, l, m, r): print(f\"{a}, {l}, {m}, {r})\") n1 = m - l + 1 n2 = r - m L = [0] * n1 R = [0] * n2 for i in range(0, n1): L[i] = a[l + i] for i in range(0, n2): R[i] = a[m + i + 1] i, j, k = 0, 0, l while i < n1 and j < n2: if L[i] <= R[j]: a[k] = L[i] i += 1 else: a[k] = R[j] j += 1 k += 1 while i < n1: a[k] = L[i] i += 1 k += 1 while j < n2: a[k] = R[j] j += 1 k += 1 # Driver code a = [-74,48,-20,2,10,-84,-5,-9,11,-24,-91,2,-71,64,63,80,28,-30,-58,-11,-44,-87,-22,54,-74,-10,-55,-28,-46,29,10,50,-72,34,26,25,8,51,13,30,35,-8,50,65,-6,16,-2,21,-78,35,-13,14,23,-3,26,-90,86,25,-56,91,-13,92,-25,37,57,-20,-69,98,95,45,47,29,86,-28,73,-44,-46,65,-84,-96,-24,-12,72,-68,93,57,92,52,-45,-2,85,-63,56,55,12,-85,77,-39] print(\"Given array is \") print(a) mergeSort(a) print(\"Sorted array is \") print(a)","title":"Iterative Merge Sort:"},{"location":"notes/algorithms/python/names_and_dates/","text":"Names and Dates Who has the same birthday? This algorithm is a simple, find all people with the same birthday. # Initial data that we are searching through data = {'Kate Thompson':'11/23/2000','Ben Patterson':'05/12/1989','Joseph Bernanke':'11/23/1970','Lindsey Harrison':'05/12/2003','Jenny Bennington':'07/09/1998','Jeremy English':'02/27/1967'} def names_and_dates(data): unique_dates = set((i[0:5] for i in data.values())) dates_dict = {date:[] for date in unique_dates} for date in dates_dict: for name in data.keys(): if data[name][0:5] == date: dates_dict[date].append(name) same_birthdays = [dates_dict[date] for date in dates_dict.keys() if len(dates_dict[date]) > 1] return same_birthdays same_birthdays = names_and_dates(data) [print(names, \"Have the same birthday\") for names in same_birthdays]","title":"Names and Dates"},{"location":"notes/algorithms/python/names_and_dates/#names-and-dates","text":"Who has the same birthday? This algorithm is a simple, find all people with the same birthday. # Initial data that we are searching through data = {'Kate Thompson':'11/23/2000','Ben Patterson':'05/12/1989','Joseph Bernanke':'11/23/1970','Lindsey Harrison':'05/12/2003','Jenny Bennington':'07/09/1998','Jeremy English':'02/27/1967'} def names_and_dates(data): unique_dates = set((i[0:5] for i in data.values())) dates_dict = {date:[] for date in unique_dates} for date in dates_dict: for name in data.keys(): if data[name][0:5] == date: dates_dict[date].append(name) same_birthdays = [dates_dict[date] for date in dates_dict.keys() if len(dates_dict[date]) > 1] return same_birthdays same_birthdays = names_and_dates(data) [print(names, \"Have the same birthday\") for names in same_birthdays]","title":"Names and Dates"},{"location":"notes/algorithms/python/nodes_w_classes/","text":"Nodes with Classes Build a parent/child relationship with classes Consider the following code, what will it output? class Node(object): def __init__(self,sName): self._lChildren = [] self.sName = sName def __repr__(self): return \"<Node '{}'>\".format(self.sName) def append(self,*args,**kwargs): self._lChildren.append(*args,**kwargs) def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) oRoot = Node(\"root\") oChild1 = Node(\"child1\") oChild2 = Node(\"child2\") oChild3 = Node(\"child3\") oChild4 = Node(\"child4\") oChild5 = Node(\"child5\") oChild6 = Node(\"child6\") oChild7 = Node(\"child7\") oChild8 = Node(\"child8\") oChild9 = Node(\"child9\") oChild10 = Node(\"child10\") oRoot.append(oChild1) oRoot.append(oChild2) oRoot.append(oChild3) oChild1.append(oChild5) oChild2.append(oChild6) oChild4.append(oChild7) oChild3.append(oChild8) oChild3.append(oChild9) oChild6.append(oChild10) # specify output from here onwards oRoot.print_all_1() oRoot.print_all_2() Let's discuss what happens. Below is the map of the parents and children: oRoot() _ _| _ | | | oRoot1() oRoot2() oRoot3() | | _|____ oRoot5() oRoot6() | | | oRoot8() oRoot9() oRoot10() oRoot4() | oRoot7() The append method is taking the calling object and appending the passed object to an instance variable '_lChildren'. Once the above tree is setup, it's time to investigate the two print methods. oRoot().print_all_1() def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() The first action is to print the starting root. print(self). Then we loop through the children and call the print_all_1() method for each child. It's important to note that oChild5() is called before oChild2(). This is because before oChild2() is called oChild1() called print_all1() on it's children. This has the affect of traveling down branches as opposed to across them. oRoot.print_all_1() oRoot().print_all_2() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) First loop in the for loop: Do you see the generator? print_all_2() actually creates a generator function. First it creates a function called gen(o) and then creates lAll, a list for storing objects. This is an interesting use of a generator; since lAll is remembered between successive yields, it can theoretically traverse an infinite tree. So the first value to gen is self. In that for loop the first yield is hit and pauses the function after yeilding oRoot(). The for loop is looping through an iterator so the iterator is telling the for loop to keep going. So finish the first for loop and print the oRoot(). The second: So the for loop picks up and rechecks the while condition, which is true as we added oRoot()'s direct children to lAll. Now we pop oChild1() off of lAll and add it's children to lAll. Here is what lAll looks like at this point: [oChild2(), oChild3(), oChild5()]. We yield and print oChild1() and check the while again. Its true so we grab the next element off lAll which is oChild2(). Append its children and yield it and continue horizontaly and left to right across the tree. Check out the final route taken: oRoot.print_all_2()","title":"Node w Classes"},{"location":"notes/algorithms/python/nodes_w_classes/#nodes-with-classes","text":"Build a parent/child relationship with classes Consider the following code, what will it output? class Node(object): def __init__(self,sName): self._lChildren = [] self.sName = sName def __repr__(self): return \"<Node '{}'>\".format(self.sName) def append(self,*args,**kwargs): self._lChildren.append(*args,**kwargs) def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) oRoot = Node(\"root\") oChild1 = Node(\"child1\") oChild2 = Node(\"child2\") oChild3 = Node(\"child3\") oChild4 = Node(\"child4\") oChild5 = Node(\"child5\") oChild6 = Node(\"child6\") oChild7 = Node(\"child7\") oChild8 = Node(\"child8\") oChild9 = Node(\"child9\") oChild10 = Node(\"child10\") oRoot.append(oChild1) oRoot.append(oChild2) oRoot.append(oChild3) oChild1.append(oChild5) oChild2.append(oChild6) oChild4.append(oChild7) oChild3.append(oChild8) oChild3.append(oChild9) oChild6.append(oChild10) # specify output from here onwards oRoot.print_all_1() oRoot.print_all_2() Let's discuss what happens. Below is the map of the parents and children: oRoot() _ _| _ | | | oRoot1() oRoot2() oRoot3() | | _|____ oRoot5() oRoot6() | | | oRoot8() oRoot9() oRoot10() oRoot4() | oRoot7() The append method is taking the calling object and appending the passed object to an instance variable '_lChildren'. Once the above tree is setup, it's time to investigate the two print methods. oRoot().print_all_1() def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() The first action is to print the starting root. print(self). Then we loop through the children and call the print_all_1() method for each child. It's important to note that oChild5() is called before oChild2(). This is because before oChild2() is called oChild1() called print_all1() on it's children. This has the affect of traveling down branches as opposed to across them. oRoot.print_all_1() oRoot().print_all_2() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) First loop in the for loop: Do you see the generator? print_all_2() actually creates a generator function. First it creates a function called gen(o) and then creates lAll, a list for storing objects. This is an interesting use of a generator; since lAll is remembered between successive yields, it can theoretically traverse an infinite tree. So the first value to gen is self. In that for loop the first yield is hit and pauses the function after yeilding oRoot(). The for loop is looping through an iterator so the iterator is telling the for loop to keep going. So finish the first for loop and print the oRoot(). The second: So the for loop picks up and rechecks the while condition, which is true as we added oRoot()'s direct children to lAll. Now we pop oChild1() off of lAll and add it's children to lAll. Here is what lAll looks like at this point: [oChild2(), oChild3(), oChild5()]. We yield and print oChild1() and check the while again. Its true so we grab the next element off lAll which is oChild2(). Append its children and yield it and continue horizontaly and left to right across the tree. Check out the final route taken: oRoot.print_all_2()","title":"Nodes with Classes"},{"location":"notes/algorithms/python/print_directory/","text":"Print Directory This function takes the name of a directory and prints out the paths files within that directory as well as any files contained in contained directories. This function is similar to os.walk. Please don't use os.walk in your answer. We are interested in your ability to work with nested structures. def print_directory_contents(sPath): import os for sChild in os.listdir(sPath): # Everything gets turned into a new path # Then we see which can go deeper... sChildPath = os.path.join(sPath,sChild) if os.path.isdir(sChildPath): # If we have a folder, recall this function print_directory_contents(sChildPath) else: # If we hit a base case then print as we have found a file print(sChildPath)","title":"Print Directory"},{"location":"notes/algorithms/python/print_directory/#print-directory","text":"This function takes the name of a directory and prints out the paths files within that directory as well as any files contained in contained directories. This function is similar to os.walk. Please don't use os.walk in your answer. We are interested in your ability to work with nested structures. def print_directory_contents(sPath): import os for sChild in os.listdir(sPath): # Everything gets turned into a new path # Then we see which can go deeper... sChildPath = os.path.join(sPath,sChild) if os.path.isdir(sChildPath): # If we have a folder, recall this function print_directory_contents(sChildPath) else: # If we hit a base case then print as we have found a file print(sChildPath)","title":"Print Directory"},{"location":"notes/algorithms/python/quick_sort/","text":"Quick Sort After sorting you can search the array with binary search . Take last element as pivot and low and high are indexes of full array's start and end import pdb def partition(arr, low, high): i = (low-1) # index of smaller element pivot = arr[high] # pivot, takes right most element # print(\"call to partition\") # pdb.set_trace() for j in range(low, high): # If current element is smaller than or # equal to pivot if arr[j] <= pivot: # increment index of smaller element i = i+1 arr[i], arr[j] = arr[j], arr[i] # print(arr) arr[i+1], arr[high] = arr[high], arr[i+1] # print(arr) return (i+1) # low and high are indexes of full array's start and end def quickSort(arr, low, high): if len(arr) == 1: return arr if low < high: # pi is partitioning index, arr[p] is now # at right place pi = partition(arr, low, high) # pdb.set_trace() # Separately sort elements before # partition and after partition quickSort(arr, low, pi-1) quickSort(arr, pi+1, high) # Driver code to test above arr = [2, 7, 8, 9, 1, 5] n = len(arr) quickSort(arr, 0, n-1) print(\"Sorted array is:\",arr) # for i in range(n): # print(\"%d\" % arr[i]), Big O notation for this algorithm is O(log(n)). However according to medium.com , O(log (n)) speed is a best-case/average time, in worst case scenarios it can be O(n2) depending on the implementation.","title":"Quick Sort"},{"location":"notes/algorithms/python/quick_sort/#quick-sort","text":"After sorting you can search the array with binary search . Take last element as pivot and low and high are indexes of full array's start and end import pdb def partition(arr, low, high): i = (low-1) # index of smaller element pivot = arr[high] # pivot, takes right most element # print(\"call to partition\") # pdb.set_trace() for j in range(low, high): # If current element is smaller than or # equal to pivot if arr[j] <= pivot: # increment index of smaller element i = i+1 arr[i], arr[j] = arr[j], arr[i] # print(arr) arr[i+1], arr[high] = arr[high], arr[i+1] # print(arr) return (i+1) # low and high are indexes of full array's start and end def quickSort(arr, low, high): if len(arr) == 1: return arr if low < high: # pi is partitioning index, arr[p] is now # at right place pi = partition(arr, low, high) # pdb.set_trace() # Separately sort elements before # partition and after partition quickSort(arr, low, pi-1) quickSort(arr, pi+1, high) # Driver code to test above arr = [2, 7, 8, 9, 1, 5] n = len(arr) quickSort(arr, 0, n-1) print(\"Sorted array is:\",arr) # for i in range(n): # print(\"%d\" % arr[i]), Big O notation for this algorithm is O(log(n)). However according to medium.com , O(log (n)) speed is a best-case/average time, in worst case scenarios it can be O(n2) depending on the implementation.","title":"Quick Sort"},{"location":"notes/algorithms/python/retry/","text":"retry() Retry a GET request a maximum of 5 times. Let's assume you want to get some response from a get command. For all intensive purposes we will assume this is all happening inside a function called get_response(choice) Next youy want to make a function try_again(response='pass') that will keep trying to execute get_response(choice) if we get a '401' response, which means that the client could not be authenticated. The catch is that we want the maximum retries to be 5 before accepting the failed authentication. Furthermore each time the retry function is called the function should sleep for n^2 seconds. It shouldn't wait anytime to execute the first get_response(choice) , which is the one initially passed to try_again() . Finally if we get '404' then return a '404 Not Found Error'. We start by defining get_response() def get_response(choice=3): choices = {1:'404',2:'pass',3:'401'} return choices[choice] Now we need try_again(). I have to give credit to Zagaran which you can find a link to on the Sources tab for this algorithm. During an interview I attempted a recursive solution, but they said a for loop would of done. So for extra practice I implemented both. def try_again_recursive(response, try_count=1): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '401', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" if response == '404': return '404 Not Found Error' elif response == '401': if try_count == 5: print('for n = {0}, waiting {1} seconds.'.format(try_count, 0)) print('client could not be authenticated.') else: print('for n = {0}, waiting {1} seconds.'.format(try_count, try_count**2)) time.sleep(try_count**2) try_again_recursive(get_response(), try_count=try_count+1) else: return 'we connected!' return response Check out the output below # Tests # >>>try_again_recursive(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' One thing to note, I cheated a little. For the sake of practicing the multiple calls, I made sure the get_response() function always returned a '401'. We will address this a little later. For now let's implement the for loop version. # For Loop Solution: def try_again_for_loop(response): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '404', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" for i in range(1,6): # [1,2,3,4,5] if response == '404': return '404 Not Found Error' elif response == '401': if i == 5: print('for n = {0}, waiting {1} seconds.'.format(i, 0)) print('client could not be authenticated.') else: # pdb.set_trace() print('for n = {0}, waiting {1} seconds.'.format(i, i**2)) time.sleep(i**2) response = get_response() else: return 'we connected!' return response Output: # >>> try_again_for_loop(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Now we need to simulate reality a little better to give the other logic a chance to be executed. Let's make it so get_response() could return any of the other values ('pass','404'). Go into the recursive version of try_again() and find this line try_again_recursive(get_response(), try_count=try_count+1) , and change it to try_again_recursive(get_response(random.randint(1,3)), try_count=try_count+1) . You will need to import random at the top of your script. Then also add to your call of try_again_recursive() Results: # Simulation # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # 'we connected!' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # '404 Not Found Error' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Again, the for loop version changes the same thing, ie. response = get_response( to this response = get_response(random.rendint(1,3)) . # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # '404 Not Found Error' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # '404 Not Found Error'","title":"Retry"},{"location":"notes/algorithms/python/retry/#retry","text":"Retry a GET request a maximum of 5 times. Let's assume you want to get some response from a get command. For all intensive purposes we will assume this is all happening inside a function called get_response(choice) Next youy want to make a function try_again(response='pass') that will keep trying to execute get_response(choice) if we get a '401' response, which means that the client could not be authenticated. The catch is that we want the maximum retries to be 5 before accepting the failed authentication. Furthermore each time the retry function is called the function should sleep for n^2 seconds. It shouldn't wait anytime to execute the first get_response(choice) , which is the one initially passed to try_again() . Finally if we get '404' then return a '404 Not Found Error'. We start by defining get_response() def get_response(choice=3): choices = {1:'404',2:'pass',3:'401'} return choices[choice] Now we need try_again(). I have to give credit to Zagaran which you can find a link to on the Sources tab for this algorithm. During an interview I attempted a recursive solution, but they said a for loop would of done. So for extra practice I implemented both. def try_again_recursive(response, try_count=1): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '401', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" if response == '404': return '404 Not Found Error' elif response == '401': if try_count == 5: print('for n = {0}, waiting {1} seconds.'.format(try_count, 0)) print('client could not be authenticated.') else: print('for n = {0}, waiting {1} seconds.'.format(try_count, try_count**2)) time.sleep(try_count**2) try_again_recursive(get_response(), try_count=try_count+1) else: return 'we connected!' return response Check out the output below # Tests # >>>try_again_recursive(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' One thing to note, I cheated a little. For the sake of practicing the multiple calls, I made sure the get_response() function always returned a '401'. We will address this a little later. For now let's implement the for loop version. # For Loop Solution: def try_again_for_loop(response): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '404', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" for i in range(1,6): # [1,2,3,4,5] if response == '404': return '404 Not Found Error' elif response == '401': if i == 5: print('for n = {0}, waiting {1} seconds.'.format(i, 0)) print('client could not be authenticated.') else: # pdb.set_trace() print('for n = {0}, waiting {1} seconds.'.format(i, i**2)) time.sleep(i**2) response = get_response() else: return 'we connected!' return response Output: # >>> try_again_for_loop(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Now we need to simulate reality a little better to give the other logic a chance to be executed. Let's make it so get_response() could return any of the other values ('pass','404'). Go into the recursive version of try_again() and find this line try_again_recursive(get_response(), try_count=try_count+1) , and change it to try_again_recursive(get_response(random.randint(1,3)), try_count=try_count+1) . You will need to import random at the top of your script. Then also add to your call of try_again_recursive() Results: # Simulation # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # 'we connected!' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # '404 Not Found Error' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Again, the for loop version changes the same thing, ie. response = get_response( to this response = get_response(random.rendint(1,3)) . # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # '404 Not Found Error' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # '404 Not Found Error'","title":"retry()"},{"location":"notes/algorithms/python/stairs/","text":"Stairs How many ways can you traverse n stairs if you can only take 1 or 2 steps. This is a cool little chunck of code illustrating and intersting way recurtsion solves a tough problem. _ |_ |_ |... how many ways can you traverse n stairs if you can only take 1 or 2 steps. Example: n = 1 (1) answer = 1 Example: n = 2 (1,1), (2) answer = 2 Example: n = 3 (1,1,1), (1,2), (2,1) answer = 3 Example: n = 4 (1,1,1,1), (1,1,2), (1,2,1), (2,1,1), (2,2) answer = 5 Example: n = 5 (1,1,1,1,1), (1,1,1,2), (1,1,2,1), (1,2,1,1), (2,1,1,1), (1,2,2), (2,1,2), (2,2,1) answer = 8 Pattern: n stairs_combo(n) 1 1 2 2 3 3 4 5 5 8 What do we notice? Answer Pattern: n stairs_combo(n) 1 1 = pattern doesn't hold 2 2 = pattern doesn't hold 3 3 = stairs_combo(2) + stairs_combo(1) = 2 + 1 = 3 4 5 = stairs_combo(3) + stairs_combo(2) = 3 + 2 = 5 5 8 = stairs_combo(4) + stairs_combo(3) = 5 + 3 = 8 We can use recursion on this since the answer depends on answers before it. For the first two values n can have (1,2) we will setup base cases to kill the recursive stack. Here is the answer: def stairs_combo(n=0): if n == 0: return 0 elif n == 1: return 1 elif n == 2: return 2 else: return stairs_combo(n-1) + stairs_combo(n-2) print(stairs_combo()) # 0 print(stairs_combo(1)) # 1 print(stairs_combo(2)) # 2 print(stairs_combo(3)) # 3 print(stairs_combo(4)) # 5 print(stairs_combo(5)) # 8 The other solution is to figure out the combinations for real! ;)","title":"Stairs"},{"location":"notes/algorithms/python/stairs/#stairs","text":"How many ways can you traverse n stairs if you can only take 1 or 2 steps. This is a cool little chunck of code illustrating and intersting way recurtsion solves a tough problem. _ |_ |_ |... how many ways can you traverse n stairs if you can only take 1 or 2 steps. Example: n = 1 (1) answer = 1 Example: n = 2 (1,1), (2) answer = 2 Example: n = 3 (1,1,1), (1,2), (2,1) answer = 3 Example: n = 4 (1,1,1,1), (1,1,2), (1,2,1), (2,1,1), (2,2) answer = 5 Example: n = 5 (1,1,1,1,1), (1,1,1,2), (1,1,2,1), (1,2,1,1), (2,1,1,1), (1,2,2), (2,1,2), (2,2,1) answer = 8 Pattern: n stairs_combo(n) 1 1 2 2 3 3 4 5 5 8 What do we notice? Answer Pattern: n stairs_combo(n) 1 1 = pattern doesn't hold 2 2 = pattern doesn't hold 3 3 = stairs_combo(2) + stairs_combo(1) = 2 + 1 = 3 4 5 = stairs_combo(3) + stairs_combo(2) = 3 + 2 = 5 5 8 = stairs_combo(4) + stairs_combo(3) = 5 + 3 = 8 We can use recursion on this since the answer depends on answers before it. For the first two values n can have (1,2) we will setup base cases to kill the recursive stack. Here is the answer: def stairs_combo(n=0): if n == 0: return 0 elif n == 1: return 1 elif n == 2: return 2 else: return stairs_combo(n-1) + stairs_combo(n-2) print(stairs_combo()) # 0 print(stairs_combo(1)) # 1 print(stairs_combo(2)) # 2 print(stairs_combo(3)) # 3 print(stairs_combo(4)) # 5 print(stairs_combo(5)) # 8 The other solution is to figure out the combinations for real! ;)","title":"Stairs"},{"location":"notes/algorithms/python/swap_case/","text":"sWAP cASE Switch case of all letters in a string. You are given a string and your task is to swap cases. In other words, convert all lowercase letters to uppercase letters and vice versa. For Example: Www.HackerRank.com \u2192 wWW.hACKERrANK.COM Pythonist 2 \u2192 pYTHONIST 2 Input Format: A single line containing a string S. Constraints: * 0<len(S)<=1000 Output Format: Print the modified string S. Sample Input: HackerRank.com presents \"Pythonist 2\". Sample Output: hACKERrANK.COM PRESENTS \"pYTHONIST 2\". Code: A = 'HackerRank.com presents \"Pythonist 2\".' def swap_case(s): S = list(s) for _ in range(len(S)): if S[_] == S[_].lower(): S[_] = S[_].upper() elif S[_] == S[_].upper(): S[_] = S[_].lower() return ''.join(S) print(swap_case(A))","title":"sWAP cASE"},{"location":"notes/algorithms/python/swap_case/#swap-case","text":"Switch case of all letters in a string. You are given a string and your task is to swap cases. In other words, convert all lowercase letters to uppercase letters and vice versa. For Example: Www.HackerRank.com \u2192 wWW.hACKERrANK.COM Pythonist 2 \u2192 pYTHONIST 2 Input Format: A single line containing a string S. Constraints: * 0<len(S)<=1000 Output Format: Print the modified string S. Sample Input: HackerRank.com presents \"Pythonist 2\". Sample Output: hACKERrANK.COM PRESENTS \"pYTHONIST 2\". Code: A = 'HackerRank.com presents \"Pythonist 2\".' def swap_case(s): S = list(s) for _ in range(len(S)): if S[_] == S[_].lower(): S[_] = S[_].upper() elif S[_] == S[_].upper(): S[_] = S[_].lower() return ''.join(S) print(swap_case(A))","title":"sWAP cASE"},{"location":"notes/algorithms/python/tim_sort/","text":"Timsort Source: geeksforgeeks TimSort is a sorting algorithm based on Insertion Sort and Merge Sort. We divide the Array into blocks known as Run. We sort those runs using insertion sort one by one and then merge those runs using the combine function used in merge sort. If the size of the Array is less than run, then Array gets sorted just by using Insertion Sort. The size of the run may vary from 32 to 64 depending upon the size of the array. Note that the merge function performs well when size subarrays are powers of 2. The idea is based on the fact that insertion sort performs well for small arrays. # Python3 program to perform basic timSort MIN_MERGE = 8 def calcMinRun(n): # print(f\"n: {n}\") # array_len = n \"\"\"Returns the minimum length of a run from 23 - 64 so that the len(array)/minrun is less than or equal to a power of 2. e.g. 1=>1, ..., 63=>63, 64=>32, 65=>33, ..., 127=>64, 128=>32, ... \"\"\" r = 0 # print(f\"In calcMinRun: r:{r}\") while n >= MIN_MERGE: # print(f\"r: {r} n:{n}\") # pdb.set_trace() r |= n & 1 # r = r | (n & 1) n >>= 1 # print(f\"r: {r} n:{n}\") # print(f\"minRunn: {n + r} and len(array)/minrun = {array_len/(n+r)}\") return n + r # This function sorts array from left index to # to right index which is of size atmost RUN def insertionSort(arr, left, right): print(f\"arr: {arr} left: {left} right: {right}\") for i in range(left + 1, right + 1): j = i while j > left and arr[j] < arr[j - 1]: arr[j], arr[j - 1] = arr[j - 1], arr[j] j -= 1 print(f\"arr: {arr}\") # Merge function merges the sorted runs def merge(arr, l, m, r): # original array is broken in two parts # left and right array print(\"In merge()\") print(f\"arr: {arr} l: {l} m: {m} r: {r} \") len1, len2 = m - l + 1, r - m print(f\"len1: {len1} len2: {len2}\") left, right = [], [] print(f\"left: {left} right: {right}\") for i in range(0, len1): left.append(arr[l + i]) print(f\"left: {left} right: {right}\") for i in range(0, len2): right.append(arr[m + 1 + i]) print(f\"left: {left} right: {right}\") i, j, k = 0, 0, l # after comparing, we merge those two array # in larger sub array while i < len1 and j < len2: if left[i] <= right[j]: arr[k] = left[i] i += 1 print(f\"arr: {arr} left: {left} right: {right}\") else: arr[k] = right[j] j += 1 print(f\"arr: {arr} left: {left} right: {right}\") k += 1 print(\"# Copy remaining elements of left, if any)\") # Copy remaining elements of left, if any while i < len1: arr[k] = left[i] k += 1 i += 1 print(f\"arr: {arr} left: {left} right: {right}\") # Copy remaining element of right, if any while j < len2: arr[k] = right[j] k += 1 j += 1 print(f\"arr: {arr} left: {left} right: {right}\") # Iterative Timsort function to sort the # array[0...n-1] (similar to merge sort) def timSort(arr): n = len(arr) print(f\"n: {n}; calling calcMinRun(n)\") minRun = calcMinRun(n) print(f\"midRun: {minRun}\") # Sort individual subarrays of size RUN for start in range(0, n, minRun): print(f\"start: {start} of minRun: {minRun}\") end = min(start + minRun - 1, n - 1) print(f\"calling insertionSort({arr}, {start}, {end})\") insertionSort(arr, start, end) # Start merging from size RUN (or 32). It will merge # to form size 64, then 128, 256 and so on .... size = minRun print(f\"size: {size}\") while size < n: # Pick starting point of left sub array. We # are going to merge arr[left..left+size-1] # and arr[left+size, left+2*size-1] # After every merge, we increase left by 2*size for left in range(0, n, 2 * size): # Find ending point of left sub array # mid+1 is starting point of right sub array mid = min(n - 1, left + size - 1) right = min((left + 2 * size - 1), (n - 1)) # Merge sub array arr[left.....mid] & # arr[mid+1....right] print(f\"mid: {mid} left: {left} right: {right}\") if mid < right: print(f\"calling merge({arr}, {left}, {mid}, {right})\") merge(arr, left, mid, right) size = 2 * size # Driver program to test above function # if __name__ == \"__main__\": arr = [-2, 7, 15, -14, 0, 15, 0, 7, -7, -4, -13, 5, 8, -14, 12] print(\"Given Array is\") print(arr) # Function Call timSort(arr) print(\"After Sorting Array is\") print(arr) # [-14, -14, -13, -7, -4, -2, 0, 0, # 5, 7, 7, 8, 12, 15, 15]","title":"Tim Sort"},{"location":"notes/algorithms/python/tim_sort/#timsort","text":"Source: geeksforgeeks TimSort is a sorting algorithm based on Insertion Sort and Merge Sort. We divide the Array into blocks known as Run. We sort those runs using insertion sort one by one and then merge those runs using the combine function used in merge sort. If the size of the Array is less than run, then Array gets sorted just by using Insertion Sort. The size of the run may vary from 32 to 64 depending upon the size of the array. Note that the merge function performs well when size subarrays are powers of 2. The idea is based on the fact that insertion sort performs well for small arrays. # Python3 program to perform basic timSort MIN_MERGE = 8 def calcMinRun(n): # print(f\"n: {n}\") # array_len = n \"\"\"Returns the minimum length of a run from 23 - 64 so that the len(array)/minrun is less than or equal to a power of 2. e.g. 1=>1, ..., 63=>63, 64=>32, 65=>33, ..., 127=>64, 128=>32, ... \"\"\" r = 0 # print(f\"In calcMinRun: r:{r}\") while n >= MIN_MERGE: # print(f\"r: {r} n:{n}\") # pdb.set_trace() r |= n & 1 # r = r | (n & 1) n >>= 1 # print(f\"r: {r} n:{n}\") # print(f\"minRunn: {n + r} and len(array)/minrun = {array_len/(n+r)}\") return n + r # This function sorts array from left index to # to right index which is of size atmost RUN def insertionSort(arr, left, right): print(f\"arr: {arr} left: {left} right: {right}\") for i in range(left + 1, right + 1): j = i while j > left and arr[j] < arr[j - 1]: arr[j], arr[j - 1] = arr[j - 1], arr[j] j -= 1 print(f\"arr: {arr}\") # Merge function merges the sorted runs def merge(arr, l, m, r): # original array is broken in two parts # left and right array print(\"In merge()\") print(f\"arr: {arr} l: {l} m: {m} r: {r} \") len1, len2 = m - l + 1, r - m print(f\"len1: {len1} len2: {len2}\") left, right = [], [] print(f\"left: {left} right: {right}\") for i in range(0, len1): left.append(arr[l + i]) print(f\"left: {left} right: {right}\") for i in range(0, len2): right.append(arr[m + 1 + i]) print(f\"left: {left} right: {right}\") i, j, k = 0, 0, l # after comparing, we merge those two array # in larger sub array while i < len1 and j < len2: if left[i] <= right[j]: arr[k] = left[i] i += 1 print(f\"arr: {arr} left: {left} right: {right}\") else: arr[k] = right[j] j += 1 print(f\"arr: {arr} left: {left} right: {right}\") k += 1 print(\"# Copy remaining elements of left, if any)\") # Copy remaining elements of left, if any while i < len1: arr[k] = left[i] k += 1 i += 1 print(f\"arr: {arr} left: {left} right: {right}\") # Copy remaining element of right, if any while j < len2: arr[k] = right[j] k += 1 j += 1 print(f\"arr: {arr} left: {left} right: {right}\") # Iterative Timsort function to sort the # array[0...n-1] (similar to merge sort) def timSort(arr): n = len(arr) print(f\"n: {n}; calling calcMinRun(n)\") minRun = calcMinRun(n) print(f\"midRun: {minRun}\") # Sort individual subarrays of size RUN for start in range(0, n, minRun): print(f\"start: {start} of minRun: {minRun}\") end = min(start + minRun - 1, n - 1) print(f\"calling insertionSort({arr}, {start}, {end})\") insertionSort(arr, start, end) # Start merging from size RUN (or 32). It will merge # to form size 64, then 128, 256 and so on .... size = minRun print(f\"size: {size}\") while size < n: # Pick starting point of left sub array. We # are going to merge arr[left..left+size-1] # and arr[left+size, left+2*size-1] # After every merge, we increase left by 2*size for left in range(0, n, 2 * size): # Find ending point of left sub array # mid+1 is starting point of right sub array mid = min(n - 1, left + size - 1) right = min((left + 2 * size - 1), (n - 1)) # Merge sub array arr[left.....mid] & # arr[mid+1....right] print(f\"mid: {mid} left: {left} right: {right}\") if mid < right: print(f\"calling merge({arr}, {left}, {mid}, {right})\") merge(arr, left, mid, right) size = 2 * size # Driver program to test above function # if __name__ == \"__main__\": arr = [-2, 7, 15, -14, 0, 15, 0, 7, -7, -4, -13, 5, 8, -14, 12] print(\"Given Array is\") print(arr) # Function Call timSort(arr) print(\"After Sorting Array is\") print(arr) # [-14, -14, -13, -7, -4, -2, 0, 0, # 5, 7, 7, 8, 12, 15, 15]","title":"Timsort"},{"location":"notes/algorithms/python/wheel/wheel/","text":"Make a Spinning Wheel To start we need picture like states. We can make these with strings. Import what you need import os import sys import time import math state_1 = \"\"\" -|- | / | \\\\ | | | | | | | | | | | | \\\\ | / | | -|- \"\"\" state_2 = \"\"\" / / / / / / / / / / / / / / \"\"\" state_3 = \"\"\" --------------- --------------- --------------- \"\"\" state_4 = \"\"\" - / \\\\ ---------------------------------------- \\\\ / - \"\"\" state_5 = \"\"\" --------------- --------------- --------------- \"\"\" state_6 = \"\"\" \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \"\"\" Now define a generator to access the wheel states infintely: def spinning_cursor(): while True: for cursor in [state_1,state_2,state_3,state_4,state_5,state_6]: yield cursor Next wheel size and set placeholders to keep track of the elapsed time and velocity : radius = 1 # ft spinner = spinning_cursor() elapsed_time = 0 velocity = 0 Choose the speed in terms of rotations per second: rotations_per_second = 3 rotations_desired = 2 \"DJ spin that shit!\" - KS - for _ in range(6*rotations_desired): # there are 6 states and we need all of them per rotation state = next(spinner) # call the next state if state == state_1 and elapsed_time != 0: # Once we've done a full rotation we should have enough info to calculate the velocity velocity = 2*math.pi*radius / elapsed_time # ft/s # In 1 second we do 6 states * rotations_per_second. # We can divide 1 second into these partitions to find out # how much time has passed between states. elapsed_time += 1.0 / (6*rotations_per_second) # seconds sys.stdout.write(state) # see the state sys.stdout.flush() # clear screen time.sleep(1.0/(6*rotations_per_second)) # control the rate of execution os.system('clear') # clear the screen of any states os.system('clear') # clear the screen of any states print(\"velocity: {} in miles/hour\".format(velocity/5280*60*60)) # velocity is currently in ft/s. # 5280 ft in a mile # 60 seconds in a min # 60 min in an hour Your browser does not support HTML5 video.","title":"Wheel"},{"location":"notes/algorithms/python/wheel/wheel/#make-a-spinning-wheel","text":"To start we need picture like states. We can make these with strings. Import what you need import os import sys import time import math state_1 = \"\"\" -|- | / | \\\\ | | | | | | | | | | | | \\\\ | / | | -|- \"\"\" state_2 = \"\"\" / / / / / / / / / / / / / / \"\"\" state_3 = \"\"\" --------------- --------------- --------------- \"\"\" state_4 = \"\"\" - / \\\\ ---------------------------------------- \\\\ / - \"\"\" state_5 = \"\"\" --------------- --------------- --------------- \"\"\" state_6 = \"\"\" \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \"\"\" Now define a generator to access the wheel states infintely: def spinning_cursor(): while True: for cursor in [state_1,state_2,state_3,state_4,state_5,state_6]: yield cursor Next wheel size and set placeholders to keep track of the elapsed time and velocity : radius = 1 # ft spinner = spinning_cursor() elapsed_time = 0 velocity = 0 Choose the speed in terms of rotations per second: rotations_per_second = 3 rotations_desired = 2 \"DJ spin that shit!\" - KS - for _ in range(6*rotations_desired): # there are 6 states and we need all of them per rotation state = next(spinner) # call the next state if state == state_1 and elapsed_time != 0: # Once we've done a full rotation we should have enough info to calculate the velocity velocity = 2*math.pi*radius / elapsed_time # ft/s # In 1 second we do 6 states * rotations_per_second. # We can divide 1 second into these partitions to find out # how much time has passed between states. elapsed_time += 1.0 / (6*rotations_per_second) # seconds sys.stdout.write(state) # see the state sys.stdout.flush() # clear screen time.sleep(1.0/(6*rotations_per_second)) # control the rate of execution os.system('clear') # clear the screen of any states os.system('clear') # clear the screen of any states print(\"velocity: {} in miles/hour\".format(velocity/5280*60*60)) # velocity is currently in ft/s. # 5280 ft in a mile # 60 seconds in a min # 60 min in an hour Your browser does not support HTML5 video.","title":"Make a Spinning Wheel"},{"location":"notes/android/hello-world/","text":"Hello World Download Android Studio Android Studio Place extracted folder in: /home/bbearce/Documents/android-studio-2021.1.1.21-linux Ex: bbearce@terry:~/Documents/android-studio-2021.1.1.21-linux$ ls -la total 12 drwxrwxr-x 3 bbearce bbearce 4096 Feb 22 14:59 . drwxr-xr-x 45 bbearce bbearce 4096 Feb 22 14:59 .. drwxrwxr-x 7 bbearce bbearce 4096 Dec 31 2009 android-studio bbearce@terry:~/Documents/android-studio-2021.1.1.21-linux$ ls -la android-studio/ total 3336 drwxrwxr-x 7 bbearce bbearce 4096 Dec 31 2009 . drwxrwxr-x 3 bbearce bbearce 4096 Feb 22 14:59 .. drwxrwxr-x 4 bbearce bbearce 4096 Dec 31 2009 bin -rwxr-xr-x 1 bbearce bbearce 105333 Dec 31 2009 brokenPlugins.db -rw-rw-r-- 1 bbearce bbearce 27 Dec 31 2009 build.txt -rwxr-xr-x 1 bbearce bbearce 7554 Dec 31 2009 classpath.txt -rwxr-xr-x 1 bbearce bbearce 3231744 Dec 31 2009 icons.db -rwxr-xr-x 1 bbearce bbearce 1852 Dec 31 2009 Install-Linux-tar.txt drwxrwxr-x 6 bbearce bbearce 4096 Dec 31 2009 jre drwxrwxr-x 5 bbearce bbearce 16384 Dec 31 2009 lib drwxrwxr-x 2 bbearce bbearce 4096 Dec 31 2009 license -rwxr-xr-x 1 bbearce bbearce 11352 Dec 31 2009 LICENSE.txt -rwxr-xr-x 1 bbearce bbearce 396 Dec 31 2009 NOTICE.txt drwxrwxr-x 57 bbearce bbearce 4096 Dec 31 2009 plugins -rw-rw-r-- 1 bbearce bbearce 418 Dec 31 2009 product-info.json Run with: /home/bbearce/Documents/android-studio-2021.1.1.21-linux/android-studio/bin/studio.sh Android Studio Reference: Build Your First App Apps provide multiple entry points Android apps are built as a combination of components that can be invoked individually. For example, an activity is a type of app component that provides a user interface (UI). Other components, such as WorkManager , allow your app to perform background tasks without a UI. If any of your app's features need specific hardware, such as a camera, you can query at runtime whether the device has that hardware or not, and then disable the corresponding features if it doesn't. You can specify that your app requires certain hardware so that Google Play won't allow the app to be installed on devices without them. After you build your first app, you can learn more about the other app components at Application fundamentals . After you build your first app, learn more about device configurations at Device compatibility overview . Create First Project https://developer.android.com/training/basics/firstapp/creating-project There are notes about emulators and connecting to your phone but they are simple. Read if necessary from above links. Build a Simple User Interface In this lesson, you learn how to use the [Android Studio Layout Editor(https://developer.android.com/studio/write/layout-editor)] to create a layout that includes a text box and a button. This sets up the next lesson, where you learn how to make the app send the content of the text box to another activity when the button is tapped. The user interface (UI) for an Android app is built as a hierarchy of layouts and widgets . The layouts are ViewGroup objects, containers that control how their child views are positioned on the screen. Widgets are View objects, UI components such as buttons and text boxes. Android provides an XML vocabulary for ViewGroup and View classes, so most of your UI is defined in XML files. However, rather than teach you to write XML, this lesson shows you how to create a layout using Android Studio's Layout Editor. The Layout Editor writes the XML for you as you drag and drop views to build your layout. In the Project window, open app > res > layout > activity_main.xml . building-ui Build This UI starting-activity Source: starting-activity MainActivity We skipped over the MainActivity.java code that was pre-defined: package com.example.myfirstapp; import androidx.appcompat.app.AppCompatActivity; import android.os.Bundle; public class MainActivity extends AppCompatActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); } } Respond to Send Button Add this method to class MainActivity: /** Called when the user taps the Send button */ public void sendMessage(View view) { // Do something in response to button } Note: we need to import the View class.: import android.view.View; ... Add the Method sendMessage to the button under Button Attributes -> onCLick. Build an Intent An Intent is an object that provides runtime binding between separate components, such as two activities. The Intent represents an app\u2019s intent to do something. You can use intents for a wide variety of tasks, but in this lesson, your intent starts another activity. Edit sendMessage() : public void sendMessage(View view) { Intent intent = new Intent(this, DisplayMessageActivity.class); EditText editText = (EditText) findViewById(R.id.editTextTextPersonName); String message = editText.getText().toString(); intent.putExtra(EXTRA_MESSAGE, message); startActivity(intent); } Here is what is going on: The Intent constructor takes two parameters, a Context and a Class . The Context parameter is used first because the Activity class is a subclass of Context. The Class parameter of the app component, to which the system delivers the Intent , is, in this case, the activity to start. The putExtra() method adds the value of EditText to the intent. An Intent can carry data types as key-value pairs called extras. Your key is a public constant EXTRA_MESSAGE because the next activity uses the key to retrieve the text value. It's a good practice to define keys for intent extras with your app's package name as a prefix. This ensures that the keys are unique, in case your app interacts with other apps. The startActivity() method starts an instance of the DisplayMessageActivity that's specified by the Intent . Next, you need to create that class. Create The Second Activity To create the second activity, follow these steps: In the Project window, right-click the app folder and select New > Activity > Empty Activity . In the Configure Activity window, enter \"DisplayMessageActivity\" for Activity Name . Leave all other properties set to their defaults and click Finish . Android Studio automatically does three things: Creates the DisplayMessageActivity file. Creates the layout file activity_display_message.xml , which corresponds with the DisplayMessageActivity file. Adds the required <activity> element in AndroidManifest.xml . DisplayMessageActivity should look like this: package com.example.myfirstapp; import androidx.appcompat.app.AppCompatActivity; import android.content.Intent; import android.os.Bundle; import android.widget.TextView; public class DisplayMessageActivity extends AppCompatActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_display_message); // Get the Intent that started this activity and extract the string Intent intent = getIntent(); String message = intent.getStringExtra(MainActivity.EXTRA_MESSAGE); // Capture the layout's TextView and set the string as its text TextView textView = findViewById(R.id.textView); textView.setText(message); } } Add Upward Navigation Each screen in your app that's not the main entry point, which are all the screens that aren't the home screen, must provide navigation that directs the user to the logical parent screen in the app's hierarchy. To do this, add an Up button in the app bar.","title":"Hello World"},{"location":"notes/android/hello-world/#hello-world","text":"","title":"Hello World"},{"location":"notes/android/hello-world/#download-android-studio","text":"Android Studio Place extracted folder in: /home/bbearce/Documents/android-studio-2021.1.1.21-linux Ex: bbearce@terry:~/Documents/android-studio-2021.1.1.21-linux$ ls -la total 12 drwxrwxr-x 3 bbearce bbearce 4096 Feb 22 14:59 . drwxr-xr-x 45 bbearce bbearce 4096 Feb 22 14:59 .. drwxrwxr-x 7 bbearce bbearce 4096 Dec 31 2009 android-studio bbearce@terry:~/Documents/android-studio-2021.1.1.21-linux$ ls -la android-studio/ total 3336 drwxrwxr-x 7 bbearce bbearce 4096 Dec 31 2009 . drwxrwxr-x 3 bbearce bbearce 4096 Feb 22 14:59 .. drwxrwxr-x 4 bbearce bbearce 4096 Dec 31 2009 bin -rwxr-xr-x 1 bbearce bbearce 105333 Dec 31 2009 brokenPlugins.db -rw-rw-r-- 1 bbearce bbearce 27 Dec 31 2009 build.txt -rwxr-xr-x 1 bbearce bbearce 7554 Dec 31 2009 classpath.txt -rwxr-xr-x 1 bbearce bbearce 3231744 Dec 31 2009 icons.db -rwxr-xr-x 1 bbearce bbearce 1852 Dec 31 2009 Install-Linux-tar.txt drwxrwxr-x 6 bbearce bbearce 4096 Dec 31 2009 jre drwxrwxr-x 5 bbearce bbearce 16384 Dec 31 2009 lib drwxrwxr-x 2 bbearce bbearce 4096 Dec 31 2009 license -rwxr-xr-x 1 bbearce bbearce 11352 Dec 31 2009 LICENSE.txt -rwxr-xr-x 1 bbearce bbearce 396 Dec 31 2009 NOTICE.txt drwxrwxr-x 57 bbearce bbearce 4096 Dec 31 2009 plugins -rw-rw-r-- 1 bbearce bbearce 418 Dec 31 2009 product-info.json Run with: /home/bbearce/Documents/android-studio-2021.1.1.21-linux/android-studio/bin/studio.sh","title":"Download Android Studio"},{"location":"notes/android/hello-world/#android-studio","text":"Reference: Build Your First App","title":"Android Studio"},{"location":"notes/android/hello-world/#apps-provide-multiple-entry-points","text":"Android apps are built as a combination of components that can be invoked individually. For example, an activity is a type of app component that provides a user interface (UI). Other components, such as WorkManager , allow your app to perform background tasks without a UI. If any of your app's features need specific hardware, such as a camera, you can query at runtime whether the device has that hardware or not, and then disable the corresponding features if it doesn't. You can specify that your app requires certain hardware so that Google Play won't allow the app to be installed on devices without them. After you build your first app, you can learn more about the other app components at Application fundamentals . After you build your first app, learn more about device configurations at Device compatibility overview .","title":"Apps provide multiple entry points"},{"location":"notes/android/hello-world/#create-first-project","text":"https://developer.android.com/training/basics/firstapp/creating-project There are notes about emulators and connecting to your phone but they are simple. Read if necessary from above links.","title":"Create First Project"},{"location":"notes/android/hello-world/#build-a-simple-user-interface","text":"In this lesson, you learn how to use the [Android Studio Layout Editor(https://developer.android.com/studio/write/layout-editor)] to create a layout that includes a text box and a button. This sets up the next lesson, where you learn how to make the app send the content of the text box to another activity when the button is tapped. The user interface (UI) for an Android app is built as a hierarchy of layouts and widgets . The layouts are ViewGroup objects, containers that control how their child views are positioned on the screen. Widgets are View objects, UI components such as buttons and text boxes. Android provides an XML vocabulary for ViewGroup and View classes, so most of your UI is defined in XML files. However, rather than teach you to write XML, this lesson shows you how to create a layout using Android Studio's Layout Editor. The Layout Editor writes the XML for you as you drag and drop views to build your layout. In the Project window, open app > res > layout > activity_main.xml .","title":"Build a Simple User Interface"},{"location":"notes/android/hello-world/#building-ui","text":"Build This UI","title":"building-ui"},{"location":"notes/android/hello-world/#starting-activity","text":"Source: starting-activity","title":"starting-activity"},{"location":"notes/android/hello-world/#mainactivity","text":"We skipped over the MainActivity.java code that was pre-defined: package com.example.myfirstapp; import androidx.appcompat.app.AppCompatActivity; import android.os.Bundle; public class MainActivity extends AppCompatActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); } }","title":"MainActivity"},{"location":"notes/android/hello-world/#respond-to-send-button","text":"Add this method to class MainActivity: /** Called when the user taps the Send button */ public void sendMessage(View view) { // Do something in response to button } Note: we need to import the View class.: import android.view.View; ... Add the Method sendMessage to the button under Button Attributes -> onCLick.","title":"Respond to Send Button"},{"location":"notes/android/hello-world/#build-an-intent","text":"An Intent is an object that provides runtime binding between separate components, such as two activities. The Intent represents an app\u2019s intent to do something. You can use intents for a wide variety of tasks, but in this lesson, your intent starts another activity. Edit sendMessage() : public void sendMessage(View view) { Intent intent = new Intent(this, DisplayMessageActivity.class); EditText editText = (EditText) findViewById(R.id.editTextTextPersonName); String message = editText.getText().toString(); intent.putExtra(EXTRA_MESSAGE, message); startActivity(intent); } Here is what is going on: The Intent constructor takes two parameters, a Context and a Class . The Context parameter is used first because the Activity class is a subclass of Context. The Class parameter of the app component, to which the system delivers the Intent , is, in this case, the activity to start. The putExtra() method adds the value of EditText to the intent. An Intent can carry data types as key-value pairs called extras. Your key is a public constant EXTRA_MESSAGE because the next activity uses the key to retrieve the text value. It's a good practice to define keys for intent extras with your app's package name as a prefix. This ensures that the keys are unique, in case your app interacts with other apps. The startActivity() method starts an instance of the DisplayMessageActivity that's specified by the Intent . Next, you need to create that class.","title":"Build an Intent"},{"location":"notes/android/hello-world/#create-the-second-activity","text":"To create the second activity, follow these steps: In the Project window, right-click the app folder and select New > Activity > Empty Activity . In the Configure Activity window, enter \"DisplayMessageActivity\" for Activity Name . Leave all other properties set to their defaults and click Finish . Android Studio automatically does three things: Creates the DisplayMessageActivity file. Creates the layout file activity_display_message.xml , which corresponds with the DisplayMessageActivity file. Adds the required <activity> element in AndroidManifest.xml . DisplayMessageActivity should look like this: package com.example.myfirstapp; import androidx.appcompat.app.AppCompatActivity; import android.content.Intent; import android.os.Bundle; import android.widget.TextView; public class DisplayMessageActivity extends AppCompatActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_display_message); // Get the Intent that started this activity and extract the string Intent intent = getIntent(); String message = intent.getStringExtra(MainActivity.EXTRA_MESSAGE); // Capture the layout's TextView and set the string as its text TextView textView = findViewById(R.id.textView); textView.setText(message); } }","title":"Create The Second Activity"},{"location":"notes/android/hello-world/#add-upward-navigation","text":"Each screen in your app that's not the main entry point, which are all the screens that aren't the home screen, must provide navigation that directs the user to the logical parent screen in the app's hierarchy. To do this, add an Up button in the app bar.","title":"Add Upward Navigation"},{"location":"notes/apache/apache/","text":"Apache Intro Intro Source: Medium Hypertext Transfer Protocol Secure is usually through port 443 with the unsecured protocol being through port 80. The Apache server is configured via config files in which modules are used to control its behavior. By default, Apache listens to the IP addresses configured in its config files that are being requested. This is where one of Apaches many strengths come into play. Flask Source: codementor.io","title":"Basics"},{"location":"notes/apache/apache/#apache","text":"","title":"Apache"},{"location":"notes/apache/apache/#intro","text":"Intro Source: Medium Hypertext Transfer Protocol Secure is usually through port 443 with the unsecured protocol being through port 80. The Apache server is configured via config files in which modules are used to control its behavior. By default, Apache listens to the IP addresses configured in its config files that are being requested. This is where one of Apaches many strengths come into play.","title":"Intro"},{"location":"notes/apache/apache/#flask","text":"Source: codementor.io","title":"Flask"},{"location":"notes/bash/basics/","text":"Basic bash commands Help from the man ;) This small section is to show you that help is on the way with man . Type this prefacing any other command in bash and you can get help on it: $ man ls NAME ls - list directory contents . . . Where are you? Find your location with pwd : bbearce@bbearce-XPS-15-9560:~$ pwd /home/bbearce Look around with ls : bbearce@bbearce-XPS-15-9560:~$ ls check-config.sh gems Slicer-4.10.2-linux-amd64 Desktop Music snap docker_practice pgAdmin4 src Documents Pictures Templates Downloads Public Videos Dropbox (Partners HealthCare) R wget-log examples.desktop seaborn-data -l With -l you can see things listed out vertically bbearce@bbearce-XPS-15-9560:~$ ls -l total 2268 -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 3 bbearce bbearce 4096 Aug 8 09:10 pgAdmin4 drwxr-xr-x 2 bbearce bbearce 4096 Jun 17 15:07 Pictures drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Public drwxrwxr-x 3 bbearce bbearce 4096 May 17 09:02 R drwxrwxr-x 2 bbearce bbearce 4096 Jul 19 17:36 seaborn-data drwxrwxr-x 8 bbearce bbearce 4096 Jun 4 13:06 Slicer-4.10.2-linux-amd64 drwxr-xr-x 8 bbearce bbearce 4096 Jul 25 16:43 snap drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:12 src drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Templates drwxr-xr-x 2 bbearce bbearce 4096 Aug 30 13:55 Videos -rw-rw-r-- 1 bbearce bbearce 2220414 Aug 16 17:05 wget-log The columns are using ls -lai : +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | index number | file permissions | number of links | owner | group | size | month | day | time | filename | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | 933442 | -rwxrw-r-- | 10 | root | root | 2048 | Jan | 13 | 07:11 | afile.exe | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ -a With -a you show hidden files: Hidden files are denoted with a .<file_name> bbearce@bbearce-XPS-15-9560:~$ ls -a . gems .pyenv .. .gitconfig .pylint.d .azure .gksu.lock .python_history .azure-shell .gnome R .bash_history .gnupg .Rhistory .bash_logout .hplip .rstudio-desktop .bashrc .ICEauthority seaborn-data .bundle .ipython Slicer-4.10.2-linux-amd64 .cache .java snap check-config.sh .jupyter .sqlite_history .compiz .kde src .config .lesshst .ssh .DataGrip2019.1 .local .sudo_as_admin_successful Desktop .mozilla .systemtap .dmrc Music Templates .docker .nano Videos docker_practice .node_repl_history .viminfo Documents .pgadmin .vscode Downloads pgAdmin4 .wget-hsts .dropbox Pictures wget-log -la Using both -la : bbearce@bbearce-XPS-15-9560:~$ ls -la total 2564 drwxrwxrwx 50 bbearce bbearce 4096 Sep 19 22:01 . drwxr-xr-x 3 root root 4096 May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 4096 May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 56352 Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 220 May 16 16:13 .bash_logout -rw-r--r-- 1 bbearce bbearce 3865 Jun 20 16:55 .bashrc drwxrwxr-x 4 bbearce bbearce 4096 Jul 10 14:28 .bundle drwx------ 48 bbearce bbearce 4096 Sep 9 13:31 .cache -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwx------ 3 bbearce bbearce 4096 May 16 16:50 .compiz drwx------ 43 bbearce bbearce 4096 Sep 12 11:10 .config drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .DataGrip2019.1 drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop -rw-r--r-- 1 bbearce bbearce 25 May 16 16:49 .dmrc drwxrwx--- 3 bbearce bbearce 4096 Sep 5 17:35 .docker drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 19 22:01 .dropbox drwxrwxr-x 3 bbearce bbearce 4096 Sep 18 03:24 .dropbox-dist drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwx------ 2 bbearce bbearce 4096 Sep 17 06:50 .gconf drwxrwxr-x 4 bbearce bbearce 4096 Jun 20 16:33 .gem drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems -rw-rw-r-- 1 bbearce bbearce 57 May 21 17:11 .gitconfig -rw-r----- 1 bbearce bbearce 0 May 31 12:23 .gksu.lock drwx------ 3 bbearce bbearce 4096 May 17 08:25 .gnome drwx------ 3 bbearce bbearce 4096 Sep 19 22:01 .gnupg drwxr-xr-x 2 bbearce bbearce 4096 Jun 28 10:21 .hplip -rw------- 1 bbearce bbearce 27762 Sep 19 22:01 .ICEauthority drwxr-xr-x 5 bbearce bbearce 4096 Jun 16 18:30 .ipython drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .java drwxrwxr-x 3 bbearce bbearce 4096 Jun 16 22:35 .jupyter drwx------ 3 bbearce bbearce 4096 Jun 9 21:43 .kde -rw------- 1 bbearce bbearce 99 Sep 5 19:01 .lesshst drwx------ 6 bbearce bbearce 4096 Jun 16 18:30 .local drwx------ 5 bbearce bbearce 4096 May 17 08:13 .mozilla drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 2 bbearce bbearce 4096 May 22 15:23 .nano -rw-rw-r-- 1 bbearce bbearce 55 Aug 6 09:48 .node_repl_history --block_size=SIZE SIZE units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). bbearce@bbearce-XPS-15-9560:~$ ls -la --block-size=M total 3M drwxrwxrwx 50 bbearce bbearce 1M Sep 20 09:32 . drwxr-xr-x 3 root root 1M May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 1M May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 1M May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 1M Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 1M May 16 16:13 .bash_logout ... Count files Use wc to count things and get their byte sizes. The -l flag stands for lines and can be used with ls and | to count files in a directory: Ex: user@server:~/$ ls data/rsna | wc -l 26684 sudo, su Notes courtesy of maketecheasier.com Using sudo you can execute code as superuser. It stands for super user do . $ man sudo NAME sudo, sudoedit \u2014 execute a command as another user . . . $ man su NAME su - change user ID or become superuser . . . $ su The su command substitutes the current user in use by the system in the shell. This will tell the system to switch (and essentially log out of) the current user to the one specified.. su is best used when a user wants direct access to the root account on the system. It doesn\u2019t go through sudo or anything like that. It is disabled by default on Ubuntu. It is recommended to use sudo -i in this case. As the note above mentions, this drops you at /root . $ sudo su This command is essentially the same as just running su in the shell. Instead of telling the system to \u201cswitch users\u201d directly, you\u2019re telling it to run the \u201csu\u201d command as root. When sudo su is run, \u201c.profile,\u201d \u201c.bashrc\u201d and \u201c/etc/profile\u201d will be started, much like running su (or su root ). This is because if any command is run with sudo in front of it, it\u2019s a command that is given root privileges. Though there isn\u2019t very much difference from \u201csu,\u201d sudo su is still a very useful command for one important reason: When a user is running \u201csu\u201d to gain root access on a system, they must know the root password. The way root is given with sudo su is by requesting the current user\u2019s password. This makes it possible to gain root without the root password which increases security. $ sudo -i Using sudo -i is virtually the same as the sudo su command. Users can gain root by \u201csudo\u201d and not by switching to the root user. Much like sudo su , the -i flag allows a user to get a root environment without having to know the root account password. sudo -i is also very similar to using sudo su in that it\u2019ll read all of the environmental files (.profile, etc.) and set the environment inside the shell with it. Where it differs from \u201csudo su\u201d is that sudo -i is a much cleaner way of gaining root and a root environment without directly interacting with the root user. How? With sudo su you\u2019re using more than one root setuid commands. This fact makes it much more challenging to figure out what environmental variables will be kept and which ones will be changed (when swamping to the root environment). This is not true with sudo -i , and it is because of this most people view it as the preferred method to gain root without logging in directly. $ sudo -s The -s switch for \u201csudo\u201d command reads the $SHELL variable of the current user executing commands. This command works as if the user is running sudo /bin/bash . sudo -s is a \u201cnon-login\u201d style shell. This means that unlike a command like sudo -i or sudo su , the system will not read any environmental files. This means that when a user tells the shell to run sudo -s , it gains root but will not change the user or the user environment. Your home will not be the root home, etc. This command is best used when the user doesn\u2019t want to touch root at all and just wants a root shell for easy command execution. Other commands talked about above gain root access, but touch root environmental files, and allow users more fuller access to root (which can be a security issue). Summary and demo of pwd when running these commands: bbearce@bbearce-XPS-15-9560:~$ sudo su root@bbearce-XPS-15-9560:/home/bbearce# pwd /home/bbearce root@bbearce-XPS-15-9560:/home/bbearce# exit exit bbearce@bbearce-XPS-15-9560:~$ sudo -i root@bbearce-XPS-15-9560:~# pwd /root root@bbearce-XPS-15-9560:~# exit logout bbearce@bbearce-XPS-15-9560:~$ sudo -s root@bbearce-XPS-15-9560:~# pwd /home/bbearce root@bbearce-XPS-15-9560:~# exit exit btw: exit is for leaving a current logged in session. bash or sh or dash? Notes courtesy of diffzi.com Bash ( bash ) is one of many available (yet the most commonly used) Unix shells. Bash stands for \" B ourne A gain SH ell\", and is a replacement/improvement of the original Bourne shell ( sh ). What is Bash? Bash is the Bourne-Again shell. Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. Bash is designed for human beings and provides a superset of POSIX functionality. What is Dash? Dash is the Debian Almquist Shell. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. Dash is for non-interactive script execution. Dash Only supports POSIX compliant features. Key Differences Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. However, it is rather large and slow to start up and operate by comparison with dash. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. But some of the \u201cbashisms\u201d are convenient, would add little to the size of dash, and would make it far easier to use dash as an alternative. A lot of shell scripts which contain the command set \u2013k are not supported by dash but supported by bash. Bash Supports the same scripting commands as Dash as well as its own additional commands, Dash Only supports POSIX compliant features. Bash is designed for human beings and provides a superset of POSIX functionality, Dash is for non-interactive script execution. Bash supports tab completion and Supports a command history. Dash is only 100K compared to Bash\u2019s 900K. Dash is for Faster start-up and script execution as compared to Bash. Moving and Copying Use the mv command to move things or rename them. Use the cp command to copy things. Use the rsync command to move things. Use the -a flag to archive and retain permissions and time stamps. scp Local to local: $ scp <from> <to> Local to Remote: $ scp [-i identity_file] <from_local> user@remote:<to_remote> Remote to Local: $ scp [-i identity_file] user@remote:<from_remote> <to_local> rsync Local to local: $ rsync <from> <to> Local to Remote: $ rsync [-e \"ssh -i <path to identity_file>\"] <from_local> user@remote:<to_remote> Remote to Local: $ rsync [-e \"ssh -i <path to identity_file>\"] user@remote:<from_remote> <to_local> Most important flags: -a -v -z rsync -avz foo:src/bar /data/tmp This would recursively transfer all files from the directory src/bar on the machine foo into the /data/tmp/bar directory on the local machine. The files are transferred in \"archive\" mode, which ensures that symbolic links, devices, attributes, permissions, ownerships, etc. are preserved in the transfer. Additionally, compression will be used to reduce the size of data portions of the transfer. -a : -a, --archive archive mode; equals -rlptgoD (no -H,-A,-X) -v : -v, --verbose increase verbosity -z : -z, --compress --progress : This option tells rsync to print information showing the progress of the transfer. This gives a bored user something to watch. Check OS Version $ cat /etc/os-release make files Courtesty of stackoverflow make is part of the build system commonly used in unix type systems - binutils . It looks at make files which hold configuration information and build targets. Specifically: ./configure - this is a script that sets up the environment for the build make - calls make with the default build target. Normally builds the app. make install - calls make with the install build target. Normally installs the app. sshfs Courtesy of github About SSHFS allows you to mount a remote filesystem using SFTP. Most SSH servers support and enable this SFTP access by default, so SSHFS is very simple to use - there's nothing to do on the server-side. Development Status SSHFS is shipped by all major Linux distributions and has been in production use across a wide range of systems for many years. However, at present SSHFS does not have any active, regular contributors, and there are a number of known issues (see the bugtracker). The current maintainer continues to apply pull requests and makes regular releases, but unfortunately has no capacity to do any development beyond addressing high-impact issues. When reporting bugs, please understand that unless you are including a pull request or are reporting a critical issue, you will probably not get a response. How to use Once sshfs is installed (see next section) running it is very simple: $ sshfs [user@]hostname:[directory] mountpoint It is recommended to run SSHFS as regular user (not as root). For this to work the mountpoint must be owned by the user. If username is omitted SSHFS will use the local username. If the directory is omitted, SSHFS will mount the (remote) home directory. If you need to enter a password sshfs will ask for it (actually it just runs ssh which ask for the password if needed). Also many ssh options can be specified (see the manual pages for sftp(1) and ssh_config(5)), including the remote port number (-oport=PORT) To unmount the filesystem: fusermount -u mountpoint On BSD and macOS, to unmount the filesystem: umount mountpoint If you get an error: fuse: mountpoint is not empty fuse: if you are sure this is safe, use the 'nonempty' mount option You might need to kill all sshfs processes and restart: ~$ killall sshfs Then restart your sshfs mounting procedure Mouse sensitivity Source: askubuntu Sometimes (happened to me) the mouse UI options in settings are not enough to control the mouse sensitivity. If that is the case then try this:","title":"Basics"},{"location":"notes/bash/basics/#basic-bash-commands","text":"","title":"Basic bash commands"},{"location":"notes/bash/basics/#help-from-the-man","text":"This small section is to show you that help is on the way with man . Type this prefacing any other command in bash and you can get help on it: $ man ls NAME ls - list directory contents . . .","title":"Help from the man ;)"},{"location":"notes/bash/basics/#where-are-you","text":"Find your location with pwd : bbearce@bbearce-XPS-15-9560:~$ pwd /home/bbearce Look around with ls : bbearce@bbearce-XPS-15-9560:~$ ls check-config.sh gems Slicer-4.10.2-linux-amd64 Desktop Music snap docker_practice pgAdmin4 src Documents Pictures Templates Downloads Public Videos Dropbox (Partners HealthCare) R wget-log examples.desktop seaborn-data","title":"Where are you?"},{"location":"notes/bash/basics/#-l","text":"With -l you can see things listed out vertically bbearce@bbearce-XPS-15-9560:~$ ls -l total 2268 -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 3 bbearce bbearce 4096 Aug 8 09:10 pgAdmin4 drwxr-xr-x 2 bbearce bbearce 4096 Jun 17 15:07 Pictures drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Public drwxrwxr-x 3 bbearce bbearce 4096 May 17 09:02 R drwxrwxr-x 2 bbearce bbearce 4096 Jul 19 17:36 seaborn-data drwxrwxr-x 8 bbearce bbearce 4096 Jun 4 13:06 Slicer-4.10.2-linux-amd64 drwxr-xr-x 8 bbearce bbearce 4096 Jul 25 16:43 snap drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:12 src drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Templates drwxr-xr-x 2 bbearce bbearce 4096 Aug 30 13:55 Videos -rw-rw-r-- 1 bbearce bbearce 2220414 Aug 16 17:05 wget-log The columns are using ls -lai : +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | index number | file permissions | number of links | owner | group | size | month | day | time | filename | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | 933442 | -rwxrw-r-- | 10 | root | root | 2048 | Jan | 13 | 07:11 | afile.exe | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+","title":"-l"},{"location":"notes/bash/basics/#-a","text":"With -a you show hidden files: Hidden files are denoted with a .<file_name> bbearce@bbearce-XPS-15-9560:~$ ls -a . gems .pyenv .. .gitconfig .pylint.d .azure .gksu.lock .python_history .azure-shell .gnome R .bash_history .gnupg .Rhistory .bash_logout .hplip .rstudio-desktop .bashrc .ICEauthority seaborn-data .bundle .ipython Slicer-4.10.2-linux-amd64 .cache .java snap check-config.sh .jupyter .sqlite_history .compiz .kde src .config .lesshst .ssh .DataGrip2019.1 .local .sudo_as_admin_successful Desktop .mozilla .systemtap .dmrc Music Templates .docker .nano Videos docker_practice .node_repl_history .viminfo Documents .pgadmin .vscode Downloads pgAdmin4 .wget-hsts .dropbox Pictures wget-log","title":"-a"},{"location":"notes/bash/basics/#-la","text":"Using both -la : bbearce@bbearce-XPS-15-9560:~$ ls -la total 2564 drwxrwxrwx 50 bbearce bbearce 4096 Sep 19 22:01 . drwxr-xr-x 3 root root 4096 May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 4096 May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 56352 Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 220 May 16 16:13 .bash_logout -rw-r--r-- 1 bbearce bbearce 3865 Jun 20 16:55 .bashrc drwxrwxr-x 4 bbearce bbearce 4096 Jul 10 14:28 .bundle drwx------ 48 bbearce bbearce 4096 Sep 9 13:31 .cache -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwx------ 3 bbearce bbearce 4096 May 16 16:50 .compiz drwx------ 43 bbearce bbearce 4096 Sep 12 11:10 .config drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .DataGrip2019.1 drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop -rw-r--r-- 1 bbearce bbearce 25 May 16 16:49 .dmrc drwxrwx--- 3 bbearce bbearce 4096 Sep 5 17:35 .docker drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 19 22:01 .dropbox drwxrwxr-x 3 bbearce bbearce 4096 Sep 18 03:24 .dropbox-dist drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwx------ 2 bbearce bbearce 4096 Sep 17 06:50 .gconf drwxrwxr-x 4 bbearce bbearce 4096 Jun 20 16:33 .gem drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems -rw-rw-r-- 1 bbearce bbearce 57 May 21 17:11 .gitconfig -rw-r----- 1 bbearce bbearce 0 May 31 12:23 .gksu.lock drwx------ 3 bbearce bbearce 4096 May 17 08:25 .gnome drwx------ 3 bbearce bbearce 4096 Sep 19 22:01 .gnupg drwxr-xr-x 2 bbearce bbearce 4096 Jun 28 10:21 .hplip -rw------- 1 bbearce bbearce 27762 Sep 19 22:01 .ICEauthority drwxr-xr-x 5 bbearce bbearce 4096 Jun 16 18:30 .ipython drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .java drwxrwxr-x 3 bbearce bbearce 4096 Jun 16 22:35 .jupyter drwx------ 3 bbearce bbearce 4096 Jun 9 21:43 .kde -rw------- 1 bbearce bbearce 99 Sep 5 19:01 .lesshst drwx------ 6 bbearce bbearce 4096 Jun 16 18:30 .local drwx------ 5 bbearce bbearce 4096 May 17 08:13 .mozilla drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 2 bbearce bbearce 4096 May 22 15:23 .nano -rw-rw-r-- 1 bbearce bbearce 55 Aug 6 09:48 .node_repl_history","title":"-la"},{"location":"notes/bash/basics/#-block_sizesize","text":"SIZE units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). bbearce@bbearce-XPS-15-9560:~$ ls -la --block-size=M total 3M drwxrwxrwx 50 bbearce bbearce 1M Sep 20 09:32 . drwxr-xr-x 3 root root 1M May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 1M May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 1M May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 1M Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 1M May 16 16:13 .bash_logout ...","title":"--block_size=SIZE"},{"location":"notes/bash/basics/#count-files","text":"Use wc to count things and get their byte sizes. The -l flag stands for lines and can be used with ls and | to count files in a directory: Ex: user@server:~/$ ls data/rsna | wc -l 26684","title":"Count files"},{"location":"notes/bash/basics/#sudo-su","text":"Notes courtesy of maketecheasier.com Using sudo you can execute code as superuser. It stands for super user do . $ man sudo NAME sudo, sudoedit \u2014 execute a command as another user . . . $ man su NAME su - change user ID or become superuser . . .","title":"sudo, su"},{"location":"notes/bash/basics/#su","text":"The su command substitutes the current user in use by the system in the shell. This will tell the system to switch (and essentially log out of) the current user to the one specified.. su is best used when a user wants direct access to the root account on the system. It doesn\u2019t go through sudo or anything like that. It is disabled by default on Ubuntu. It is recommended to use sudo -i in this case. As the note above mentions, this drops you at /root .","title":"$ su"},{"location":"notes/bash/basics/#sudo-su_1","text":"This command is essentially the same as just running su in the shell. Instead of telling the system to \u201cswitch users\u201d directly, you\u2019re telling it to run the \u201csu\u201d command as root. When sudo su is run, \u201c.profile,\u201d \u201c.bashrc\u201d and \u201c/etc/profile\u201d will be started, much like running su (or su root ). This is because if any command is run with sudo in front of it, it\u2019s a command that is given root privileges. Though there isn\u2019t very much difference from \u201csu,\u201d sudo su is still a very useful command for one important reason: When a user is running \u201csu\u201d to gain root access on a system, they must know the root password. The way root is given with sudo su is by requesting the current user\u2019s password. This makes it possible to gain root without the root password which increases security.","title":"$ sudo su"},{"location":"notes/bash/basics/#sudo-i","text":"Using sudo -i is virtually the same as the sudo su command. Users can gain root by \u201csudo\u201d and not by switching to the root user. Much like sudo su , the -i flag allows a user to get a root environment without having to know the root account password. sudo -i is also very similar to using sudo su in that it\u2019ll read all of the environmental files (.profile, etc.) and set the environment inside the shell with it. Where it differs from \u201csudo su\u201d is that sudo -i is a much cleaner way of gaining root and a root environment without directly interacting with the root user. How? With sudo su you\u2019re using more than one root setuid commands. This fact makes it much more challenging to figure out what environmental variables will be kept and which ones will be changed (when swamping to the root environment). This is not true with sudo -i , and it is because of this most people view it as the preferred method to gain root without logging in directly.","title":"$ sudo -i"},{"location":"notes/bash/basics/#sudo-s","text":"The -s switch for \u201csudo\u201d command reads the $SHELL variable of the current user executing commands. This command works as if the user is running sudo /bin/bash . sudo -s is a \u201cnon-login\u201d style shell. This means that unlike a command like sudo -i or sudo su , the system will not read any environmental files. This means that when a user tells the shell to run sudo -s , it gains root but will not change the user or the user environment. Your home will not be the root home, etc. This command is best used when the user doesn\u2019t want to touch root at all and just wants a root shell for easy command execution. Other commands talked about above gain root access, but touch root environmental files, and allow users more fuller access to root (which can be a security issue).","title":"$ sudo -s"},{"location":"notes/bash/basics/#summary-and-demo-of-pwd-when-running-these-commands","text":"bbearce@bbearce-XPS-15-9560:~$ sudo su root@bbearce-XPS-15-9560:/home/bbearce# pwd /home/bbearce root@bbearce-XPS-15-9560:/home/bbearce# exit exit bbearce@bbearce-XPS-15-9560:~$ sudo -i root@bbearce-XPS-15-9560:~# pwd /root root@bbearce-XPS-15-9560:~# exit logout bbearce@bbearce-XPS-15-9560:~$ sudo -s root@bbearce-XPS-15-9560:~# pwd /home/bbearce root@bbearce-XPS-15-9560:~# exit exit btw: exit is for leaving a current logged in session.","title":"Summary and demo of pwd when running these commands:"},{"location":"notes/bash/basics/#bash-or-sh-or-dash","text":"Notes courtesy of diffzi.com Bash ( bash ) is one of many available (yet the most commonly used) Unix shells. Bash stands for \" B ourne A gain SH ell\", and is a replacement/improvement of the original Bourne shell ( sh ).","title":"bash or sh or dash?"},{"location":"notes/bash/basics/#what-is-bash","text":"Bash is the Bourne-Again shell. Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. Bash is designed for human beings and provides a superset of POSIX functionality.","title":"What is Bash?"},{"location":"notes/bash/basics/#what-is-dash","text":"Dash is the Debian Almquist Shell. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. Dash is for non-interactive script execution. Dash Only supports POSIX compliant features.","title":"What is Dash?"},{"location":"notes/bash/basics/#key-differences","text":"Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. However, it is rather large and slow to start up and operate by comparison with dash. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. But some of the \u201cbashisms\u201d are convenient, would add little to the size of dash, and would make it far easier to use dash as an alternative. A lot of shell scripts which contain the command set \u2013k are not supported by dash but supported by bash. Bash Supports the same scripting commands as Dash as well as its own additional commands, Dash Only supports POSIX compliant features. Bash is designed for human beings and provides a superset of POSIX functionality, Dash is for non-interactive script execution. Bash supports tab completion and Supports a command history. Dash is only 100K compared to Bash\u2019s 900K. Dash is for Faster start-up and script execution as compared to Bash.","title":"Key Differences"},{"location":"notes/bash/basics/#moving-and-copying","text":"Use the mv command to move things or rename them. Use the cp command to copy things. Use the rsync command to move things. Use the -a flag to archive and retain permissions and time stamps.","title":"Moving and Copying"},{"location":"notes/bash/basics/#scp","text":"Local to local: $ scp <from> <to> Local to Remote: $ scp [-i identity_file] <from_local> user@remote:<to_remote> Remote to Local: $ scp [-i identity_file] user@remote:<from_remote> <to_local>","title":"scp"},{"location":"notes/bash/basics/#rsync","text":"Local to local: $ rsync <from> <to> Local to Remote: $ rsync [-e \"ssh -i <path to identity_file>\"] <from_local> user@remote:<to_remote> Remote to Local: $ rsync [-e \"ssh -i <path to identity_file>\"] user@remote:<from_remote> <to_local> Most important flags: -a -v -z rsync -avz foo:src/bar /data/tmp This would recursively transfer all files from the directory src/bar on the machine foo into the /data/tmp/bar directory on the local machine. The files are transferred in \"archive\" mode, which ensures that symbolic links, devices, attributes, permissions, ownerships, etc. are preserved in the transfer. Additionally, compression will be used to reduce the size of data portions of the transfer. -a : -a, --archive archive mode; equals -rlptgoD (no -H,-A,-X) -v : -v, --verbose increase verbosity -z : -z, --compress --progress : This option tells rsync to print information showing the progress of the transfer. This gives a bored user something to watch.","title":"rsync"},{"location":"notes/bash/basics/#check-os-version","text":"$ cat /etc/os-release","title":"Check OS Version"},{"location":"notes/bash/basics/#make-files","text":"Courtesty of stackoverflow make is part of the build system commonly used in unix type systems - binutils . It looks at make files which hold configuration information and build targets. Specifically: ./configure - this is a script that sets up the environment for the build make - calls make with the default build target. Normally builds the app. make install - calls make with the install build target. Normally installs the app.","title":"make files"},{"location":"notes/bash/basics/#sshfs","text":"Courtesy of github","title":"sshfs"},{"location":"notes/bash/basics/#about","text":"SSHFS allows you to mount a remote filesystem using SFTP. Most SSH servers support and enable this SFTP access by default, so SSHFS is very simple to use - there's nothing to do on the server-side.","title":"About"},{"location":"notes/bash/basics/#development-status","text":"SSHFS is shipped by all major Linux distributions and has been in production use across a wide range of systems for many years. However, at present SSHFS does not have any active, regular contributors, and there are a number of known issues (see the bugtracker). The current maintainer continues to apply pull requests and makes regular releases, but unfortunately has no capacity to do any development beyond addressing high-impact issues. When reporting bugs, please understand that unless you are including a pull request or are reporting a critical issue, you will probably not get a response.","title":"Development Status"},{"location":"notes/bash/basics/#how-to-use","text":"Once sshfs is installed (see next section) running it is very simple: $ sshfs [user@]hostname:[directory] mountpoint It is recommended to run SSHFS as regular user (not as root). For this to work the mountpoint must be owned by the user. If username is omitted SSHFS will use the local username. If the directory is omitted, SSHFS will mount the (remote) home directory. If you need to enter a password sshfs will ask for it (actually it just runs ssh which ask for the password if needed). Also many ssh options can be specified (see the manual pages for sftp(1) and ssh_config(5)), including the remote port number (-oport=PORT) To unmount the filesystem: fusermount -u mountpoint On BSD and macOS, to unmount the filesystem: umount mountpoint If you get an error: fuse: mountpoint is not empty fuse: if you are sure this is safe, use the 'nonempty' mount option You might need to kill all sshfs processes and restart: ~$ killall sshfs Then restart your sshfs mounting procedure","title":"How to use"},{"location":"notes/bash/basics/#mouse-sensitivity","text":"Source: askubuntu Sometimes (happened to me) the mouse UI options in settings are not enough to control the mouse sensitivity. If that is the case then try this:","title":"Mouse sensitivity"},{"location":"notes/bash/environment_variables/","text":"Environment Variables $PATH Courtesy of smallbusiness.chron.com Ubuntu Linux, as well as all other Linux distributions, uses the PATH variable to tell the operating system where to look for executable commands. Typically these commands are located in the /usr/sbin, usr/bin and /sbin, and /bin directories. Other command directories can be added to this list of directories by adding them to the PATH variable. You can choose to make a user-specified directory available for a single user or the entire system, depending on your company's requirements. Add to Path Add a path to the PATH variable: export PATH=$PATH:/my/custom/path This appends your new path to the old path and overwrites PATH. Remove from Path Execute echo $PATH to see full path variable and copy it to clipboard. Then remove a specific path between colons ...:<some path>:<path to be removed>:... and paste that into this command: export PATH=<new full path minus the path you do not want> Note thatwe aren't appending to $PATH this time, but overwriting it.","title":"Environment Variables"},{"location":"notes/bash/environment_variables/#environment-variables","text":"","title":"Environment Variables"},{"location":"notes/bash/environment_variables/#path","text":"Courtesy of smallbusiness.chron.com Ubuntu Linux, as well as all other Linux distributions, uses the PATH variable to tell the operating system where to look for executable commands. Typically these commands are located in the /usr/sbin, usr/bin and /sbin, and /bin directories. Other command directories can be added to this list of directories by adding them to the PATH variable. You can choose to make a user-specified directory available for a single user or the entire system, depending on your company's requirements.","title":"$PATH"},{"location":"notes/bash/environment_variables/#add-to-path","text":"Add a path to the PATH variable: export PATH=$PATH:/my/custom/path This appends your new path to the old path and overwrites PATH.","title":"Add to Path"},{"location":"notes/bash/environment_variables/#remove-from-path","text":"Execute echo $PATH to see full path variable and copy it to clipboard. Then remove a specific path between colons ...:<some path>:<path to be removed>:... and paste that into this command: export PATH=<new full path minus the path you do not want> Note thatwe aren't appending to $PATH this time, but overwriting it.","title":"Remove from Path"},{"location":"notes/bash/mount/","text":"Mount Courtesy of Linuxize Example, to mount the /dev/sdb1 file system to the /mnt/media directory you would use: sudo mount /dev/sdb1 /mnt/media","title":"Mount"},{"location":"notes/bash/mount/#mount","text":"Courtesy of Linuxize Example, to mount the /dev/sdb1 file system to the /mnt/media directory you would use: sudo mount /dev/sdb1 /mnt/media","title":"Mount"},{"location":"notes/bash/pdfs/","text":"PDFs Command line utility pdftk is used to create pdfs. Install: sudo apt instal pdftk Use: pdftk page1.pdf page2.pdf output mergedfile.pdf","title":"PDFs"},{"location":"notes/bash/pdfs/#pdfs","text":"Command line utility pdftk is used to create pdfs. Install: sudo apt instal pdftk Use: pdftk page1.pdf page2.pdf output mergedfile.pdf","title":"PDFs"},{"location":"notes/bash/processes/","text":"Processes Kill a process (TL;DR) $ kill <pid> Ex: $ kill 1234 By Port Source mr-khan.gitlab.io Sometimes I fed up with searching my program PID. As you know the port number so you can easily find the port PID and kill it. If you want to kill a process running on port number 8000 then first you need to find the PID and then kill it. Run the following command to find port number PID: sudo lsof -t -i:8000 then kill: sudo kill $(sudo lsof -t -i:8000) Courtesy of booleanworld.com ...more...below: Use top to see running processes. azureuser@miccai:~$ top top - 19:14:36 up 28 min, 1 user, load average: 0.00, 0.02, 0.05 Tasks: 115 total, 1 running, 114 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 3514568 total, 715812 used, 2798756 free, 32776 buffers KiB Swap: 0 total, 0 used, 0 free. 336792 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.10 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:00.19 kworker/u256:0 7 root 20 0 0 0 0 S 0.0 0.0 0:00.90 rcu_sched 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:00.99 rcuos/0 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/0 11 root rt 0 0 0 0 S 0.0 0.0 0:00.07 migration/0 12 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/0 13 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/1 14 root rt 0 0 0 0 S 0.0 0.0 0:00.03 migration/1 15 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/1 17 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/1:0H 18 root 20 0 0 0 0 S 0.0 0.0 0:00.42 rcuos/1 19 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/1 20 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 khelper 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 22 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 netns 23 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 perf 24 root 20 0 0 0 0 S 0.0 0.0 0:00.00 khungtaskd 25 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 writeback 26 root 25 5 0 0 0 S 0.0 0.0 0:00.00 ksmd 27 root 39 19 0 0 0 S 0.0 0.0 0:00.03 khugepaged 28 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 crypto 29 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kintegrityd 30 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 bioset 31 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kblockd ... ... ... The other option is $ ps aux . This will give the command used to run the process (this can sometimes be more helpful). azureuser@miccai:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.1 35080 5100 ? Ss 18:46 0:04 /sbin/init root 2 0.0 0.0 0 0 ? S 18:46 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 18:46 0:00 [ksoftirqd/0] root 5 0.0 0.0 0 0 ? S< 18:46 0:00 [kworker/0:0H] root 6 0.0 0.0 0 0 ? S 18:46 0:00 [kworker/u256:0] root 7 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_sched] root 8 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_bh] root 9 0.0 0.0 0 0 ? S 18:46 0:01 [rcuos/0] root 10 0.0 0.0 0 0 ? S 18:46 0:00 [rcuob/0] root 11 0.0 0.0 0 0 ? S 18:46 0:00 [migration/0] root 12 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/0] root 13 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/1] root 14 0.0 0.0 0 0 ? S 18:46 0:00 [migration/1] ... ... ... The advantage of using ps is that you can easily filter this list with the grep command. For example, to find a process associated with the term \"SCREEN\", you can use: azureuser@miccai:~$ ps aux | grep -i SCREEN azureus+ 1813 0.0 0.0 26104 2756 ? Ss 19:02 0:00 SCREEN -S mysql azureus+ 2058 0.0 0.0 8212 2148 pts/0 S+ 19:17 0:00 grep --color=auto -i SCREEN Thus, even when there are no \u201cvnstat\u201d related processes running, we would get one entry showing the grep process: azureuser@miccai:~$ ps aux | grep -i \"vnstat\" azureus+ 2070 0.0 0.0 8212 2212 pts/0 S+ 19:18 0:00 grep --color=auto -i vnstat Killing a process: There are various commands you can use to kill a process \u2014 kill , killall , pkill and top . We will begin from the simplest one: the killall command. killall Killing processes with the killall command: The killall command is one of the easiest ways to kill a process. If you know the exact name of a process, and you know that it\u2019s not running as another user and it is not in the Z or D states, then you can use this command directly; there\u2019s no need to manually locate the process as we described above. By default, For example, to kill a process named \u201cfirefox\u201d, run: $ killall firefox To forcibly kill the process with SIGKILL , run: $ killall -9 firefox You can also use -SIGKILL instead of -9 . If you want to kill processes interactively, you can use -i like so: $ killall -i firefox If you want to kill a process running as a different user, you can use sudo : $ sudo killall firefox You can also kill a process that has been running for a certain period of time with the -o and -y flags. So, if you want to kill a process that has been running for more than 30 minutes, use: $ killall -o 30m <process-name> If you want to kill a process that has been running for less than 30 minutes, use: $ killall -y 30m <process-name> Similarly, use the following abbreviations for the respective units of time: s seconds m minutes h hours d days w weeks M months y years pkill Killing processes with the pkill command Sometimes, you only know part of a program\u2019s name. Just like pgrep , pkill allows you to kill processes based on partial matches. For example, if you want to kill all processes containing the name apache in the name, run: pkill apache If you want to use a SIGKILL instead of a SIGTERM , use: pkill -9 apache Again, you can also use -SIGKILL instead of -9 . kill Killing processes with the kill command: Using the kill command is straightforward. Once you have found out the PID of the process that you want to kill , you can terminate it using the kill command. For example, if you want to kill a process having a PID of 1234, then use the following command: kill 1234 As we mentioned previously, the default is to use a SIGTERM . To use a SIGKILL , use -9 or -SIGKILL as we have seen before: kill -9 1234 Using top Killing processes with the top command: It is very easy to kill processes using the top command. First, search for the process that you want to kill and note the PID. Then, press k while top is running (this is case sensitive). It will prompt you to enter the PID of the process that you want to kill.","title":"Processes"},{"location":"notes/bash/processes/#processes","text":"","title":"Processes"},{"location":"notes/bash/processes/#kill-a-process-tldr","text":"$ kill <pid> Ex: $ kill 1234","title":"Kill a process (TL;DR)"},{"location":"notes/bash/processes/#by-port","text":"Source mr-khan.gitlab.io Sometimes I fed up with searching my program PID. As you know the port number so you can easily find the port PID and kill it. If you want to kill a process running on port number 8000 then first you need to find the PID and then kill it. Run the following command to find port number PID: sudo lsof -t -i:8000 then kill: sudo kill $(sudo lsof -t -i:8000) Courtesy of booleanworld.com ...more...below: Use top to see running processes. azureuser@miccai:~$ top top - 19:14:36 up 28 min, 1 user, load average: 0.00, 0.02, 0.05 Tasks: 115 total, 1 running, 114 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 3514568 total, 715812 used, 2798756 free, 32776 buffers KiB Swap: 0 total, 0 used, 0 free. 336792 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.10 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:00.19 kworker/u256:0 7 root 20 0 0 0 0 S 0.0 0.0 0:00.90 rcu_sched 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:00.99 rcuos/0 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/0 11 root rt 0 0 0 0 S 0.0 0.0 0:00.07 migration/0 12 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/0 13 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/1 14 root rt 0 0 0 0 S 0.0 0.0 0:00.03 migration/1 15 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/1 17 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/1:0H 18 root 20 0 0 0 0 S 0.0 0.0 0:00.42 rcuos/1 19 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/1 20 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 khelper 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 22 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 netns 23 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 perf 24 root 20 0 0 0 0 S 0.0 0.0 0:00.00 khungtaskd 25 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 writeback 26 root 25 5 0 0 0 S 0.0 0.0 0:00.00 ksmd 27 root 39 19 0 0 0 S 0.0 0.0 0:00.03 khugepaged 28 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 crypto 29 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kintegrityd 30 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 bioset 31 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kblockd ... ... ... The other option is $ ps aux . This will give the command used to run the process (this can sometimes be more helpful). azureuser@miccai:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.1 35080 5100 ? Ss 18:46 0:04 /sbin/init root 2 0.0 0.0 0 0 ? S 18:46 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 18:46 0:00 [ksoftirqd/0] root 5 0.0 0.0 0 0 ? S< 18:46 0:00 [kworker/0:0H] root 6 0.0 0.0 0 0 ? S 18:46 0:00 [kworker/u256:0] root 7 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_sched] root 8 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_bh] root 9 0.0 0.0 0 0 ? S 18:46 0:01 [rcuos/0] root 10 0.0 0.0 0 0 ? S 18:46 0:00 [rcuob/0] root 11 0.0 0.0 0 0 ? S 18:46 0:00 [migration/0] root 12 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/0] root 13 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/1] root 14 0.0 0.0 0 0 ? S 18:46 0:00 [migration/1] ... ... ... The advantage of using ps is that you can easily filter this list with the grep command. For example, to find a process associated with the term \"SCREEN\", you can use: azureuser@miccai:~$ ps aux | grep -i SCREEN azureus+ 1813 0.0 0.0 26104 2756 ? Ss 19:02 0:00 SCREEN -S mysql azureus+ 2058 0.0 0.0 8212 2148 pts/0 S+ 19:17 0:00 grep --color=auto -i SCREEN Thus, even when there are no \u201cvnstat\u201d related processes running, we would get one entry showing the grep process: azureuser@miccai:~$ ps aux | grep -i \"vnstat\" azureus+ 2070 0.0 0.0 8212 2212 pts/0 S+ 19:18 0:00 grep --color=auto -i vnstat Killing a process: There are various commands you can use to kill a process \u2014 kill , killall , pkill and top . We will begin from the simplest one: the killall command.","title":"By Port"},{"location":"notes/bash/processes/#killall","text":"Killing processes with the killall command: The killall command is one of the easiest ways to kill a process. If you know the exact name of a process, and you know that it\u2019s not running as another user and it is not in the Z or D states, then you can use this command directly; there\u2019s no need to manually locate the process as we described above. By default, For example, to kill a process named \u201cfirefox\u201d, run: $ killall firefox To forcibly kill the process with SIGKILL , run: $ killall -9 firefox You can also use -SIGKILL instead of -9 . If you want to kill processes interactively, you can use -i like so: $ killall -i firefox If you want to kill a process running as a different user, you can use sudo : $ sudo killall firefox You can also kill a process that has been running for a certain period of time with the -o and -y flags. So, if you want to kill a process that has been running for more than 30 minutes, use: $ killall -o 30m <process-name> If you want to kill a process that has been running for less than 30 minutes, use: $ killall -y 30m <process-name> Similarly, use the following abbreviations for the respective units of time: s seconds m minutes h hours d days w weeks M months y years","title":"killall"},{"location":"notes/bash/processes/#pkill","text":"Killing processes with the pkill command Sometimes, you only know part of a program\u2019s name. Just like pgrep , pkill allows you to kill processes based on partial matches. For example, if you want to kill all processes containing the name apache in the name, run: pkill apache If you want to use a SIGKILL instead of a SIGTERM , use: pkill -9 apache Again, you can also use -SIGKILL instead of -9 .","title":"pkill"},{"location":"notes/bash/processes/#kill","text":"Killing processes with the kill command: Using the kill command is straightforward. Once you have found out the PID of the process that you want to kill , you can terminate it using the kill command. For example, if you want to kill a process having a PID of 1234, then use the following command: kill 1234 As we mentioned previously, the default is to use a SIGTERM . To use a SIGKILL , use -9 or -SIGKILL as we have seen before: kill -9 1234","title":"kill"},{"location":"notes/bash/processes/#using-top","text":"Killing processes with the top command: It is very easy to kill processes using the top command. First, search for the process that you want to kill and note the PID. Then, press k while top is running (this is case sensitive). It will prompt you to enter the PID of the process that you want to kill.","title":"Using top"},{"location":"notes/bash/screen/","text":"Screen Courtesy of linuxize.com and this video by Linux Leech . Basics: C-a means Control+a and seems to be the basis of most commands. C-a ? Means to press control+a and then the ? for help about other command for screen. Start a new screen with the word screen . Name a screen session with screen \u2013S secondscreen Rename Screen: To list running screen sessions, use screen -ls $screen -ls There is a screen on: 12129.testsession (Detached) 1 Socket in /var/run/screen/S-root. Yeah, here we go!! Renaming the screen session name testsession to something else. Here is the command to rename the existing session. Note: sessionname as used below is a command so it is always necessary $ screen -S 12129.testsession -X sessionname newname C-a d is to detach. Once you detach you can see all screens with screen \u2013ls Now connect to a screen... bbearce@bbearce-XPS-15-9560:~$ screen -ls There are screens on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 370.pts-4.bbearce-XPS-15-9560 (05/28/2019 03:24:18 PM) (Detached) 2 Sockets in /var/run/screen/S-bbearce. Connect to either with the name of the screen or the PID (prefacing numbers {530, 370}) To get rid of a screen: bbearce@bbearce-XPS-15-9560:~$ screen -X -S 370 quit The \u2013X is for sending a command to a screen and \u2013S is to identify the name of the screen to send the command. The command is quit . Now use screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls There is a screen on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 1 Socket in /var/run/screen/S-bbearce. The other way to kill a screen is from within it. Keep in mind this is technically for windows and not screens, but will kill a screen if there is only 1 window C-a k This will prompt you for whether or not you are sure. (y/n) Now screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls No Sockets found in /var/run/screen/S-bbearce. Windows: Once in a screen use C-a c to create a new window. C-a n is for next C-a p is for previous C-a w is for listing windows C-a \" is for showing a menu of windows Don't forget C-a k will kill a window and eventually the screen if there is only one window. If you make 4 screens and echo 0-3 in them, we can jump to each with these commands: C-a 0 will jump to the first window with echo 0 in in it C-a 1 will jump to the first window with echo 1 in in it C-a 2 will jump to the first window with echo 2 in in it C-a 3 will jump to the first window with echo 3 in in it C-a \" will show them all and notice they are all named bash. We can rename them to be more useful. Ex: Num Name 0 bash 1 bash 2 bash 3 bash If you press C-a A we can rename our windows. Notice what happens during C-a \" now after renaming: Ex: Num Name 0 bash 1 bash 2 window-2 3 bash Panes: C-a | will split the window vertically C-a S will split the window horizontally C-a tab to change panes C-a X to exit panes C-a x to lock the terminal\\screen - you will need a password to get back in. C-a t to to get the time and load on the system Tab over to a new pane that is empty and open a window with the general window commands. C-a X will close a pane as well as performing C-a : which will bring up a prompt starting with : . At the prompt type remove and press enter. This removes the pane as well. Run Commands with Screen: Use VI to make counter.py file as such: $ vi counter.py ...write the code below import time for i in range(5): print(time.ctime(time.time())) time.sleep(1) Now we can run this program in a screen but will kill the screen when complete $ screen -d -m counter.py You can see the screen momentarily before it quits by running screen \u2013r . Also we can run this in a screen and not have it automatically quit by connecting first. Problems There is no screen to be resumed matching <screen-name> azureuser@cbibop3:~$ screen -r codalab There is a screen on: 8967.codalab (10/18/2019 06:56:52 PM) (Attached) There is no screen to be resumed matching codalab. As screen -r says, there is one screen, but it is attached. To resume it on your current terminal, you have to detach it from the other one first: screen -d -r 8967 , see manpage -d . Edit: use -d instead of -x . Edit2: @alex78191: When using -x , screen attaches to the currently running session, resulting in a \"multi-display mode\": you see the session on both terminals simultaneously, i.e., when entering a command on one terminal, it also appears on the second. However, detaching from a multi-display mode just detaches the current terminal. You hence get the message that it is still attached (on the other terminal). Resize Region gnu Find section \"resize\" under \"9.5 - Regions\" The amount of lines to add or remove can be expressed a couple of different ways. By specifying a number n by itself will resize the region by that absolute amount. You can specify a relative amount by prefixing a plus \u2018+\u2019 or minus \u2018-\u2019 to the amount, such as adding +n lines or removing -n lines. Resizing can also be expressed as an absolute or relative percentage by postfixing a percent sign \u2018%\u2019. Using zero \u20180\u2019 is a synonym for min and using an underscore \u2018_\u2019 is a synonym for max. Some examples are: resize +N increase current region by N resize -N decrease current region by N resize N set current region to N resize 20% set current region to 20% of original size resize +20% increase current region by 20% resize -b = make all windows equally resize max maximize current region resize min minimize current region Without any arguments, screen will prompt for how you would like to resize the current region.","title":"Screen"},{"location":"notes/bash/screen/#screen","text":"Courtesy of linuxize.com and this video by Linux Leech .","title":"Screen"},{"location":"notes/bash/screen/#basics","text":"C-a means Control+a and seems to be the basis of most commands. C-a ? Means to press control+a and then the ? for help about other command for screen. Start a new screen with the word screen . Name a screen session with screen \u2013S secondscreen","title":"Basics:"},{"location":"notes/bash/screen/#rename-screen","text":"To list running screen sessions, use screen -ls $screen -ls There is a screen on: 12129.testsession (Detached) 1 Socket in /var/run/screen/S-root. Yeah, here we go!! Renaming the screen session name testsession to something else. Here is the command to rename the existing session. Note: sessionname as used below is a command so it is always necessary $ screen -S 12129.testsession -X sessionname newname C-a d is to detach. Once you detach you can see all screens with screen \u2013ls Now connect to a screen... bbearce@bbearce-XPS-15-9560:~$ screen -ls There are screens on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 370.pts-4.bbearce-XPS-15-9560 (05/28/2019 03:24:18 PM) (Detached) 2 Sockets in /var/run/screen/S-bbearce. Connect to either with the name of the screen or the PID (prefacing numbers {530, 370})","title":"Rename Screen:"},{"location":"notes/bash/screen/#to-get-rid-of-a-screen","text":"bbearce@bbearce-XPS-15-9560:~$ screen -X -S 370 quit The \u2013X is for sending a command to a screen and \u2013S is to identify the name of the screen to send the command. The command is quit . Now use screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls There is a screen on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 1 Socket in /var/run/screen/S-bbearce. The other way to kill a screen is from within it. Keep in mind this is technically for windows and not screens, but will kill a screen if there is only 1 window C-a k This will prompt you for whether or not you are sure. (y/n) Now screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls No Sockets found in /var/run/screen/S-bbearce.","title":"To get rid of a screen:"},{"location":"notes/bash/screen/#windows","text":"Once in a screen use C-a c to create a new window. C-a n is for next C-a p is for previous C-a w is for listing windows C-a \" is for showing a menu of windows Don't forget C-a k will kill a window and eventually the screen if there is only one window. If you make 4 screens and echo 0-3 in them, we can jump to each with these commands: C-a 0 will jump to the first window with echo 0 in in it C-a 1 will jump to the first window with echo 1 in in it C-a 2 will jump to the first window with echo 2 in in it C-a 3 will jump to the first window with echo 3 in in it C-a \" will show them all and notice they are all named bash. We can rename them to be more useful. Ex: Num Name 0 bash 1 bash 2 bash 3 bash If you press C-a A we can rename our windows. Notice what happens during C-a \" now after renaming: Ex: Num Name 0 bash 1 bash 2 window-2 3 bash","title":"Windows:"},{"location":"notes/bash/screen/#panes","text":"C-a | will split the window vertically C-a S will split the window horizontally C-a tab to change panes C-a X to exit panes C-a x to lock the terminal\\screen - you will need a password to get back in. C-a t to to get the time and load on the system Tab over to a new pane that is empty and open a window with the general window commands. C-a X will close a pane as well as performing C-a : which will bring up a prompt starting with : . At the prompt type remove and press enter. This removes the pane as well.","title":"Panes:"},{"location":"notes/bash/screen/#run-commands-with-screen","text":"Use VI to make counter.py file as such: $ vi counter.py ...write the code below import time for i in range(5): print(time.ctime(time.time())) time.sleep(1) Now we can run this program in a screen but will kill the screen when complete $ screen -d -m counter.py You can see the screen momentarily before it quits by running screen \u2013r . Also we can run this in a screen and not have it automatically quit by connecting first.","title":"Run Commands with Screen:"},{"location":"notes/bash/screen/#problems","text":"There is no screen to be resumed matching <screen-name> azureuser@cbibop3:~$ screen -r codalab There is a screen on: 8967.codalab (10/18/2019 06:56:52 PM) (Attached) There is no screen to be resumed matching codalab. As screen -r says, there is one screen, but it is attached. To resume it on your current terminal, you have to detach it from the other one first: screen -d -r 8967 , see manpage -d . Edit: use -d instead of -x . Edit2: @alex78191: When using -x , screen attaches to the currently running session, resulting in a \"multi-display mode\": you see the session on both terminals simultaneously, i.e., when entering a command on one terminal, it also appears on the second. However, detaching from a multi-display mode just detaches the current terminal. You hence get the message that it is still attached (on the other terminal).","title":"Problems"},{"location":"notes/bash/screen/#resize-region","text":"gnu Find section \"resize\" under \"9.5 - Regions\" The amount of lines to add or remove can be expressed a couple of different ways. By specifying a number n by itself will resize the region by that absolute amount. You can specify a relative amount by prefixing a plus \u2018+\u2019 or minus \u2018-\u2019 to the amount, such as adding +n lines or removing -n lines. Resizing can also be expressed as an absolute or relative percentage by postfixing a percent sign \u2018%\u2019. Using zero \u20180\u2019 is a synonym for min and using an underscore \u2018_\u2019 is a synonym for max. Some examples are: resize +N increase current region by N resize -N decrease current region by N resize N set current region to N resize 20% set current region to 20% of original size resize +20% increase current region by 20% resize -b = make all windows equally resize max maximize current region resize min minimize current region Without any arguments, screen will prompt for how you would like to resize the current region.","title":"Resize Region"},{"location":"notes/bash/sed/","text":"sed Search grep is likely better $ sed -n \"/pattern/p\" file.txt $ cat test.txt asdfasdf fjfjfjfjfj aaaaa $ sed -n \"/a/p\" test.txt asdfasdf aaaaa Find and Replate g is for global or replace all $ sed -i -e \"s/pattern_to_find/replace_string/\" file.txt $ sed -i -e \"s/pattern_to_find/replace_string/g\" file.txt Flags: -i[SUFFIX], --in-place[=SUFFIX] edit files in place (makes backup if SUFFIX supplied) -e script, --expression=script add the script to the commands to be executed Ex: $ cat deleteme.txt abc abc abc $ sed -i -e \"s/abc/XYZ/g\" deleteme.txt $ cat deleteme.txt XYZ XYZ XYZ Find Line Source $ cat test.txt asdfasdf fjfjfjfjfj aaaaa $ sed -n \"1p\" test.txt asdfasdf $ sed -n \"2p\" test.txt fjfjfjfjfj $ sed -n \"1,2p\" test.txt asdfasdf fjfjfjfjfj $ sed -n \"1~2p\" test.txt asdfasdf aaaaa $ sed -n \"2{p;q}\" test.txt fjfjfjfjfj $ sed -n \"1,2{p;q}\" test.txt asdfasdf * -n, --quiet, --silent suppress automatic printing of pattern space * \u201c-n \u20185p\u2019\u201d means print only the fifth line * The sed has provided a \u2018q\u2018 command that allows to \u201cquit\u201d further processing. We can put the \u2018q\u2018 command in the two one-liners * {p;q} is just to apply the p and the q command","title":"Sed"},{"location":"notes/bash/sed/#sed","text":"","title":"sed"},{"location":"notes/bash/sed/#search","text":"grep is likely better $ sed -n \"/pattern/p\" file.txt $ cat test.txt asdfasdf fjfjfjfjfj aaaaa $ sed -n \"/a/p\" test.txt asdfasdf aaaaa","title":"Search"},{"location":"notes/bash/sed/#find-and-replate","text":"g is for global or replace all $ sed -i -e \"s/pattern_to_find/replace_string/\" file.txt $ sed -i -e \"s/pattern_to_find/replace_string/g\" file.txt Flags: -i[SUFFIX], --in-place[=SUFFIX] edit files in place (makes backup if SUFFIX supplied) -e script, --expression=script add the script to the commands to be executed Ex: $ cat deleteme.txt abc abc abc $ sed -i -e \"s/abc/XYZ/g\" deleteme.txt $ cat deleteme.txt XYZ XYZ XYZ","title":"Find and Replate"},{"location":"notes/bash/sed/#find-line","text":"Source $ cat test.txt asdfasdf fjfjfjfjfj aaaaa $ sed -n \"1p\" test.txt asdfasdf $ sed -n \"2p\" test.txt fjfjfjfjfj $ sed -n \"1,2p\" test.txt asdfasdf fjfjfjfjfj $ sed -n \"1~2p\" test.txt asdfasdf aaaaa $ sed -n \"2{p;q}\" test.txt fjfjfjfjfj $ sed -n \"1,2{p;q}\" test.txt asdfasdf * -n, --quiet, --silent suppress automatic printing of pattern space * \u201c-n \u20185p\u2019\u201d means print only the fifth line * The sed has provided a \u2018q\u2018 command that allows to \u201cquit\u201d further processing. We can put the \u2018q\u2018 command in the two one-liners * {p;q} is just to apply the p and the q command","title":"Find Line"},{"location":"notes/bash/ssh/","text":"SSH Useage $ ssh <username>@<host-ip> Keys Courtesty of stackexchange Generating a Key Pair the Proper way On Local server ssh-keygen -t rsa On remote Server ssh root@remote_servers_ip \"mkdir -p .ssh\" Uploading Generated Public Keys to the Remote Server cat ~/.ssh/id_rsa.pub | ssh root@remote_servers_ip \"cat >> ~/.ssh/authorized_keys\" Set Permissions on Remote server ssh root@remote_servers_ip \"chmod 700 ~/.ssh; chmod 640 ~/.ssh/authorized_keys\" Login ssh root@remote_servers_ip Enabling SSH Protocol v2 uncomment \"Protocol 2\" in /etc/ssh/sshd_config Enabling public key authorization in sshd uncomment \"PubkeyAuthentication yes\" in /etc/ssh/sshd_config If StrictModes is set to yes in /etc/ssh/sshd_config then restorecon -Rv ~/.ssh If you get \"The program 'restorecon' is currently not installed. You can install it by typing\" sudo apt install policycoreutils ...then run that Over Two ssh Hops larkinweb ssh from systemA trough systemB into systemC: $ ssh -t user1@systemB \"ssh user2@systemC\" Mount file system under similar circumstances:","title":"SSH"},{"location":"notes/bash/ssh/#ssh","text":"","title":"SSH"},{"location":"notes/bash/ssh/#useage","text":"$ ssh <username>@<host-ip>","title":"Useage"},{"location":"notes/bash/ssh/#keys","text":"Courtesty of stackexchange Generating a Key Pair the Proper way On Local server ssh-keygen -t rsa On remote Server ssh root@remote_servers_ip \"mkdir -p .ssh\" Uploading Generated Public Keys to the Remote Server cat ~/.ssh/id_rsa.pub | ssh root@remote_servers_ip \"cat >> ~/.ssh/authorized_keys\" Set Permissions on Remote server ssh root@remote_servers_ip \"chmod 700 ~/.ssh; chmod 640 ~/.ssh/authorized_keys\" Login ssh root@remote_servers_ip Enabling SSH Protocol v2 uncomment \"Protocol 2\" in /etc/ssh/sshd_config Enabling public key authorization in sshd uncomment \"PubkeyAuthentication yes\" in /etc/ssh/sshd_config If StrictModes is set to yes in /etc/ssh/sshd_config then restorecon -Rv ~/.ssh If you get \"The program 'restorecon' is currently not installed. You can install it by typing\" sudo apt install policycoreutils ...then run that","title":"Keys"},{"location":"notes/bash/ssh/#over-two-ssh-hops","text":"larkinweb ssh from systemA trough systemB into systemC: $ ssh -t user1@systemB \"ssh user2@systemC\" Mount file system under similar circumstances:","title":"Over Two ssh Hops"},{"location":"notes/bash/startup/","text":"Run On Startup Proper instructions aren't working so I made a run.sh script that I can run from /home/bbearce #!/bin/bash ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs if [ \"$1\" == mount ]; then sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # Postgres and pgAdmin elif [ \"$1\" == postgres ] || [ \"$1\" == pg ]; then cd /home/bbearce/pgAdmin4/pgAdmin4; . bin/activate; python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py; elif [ \"$1\" == code-journal ]; then cd /home/bbearce/Documents/code-journal; . venv/bin/activate; mkdocs serve; elif [ \"$1\" == cj-commit ]; then cd /home/bbearce/Documents/code-journal/; git add .; git commit -m \"\"$2\"\"; echo \"$2\" git push origin master; cd /home/bbearce/Documents/code-journal; . venv/bin/activate; cd /home/bbearce/Documents/bbearce.github.io; mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master; elif [ \"$1\" == key_github ] || [ \"$1\" == gk ]; then # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; elif [ \"$1\" == qtim-site ] || [ \"$1\" == qs ]; then cd /home/bbearce/Documents/qtim-lab.github.io; bundle exec jekyll serve; elif [ \"$1\" == qs-commit ]; then cd /home/bbearce/Documents/qtim-lab.github.io; git add .; git commit -m \"\"$2\"\"; git push origin master; elif [ \"$1\" == thegratefulbrauer ] || [ \"$1\" == tgb ]; then cd /home/bbearce/Documents/thegratefulbrauer; . venv3.6/bin/activate; python app.py; else echo \"not sure what you want...\" fi ### BB - Add startup commands here ### Courtesy of ghacks Find file /etc/rc.local . You can place scripts that you want to run at startup. /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; ### BB - Add startup commands here ### exit 0","title":"On Startup"},{"location":"notes/bash/startup/#run-on-startup","text":"Proper instructions aren't working so I made a run.sh script that I can run from /home/bbearce #!/bin/bash ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs if [ \"$1\" == mount ]; then sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # Postgres and pgAdmin elif [ \"$1\" == postgres ] || [ \"$1\" == pg ]; then cd /home/bbearce/pgAdmin4/pgAdmin4; . bin/activate; python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py; elif [ \"$1\" == code-journal ]; then cd /home/bbearce/Documents/code-journal; . venv/bin/activate; mkdocs serve; elif [ \"$1\" == cj-commit ]; then cd /home/bbearce/Documents/code-journal/; git add .; git commit -m \"\"$2\"\"; echo \"$2\" git push origin master; cd /home/bbearce/Documents/code-journal; . venv/bin/activate; cd /home/bbearce/Documents/bbearce.github.io; mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master; elif [ \"$1\" == key_github ] || [ \"$1\" == gk ]; then # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; elif [ \"$1\" == qtim-site ] || [ \"$1\" == qs ]; then cd /home/bbearce/Documents/qtim-lab.github.io; bundle exec jekyll serve; elif [ \"$1\" == qs-commit ]; then cd /home/bbearce/Documents/qtim-lab.github.io; git add .; git commit -m \"\"$2\"\"; git push origin master; elif [ \"$1\" == thegratefulbrauer ] || [ \"$1\" == tgb ]; then cd /home/bbearce/Documents/thegratefulbrauer; . venv3.6/bin/activate; python app.py; else echo \"not sure what you want...\" fi ### BB - Add startup commands here ### Courtesy of ghacks Find file /etc/rc.local . You can place scripts that you want to run at startup. /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; ### BB - Add startup commands here ### exit 0","title":"Run On Startup"},{"location":"notes/bash/users/","text":"Users Courtesy of cyberciti.biz Introduction : By default, the cloud server comes with a user named ubuntu . You can use such primary user account for sysadmin tasks on Ubuntu. However, sometimes you need to add a user account on Ubuntu for additional sysadmin tasks. This page shows how to create a regular user account or sysadmin account on the Ubuntu server. Add User Steps to create a user account on Ubuntu Linux 1. Open the terminal application 2. Log in to remote box by running the ssh user@your-ubuntu-box-ip 3. To add a new user in Ubuntu run sudo adduser userNameHere 4. Enter password and other needed info to create a user account on Ubuntu server 5. New username would be added to /etc/passwd file, and encrypted password stored in the /etc/shadow file Example bbearce@bbearce-XPS-15-9560:~$ sudo adduser vivek [sudo] password for bbearce: Adding user `vivek' ... Adding new group `vivek' (1001) ... Adding new user `vivek' (1001) with group `vivek' ... Creating home directory `/home/vivek' ... Copying files from `/etc/skel' ... Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Changing the user information for vivek Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y Verification Use the grep command or cat command as follows: $ cat /etc/passwd . . . vivek:x:1001:1001:,,,:/home/vivek:/bin/bash or $ grep '^vivek' /etc/passwd vivek:x:1001:1001:,,,:/home/vivek:/bin/bash Check group, groupid and userid of user: Use id : $ id <user> Ex: bbearce@bbearce-XPS-15-9560:~$ id bbearce uid=1000(bbearce) gid=1000(bbearce) groups=1000(bbearce),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare),130(docker),999(microk8s) Delete User Remove the user Log in to your server via SSH. Switch to the root user: $ sudo su - Use the userdel command to remove the old user: $ userdel <user's username> Optional: You can also delete that user's home directory and mail spool by using the -r flag with the command: $ userdel -r user's username Warning: Only delete a user's home directory if you are certain you no longer need their files or mail. Change Password Source: cyberciti Type following passwd command to change your own password: $ passwd Sample Outputs: Changing password for vivek (current) UNIX password: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Add User to Sudo Source phoenixnap usermod \u2013aG sudo newuser","title":"Users"},{"location":"notes/bash/users/#users","text":"Courtesy of cyberciti.biz Introduction : By default, the cloud server comes with a user named ubuntu . You can use such primary user account for sysadmin tasks on Ubuntu. However, sometimes you need to add a user account on Ubuntu for additional sysadmin tasks. This page shows how to create a regular user account or sysadmin account on the Ubuntu server.","title":"Users"},{"location":"notes/bash/users/#add-user","text":"Steps to create a user account on Ubuntu Linux 1. Open the terminal application 2. Log in to remote box by running the ssh user@your-ubuntu-box-ip 3. To add a new user in Ubuntu run sudo adduser userNameHere 4. Enter password and other needed info to create a user account on Ubuntu server 5. New username would be added to /etc/passwd file, and encrypted password stored in the /etc/shadow file","title":"Add User"},{"location":"notes/bash/users/#example","text":"bbearce@bbearce-XPS-15-9560:~$ sudo adduser vivek [sudo] password for bbearce: Adding user `vivek' ... Adding new group `vivek' (1001) ... Adding new user `vivek' (1001) with group `vivek' ... Creating home directory `/home/vivek' ... Copying files from `/etc/skel' ... Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Changing the user information for vivek Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y","title":"Example"},{"location":"notes/bash/users/#verification","text":"Use the grep command or cat command as follows: $ cat /etc/passwd . . . vivek:x:1001:1001:,,,:/home/vivek:/bin/bash or $ grep '^vivek' /etc/passwd vivek:x:1001:1001:,,,:/home/vivek:/bin/bash","title":"Verification"},{"location":"notes/bash/users/#check-group-groupid-and-userid-of-user","text":"Use id : $ id <user> Ex: bbearce@bbearce-XPS-15-9560:~$ id bbearce uid=1000(bbearce) gid=1000(bbearce) groups=1000(bbearce),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare),130(docker),999(microk8s)","title":"Check group, groupid and userid of user:"},{"location":"notes/bash/users/#delete-user","text":"","title":"Delete User"},{"location":"notes/bash/users/#remove-the-user","text":"Log in to your server via SSH. Switch to the root user: $ sudo su - Use the userdel command to remove the old user: $ userdel <user's username> Optional: You can also delete that user's home directory and mail spool by using the -r flag with the command: $ userdel -r user's username Warning: Only delete a user's home directory if you are certain you no longer need their files or mail.","title":"Remove the user"},{"location":"notes/bash/users/#change-password","text":"Source: cyberciti Type following passwd command to change your own password: $ passwd Sample Outputs: Changing password for vivek (current) UNIX password: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully","title":"Change Password"},{"location":"notes/bash/users/#add-user-to-sudo","text":"Source phoenixnap usermod \u2013aG sudo newuser","title":"Add User to Sudo"},{"location":"notes/bash/vi/","text":"VI Find and Replace Source linfo TL;DR Single Line scope (single replacement): :s/search_for_text/replace_text/ Single Line scope (global replacement): :s/search_for_text/replace_text/g Whole file scope: :s%/search_for_text/replace_text/<g or no g> Formal Notes: vi also has powerful search and replace capabilities. To search the text of an open file for a specific string (combination of characters or words), in the command mode type a colon (:), \"s,\" forward slash (/) and the search string itself. What you type will appear on the bottom line of the display screen. Finally, press ENTER, and the matching area of the text will be highlighted, if it exists. If the matching string is on an area of text that is not currently displayed on the screen, the text will scroll to show that area. The formal syntax for searching is: :s/string For example, suppose you want to search some text for the string \"cherry.\" Type the following and press ENTER: :s/cherry The first match for \"cherry\" in your text will then be highlighted. To see if there are additional occurrences of the same string in the text, type n, and the highlight will switch to the next match, if one exists. The syntax for replacing one string with another string in the current line is :s/pattern/replace/ Here \"pattern\" represents the old string and \"replace\" represents the new string. For example, to replace each occurrence of the word \"lemon\" in a line with \"orange,\" type: :s/lemon/orange/ The syntax for replacing every occurrence of a string in the entire text is similar. The only difference is the addition of a \"%\" in front of the \"s\": :%s/pattern/replace/ Thus repeating the previous example for the entire text instead of just for a single line would be: :%s/lemon/orange/ Next: Working With Multiple Files","title":"VI"},{"location":"notes/bash/vi/#vi","text":"","title":"VI"},{"location":"notes/bash/vi/#find-and-replace","text":"Source linfo TL;DR Single Line scope (single replacement): :s/search_for_text/replace_text/ Single Line scope (global replacement): :s/search_for_text/replace_text/g Whole file scope: :s%/search_for_text/replace_text/<g or no g>","title":"Find and Replace"},{"location":"notes/bash/vi/#formal-notes","text":"vi also has powerful search and replace capabilities. To search the text of an open file for a specific string (combination of characters or words), in the command mode type a colon (:), \"s,\" forward slash (/) and the search string itself. What you type will appear on the bottom line of the display screen. Finally, press ENTER, and the matching area of the text will be highlighted, if it exists. If the matching string is on an area of text that is not currently displayed on the screen, the text will scroll to show that area. The formal syntax for searching is: :s/string For example, suppose you want to search some text for the string \"cherry.\" Type the following and press ENTER: :s/cherry The first match for \"cherry\" in your text will then be highlighted. To see if there are additional occurrences of the same string in the text, type n, and the highlight will switch to the next match, if one exists. The syntax for replacing one string with another string in the current line is :s/pattern/replace/ Here \"pattern\" represents the old string and \"replace\" represents the new string. For example, to replace each occurrence of the word \"lemon\" in a line with \"orange,\" type: :s/lemon/orange/ The syntax for replacing every occurrence of a string in the entire text is similar. The only difference is the addition of a \"%\" in front of the \"s\": :%s/pattern/replace/ Thus repeating the previous example for the entire text instead of just for a single line would be: :%s/lemon/orange/ Next: Working With Multiple Files","title":"Formal Notes:"},{"location":"notes/bash/zip_and_unzip/","text":"Zipping and Unzipping zip, unzip Courtesty of geeksforgeeks.org zip $ man zip NAME zip - package and compress (archive) files . . . zipping files To zip files into a .zip file run this command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip wheel.py something.txt adding: wheel.py (deflated 79%) adding: something.txt (stored 0%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py zipping folder contents To zip eveything in a directory into zip file use ./* This is useful for pointing to <some directory>/* bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py Folders will be picked up but not their contents without -r : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory/ test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: directory/ (stored 0%) adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ zip -r example.zip ./* updating: directory/ (stored 0%) updating: something.txt (stored 0%) updating: wheel.py (deflated 79%) adding: directory/test.txt (stored 0%) Notice how zipping to example.zip again updates pre-existing files and adds new ones. Also flag -j can be used to make sure a folder's contents only get zipped. By default if you include a folder name, the folder is included in the zip file. Using -j will explicitly exclude the folder. unzip $ man unzip NAME unzip - list, test and extract compressed files in a ZIP archive . . . To unzip simply use the unzip command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip Archive: example.zip creating: directory/ extracting: something.txt inflating: wheel.py extracting: directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory example.zip something.txt wheel.py We can unzip to a specific directory with the -d flag: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py If the directory already exists, we can dump the contents into it: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ mkdir test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py Tar Files .tar The tar command is used to create tar archives by converting a group of files into an archive. It supports a vast range of compression programs such as gzip , bzip2 , lzip , lzma , lzop , xz and compress . Tar was originally designed for creating archives to store files on magnetic tape which is why it has its name \u201c T ape AR chive\u201d. Create a .tar bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Here -cvf means c: create; v: verbose; f: use archive file or device (we can and did specify a filename to use) . Keep in mind we didn't use the -z flag which would make a gzip . .tar.gz .tar.gz files are just files that have been zipped with -z to make a gzip. Let's redo the above example with -z : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvzf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Note that the archive name test.tar is a .tar.gz but you have to specify that explicitly. So a canonical way of diong this is: tar -cvzf test.tar.gz test.txt . Also the [-] is optional, so tar cvzf test.tar.gz test.txt would work too. We can see that the .tar.gz is smaller with du bbearce@bbearce-XPS-15-9560:~/Desktop$ du -BK ./* 20K ./test.tar 4K ./test.tar.gz 12K ./test.txt Unzip a .tar We need the -x option: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -xvf test.tar test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt When downloading 3DSlicer you get a *.tar.gz file. This command installs it. tar zxvf Slicer-4.8.1-linux-amd64.tar.gz FYI: ( Link ) RAR Files linuxhint Install rar and unrar rar sudo apt-get install rar rar a filename.rar filename1 This will make a filename.rar with a folder filename1 in it and files won't be at root in te directory. To do that do the following: cd filename1 rar a ../filename.rar ./ cd ../ # to return to original directory unrar sudo apt-get install unrar Extract into current directory unrar e filename.rar Extract into specific directory (/home): unrar e filename.rar /home/","title":"Zip and Unzip"},{"location":"notes/bash/zip_and_unzip/#zipping-and-unzipping","text":"","title":"Zipping and Unzipping"},{"location":"notes/bash/zip_and_unzip/#zip-unzip","text":"Courtesty of geeksforgeeks.org","title":"zip, unzip"},{"location":"notes/bash/zip_and_unzip/#zip","text":"$ man zip NAME zip - package and compress (archive) files . . .","title":"zip"},{"location":"notes/bash/zip_and_unzip/#zipping-files","text":"To zip files into a .zip file run this command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip wheel.py something.txt adding: wheel.py (deflated 79%) adding: something.txt (stored 0%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py","title":"zipping files"},{"location":"notes/bash/zip_and_unzip/#zipping-folder-contents","text":"To zip eveything in a directory into zip file use ./* This is useful for pointing to <some directory>/* bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py Folders will be picked up but not their contents without -r : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory/ test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: directory/ (stored 0%) adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ zip -r example.zip ./* updating: directory/ (stored 0%) updating: something.txt (stored 0%) updating: wheel.py (deflated 79%) adding: directory/test.txt (stored 0%) Notice how zipping to example.zip again updates pre-existing files and adds new ones. Also flag -j can be used to make sure a folder's contents only get zipped. By default if you include a folder name, the folder is included in the zip file. Using -j will explicitly exclude the folder.","title":"zipping folder contents"},{"location":"notes/bash/zip_and_unzip/#unzip","text":"$ man unzip NAME unzip - list, test and extract compressed files in a ZIP archive . . . To unzip simply use the unzip command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip Archive: example.zip creating: directory/ extracting: something.txt inflating: wheel.py extracting: directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory example.zip something.txt wheel.py We can unzip to a specific directory with the -d flag: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py If the directory already exists, we can dump the contents into it: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ mkdir test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py","title":"unzip"},{"location":"notes/bash/zip_and_unzip/#tar-files","text":"","title":"Tar Files"},{"location":"notes/bash/zip_and_unzip/#tar","text":"The tar command is used to create tar archives by converting a group of files into an archive. It supports a vast range of compression programs such as gzip , bzip2 , lzip , lzma , lzop , xz and compress . Tar was originally designed for creating archives to store files on magnetic tape which is why it has its name \u201c T ape AR chive\u201d.","title":".tar"},{"location":"notes/bash/zip_and_unzip/#create-a-tar","text":"bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Here -cvf means c: create; v: verbose; f: use archive file or device (we can and did specify a filename to use) . Keep in mind we didn't use the -z flag which would make a gzip .","title":"Create a .tar"},{"location":"notes/bash/zip_and_unzip/#targz","text":".tar.gz files are just files that have been zipped with -z to make a gzip. Let's redo the above example with -z : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvzf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Note that the archive name test.tar is a .tar.gz but you have to specify that explicitly. So a canonical way of diong this is: tar -cvzf test.tar.gz test.txt . Also the [-] is optional, so tar cvzf test.tar.gz test.txt would work too. We can see that the .tar.gz is smaller with du bbearce@bbearce-XPS-15-9560:~/Desktop$ du -BK ./* 20K ./test.tar 4K ./test.tar.gz 12K ./test.txt","title":".tar.gz"},{"location":"notes/bash/zip_and_unzip/#unzip-a-tar","text":"We need the -x option: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -xvf test.tar test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt When downloading 3DSlicer you get a *.tar.gz file. This command installs it. tar zxvf Slicer-4.8.1-linux-amd64.tar.gz FYI: ( Link )","title":"Unzip a .tar"},{"location":"notes/bash/zip_and_unzip/#rar-files","text":"linuxhint","title":"RAR Files"},{"location":"notes/bash/zip_and_unzip/#install-rar-and-unrar","text":"","title":"Install rar and unrar"},{"location":"notes/bash/zip_and_unzip/#rar","text":"sudo apt-get install rar rar a filename.rar filename1 This will make a filename.rar with a folder filename1 in it and files won't be at root in te directory. To do that do the following: cd filename1 rar a ../filename.rar ./ cd ../ # to return to original directory","title":"rar"},{"location":"notes/bash/zip_and_unzip/#unrar","text":"sudo apt-get install unrar Extract into current directory unrar e filename.rar Extract into specific directory (/home): unrar e filename.rar /home/","title":"unrar"},{"location":"notes/bash/sftp/sftp/","text":"SFTP Setup an SFTP server Source: linuxconfig.org \"Secure File Transfer Protocol\" In this tutorial you will learn: How to install and configure SSH daemon How to setup an SFTP user account and group How to connect to SFTP server via command line How to connect to SFTP server via command line SFTP requires SSH, so if SSH server is not already installed on your system, install it with the following command: $ sudo apt install ssh Once SSH is installed, we need to make some changes to the SSHD configuration file. Use nano or your favorite text editor to open it: $ sudo vi /etc/ssh/sshd_config Scroll to the bottom of the file and add the following 5 lines at the very end: Match group sftp ChrootDirectory /home X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp Restart the SSH service for these new changes to take effect: $ sudo systemctl restart ssh Create a new user group called sftp. All of our SFTP users will need to belong to this group. $ sudo addgroup sftp Adding group `sftp' (GID 1002) ... Done. Login bbearce@terry:~$ sftp challenge-organizer@<server ip address> challenge-organizer@<server ip address>'s password: Connected to <server ip address>. sftp> ls azureuser challenge-organizer sftp> Use FileZilla Real Example: azureuser@codalab-vm:~$ ls -la /home total 16 drwxr-xr-x 4 root root 4096 Feb 5 16:13 . drwxr-xr-x 23 root root 4096 Jan 21 06:22 .. drwxr-x--- 13 azureuser azureuser 4096 Feb 7 03:13 azureuser drwx------ 6 challenge-organizer sftp 4096 Feb 7 02:43 challenge-organizer $ sudo vi /etc/ssh/sshd_config sshd_config: Note: ChrootDirectories have to be owned by root. Match group sftp #Match user challenge-organizer # other option ChrootDirectory /home X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp Notice how to the sftp user they are at / but for them that is /home . You don't want to give them access to the real / .","title":"SFTP"},{"location":"notes/bash/sftp/sftp/#sftp","text":"","title":"SFTP"},{"location":"notes/bash/sftp/sftp/#setup-an-sftp-server","text":"Source: linuxconfig.org \"Secure File Transfer Protocol\" In this tutorial you will learn: How to install and configure SSH daemon How to setup an SFTP user account and group How to connect to SFTP server via command line How to connect to SFTP server via command line SFTP requires SSH, so if SSH server is not already installed on your system, install it with the following command: $ sudo apt install ssh Once SSH is installed, we need to make some changes to the SSHD configuration file. Use nano or your favorite text editor to open it: $ sudo vi /etc/ssh/sshd_config Scroll to the bottom of the file and add the following 5 lines at the very end: Match group sftp ChrootDirectory /home X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp Restart the SSH service for these new changes to take effect: $ sudo systemctl restart ssh Create a new user group called sftp. All of our SFTP users will need to belong to this group. $ sudo addgroup sftp Adding group `sftp' (GID 1002) ... Done.","title":"Setup an SFTP server"},{"location":"notes/bash/sftp/sftp/#login","text":"bbearce@terry:~$ sftp challenge-organizer@<server ip address> challenge-organizer@<server ip address>'s password: Connected to <server ip address>. sftp> ls azureuser challenge-organizer sftp>","title":"Login"},{"location":"notes/bash/sftp/sftp/#use-filezilla","text":"Real Example: azureuser@codalab-vm:~$ ls -la /home total 16 drwxr-xr-x 4 root root 4096 Feb 5 16:13 . drwxr-xr-x 23 root root 4096 Jan 21 06:22 .. drwxr-x--- 13 azureuser azureuser 4096 Feb 7 03:13 azureuser drwx------ 6 challenge-organizer sftp 4096 Feb 7 02:43 challenge-organizer $ sudo vi /etc/ssh/sshd_config sshd_config: Note: ChrootDirectories have to be owned by root. Match group sftp #Match user challenge-organizer # other option ChrootDirectory /home X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp Notice how to the sftp user they are at / but for them that is /home . You don't want to give them access to the real / .","title":"Use FileZilla"},{"location":"notes/bash/tmux/tmux/","text":"TMUX Source leimao.github.io Cheat Sheets and Config Tmux Cheat Sheet Customize Config Concepts Tmux has sessions, windows, and panes. The hierarchy is that Tmux could have multiple sessions, a session could have multiple windows, a window could have multiple panes. On the server, users could follow certain conventions or rules to manage Tmux. For example, we could create a session for a specific project. In the project session, we could create multiple windows, and each window would be used for each specific task for the project. In the window, in order to finish the task more efficiently, we create multiple panes for purposes such as process monitoring and file management. Dual Interface Similar to Docker, Tmux has two layers of the interface, the local terminal outside Tmux, and the terminal inside Tmux. We could manage Tmux in both layers. While typing bash commands are equivalent in both interface, to manage the Tmux related stuff inside Tmux, we would need to use hotkeys so Tmux knows when to manage the Tmux related stuff. All the hotkeys are prefixed by Ctrl + b . Tmux Console In the Tmux terminal, we could call out Tmux console by Ctrl + b + : and run all the Tmux commands available for the local terminal without tmux prefix. For example, if there is a Tmux command for the local terminal like this. $ tmux xxxxx In the Tmux console in the Tmux terminal, we could do the equivalent thing by running the following command. :xxxxx Note that : is the prefix of the Tmux console which we don\u2019t type. : could be thought as $ tmux in the local terminal. Create Sessions In the local terminal, we create Tmux sessions by simply running one of the following three equivalent commands. $ tmux $ tmux new $ tmux new-session This will create a new session to the existing Tmux. If there is no previous Tmux session running, this will create the first Tmux session. If there are already Tmux sessions running, this will create an additional one. This has auto named the session and will not have a custom name but rather \"session 1\"...\"session n\". Tmux requires at least one session to run. If the last session was closed, Tmux server will automatically close. Detach Sessions To return to the local terminal from Tmux sessions, we usually do detach by hitting Ctrl + b + d. Everything would be still running in the backend. In some scenarios, we could return to the local terminal by running the following command in the Tmux terminal. $ exit However, bear in mind that using this method the current session will exit and all the information in the current session will be lost. Create Sessions With Names Tmux, by default, uses natural integers as the name for sessions. This is sometimes inconvenient for project management. We could create sessions with names using the following commands in the local terminal. $ tmux new -s [session-name] View Sessions To view Tmux sessions from the local terminal, run one of the following commands. $ tmux ls $ tmux list-sessions In Tmux terminal, we check Tmux sessions by hitting Ctrl + b + s. The following information will show up. Hit Esc or q to exit the information. Rename Sessions To rename sessions, from the local terminal, we run the following command. $ tmux rename-session [-t session-name] [new-session-name] If [-t session-name] is not provided, the last session used will be renamed. Ex: Alternatively, we may also hit Ctrl + b + $ to rename the current session in the Tmux terminal. bb927@bryce:~$ tmux ls 0-newer: 1 windows (created Wed Sep 22 09:38:41 2021) [105x14] (attached) 1: 1 windows (created Wed Sep 22 09:42:48 2021) [56x13] session_bb: 1 windows (created Wed Sep 22 09:48:34 2021) [105x14] Kill Sessions To kill all sessions, from the local terminal, we run the following command. $ tmux kill-server Ex: bb927@bryce:~$ tmux kill-server bb927@bryce:~$ tmux ls no server running on /tmp/tmux-4643095/default To kill specific sessions, from the local terminal, we run the following command. $ tmux kill-session -t [session-name] Attach Sessions To attach to specific sessions, from the local terminal, we run the following command. $ tmux attach -t [session-name] Create/Close Windows In Tmux session, we could have multiple windows. To create a window, in the Tmux terminal, we hit Ctrl + b + c . To kill the current window, in the Tmux terminal, we hit Ctrl + b + & ( & is Shift + 7 ). The windows in the sessions could have names. We rename the current window by hitting Ctrl + b + , . Select Windows Each window in the session, regardless of whether it has a name or not (actually its default name is always bash), would have a window id of natural integer 0, 1, etc. We select specific window by hitting Ctrl + b + window id . Sometimes it is also convenient to use Ctrl + b + n to move to the next window, or Ctrl + b + p to move to the previous window. Create/Close Panes Each window in the session could have multiple panes, just like Gnome Terminator. To split the pane vertically, we hit Ctrl + b + % . To split the pane horizontally, we hit Ctrl + b + \" . To close the current pane, we we hit Ctrl + b + x . To toggle between panes in the window, we simply hit Ctrl + b + \u2191/\u2193/\u2190/\u2192 . Use Mouse To scroll using mouse, we have to enable mouse mode in Tmux. :set -g mouse on To disable mouse mode in Tmux :set -g mouse off Copy and Paste To do copy and paste in Tmux or between Tmux and host, holding Shift while left clicking and selecting text followed by pressing Ctrl-Shift-C will copy to the X clipboard. Ctrl-Shift-V will paste from the X clipboard as well. Alternatively, holding Shift while left clicking and selecting text followed by right clicking. To paste, holding Shift while clicking the middle button of the mouse. Tmux Resurrect Usages Installation To install Tmux Resurrect, it is recommended to install Tmux Plugin Manager first. Please check the GitHub repo for installation instructions. Then we add new plugin Tmux Resurrect to Tmux by adding set -g @plugin 'tmux-plugins/tmux-resurrect' to ~/.tmux.conf. An example of the ~/.tmux.conf would be $ cat ~/.tmux.conf # List of plugins set -g @plugin 'tmux-plugins/tpm' set -g @plugin 'tmux-plugins/tmux-sensible' set -g @plugin 'tmux-plugins/tmux-resurrect' # Other examples: # set -g @plugin 'github_username/plugin_name' # set -g @plugin 'git@github.com/user/plugin' # set -g @plugin 'git@bitbucket.com/user/plugin' # Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf) run -b '~/.tmux/plugins/tpm/tpm' Finally, we install the plugin by hitting Ctrl + b + I in the Tmux terminal. We would see the following information if the installation was successful. [0/0] TMUX environment reloaded. Done, press ESCAPE to continue. Save and Restore Tmux Environment To save the Tmux environment, we hit Ctrl + b + Ctrl + s in the Tmux Terminal. If the save was successful, a message of Tmux environment saved! would pop up. To restore the Tmux environment, we hit Ctrl + b + Ctrl + r in the Tmux Terminal. If the restore was successful, a message of Tmux restore complete! would pop up. All the sessions, windows, and panels would be saved and restored with Tmux Resurrect. Some of the running commands, such as htop, would be restored as well. Last Tricks Prefix Key Binding Sometimes hitting the hotkey prefix Ctrl + b could be tedious. We could set a single button hit for Ctrl + b . The right \u229e Win key on my keyboard seems to be useless in Ubuntu, and we could bind the right \u229e Win key to Ctrl + b . Acknowledgement Thank my friend Dong Meng for recommending Tmux Resurrect to me. Final Remarks We would see concept similarities between Tmux, Docker, and Vim. More comprehensive Tmux commands could be found on Tmux Cheat Sheet .","title":"TMUX"},{"location":"notes/bash/tmux/tmux/#tmux","text":"Source leimao.github.io","title":"TMUX"},{"location":"notes/bash/tmux/tmux/#cheat-sheets-and-config","text":"Tmux Cheat Sheet Customize Config","title":"Cheat Sheets and Config"},{"location":"notes/bash/tmux/tmux/#concepts","text":"Tmux has sessions, windows, and panes. The hierarchy is that Tmux could have multiple sessions, a session could have multiple windows, a window could have multiple panes. On the server, users could follow certain conventions or rules to manage Tmux. For example, we could create a session for a specific project. In the project session, we could create multiple windows, and each window would be used for each specific task for the project. In the window, in order to finish the task more efficiently, we create multiple panes for purposes such as process monitoring and file management.","title":"Concepts"},{"location":"notes/bash/tmux/tmux/#dual-interface","text":"Similar to Docker, Tmux has two layers of the interface, the local terminal outside Tmux, and the terminal inside Tmux. We could manage Tmux in both layers. While typing bash commands are equivalent in both interface, to manage the Tmux related stuff inside Tmux, we would need to use hotkeys so Tmux knows when to manage the Tmux related stuff. All the hotkeys are prefixed by Ctrl + b .","title":"Dual Interface"},{"location":"notes/bash/tmux/tmux/#tmux-console","text":"In the Tmux terminal, we could call out Tmux console by Ctrl + b + : and run all the Tmux commands available for the local terminal without tmux prefix. For example, if there is a Tmux command for the local terminal like this. $ tmux xxxxx In the Tmux console in the Tmux terminal, we could do the equivalent thing by running the following command. :xxxxx Note that : is the prefix of the Tmux console which we don\u2019t type. : could be thought as $ tmux in the local terminal.","title":"Tmux Console"},{"location":"notes/bash/tmux/tmux/#create-sessions","text":"In the local terminal, we create Tmux sessions by simply running one of the following three equivalent commands. $ tmux $ tmux new $ tmux new-session This will create a new session to the existing Tmux. If there is no previous Tmux session running, this will create the first Tmux session. If there are already Tmux sessions running, this will create an additional one. This has auto named the session and will not have a custom name but rather \"session 1\"...\"session n\". Tmux requires at least one session to run. If the last session was closed, Tmux server will automatically close.","title":"Create Sessions"},{"location":"notes/bash/tmux/tmux/#detach-sessions","text":"To return to the local terminal from Tmux sessions, we usually do detach by hitting Ctrl + b + d. Everything would be still running in the backend. In some scenarios, we could return to the local terminal by running the following command in the Tmux terminal. $ exit However, bear in mind that using this method the current session will exit and all the information in the current session will be lost.","title":"Detach Sessions"},{"location":"notes/bash/tmux/tmux/#create-sessions-with-names","text":"Tmux, by default, uses natural integers as the name for sessions. This is sometimes inconvenient for project management. We could create sessions with names using the following commands in the local terminal. $ tmux new -s [session-name]","title":"Create Sessions With Names"},{"location":"notes/bash/tmux/tmux/#view-sessions","text":"To view Tmux sessions from the local terminal, run one of the following commands. $ tmux ls $ tmux list-sessions In Tmux terminal, we check Tmux sessions by hitting Ctrl + b + s. The following information will show up. Hit Esc or q to exit the information.","title":"View Sessions"},{"location":"notes/bash/tmux/tmux/#rename-sessions","text":"To rename sessions, from the local terminal, we run the following command. $ tmux rename-session [-t session-name] [new-session-name] If [-t session-name] is not provided, the last session used will be renamed. Ex: Alternatively, we may also hit Ctrl + b + $ to rename the current session in the Tmux terminal. bb927@bryce:~$ tmux ls 0-newer: 1 windows (created Wed Sep 22 09:38:41 2021) [105x14] (attached) 1: 1 windows (created Wed Sep 22 09:42:48 2021) [56x13] session_bb: 1 windows (created Wed Sep 22 09:48:34 2021) [105x14]","title":"Rename Sessions"},{"location":"notes/bash/tmux/tmux/#kill-sessions","text":"To kill all sessions, from the local terminal, we run the following command. $ tmux kill-server Ex: bb927@bryce:~$ tmux kill-server bb927@bryce:~$ tmux ls no server running on /tmp/tmux-4643095/default To kill specific sessions, from the local terminal, we run the following command. $ tmux kill-session -t [session-name]","title":"Kill Sessions"},{"location":"notes/bash/tmux/tmux/#attach-sessions","text":"To attach to specific sessions, from the local terminal, we run the following command. $ tmux attach -t [session-name]","title":"Attach Sessions"},{"location":"notes/bash/tmux/tmux/#createclose-windows","text":"In Tmux session, we could have multiple windows. To create a window, in the Tmux terminal, we hit Ctrl + b + c . To kill the current window, in the Tmux terminal, we hit Ctrl + b + & ( & is Shift + 7 ). The windows in the sessions could have names. We rename the current window by hitting Ctrl + b + , .","title":"Create/Close Windows"},{"location":"notes/bash/tmux/tmux/#select-windows","text":"Each window in the session, regardless of whether it has a name or not (actually its default name is always bash), would have a window id of natural integer 0, 1, etc. We select specific window by hitting Ctrl + b + window id . Sometimes it is also convenient to use Ctrl + b + n to move to the next window, or Ctrl + b + p to move to the previous window.","title":"Select Windows"},{"location":"notes/bash/tmux/tmux/#createclose-panes","text":"Each window in the session could have multiple panes, just like Gnome Terminator. To split the pane vertically, we hit Ctrl + b + % . To split the pane horizontally, we hit Ctrl + b + \" . To close the current pane, we we hit Ctrl + b + x . To toggle between panes in the window, we simply hit Ctrl + b + \u2191/\u2193/\u2190/\u2192 .","title":"Create/Close Panes"},{"location":"notes/bash/tmux/tmux/#use-mouse","text":"To scroll using mouse, we have to enable mouse mode in Tmux. :set -g mouse on To disable mouse mode in Tmux :set -g mouse off","title":"Use Mouse"},{"location":"notes/bash/tmux/tmux/#copy-and-paste","text":"To do copy and paste in Tmux or between Tmux and host, holding Shift while left clicking and selecting text followed by pressing Ctrl-Shift-C will copy to the X clipboard. Ctrl-Shift-V will paste from the X clipboard as well. Alternatively, holding Shift while left clicking and selecting text followed by right clicking. To paste, holding Shift while clicking the middle button of the mouse.","title":"Copy and Paste"},{"location":"notes/bash/tmux/tmux/#tmux-resurrect-usages","text":"","title":"Tmux Resurrect Usages"},{"location":"notes/bash/tmux/tmux/#installation","text":"To install Tmux Resurrect, it is recommended to install Tmux Plugin Manager first. Please check the GitHub repo for installation instructions. Then we add new plugin Tmux Resurrect to Tmux by adding set -g @plugin 'tmux-plugins/tmux-resurrect' to ~/.tmux.conf. An example of the ~/.tmux.conf would be $ cat ~/.tmux.conf # List of plugins set -g @plugin 'tmux-plugins/tpm' set -g @plugin 'tmux-plugins/tmux-sensible' set -g @plugin 'tmux-plugins/tmux-resurrect' # Other examples: # set -g @plugin 'github_username/plugin_name' # set -g @plugin 'git@github.com/user/plugin' # set -g @plugin 'git@bitbucket.com/user/plugin' # Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf) run -b '~/.tmux/plugins/tpm/tpm' Finally, we install the plugin by hitting Ctrl + b + I in the Tmux terminal. We would see the following information if the installation was successful. [0/0] TMUX environment reloaded. Done, press ESCAPE to continue.","title":"Installation"},{"location":"notes/bash/tmux/tmux/#save-and-restore-tmux-environment","text":"To save the Tmux environment, we hit Ctrl + b + Ctrl + s in the Tmux Terminal. If the save was successful, a message of Tmux environment saved! would pop up. To restore the Tmux environment, we hit Ctrl + b + Ctrl + r in the Tmux Terminal. If the restore was successful, a message of Tmux restore complete! would pop up. All the sessions, windows, and panels would be saved and restored with Tmux Resurrect. Some of the running commands, such as htop, would be restored as well.","title":"Save and Restore Tmux Environment"},{"location":"notes/bash/tmux/tmux/#last-tricks","text":"Prefix Key Binding Sometimes hitting the hotkey prefix Ctrl + b could be tedious. We could set a single button hit for Ctrl + b . The right \u229e Win key on my keyboard seems to be useless in Ubuntu, and we could bind the right \u229e Win key to Ctrl + b . Acknowledgement Thank my friend Dong Meng for recommending Tmux Resurrect to me. Final Remarks We would see concept similarities between Tmux, Docker, and Vim. More comprehensive Tmux commands could be found on Tmux Cheat Sheet .","title":"Last Tricks"},{"location":"notes/codalab/docker_submission/","text":"Docker Submission This is a documentation for adding docker submissions to the open source project CodaLab . Introduction Codalab comes with essentially 2 types of submissions from participants: Results Submission: Participants submit the \"results\" of their algorithms. Code Submission: Participants submit actual code to be executed by the challenge organizer. The code can be executed directly as an executable ($ python sample.py...) or participants can define classes that the organizer calls themselves (hence the participants do not have to read the input data themselves). We will be adding a third option where participants can submit a docker image with their code and libraries baked in. Step 1: Install Codalab First we need a running instance installed. Setup the challenge to accept a results submission, as we will tweak this process to evaluate the output of a docker container. Step 2: Add Database Option for Docker Submission Add New Column in Competition Table Find file /codalab/apps/web/models.py . Approximately on line 233 is the Competition model definition. It has definitions for the columns of this table. It looks like this: class Competition(ChaHubSaveMixin, models.Model): ... Add enable_docker_submission as a BooleanField : Add this line: enable_docker_submission = models.BooleanField(default=False, verbose_name=\"Allow participant docker submission\") It doesn't matter what order the columns are in, so if you are familiar with setting up models in Django or Flask projects, just add a column that we can reference later. Migrate Database So We Can See Our Changes Codalab as of this writing (5/1/2020) is deployed with an orchestration of docker containers. One of these is the database (postgres). Using docker ps we can see our images: bbearce@MedICI-CodaLab-Master:~/src$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b8196a2e9fc5 nginx \"bash -x /app/docker\u2026\" 4 months ago Up 2 months 0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp nginx 528d2dc976a1 codalab-competitions_django \"bash /app/docker/ru\u2026\" 4 months ago Up 2 months 0.0.0.0:8000->8000/tcp django c56dcc7fb3e2 codalab-competitions_worker_site \"sh /app/docker/run_\u2026\" 4 months ago Up 2 months worker_site 23996bbcfbd0 codalab/competitions-v1-compute-worker:1.1.7 \"/bin/sh -c 'celery \u2026\" 4 months ago Up 2 months worker_compute 3f5a1df0f9ae codalab-competitions_flower \"/usr/bin/dumb-init \u2026\" 4 months ago Up 2 months 0.0.0.0:5555->5555/tcp flower b93b3f7e0a3b codalab-competitions_rabbit \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp rabbit 5e3492959db1 postgres:9.6.3 \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 5432/tcp postgres We need use manage.py from the django container to migrate the database. This will talk to the postgres container for us. Codalab has notes for how to do this so we will follow their lead : Note, in the link from codalab they have a placeholder for APPNAME . Substitute web in for this. root@django:/app/codalab# ./manage schemamigration web --auto ./manage: line 3: ./config/generated/startup_env.sh: No such file or directory ./manage: line 4: ../venv/bin/activate: No such file or directory /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) + Added field enable_docker_submission on web.Competition Created 0085_auto__add_field_competition_enable_docker_submission.py. You can now apply this migration with: ./manage.py migrate web Once you have that migrate the database. ./manage migrate This will apply the migration and look for all changes to all tables. Looking in the results we can see our migration in action: ~ ~ Running migrations for web: - Migrating forwards to 0085_auto__add_field_competition_enable_docker_submission. > web:0085_auto__add_field_competition_enable_docker_submission ~ ~ Now let's verify our change. First create a database superuser: docker exec -it django python manage.py createsuperuser This will prompt you to create a superuser: bbearce@miccai2019:~/src/codalab-competitions/codalab$ docker exec -it django python manage.py createsuperuser /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) Username: admin Email address: bbearce@mgh.harvard.edu Password: Password (again): Superuser created successfully. Now proceed to http://<your domain>.com/admin and log in with the credentials you made. Find the \"Web\" app section and click \"Competitions\". Next scroll till you see \"Allow participant docker submission\" or whatever you put as your new column. Add Our new Option to the CompetitionForm Class Django or CodaLab uses apps.web.forms.CompetitionForm to manage the edit competiton form. class CompetitionForm(forms.ModelForm): class Meta: model = models.Competition fields = ( 'title', 'description', 'queue', 'disallow_leaderboard_modifying', 'force_submission_to_leaderboard', 'image', 'end_date', 'published', 'enable_medical_image_viewer', 'enable_detailed_results', # change start 'enable_docker_submission', # change end 'admins', ... This will allow us to see this option in the edit competition view: This has the effect of allowing access to this option from multiple views so we can conditionally render a docker submission form or a regular result submission form. Step 3: Add Conditional Rendering for Docker\\Result Submission Add UI Elements Find template apps/web/templates/web/competitions/_submit_results_page.html . Roughly on line 70 there is a {% if USE_AWS %} block and in the {% else %} block we can add our code. Replace the inside of the else block with this code: {% if phase.competition.enable_docker_submission %} <h1>DOCKER SUBMISSION ENABLED </h1> <!-- Submit Docker Button --> <button id=submit-button type=\"button\" data-toggle=\"modal\" data-target=\"#submit-docker-dialog\" class=\"btn btn-secondary\">Submit Docker</button> <!-- Submit Docker Button --> {% else %} <h1>DOCKER SUBMISSION DISABLED</h1> <!-- original code from codalab --> <button id=\"fileUploadButton\" class=\"button btn btn-primary {% if not phase.reference_data %}disabled{% endif %}\" {% if not phase.reference_data %}disabled=\"disabled\"{% endif %}> Submit </button> <!-- original code from codalab --> {% endif %} Our If Else block has the original code in the else. At this point if you refresh the page and navigate to the \"Participate\" tab, you should see this. Now if we go to \"Edit\" and look at the competition options, we see that if we check the box \"Allow participant docker submission\" we can see the text change on the \"Participate\" tab. Next we need the \"Submit Docker\" button to do something. Let's add a modal that get's triggered when you click the button. Add this code just before the line that has {% include \"web/common/_submission_details_template.html\" %} on line ~167: <div class=\"modal fade\" id=\"submit-docker-dialog\"> <div class=\"modal-dialog\"> <div class=\"modal-content\"> <div class=\"modal-header\"> <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</button> <h4 class=\"modal-title\">Submit Docker</h4> </div> <div class=\"modal-body\"> <h4>Submit Docker Run Command</h4> <textarea id=\"docker-run-command\" type=text style=\"width: 500px; height: 250px;\">docker run -v /<local-path>:/<docker-container-path-1> -v /<local-path>:/<docker-container-path-2> user/repo:tag</textarea> </div> <div class=\"modal-footer\"> <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button> <button id=\"submit-docker\" class=\"btn btn-primary\">Send</button> </div> </div> </div> </div> You should see this when you click the \"Submit Docker\" button: Now let's tie the \"Send\" button from the modal to some javascript so we can call the backend to process this for us. At the bottom of the same file we've been working in, add this code and create the file apps/web/static/js/submit_docker.js: <!-- Docker Submission Front End --> <script type=\"text/javascript\" src=\"{{STATIC_URL}}js/submit_docker.js\"></script> <!-- Docker Submission Front End --> In submit_docker.js add this: $('#submit-docker').click(function(){ //copy from codalab/apps/web/static/js/Competition.js file $('#details').html('Creating new submission...'); var competitionId = $(\"#competitionId\").val() var description = $('#submission_description_textarea').val() || ''; var method_name = $('#submission_method_name').val() || ''; var method_description = $('#submission_method_description').val() || ''; var project_url = $('#submission_project_url').val() || ''; var publication_url = $('#submission_publication_url').val() || ''; var bibtex = $('#submission_bibtex').val() || ''; var team_name = $('#submission_team_name').val() || ''; var organization_or_affiliation = $('#submission_organization_or_affiliation').val() || ''; var phase_id = $('#submission_phase_id').val(); // capture docker run command var docker_run_cmd = $('#docker-run-command').val(); console.log(docker_run_cmd); // capture docker run command $('#submission_description_textarea').val(''); // console.log('/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + // '&method_name=' + encodeURIComponent(method_name) + // '&method_description=' + encodeURIComponent(method_description) + // '&project_url=' + encodeURIComponent(project_url) + // '&publication_url=' + encodeURIComponent(publication_url) + // '&bibtex=' + encodeURIComponent(bibtex) + // '&team_name=' + encodeURIComponent(team_name) + // '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + // '&phase_id=' + encodeURIComponent(phase_id)+ // '&docker-run-command='+ encodeURIComponent(docker_run_cmd) // ) $.ajax({ url: '/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + '&method_name=' + encodeURIComponent(method_name) + '&method_description=' + encodeURIComponent(method_description) + '&project_url=' + encodeURIComponent(project_url) + '&publication_url=' + encodeURIComponent(publication_url) + '&bibtex=' + encodeURIComponent(bibtex) + '&team_name=' + encodeURIComponent(team_name) + '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + '&phase_id=' + encodeURIComponent(phase_id)+ '&docker-run-command='+ encodeURIComponent(docker_run_cmd), type: 'post', cache: false, async: false, data: { 'id': 'trackingid', 'name': '', 'type': '', 'size': '' } }).done(function(response) { $('#details').html(''); $('#user_results tr.noData').remove(); $('#user_results').append(Competition.displayNewSubmission(response, description, method_name, method_description, project_url, publication_url, bibtex, team_name, organization_or_affiliation)); $('#user_results #' + response.id + ' .glyphicon-plus').on('click', function() { Competition.showOrHideSubmissionDetails(this) }); //$('#fileUploadButton').removeClass('disabled'); //$('#fileUploadButton').text(\"Submit Results...\"); $('#user_results #' + response.id + ' .glyphicon-plus').click(); location.reload(true); }).fail(function(jqXHR) { var msg = 'An unexpected error occurred.'; if (jqXHR.status == 403) { msg = jqXHR.responseJSON.detail; } $('#details').html(msg); //$('#fileUploadButton').text(\"Submit Results...\"); $('#fileUploadButton').removeClass('disabled'); }); $('#submit-docker-dialog').modal('toggle'); }); From looking at the ajax call you can see we are hitting the route for an api that ultimately triggers a POST request in view \"CompetitionSubmissionViewSet\" under the api app. The part we need to edit is the post_save() method. Step 4: Edit Site-Worker Code Replace the post_save() with this: def post_save(self, obj, created): # Original Code #if created: # evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only)) if escape(self.request.QUERY_PARAMS.get('docker-run-command')) == 'None': # submit_docker_command = None submit_docker_command = \"this is not a docker submission\" else: submit_docker_command = escape(self.request.QUERY_PARAMS.get('docker-run-command', \"\")).replace('&lt;','<').replace('&gt;','>') ## - pass extra info to submission if created: evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only, submit_docker_command)) ## - pass extra info to submission At the bottom of post_save() is evaluate_submission . That is from apps/web/tasks.py. We are passing the submit_docker_command to evaluate submission . Change evaluate_submission to accept the submit_docker_command . Also edit it to pass the command to task_func() Add submit_docker_command to these two sections: def evaluate_submission(submission_id, is_scoring_only, submit_docker_command): and: task_func(submission, job_id, submit_docker_command) task_func is actually a user defined variable for score , a function in the same file above evaluate_submission . Add submit_docker_command to the input of score . def score(submission, job_id, submit_docker_command): Towards the bottom of score , _prepare_compute_worker_run needs submit_docker_command . Pass it in, and subsequently add it to it's function definition at towards the top of the page: _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction=False) and def _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction): Inside this function you will find a data variable we need to add the docker command to: data = { \"id\": job_id, \"task_type\": \"run\", \"task_args\": { \"submission_id\": submission.pk, \"docker_image\": docker_image, \"ingestion_program_docker_image\": docker_image, \"bundle_url\": _make_url_sassy(bundle_url), \"stdout_url\": _make_url_sassy(stdout, permission='w'), \"stderr_url\": _make_url_sassy(stderr, permission='w'), \"output_url\": _make_url_sassy(output, permission='w'), \"ingestion_program_output_url\": _make_url_sassy(submission.ingestion_program_stdout_file.name, permission='w'), \"ingestion_program_stderr_url\": _make_url_sassy(submission.ingestion_program_stderr_file.name, permission='w'), \"detailed_results_url\": _make_url_sassy(submission.detailed_results_file.name, permission='w'), \"private_output_url\": _make_url_sassy(submission.private_output_file.name, permission='w'), \"secret\": submission.secret, \"execution_time_limit\": submission.phase.execution_time_limit, \"predict\": is_prediction, 'submit_docker_command':submit_docker_command, ## - Added submit_docker_command here } At this point the code hands off the data to the Compute-Worker with compute_worker_run(data, soft_time_limit=time_limit, priority=2) . Step 5: The Compute-Worker We need to enter the compute-worker docker container and edit \"worker.py\". On the machine where the compute-worker is running execute: $ docker exec -it codalab/competitions-v1-compute-worker:latest ...or use the compute image that you need. Once inside you should be able to see \"worker.py\". Open it. Find function run(task_id, task_args) . This is where we will add our code on approx line 500. Add this: if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") logger.info(\"Invoking program %s\", participant_docker_cmd) participant_docker_process = Popen(participant_docker_cmd.split(\" \")) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') This is the important line: participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") This line substitutes the results directory into a mount placeholder so that the container can put the results from the run into it. Then the compute-worker can consume the results. Update (7/17/2020): Docker Image Submission only and resulting handling: participant_docker_submission_cmd = [ 'docker', 'run', # Ask all participants to add this user '-u', 'participant', # Cut internet '--net', 'none', # Remove it after run '--rm', # Add support for GPUs and nvidia '--gpus', 'all', # Give it a name associated to task_id '--name={}'.format(\"participant_docker_submission_taskid_\"+str(task_id)), # Try the new timeout feature '--stop-timeout={}'.format(execution_time_limit), # Don't allow subprocesses to raise privileges '--security-opt=no-new-privileges', # Set the right volume '-v', '{0}:/mnt/in:ro'.format('/home/bbearce/Documents/docker_submissions/directory_of_files'), # :ro for read-only file system '-v', '{0}:/mnt/out'.format(input_dir+\"/res\"), # Set aside 512m memory for the host #'--memory', '{}MB'.format(available_memory_mib - 512), # Don't buffer python output, so we don't lose any #'-e', 'PYTHONUNBUFFERED=1', # Set current working directory #'-w', run_dir, # Note that hidden data dir is excluded here! # Set the right image task_args['submit_docker_command'], ] if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') logger.info(\"Invoking participant docker submission: %s\", participant_docker_submission_cmd) participant_docker_process = Popen(participant_docker_submission_cmd) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') Below I will discuss some of the important options... Note, we need to mount two folders ( datain:/mnt/in , dataout:/mnt/out ). We don't need to change anything about datain because we can use :ro to make it read-only inside the docker container like so /mnt/in:ro . This keeps users from tampering with input data. dataout needs to be owned by user \"participant\". We do this like so... $ chown participant:participant <path to>/dataout and make it writable as well: $ chmod 0777 <path to>/dataout If participant is the user that will be using these mounts then we need the container to run as participant . Notice we run the docker with -u participant , but how do we get that user in the docker ahead of time. In the Dockerfile for a submission add this to the top: ARG USER=participant ARG UID=1000 ARG GID=1000 # default password for user ARG PW=participant # Option1: Using unencrypted password/ specifying password RUN useradd -m ${USER} --uid=${UID} && echo \"${USER}:${PW}\" | \\ chpasswd This should add user participant with password participant . Finally we need to make sure the container has no internet. We add flag --net none to cut internet to the docker rendering it unable to transfer data. Lastly if this is an nvidia compute worker codalab/competitions-v1-nvidia-worker:latest then we have a few extra things ot take care of. On lines 274-275 there are two variables that don't exist being referenced. max_execution_time_limit = task_args['max_execution_time_limit'] previous_execution_time = task_args['previous_execution_time'] to #max_execution_time_limit = task_args['max_execution_time_limit'] # -BB- #previous_execution_time = task_args['previous_execution_time'] # -BB- and at the bottom of the file (~ line 700): signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution to #signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution -BB-","title":"Docker Submission"},{"location":"notes/codalab/docker_submission/#docker-submission","text":"This is a documentation for adding docker submissions to the open source project CodaLab .","title":"Docker Submission"},{"location":"notes/codalab/docker_submission/#introduction","text":"Codalab comes with essentially 2 types of submissions from participants: Results Submission: Participants submit the \"results\" of their algorithms. Code Submission: Participants submit actual code to be executed by the challenge organizer. The code can be executed directly as an executable ($ python sample.py...) or participants can define classes that the organizer calls themselves (hence the participants do not have to read the input data themselves). We will be adding a third option where participants can submit a docker image with their code and libraries baked in.","title":"Introduction"},{"location":"notes/codalab/docker_submission/#step-1-install-codalab","text":"First we need a running instance installed. Setup the challenge to accept a results submission, as we will tweak this process to evaluate the output of a docker container.","title":"Step 1: Install Codalab"},{"location":"notes/codalab/docker_submission/#step-2-add-database-option-for-docker-submission","text":"","title":"Step 2: Add Database Option for Docker Submission"},{"location":"notes/codalab/docker_submission/#add-new-column-in-competition-table","text":"Find file /codalab/apps/web/models.py . Approximately on line 233 is the Competition model definition. It has definitions for the columns of this table. It looks like this: class Competition(ChaHubSaveMixin, models.Model): ... Add enable_docker_submission as a BooleanField : Add this line: enable_docker_submission = models.BooleanField(default=False, verbose_name=\"Allow participant docker submission\") It doesn't matter what order the columns are in, so if you are familiar with setting up models in Django or Flask projects, just add a column that we can reference later.","title":"Add New Column in Competition Table"},{"location":"notes/codalab/docker_submission/#migrate-database-so-we-can-see-our-changes","text":"Codalab as of this writing (5/1/2020) is deployed with an orchestration of docker containers. One of these is the database (postgres). Using docker ps we can see our images: bbearce@MedICI-CodaLab-Master:~/src$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b8196a2e9fc5 nginx \"bash -x /app/docker\u2026\" 4 months ago Up 2 months 0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp nginx 528d2dc976a1 codalab-competitions_django \"bash /app/docker/ru\u2026\" 4 months ago Up 2 months 0.0.0.0:8000->8000/tcp django c56dcc7fb3e2 codalab-competitions_worker_site \"sh /app/docker/run_\u2026\" 4 months ago Up 2 months worker_site 23996bbcfbd0 codalab/competitions-v1-compute-worker:1.1.7 \"/bin/sh -c 'celery \u2026\" 4 months ago Up 2 months worker_compute 3f5a1df0f9ae codalab-competitions_flower \"/usr/bin/dumb-init \u2026\" 4 months ago Up 2 months 0.0.0.0:5555->5555/tcp flower b93b3f7e0a3b codalab-competitions_rabbit \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp rabbit 5e3492959db1 postgres:9.6.3 \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 5432/tcp postgres We need use manage.py from the django container to migrate the database. This will talk to the postgres container for us. Codalab has notes for how to do this so we will follow their lead : Note, in the link from codalab they have a placeholder for APPNAME . Substitute web in for this. root@django:/app/codalab# ./manage schemamigration web --auto ./manage: line 3: ./config/generated/startup_env.sh: No such file or directory ./manage: line 4: ../venv/bin/activate: No such file or directory /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) + Added field enable_docker_submission on web.Competition Created 0085_auto__add_field_competition_enable_docker_submission.py. You can now apply this migration with: ./manage.py migrate web Once you have that migrate the database. ./manage migrate This will apply the migration and look for all changes to all tables. Looking in the results we can see our migration in action: ~ ~ Running migrations for web: - Migrating forwards to 0085_auto__add_field_competition_enable_docker_submission. > web:0085_auto__add_field_competition_enable_docker_submission ~ ~ Now let's verify our change. First create a database superuser: docker exec -it django python manage.py createsuperuser This will prompt you to create a superuser: bbearce@miccai2019:~/src/codalab-competitions/codalab$ docker exec -it django python manage.py createsuperuser /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) Username: admin Email address: bbearce@mgh.harvard.edu Password: Password (again): Superuser created successfully. Now proceed to http://<your domain>.com/admin and log in with the credentials you made. Find the \"Web\" app section and click \"Competitions\". Next scroll till you see \"Allow participant docker submission\" or whatever you put as your new column.","title":"Migrate Database So We Can See Our Changes"},{"location":"notes/codalab/docker_submission/#add-our-new-option-to-the-competitionform-class","text":"Django or CodaLab uses apps.web.forms.CompetitionForm to manage the edit competiton form. class CompetitionForm(forms.ModelForm): class Meta: model = models.Competition fields = ( 'title', 'description', 'queue', 'disallow_leaderboard_modifying', 'force_submission_to_leaderboard', 'image', 'end_date', 'published', 'enable_medical_image_viewer', 'enable_detailed_results', # change start 'enable_docker_submission', # change end 'admins', ... This will allow us to see this option in the edit competition view: This has the effect of allowing access to this option from multiple views so we can conditionally render a docker submission form or a regular result submission form.","title":"Add Our new Option to the CompetitionForm Class"},{"location":"notes/codalab/docker_submission/#step-3-add-conditional-rendering-for-dockerresult-submission","text":"","title":"Step 3: Add Conditional Rendering for Docker\\Result Submission"},{"location":"notes/codalab/docker_submission/#add-ui-elements","text":"Find template apps/web/templates/web/competitions/_submit_results_page.html . Roughly on line 70 there is a {% if USE_AWS %} block and in the {% else %} block we can add our code. Replace the inside of the else block with this code: {% if phase.competition.enable_docker_submission %} <h1>DOCKER SUBMISSION ENABLED </h1> <!-- Submit Docker Button --> <button id=submit-button type=\"button\" data-toggle=\"modal\" data-target=\"#submit-docker-dialog\" class=\"btn btn-secondary\">Submit Docker</button> <!-- Submit Docker Button --> {% else %} <h1>DOCKER SUBMISSION DISABLED</h1> <!-- original code from codalab --> <button id=\"fileUploadButton\" class=\"button btn btn-primary {% if not phase.reference_data %}disabled{% endif %}\" {% if not phase.reference_data %}disabled=\"disabled\"{% endif %}> Submit </button> <!-- original code from codalab --> {% endif %} Our If Else block has the original code in the else. At this point if you refresh the page and navigate to the \"Participate\" tab, you should see this. Now if we go to \"Edit\" and look at the competition options, we see that if we check the box \"Allow participant docker submission\" we can see the text change on the \"Participate\" tab. Next we need the \"Submit Docker\" button to do something. Let's add a modal that get's triggered when you click the button. Add this code just before the line that has {% include \"web/common/_submission_details_template.html\" %} on line ~167: <div class=\"modal fade\" id=\"submit-docker-dialog\"> <div class=\"modal-dialog\"> <div class=\"modal-content\"> <div class=\"modal-header\"> <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</button> <h4 class=\"modal-title\">Submit Docker</h4> </div> <div class=\"modal-body\"> <h4>Submit Docker Run Command</h4> <textarea id=\"docker-run-command\" type=text style=\"width: 500px; height: 250px;\">docker run -v /<local-path>:/<docker-container-path-1> -v /<local-path>:/<docker-container-path-2> user/repo:tag</textarea> </div> <div class=\"modal-footer\"> <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button> <button id=\"submit-docker\" class=\"btn btn-primary\">Send</button> </div> </div> </div> </div> You should see this when you click the \"Submit Docker\" button: Now let's tie the \"Send\" button from the modal to some javascript so we can call the backend to process this for us. At the bottom of the same file we've been working in, add this code and create the file apps/web/static/js/submit_docker.js: <!-- Docker Submission Front End --> <script type=\"text/javascript\" src=\"{{STATIC_URL}}js/submit_docker.js\"></script> <!-- Docker Submission Front End --> In submit_docker.js add this: $('#submit-docker').click(function(){ //copy from codalab/apps/web/static/js/Competition.js file $('#details').html('Creating new submission...'); var competitionId = $(\"#competitionId\").val() var description = $('#submission_description_textarea').val() || ''; var method_name = $('#submission_method_name').val() || ''; var method_description = $('#submission_method_description').val() || ''; var project_url = $('#submission_project_url').val() || ''; var publication_url = $('#submission_publication_url').val() || ''; var bibtex = $('#submission_bibtex').val() || ''; var team_name = $('#submission_team_name').val() || ''; var organization_or_affiliation = $('#submission_organization_or_affiliation').val() || ''; var phase_id = $('#submission_phase_id').val(); // capture docker run command var docker_run_cmd = $('#docker-run-command').val(); console.log(docker_run_cmd); // capture docker run command $('#submission_description_textarea').val(''); // console.log('/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + // '&method_name=' + encodeURIComponent(method_name) + // '&method_description=' + encodeURIComponent(method_description) + // '&project_url=' + encodeURIComponent(project_url) + // '&publication_url=' + encodeURIComponent(publication_url) + // '&bibtex=' + encodeURIComponent(bibtex) + // '&team_name=' + encodeURIComponent(team_name) + // '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + // '&phase_id=' + encodeURIComponent(phase_id)+ // '&docker-run-command='+ encodeURIComponent(docker_run_cmd) // ) $.ajax({ url: '/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + '&method_name=' + encodeURIComponent(method_name) + '&method_description=' + encodeURIComponent(method_description) + '&project_url=' + encodeURIComponent(project_url) + '&publication_url=' + encodeURIComponent(publication_url) + '&bibtex=' + encodeURIComponent(bibtex) + '&team_name=' + encodeURIComponent(team_name) + '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + '&phase_id=' + encodeURIComponent(phase_id)+ '&docker-run-command='+ encodeURIComponent(docker_run_cmd), type: 'post', cache: false, async: false, data: { 'id': 'trackingid', 'name': '', 'type': '', 'size': '' } }).done(function(response) { $('#details').html(''); $('#user_results tr.noData').remove(); $('#user_results').append(Competition.displayNewSubmission(response, description, method_name, method_description, project_url, publication_url, bibtex, team_name, organization_or_affiliation)); $('#user_results #' + response.id + ' .glyphicon-plus').on('click', function() { Competition.showOrHideSubmissionDetails(this) }); //$('#fileUploadButton').removeClass('disabled'); //$('#fileUploadButton').text(\"Submit Results...\"); $('#user_results #' + response.id + ' .glyphicon-plus').click(); location.reload(true); }).fail(function(jqXHR) { var msg = 'An unexpected error occurred.'; if (jqXHR.status == 403) { msg = jqXHR.responseJSON.detail; } $('#details').html(msg); //$('#fileUploadButton').text(\"Submit Results...\"); $('#fileUploadButton').removeClass('disabled'); }); $('#submit-docker-dialog').modal('toggle'); }); From looking at the ajax call you can see we are hitting the route for an api that ultimately triggers a POST request in view \"CompetitionSubmissionViewSet\" under the api app. The part we need to edit is the post_save() method.","title":"Add UI Elements"},{"location":"notes/codalab/docker_submission/#step-4-edit-site-worker-code","text":"Replace the post_save() with this: def post_save(self, obj, created): # Original Code #if created: # evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only)) if escape(self.request.QUERY_PARAMS.get('docker-run-command')) == 'None': # submit_docker_command = None submit_docker_command = \"this is not a docker submission\" else: submit_docker_command = escape(self.request.QUERY_PARAMS.get('docker-run-command', \"\")).replace('&lt;','<').replace('&gt;','>') ## - pass extra info to submission if created: evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only, submit_docker_command)) ## - pass extra info to submission At the bottom of post_save() is evaluate_submission . That is from apps/web/tasks.py. We are passing the submit_docker_command to evaluate submission . Change evaluate_submission to accept the submit_docker_command . Also edit it to pass the command to task_func() Add submit_docker_command to these two sections: def evaluate_submission(submission_id, is_scoring_only, submit_docker_command): and: task_func(submission, job_id, submit_docker_command) task_func is actually a user defined variable for score , a function in the same file above evaluate_submission . Add submit_docker_command to the input of score . def score(submission, job_id, submit_docker_command): Towards the bottom of score , _prepare_compute_worker_run needs submit_docker_command . Pass it in, and subsequently add it to it's function definition at towards the top of the page: _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction=False) and def _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction): Inside this function you will find a data variable we need to add the docker command to: data = { \"id\": job_id, \"task_type\": \"run\", \"task_args\": { \"submission_id\": submission.pk, \"docker_image\": docker_image, \"ingestion_program_docker_image\": docker_image, \"bundle_url\": _make_url_sassy(bundle_url), \"stdout_url\": _make_url_sassy(stdout, permission='w'), \"stderr_url\": _make_url_sassy(stderr, permission='w'), \"output_url\": _make_url_sassy(output, permission='w'), \"ingestion_program_output_url\": _make_url_sassy(submission.ingestion_program_stdout_file.name, permission='w'), \"ingestion_program_stderr_url\": _make_url_sassy(submission.ingestion_program_stderr_file.name, permission='w'), \"detailed_results_url\": _make_url_sassy(submission.detailed_results_file.name, permission='w'), \"private_output_url\": _make_url_sassy(submission.private_output_file.name, permission='w'), \"secret\": submission.secret, \"execution_time_limit\": submission.phase.execution_time_limit, \"predict\": is_prediction, 'submit_docker_command':submit_docker_command, ## - Added submit_docker_command here } At this point the code hands off the data to the Compute-Worker with compute_worker_run(data, soft_time_limit=time_limit, priority=2) .","title":"Step 4: Edit Site-Worker Code"},{"location":"notes/codalab/docker_submission/#step-5-the-compute-worker","text":"We need to enter the compute-worker docker container and edit \"worker.py\". On the machine where the compute-worker is running execute: $ docker exec -it codalab/competitions-v1-compute-worker:latest ...or use the compute image that you need. Once inside you should be able to see \"worker.py\". Open it. Find function run(task_id, task_args) . This is where we will add our code on approx line 500. Add this: if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") logger.info(\"Invoking program %s\", participant_docker_cmd) participant_docker_process = Popen(participant_docker_cmd.split(\" \")) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') This is the important line: participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") This line substitutes the results directory into a mount placeholder so that the container can put the results from the run into it. Then the compute-worker can consume the results. Update (7/17/2020): Docker Image Submission only and resulting handling: participant_docker_submission_cmd = [ 'docker', 'run', # Ask all participants to add this user '-u', 'participant', # Cut internet '--net', 'none', # Remove it after run '--rm', # Add support for GPUs and nvidia '--gpus', 'all', # Give it a name associated to task_id '--name={}'.format(\"participant_docker_submission_taskid_\"+str(task_id)), # Try the new timeout feature '--stop-timeout={}'.format(execution_time_limit), # Don't allow subprocesses to raise privileges '--security-opt=no-new-privileges', # Set the right volume '-v', '{0}:/mnt/in:ro'.format('/home/bbearce/Documents/docker_submissions/directory_of_files'), # :ro for read-only file system '-v', '{0}:/mnt/out'.format(input_dir+\"/res\"), # Set aside 512m memory for the host #'--memory', '{}MB'.format(available_memory_mib - 512), # Don't buffer python output, so we don't lose any #'-e', 'PYTHONUNBUFFERED=1', # Set current working directory #'-w', run_dir, # Note that hidden data dir is excluded here! # Set the right image task_args['submit_docker_command'], ] if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') logger.info(\"Invoking participant docker submission: %s\", participant_docker_submission_cmd) participant_docker_process = Popen(participant_docker_submission_cmd) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') Below I will discuss some of the important options... Note, we need to mount two folders ( datain:/mnt/in , dataout:/mnt/out ). We don't need to change anything about datain because we can use :ro to make it read-only inside the docker container like so /mnt/in:ro . This keeps users from tampering with input data. dataout needs to be owned by user \"participant\". We do this like so... $ chown participant:participant <path to>/dataout and make it writable as well: $ chmod 0777 <path to>/dataout If participant is the user that will be using these mounts then we need the container to run as participant . Notice we run the docker with -u participant , but how do we get that user in the docker ahead of time. In the Dockerfile for a submission add this to the top: ARG USER=participant ARG UID=1000 ARG GID=1000 # default password for user ARG PW=participant # Option1: Using unencrypted password/ specifying password RUN useradd -m ${USER} --uid=${UID} && echo \"${USER}:${PW}\" | \\ chpasswd This should add user participant with password participant . Finally we need to make sure the container has no internet. We add flag --net none to cut internet to the docker rendering it unable to transfer data. Lastly if this is an nvidia compute worker codalab/competitions-v1-nvidia-worker:latest then we have a few extra things ot take care of. On lines 274-275 there are two variables that don't exist being referenced. max_execution_time_limit = task_args['max_execution_time_limit'] previous_execution_time = task_args['previous_execution_time'] to #max_execution_time_limit = task_args['max_execution_time_limit'] # -BB- #previous_execution_time = task_args['previous_execution_time'] # -BB- and at the bottom of the file (~ line 700): signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution to #signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution -BB-","title":"Step 5: The Compute-Worker"},{"location":"notes/couchdb/basics/","text":"CouchDB Good delete script Source moduliertersingvogel Supplementary Material tutorialspoint Keep in mind that your view determines how you structure the object in todelete.append( object ) #!/usr/bin/env python3 # coding: utf-8 import json import requests import sys database=sys.argv[1] if len(database)==0: sys.exit(1) # You can use views r=requests.get(\"http://localhost:5984/{}/_all_docs\".format(database)) rows=json.loads(r.text)['rows'] print(len(rows)) todelete=[] for doc in rows: # original # todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][\"rev\"]}) todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][0]}) r=requests.post(\"http://localhost:5984/{}/_bulk_docs\".format(database), json={\"docs\": todelete}) print(r.status_code)","title":"Basics"},{"location":"notes/couchdb/basics/#couchdb","text":"","title":"CouchDB"},{"location":"notes/couchdb/basics/#good-delete-script","text":"Source moduliertersingvogel Supplementary Material tutorialspoint Keep in mind that your view determines how you structure the object in todelete.append( object ) #!/usr/bin/env python3 # coding: utf-8 import json import requests import sys database=sys.argv[1] if len(database)==0: sys.exit(1) # You can use views r=requests.get(\"http://localhost:5984/{}/_all_docs\".format(database)) rows=json.loads(r.text)['rows'] print(len(rows)) todelete=[] for doc in rows: # original # todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][\"rev\"]}) todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][0]}) r=requests.post(\"http://localhost:5984/{}/_bulk_docs\".format(database), json={\"docs\": todelete}) print(r.status_code)","title":"Good delete script"},{"location":"notes/couchdb/container/","text":"From Container Source docker $ docker pull couchdb This will get you latest. Then as per their instructoins run this command: $ docker run -d --name my-couchdb couchdb:tag If you want to expose the port to the outside world, run $ docker run -p 5984:5984 -d couchdb Start not in Admin Party mode: $ docker run -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -d couchdb Example I used for a simple project: $ docker run -d --name ben-couchdb2 -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -p 5985:5984 couchdb:latest","title":"As container"},{"location":"notes/couchdb/container/#from-container","text":"Source docker $ docker pull couchdb This will get you latest. Then as per their instructoins run this command: $ docker run -d --name my-couchdb couchdb:tag If you want to expose the port to the outside world, run $ docker run -p 5984:5984 -d couchdb Start not in Admin Party mode: $ docker run -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -d couchdb Example I used for a simple project: $ docker run -d --name ben-couchdb2 -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -p 5985:5984 couchdb:latest","title":"From Container"},{"location":"notes/css/css/","text":"CSS Style something with: p { color:red; }","title":"CSS"},{"location":"notes/css/css/#css","text":"Style something with: p { color:red; }","title":"CSS"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/","text":"Geometric Operations In this video we will discuss Geometric Operations we will cover: Geometric Operations Scaling Translation Rotation We will show the one channel representation, but unless specified we can apply these operations to each channel simultaneously. We will treat the image as a function of y and x. y is the vertical direction, x is the horizontal direction. Although x and y are real numbers, we will sample integer points of x and y. for example point 0,0 or point 1,1. In a geometric transformation, we change the coordinates of the image x and y. The new image, g is a function of x prime and y- prime g of x prime and y prime has the value of f corresponding to the values that have been mapped from x and y. We will deal with a subset of Geometric Transformations called Affine transformations Scaling Scaling is where we reshape the image, we can shrink or expand the image in a horizontal and or vertical direction. Shrinking the image or making the image larger We can scale the image along the x axis. Let us use \u201ca\u201d to represent the scaling factor We can scale the image by 2. g of y and 2x is equal to f of f of y and x, hence the values of the image g will look stretched relative to image f. Consider the following image, we will only work on the corner pixels points. Consider the image at Point 0,0, the x coordinate is in red. Applying the transform nothing happens, here is point 0, 5. Applying the transform the the x coordinates is mapped from 5 to 10 in red. Applying the transform to point 5,0 nothing happens the point 5,5 is mapped to 5, 10 overlaying the image points we see the image appears stretched. Let\u2019s look at the relationship between the pixel f and g. The first column of f is mapped to the first column of g. The second column of f is mapped the third column of g the final column of f is mapped to the following column in g. The colors shows the relation between columns of image f and g. We see several column\u2019s of g have no corresponding value in f. To determine the unknown pixel value we use Interpolation. Interpolation is where we used neighbouring pixels to determine the value of an unknown pixel. In this case, we use the nearest neighbours; this just assigns the value based on the nearest pixel; there are other methods that PIL and open cv use. We can scale the horizontal axis; we will denote the vertical scale factor as d. Lets set d to two. Nothing changes when applying the transform to the top portion of the image For the point 5,0, the new value after the transform is applied is mapped to 10, 0 the point 5, 5 is mapped to 10,5. The result is the image is stretched in the horizontal direction We can use this method to make the image larger, if the values of a or d are less than zero; the image will shrink. Translation Translation is where we shift the image. We can shift an image horizontally by adding the number of pixels \u201ctx\u201d then by mapping the new location x prime. We can add two pixels. Consider the following intensity values. We can apply the transform shifting the points. The points shift, transforming the next set of points. Shifting the points, the image appears shifted. Let\u2019s see what happens to each pixel. We see that pixels have been shifted, those pixels that are on the edge have been replaced by zero values. We can increase the size of the image to include the pixels that have been shifted. We can shift the image vertically by adding pixels ty, this shifts the image vertically. We can represent a geometric transformation as a set of equations, putting the equations in matrix form, we get the Affine Transformation matrix With open cv you input this matrix as an array. We also have the shear parameters; we will not cover them in this Course. Rotate Image You can also rotate an image We can rotate an image by an angle theta, where the red lines represent the original orientation of the horizontal and vertical axis. We can use a similar matrix to rotate an image. This Rotation matrix will perform a counter-clockwise rotation; the expression for the matrix is quite complex. We will simplify by assuming the Isotropic scale factor \u201cr\u201d is 1, and we will rotate from the center of the image to simplify If you make the above assumptions, libraries like PIL and open CV only require the parameter theta Lets try PIL. PIL In PIL we can scale the image by specifying the integer number of pixel\u2019s using the method \u201cresize\". We can double the width of the image. We apply the method resize the image is twice as wide. You can also shrink the image but the input must be an integer. We can rotate the image. We use the method rotate; the input is the angle we would like to rotate the image by. OpenCV Lets try open CV. In open cv we can use the function resize to rescale the image. We scale the horizontal axis by two and leave the vertical axis as is. We can also specify the interpolation the result is the image is twice as wide. The scaling factor does not have to be an integer, and it can be less than one, Translation requires the Affine Transformation matrix M. Where tx is the number of pixels you shift the location in the horizontal direction, ty is the number of pixels you shift in the vertical direction. We will leave the scale factors as one. We input the image and matrix into the function warpAffine, we also input the shape of the output image. The image is shifted. There is a lot more you can do using this method. We can obtain the matrix to rotate the image using getRotationMatrix2D, this will rotate the image by angle \u03b8 The parameter center is the Center of the rotation in the source, scale will be set to one Like before we rotate the image. Check out the lab for more examples. Code Lab PIL # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png import matplotlib.pyplot as plt from PIL import Image import numpy as np from PIL import ImageOps First, let's define a helper function to plot two images side-by-side. You will not need to understand this code this moment, but this function will be used repeatedly in this tutorial to showcase the results. def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1,cmap=\"gray\") plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2,cmap=\"gray\") plt.title(title_2) plt.show() Geometric Transformations Geometric transformations allow you to perform different operations like translation, i.e. to shift, reshape and rotate the image. # We can resize an image using the method resize() of PIL images, which takes the resized image's width and height as paramters. # Consider the following image: image = Image.open(\"lenna.png\") plt.imshow(image) plt.show() # We can scale the horizontal axis by two and leave the vertical axis as is: width, height = image.size new_width = 2 * width new_hight = height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # In the same manner, we can scale the vertical axis by two: new_width = width new_hight = 2 * height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # We can double both the width and the height of the image: new_width = 2 * width new_hight = 2 * height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # We can shrink too new_width = width // 2 new_hight = height // 2 new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # Rotation theta = 45 new_image = image.rotate(theta) plt.imshow(new_image) plt.show() Mathematical Operations image = np.array(image) # We can then add the constant to the image array: new_image = image + 20 plt.imshow(new_image) plt.show() # We can also multiply every pixel's intensity value by a constant value. new_image = 10 * image plt.imshow(new_image) plt.show() # We can add the elements of two arrays of equal shape. In this example, we generate an array of random noises with the same shape and data type as our image. Noise = np.random.normal(0, 20, (height, width, 3)).astype(np.uint8) Noise.shape # We add the generated noise to the image and plot the result. We see the values that have corrupted the image: new_image = image + Noise plt.imshow(new_image) plt.show() # At the same time, we can multiply the elements of two arrays of equal shape. We can multiply the random image and the Lenna image and plot the result. new_image = image*Noise plt.imshow(new_image) plt.show() Matrix Operations im_gray = Image.open(\"barbara.png\") # Even though the image is gray, it has three channels; we can convert it to a one-channel image. im_gray = ImageOps.grayscale(im_gray) im_gray = np.array(im_gray ) plt.imshow(im_gray,cmap='gray') plt.show() # We can apply algorithms designed for matrices. We can use Singular Value Decomposition, decomposing our image matrix into a product of three matrices. U, s, V = np.linalg.svd(im_gray , full_matrices=True) They did not cover this so here is a tid bit and a link Source: ChatGPT to beware Singular Value Decomposition is a fundamental matrix decomposition technique in linear algebra. Given a matrix A, the SVD factorizes it into three matrices: U: The left singular vectors matrix, where each column represents an orthonormal basis for the row space of A. S: The diagonal singular values matrix, which contains the singular values of A in descending order. These values are non-negative and provide information about the relative importance of different components in the decomposition. V^T (V transpose): The right singular vectors matrix, where each column represents an orthonormal basis for the column space of A. The SVD decomposition can be expressed as A = U * S * V^T. numpy.linalg.svd allows you to compute this decomposition for a given matrix A in NumPy. You can use it for various numerical and scientific computing tasks, including dimensionality reduction, data compression, solving linear equations, and more. It's a versatile tool for linear algebra operations involving matrices. Source: Humans: geeksforgeeks We see s is not rectangular: s.shape # We can convert s to a diagonal matrix S: S = np.zeros((im_gray.shape[0], im_gray.shape[1])) S[:image.shape[0], :image.shape[0]] = np.diag(s) S.shape # (512, 512) # Plot matrix U and V plot_image(U, V, title_1=\"Matrix U\", title_2=\"Matrix V\") # We see most elements in S are zero: plt.imshow(S, cmap='gray') plt.show() # We can find the matrix product of all the matrices. First, we can perform matrix multiplication on S and U and assign it to `B` and plot the results: B = S.dot(V) plt.imshow(B,cmap='gray') plt.show() # We can find the matrix product of `U`, `S`, and `B`. We see it's the entire image: A = U.dot(B) plt.imshow(A,cmap='gray') plt.show() # It turns out that many elements are redundant. We can eliminate some rows and columns of S and V and approximate the image by finding the product: for n_component in [1,10,100,200, 500]: S_new = S[:, :n_component] V_new = V[:n_component, :] A = U.dot(S_new.dot(V_new)) plt.imshow(A,cmap='gray') plt.title(\"Number of Components:\"+str(n_component)) plt.show() OpenCV import cv2 Scaling toy_image = np.zeros((6,6)) toy_image[1:5,1:5]=255 toy_image[2:4,2:4]=0 plt.imshow(toy_image,cmap='gray') plt.show() toy_image We can rescale along a specific axis: fx : scale factor along the horizontal axis fy : scale factor along the vertical axis The parameter interpolation estimates pixel values based on neighboring pixels. INTER_NEAREST uses the nearest pixel and INTER_CUBIC uses several pixels near the pixel value we would like to estimate. new_toy = cv2.resize(toy_image,None,fx=2, fy=1, interpolation = cv2.INTER_NEAREST ) plt.imshow(new_toy,cmap='gray') plt.show() # Read image image = cv2.imread(\"lenna.png\") plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.show() # We can scale the horizontal axis by two and leave the vertical axis as is: new_image = cv2.resize(image, None, fx=2, fy=1, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # In the same manner, we can scale the vertical axis by two: new_image = cv2.resize(image, None, fx=1, fy=2, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can scale the horizontal axis and vertical axis by two. new_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can also shrink the image by setting the scaling factor to a real number between 0 and 1: new_image = cv2.resize(image, None, fx=1, fy=0.5, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can also specify the number of rows and columns: rows = 100 cols = 200 new_image = cv2.resize(image, (100, 200), interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) Translation Translation is when you shift the location of the image. tx is the number of pixels you shift the location in the horizontal direction and ty is the number of pixels you shift in the vertical direction. You can create the transformation matrix \\(M\\) to shift the image. In this example, we shift the image 100 pixels horizontally: tx = 100 ty = 0 M = np.float32([[1, 0, tx], [0, 1, ty]]) M # The shape of the image is given by: rows, cols, _ = image.shape We use the function warpAffine from the cv2 module. The first input parater is an image array, the second input parameter is the transformation matrix M , and the final input paramter is the length and width of the output image \\((cols,rows)\\) : new_image = cv2.warpAffine(image, M, (cols, rows)) # We can plot the image; the portions of the image that do not have any intensities are set to zero: plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can see some of the original image has been cut off. We can fix this by changing the output image size: (cols + tx,rows + ty): new_image = cv2.warpAffine(image, M, (cols + tx, rows + ty)) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can shift the image horizontally: tx = 0 ty = 50 M = np.float32([[1, 0, tx], [0, 1, ty]]) new_iamge = cv2.warpAffine(image, M, (cols + tx, rows + ty)) plt.imshow(cv2.cvtColor(new_iamge, cv2.COLOR_BGR2RGB)) plt.show() Rotation We can rotate an image by angle \u03b8 which is achieved by the Rotation Matrix getRotationMatrix2D . center : Center of the rotation in the source image. We will only use the center of the image. angle : Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner). scale : Isotropic scale factor, in this course the value will be one. We can rotate our toy image by 45 degrees: theta = 45.0 M = cv2.getRotationMatrix2D(center=(3, 3), angle=theta, scale=1) new_toy_image = cv2.warpAffine(toy_image, M, (6, 6)) plot_image(toy_image, new_toy_image, title_1=\"Orignal\", title_2=\"rotated image\") # Looking at intensity values, we see that many values have been interpolated: new_toy_image # We can perform the same operation on color images: cols, rows, _ = image.shape M = cv2.getRotationMatrix2D(center=(cols // 2 - 1, rows // 2 - 1), angle=theta, scale=1) new_image = cv2.warpAffine(image, M, (cols, rows)) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() Mathematical Operations Array Operations # We can perform array operations on an image; Using Python broadcasting, we can add a constant to each pixel's intensity value. new_image = image + 20 plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can also multiply every pixel's intensity value by a constant value. new_image = 10 * image plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can add the elements of two arrays of equal shape. In this example, we generate an array of random noises with the same shape and data type as our image. Noise = np.random.normal(0, 20, (rows, cols, 3)).astype(np.uint8) Noise.shape # We add the generated noise to the image and plot the result. We see the values that have corrupted the image: new_image = image + Noise plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # At the same time, we can multiply the elements of two arrays of equal shape. We can multiply the random image and the Lenna image and plot the result. new_image = image*Noise plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() Matrix Operations # Grayscale images are matrices. Consider the following grayscale image: im_gray = cv2.imread('barbara.png', cv2.IMREAD_GRAYSCALE) im_gray.shape plt.imshow(im_gray,cmap='gray') plt.show() # We can apply algorithms designed for matrices. We can use Singular Value Decomposition, decomposing our image matrix into a product of three matrices. U, s, V = np.linalg.svd(im_gray , full_matrices=True) We see s is not rectangular: s.shape # (512,) # We can convert s to a diagonal matrix S. S = np.zeros((im_gray.shape[0], im_gray.shape[1])) S[:image.shape[0], :image.shape[0]] = np.diag(s) # We can plot the matrix `U` and `V`: plot_image(U,V,title_1=\"Matrix U \",title_2=\"matrix V\") # We see most of the elements in `S` are zero: plt.imshow(S,cmap='gray') plt.show() # We can find the matrix product of all the matrices. First, we can perform matrix multiplication on `S` and `U` and assign it to `B` and plot the results: B = S.dot(V) plt.imshow(B,cmap='gray') plt.show() # We can find the matrix product of `U`, `S`, and `B`. We see it\u2019s the entire image: A = U.dot(B) plt.imshow(A,cmap='gray') plt.show() # It turns out many elements are redundant, so we can eliminate some rows and columns of `S` and `V` and approximate the image by finding the product. for n_component in [1,10,100,200, 500]: S_new = S[:, :n_component] V_new = V[:n_component, :] A = U.dot(S_new.dot(V_new)) plt.imshow(A,cmap='gray') plt.title(\"Number of Components:\"+str(n_component)) plt.show()","title":"6. Geometric Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#geometric-operations","text":"In this video we will discuss Geometric Operations we will cover: Geometric Operations Scaling Translation Rotation We will show the one channel representation, but unless specified we can apply these operations to each channel simultaneously. We will treat the image as a function of y and x. y is the vertical direction, x is the horizontal direction. Although x and y are real numbers, we will sample integer points of x and y. for example point 0,0 or point 1,1. In a geometric transformation, we change the coordinates of the image x and y. The new image, g is a function of x prime and y- prime g of x prime and y prime has the value of f corresponding to the values that have been mapped from x and y. We will deal with a subset of Geometric Transformations called Affine transformations","title":"Geometric Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#scaling","text":"Scaling is where we reshape the image, we can shrink or expand the image in a horizontal and or vertical direction. Shrinking the image or making the image larger We can scale the image along the x axis. Let us use \u201ca\u201d to represent the scaling factor We can scale the image by 2. g of y and 2x is equal to f of f of y and x, hence the values of the image g will look stretched relative to image f. Consider the following image, we will only work on the corner pixels points. Consider the image at Point 0,0, the x coordinate is in red. Applying the transform nothing happens, here is point 0, 5. Applying the transform the the x coordinates is mapped from 5 to 10 in red. Applying the transform to point 5,0 nothing happens the point 5,5 is mapped to 5, 10 overlaying the image points we see the image appears stretched. Let\u2019s look at the relationship between the pixel f and g. The first column of f is mapped to the first column of g. The second column of f is mapped the third column of g the final column of f is mapped to the following column in g. The colors shows the relation between columns of image f and g. We see several column\u2019s of g have no corresponding value in f. To determine the unknown pixel value we use Interpolation. Interpolation is where we used neighbouring pixels to determine the value of an unknown pixel. In this case, we use the nearest neighbours; this just assigns the value based on the nearest pixel; there are other methods that PIL and open cv use. We can scale the horizontal axis; we will denote the vertical scale factor as d. Lets set d to two. Nothing changes when applying the transform to the top portion of the image For the point 5,0, the new value after the transform is applied is mapped to 10, 0 the point 5, 5 is mapped to 10,5. The result is the image is stretched in the horizontal direction We can use this method to make the image larger, if the values of a or d are less than zero; the image will shrink.","title":"Scaling"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#translation","text":"Translation is where we shift the image. We can shift an image horizontally by adding the number of pixels \u201ctx\u201d then by mapping the new location x prime. We can add two pixels. Consider the following intensity values. We can apply the transform shifting the points. The points shift, transforming the next set of points. Shifting the points, the image appears shifted. Let\u2019s see what happens to each pixel. We see that pixels have been shifted, those pixels that are on the edge have been replaced by zero values. We can increase the size of the image to include the pixels that have been shifted. We can shift the image vertically by adding pixels ty, this shifts the image vertically. We can represent a geometric transformation as a set of equations, putting the equations in matrix form, we get the Affine Transformation matrix With open cv you input this matrix as an array. We also have the shear parameters; we will not cover them in this Course.","title":"Translation"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#rotate-image","text":"You can also rotate an image We can rotate an image by an angle theta, where the red lines represent the original orientation of the horizontal and vertical axis. We can use a similar matrix to rotate an image. This Rotation matrix will perform a counter-clockwise rotation; the expression for the matrix is quite complex. We will simplify by assuming the Isotropic scale factor \u201cr\u201d is 1, and we will rotate from the center of the image to simplify If you make the above assumptions, libraries like PIL and open CV only require the parameter theta Lets try PIL.","title":"Rotate Image"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#pil","text":"In PIL we can scale the image by specifying the integer number of pixel\u2019s using the method \u201cresize\". We can double the width of the image. We apply the method resize the image is twice as wide. You can also shrink the image but the input must be an integer. We can rotate the image. We use the method rotate; the input is the angle we would like to rotate the image by.","title":"PIL"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#opencv","text":"Lets try open CV. In open cv we can use the function resize to rescale the image. We scale the horizontal axis by two and leave the vertical axis as is. We can also specify the interpolation the result is the image is twice as wide. The scaling factor does not have to be an integer, and it can be less than one, Translation requires the Affine Transformation matrix M. Where tx is the number of pixels you shift the location in the horizontal direction, ty is the number of pixels you shift in the vertical direction. We will leave the scale factors as one. We input the image and matrix into the function warpAffine, we also input the shape of the output image. The image is shifted. There is a lot more you can do using this method. We can obtain the matrix to rotate the image using getRotationMatrix2D, this will rotate the image by angle \u03b8 The parameter center is the Center of the rotation in the source, scale will be set to one Like before we rotate the image. Check out the lab for more examples.","title":"OpenCV"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#code-lab","text":"","title":"Code Lab"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#pil_1","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png import matplotlib.pyplot as plt from PIL import Image import numpy as np from PIL import ImageOps First, let's define a helper function to plot two images side-by-side. You will not need to understand this code this moment, but this function will be used repeatedly in this tutorial to showcase the results. def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1,cmap=\"gray\") plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2,cmap=\"gray\") plt.title(title_2) plt.show()","title":"PIL"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#geometric-transformations","text":"Geometric transformations allow you to perform different operations like translation, i.e. to shift, reshape and rotate the image. # We can resize an image using the method resize() of PIL images, which takes the resized image's width and height as paramters. # Consider the following image: image = Image.open(\"lenna.png\") plt.imshow(image) plt.show() # We can scale the horizontal axis by two and leave the vertical axis as is: width, height = image.size new_width = 2 * width new_hight = height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # In the same manner, we can scale the vertical axis by two: new_width = width new_hight = 2 * height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # We can double both the width and the height of the image: new_width = 2 * width new_hight = 2 * height new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # We can shrink too new_width = width // 2 new_hight = height // 2 new_image = image.resize((new_width, new_hight)) plt.imshow(new_image) plt.show() # Rotation theta = 45 new_image = image.rotate(theta) plt.imshow(new_image) plt.show()","title":"Geometric Transformations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#mathematical-operations","text":"image = np.array(image) # We can then add the constant to the image array: new_image = image + 20 plt.imshow(new_image) plt.show() # We can also multiply every pixel's intensity value by a constant value. new_image = 10 * image plt.imshow(new_image) plt.show() # We can add the elements of two arrays of equal shape. In this example, we generate an array of random noises with the same shape and data type as our image. Noise = np.random.normal(0, 20, (height, width, 3)).astype(np.uint8) Noise.shape # We add the generated noise to the image and plot the result. We see the values that have corrupted the image: new_image = image + Noise plt.imshow(new_image) plt.show() # At the same time, we can multiply the elements of two arrays of equal shape. We can multiply the random image and the Lenna image and plot the result. new_image = image*Noise plt.imshow(new_image) plt.show()","title":"Mathematical Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#matrix-operations","text":"im_gray = Image.open(\"barbara.png\") # Even though the image is gray, it has three channels; we can convert it to a one-channel image. im_gray = ImageOps.grayscale(im_gray) im_gray = np.array(im_gray ) plt.imshow(im_gray,cmap='gray') plt.show() # We can apply algorithms designed for matrices. We can use Singular Value Decomposition, decomposing our image matrix into a product of three matrices. U, s, V = np.linalg.svd(im_gray , full_matrices=True) They did not cover this so here is a tid bit and a link Source: ChatGPT to beware Singular Value Decomposition is a fundamental matrix decomposition technique in linear algebra. Given a matrix A, the SVD factorizes it into three matrices: U: The left singular vectors matrix, where each column represents an orthonormal basis for the row space of A. S: The diagonal singular values matrix, which contains the singular values of A in descending order. These values are non-negative and provide information about the relative importance of different components in the decomposition. V^T (V transpose): The right singular vectors matrix, where each column represents an orthonormal basis for the column space of A. The SVD decomposition can be expressed as A = U * S * V^T. numpy.linalg.svd allows you to compute this decomposition for a given matrix A in NumPy. You can use it for various numerical and scientific computing tasks, including dimensionality reduction, data compression, solving linear equations, and more. It's a versatile tool for linear algebra operations involving matrices. Source: Humans: geeksforgeeks We see s is not rectangular: s.shape # We can convert s to a diagonal matrix S: S = np.zeros((im_gray.shape[0], im_gray.shape[1])) S[:image.shape[0], :image.shape[0]] = np.diag(s) S.shape # (512, 512) # Plot matrix U and V plot_image(U, V, title_1=\"Matrix U\", title_2=\"Matrix V\") # We see most elements in S are zero: plt.imshow(S, cmap='gray') plt.show() # We can find the matrix product of all the matrices. First, we can perform matrix multiplication on S and U and assign it to `B` and plot the results: B = S.dot(V) plt.imshow(B,cmap='gray') plt.show() # We can find the matrix product of `U`, `S`, and `B`. We see it's the entire image: A = U.dot(B) plt.imshow(A,cmap='gray') plt.show() # It turns out that many elements are redundant. We can eliminate some rows and columns of S and V and approximate the image by finding the product: for n_component in [1,10,100,200, 500]: S_new = S[:, :n_component] V_new = V[:n_component, :] A = U.dot(S_new.dot(V_new)) plt.imshow(A,cmap='gray') plt.title(\"Number of Components:\"+str(n_component)) plt.show()","title":"Matrix Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#opencv_1","text":"import cv2","title":"OpenCV"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#scaling_1","text":"toy_image = np.zeros((6,6)) toy_image[1:5,1:5]=255 toy_image[2:4,2:4]=0 plt.imshow(toy_image,cmap='gray') plt.show() toy_image We can rescale along a specific axis: fx : scale factor along the horizontal axis fy : scale factor along the vertical axis The parameter interpolation estimates pixel values based on neighboring pixels. INTER_NEAREST uses the nearest pixel and INTER_CUBIC uses several pixels near the pixel value we would like to estimate. new_toy = cv2.resize(toy_image,None,fx=2, fy=1, interpolation = cv2.INTER_NEAREST ) plt.imshow(new_toy,cmap='gray') plt.show() # Read image image = cv2.imread(\"lenna.png\") plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.show() # We can scale the horizontal axis by two and leave the vertical axis as is: new_image = cv2.resize(image, None, fx=2, fy=1, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # In the same manner, we can scale the vertical axis by two: new_image = cv2.resize(image, None, fx=1, fy=2, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can scale the horizontal axis and vertical axis by two. new_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can also shrink the image by setting the scaling factor to a real number between 0 and 1: new_image = cv2.resize(image, None, fx=1, fy=0.5, interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape) # We can also specify the number of rows and columns: rows = 100 cols = 200 new_image = cv2.resize(image, (100, 200), interpolation=cv2.INTER_CUBIC) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() print(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)","title":"Scaling"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#translation_1","text":"Translation is when you shift the location of the image. tx is the number of pixels you shift the location in the horizontal direction and ty is the number of pixels you shift in the vertical direction. You can create the transformation matrix \\(M\\) to shift the image. In this example, we shift the image 100 pixels horizontally: tx = 100 ty = 0 M = np.float32([[1, 0, tx], [0, 1, ty]]) M # The shape of the image is given by: rows, cols, _ = image.shape We use the function warpAffine from the cv2 module. The first input parater is an image array, the second input parameter is the transformation matrix M , and the final input paramter is the length and width of the output image \\((cols,rows)\\) : new_image = cv2.warpAffine(image, M, (cols, rows)) # We can plot the image; the portions of the image that do not have any intensities are set to zero: plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can see some of the original image has been cut off. We can fix this by changing the output image size: (cols + tx,rows + ty): new_image = cv2.warpAffine(image, M, (cols + tx, rows + ty)) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can shift the image horizontally: tx = 0 ty = 50 M = np.float32([[1, 0, tx], [0, 1, ty]]) new_iamge = cv2.warpAffine(image, M, (cols + tx, rows + ty)) plt.imshow(cv2.cvtColor(new_iamge, cv2.COLOR_BGR2RGB)) plt.show()","title":"Translation"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#rotation","text":"We can rotate an image by angle \u03b8 which is achieved by the Rotation Matrix getRotationMatrix2D . center : Center of the rotation in the source image. We will only use the center of the image. angle : Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner). scale : Isotropic scale factor, in this course the value will be one. We can rotate our toy image by 45 degrees: theta = 45.0 M = cv2.getRotationMatrix2D(center=(3, 3), angle=theta, scale=1) new_toy_image = cv2.warpAffine(toy_image, M, (6, 6)) plot_image(toy_image, new_toy_image, title_1=\"Orignal\", title_2=\"rotated image\") # Looking at intensity values, we see that many values have been interpolated: new_toy_image # We can perform the same operation on color images: cols, rows, _ = image.shape M = cv2.getRotationMatrix2D(center=(cols // 2 - 1, rows // 2 - 1), angle=theta, scale=1) new_image = cv2.warpAffine(image, M, (cols, rows)) plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show()","title":"Rotation"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#mathematical-operations_1","text":"","title":"Mathematical Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#array-operations","text":"# We can perform array operations on an image; Using Python broadcasting, we can add a constant to each pixel's intensity value. new_image = image + 20 plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can also multiply every pixel's intensity value by a constant value. new_image = 10 * image plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # We can add the elements of two arrays of equal shape. In this example, we generate an array of random noises with the same shape and data type as our image. Noise = np.random.normal(0, 20, (rows, cols, 3)).astype(np.uint8) Noise.shape # We add the generated noise to the image and plot the result. We see the values that have corrupted the image: new_image = image + Noise plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show() # At the same time, we can multiply the elements of two arrays of equal shape. We can multiply the random image and the Lenna image and plot the result. new_image = image*Noise plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)) plt.show()","title":"Array Operations"},{"location":"notes/data_science/Coursera/computer_vision/Geometric_Operations/#matrix-operations_1","text":"# Grayscale images are matrices. Consider the following grayscale image: im_gray = cv2.imread('barbara.png', cv2.IMREAD_GRAYSCALE) im_gray.shape plt.imshow(im_gray,cmap='gray') plt.show() # We can apply algorithms designed for matrices. We can use Singular Value Decomposition, decomposing our image matrix into a product of three matrices. U, s, V = np.linalg.svd(im_gray , full_matrices=True) We see s is not rectangular: s.shape # (512,) # We can convert s to a diagonal matrix S. S = np.zeros((im_gray.shape[0], im_gray.shape[1])) S[:image.shape[0], :image.shape[0]] = np.diag(s) # We can plot the matrix `U` and `V`: plot_image(U,V,title_1=\"Matrix U \",title_2=\"matrix V\") # We see most of the elements in `S` are zero: plt.imshow(S,cmap='gray') plt.show() # We can find the matrix product of all the matrices. First, we can perform matrix multiplication on `S` and `U` and assign it to `B` and plot the results: B = S.dot(V) plt.imshow(B,cmap='gray') plt.show() # We can find the matrix product of `U`, `S`, and `B`. We see it\u2019s the entire image: A = U.dot(B) plt.imshow(A,cmap='gray') plt.show() # It turns out many elements are redundant, so we can eliminate some rows and columns of `S` and `V` and approximate the image by finding the product. for n_component in [1,10,100,200, 500]: S_new = S[:, :n_component] V_new = V[:n_component, :] A = U.dot(S_new.dot(V_new)) plt.imshow(A,cmap='gray') plt.title(\"Number of Components:\"+str(n_component)) plt.show()","title":"Matrix Operations"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/","text":"Image Classification KNN (K-nearest neighbors) Overview We give the images labels. Say \\(y=0\\) for cat and \\(y=1\\) for dog. Concatenate the 3 channel images to a vector or use gray scale. We can calculate the Euclidean distance between two images as i.e the length of the vector connecting the two points. We have a set of images. We will refer to them as training samples. If we have an unknown sample, we calculate the distance between each image. Let's put the images in a table where each row is a sample, the second column is a class, the final column is the distance from each sample to the unknown point. We will use the distance to predict the label of the unknown class. As this is a guess or prediction of the class we use \\(\\hat{y}\\) . The hat means it's an estimate. We calculate the distance from our unknown sample. We find the nearest point or nearest neighbour. We assign the label to the unknown sample. We sometimes call this a model. KNN is simple to code yourself or you can use a software package like sklearn. We repeat the process for the next unknown sample. Test/Train Accuracy shows us how good our method works i,e the average number of times our model got it correct. Let's use the following table to calculate accuracy on some test data. The first row in the Table denotes different sample numbers; the second row shows the actual class label, the third row is the predicted value and the final row will be one if the sample is predicted correctly or else it will be zero. We count the number of times the prediction is correct, we then take the average to get the accuracy. Finding the nearest samples does not always work best, consider the following in our 2d space. The cat is next to a dog that looks like a cat, but all the other samples next to the samples are cats. To handle this we find the K closest samples. We then perform majority vote; where within the sub set we find the class with the most number of samples and assign that label to the unknown sample, Let\u2019s do an example where k=3 As before we calculate the distance. Hyper Parameter K We use a subset called the validation data to determine the best K, this is called a hyper parameter. To select the Hyperparameter we split our data set into three parts, the training set, validation set, and test set. We use the training set for different hyperparameters, we use the accuracy for K on the validation data. We select the hyperparameter K that maximizes accuracy on the validation set. Let\u2019s do an example. In this example we take two samples. We calculate the accuracy for k=1. We calculate the accuracy for k=3. We select k=3 as it has better accuracy. If our dataset has enough samples we expect this accuracy in the real world. Once you choose the value of K you can use KNN to classify an image. Let's say you build an app to recognize cats and dogs you let k equal to three you take the photo, under the hood your app will calculate the distances find the nearest neighbours and output the class as a string. Finally, its simple to add another class, for example fish. The result would look like this in our 2d space. KNN is not usually used in practice, knn is extremely slow, and it can\u2019t deal with many of the Challenges of Image Classification so let\u2019s learn about other classifiers Linear Classifier Linear classifiers are widely used in classification, and they are the building blocks of more advanced classification methods. In this video, we will discuss the two-class case we gave the images labels y equals zero for cat, y equals one for dog. We will concatenate the three-channel images to a vector, or you can do the same for grayscale images. In this video we will show you how a simple function can take an image as an input and output the image class using simple algebra. We will also show you how a similar function will take in images and input, and output a probability of how likely that image belongs to a class. For example, here the function gives a 92 percent chance, it's a dog. The equation of a line in one dimension is given by the following. Here w represents the weight term and b represents the bias term. These are called learnable parameters, and we will use them to classify the image. For arbitrary dimensions, this equation generalizes to a hyperplane. You can represent this equation as a dot product of row vector w and image x. This is called the decision plane. If you're not familiar with vectors, this is just a compact way to express the equation of a line in dots of dimensions. In this video, we will use two dimensions for visualization. Note when the letter x are not bold, it's just simple algebra, not a sample. We will set the weights w_1 to one w_2 to minus one and the bias to one. In two dimensions we can plot the equation as a plane. We can plot the plane, z equals zero. We can see the line where the plane intersects with a plane at z equals zero. If we look at it from above, we get the following image. This is the plane where z equals zero, the line is where the decision plane intersects with the planes z equals zero. This line is called the decision boundary. We can overlay our sample images. Anything on the left side of the line is a dog. Anything on the right side of the line is a cat. Let's see how we can use the value of z to determine if it's a dog or a cat. Let's look at the following sample. Consider the following values for x_1. We can plug those values into an equation and get a value of z equals two. We see z as positive. Similarly, for x_2 and x_3, every point on the left side of the line will give you a positive value. The label for x_4 is a cat and the point lies on the right side of the line. Plugging the value of x into the equation, we see z is negative. Similarly, for sample x_4 and x_6, every point on the right side of the line will output a negative value. If we use z to calculate the class of the points, it always returns real numbers such as negative one, three, negative two, and so on. But we need a class between zero and one, so how do we convert these numbers? We'll use something called a threshold function pictured here. If z is greater than zero, it will return a one, and if z is less than zero, it will return to zero. For the following image, the output is one corresponding to dog. Every sample on this side of the plane will be classified as a cat, every sample on this side of the plane will be classified as a dog. A plane can't always separate the data. The sample is misclassified. In this case, the data is not linearly separable. The logistic function resembles the threshold function and is given by the following expression known as the sigmoid function. It will give us a probability of how likely our estimate is. In addition, it has better performance than the threshold function, for reasons we will discuss later. If the value of z is a very large negative number, the expression is approximately zero and for a very large positive value of z, the expression is approximately one. For everything in the middle, the value is between zero and one. To determine y hat as a discrete class, we use a threshold shown by the line. If the output of the logistic function is larger than 0.5, we set the prediction value y hat to one. If it's less, we set y hat to zero. Let's try out some values of x that we used previously with the linear classification. If we set x equal to zero 0.32 into the equation, we get the value of z as two, we pass the value through the sigmoid function. Since the value of the sigmoid function is greater than 0.5, we set y hat to one. Similarly, if we plug in the value of x as minus two, minus three, the value of z is minus two. We pass the value through the sigmoid function. We see the result is less than 0.5. We set y hat to zero. We can also represent the logistic function as a probability. We can find the probability of the image being a dog, that is y hat equals one. We can find the probability of the image being a cat, y hat equals zero by using the following. We see the image is more likely a dog. Given the following image, we can find the probability of being a dog y hat equals one. We find the probability of it being cat, that is y hat equals zero. We see the image is more likely a cat. If you have the learnable parameters, you can use a linear classifier, you take the photo. Under the hood, you app will use the linear classifier to make a prediction and output the class as a string.","title":"8. Image Classification"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#image-classification","text":"","title":"Image Classification"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#knn-k-nearest-neighbors","text":"","title":"KNN (K-nearest neighbors)"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#overview","text":"We give the images labels. Say \\(y=0\\) for cat and \\(y=1\\) for dog. Concatenate the 3 channel images to a vector or use gray scale. We can calculate the Euclidean distance between two images as i.e the length of the vector connecting the two points. We have a set of images. We will refer to them as training samples. If we have an unknown sample, we calculate the distance between each image. Let's put the images in a table where each row is a sample, the second column is a class, the final column is the distance from each sample to the unknown point. We will use the distance to predict the label of the unknown class. As this is a guess or prediction of the class we use \\(\\hat{y}\\) . The hat means it's an estimate. We calculate the distance from our unknown sample. We find the nearest point or nearest neighbour. We assign the label to the unknown sample. We sometimes call this a model. KNN is simple to code yourself or you can use a software package like sklearn. We repeat the process for the next unknown sample.","title":"Overview"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#testtrain","text":"Accuracy shows us how good our method works i,e the average number of times our model got it correct. Let's use the following table to calculate accuracy on some test data. The first row in the Table denotes different sample numbers; the second row shows the actual class label, the third row is the predicted value and the final row will be one if the sample is predicted correctly or else it will be zero. We count the number of times the prediction is correct, we then take the average to get the accuracy. Finding the nearest samples does not always work best, consider the following in our 2d space. The cat is next to a dog that looks like a cat, but all the other samples next to the samples are cats. To handle this we find the K closest samples. We then perform majority vote; where within the sub set we find the class with the most number of samples and assign that label to the unknown sample, Let\u2019s do an example where k=3 As before we calculate the distance.","title":"Test/Train"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#hyper-parameter-k","text":"We use a subset called the validation data to determine the best K, this is called a hyper parameter. To select the Hyperparameter we split our data set into three parts, the training set, validation set, and test set. We use the training set for different hyperparameters, we use the accuracy for K on the validation data. We select the hyperparameter K that maximizes accuracy on the validation set. Let\u2019s do an example. In this example we take two samples. We calculate the accuracy for k=1. We calculate the accuracy for k=3. We select k=3 as it has better accuracy. If our dataset has enough samples we expect this accuracy in the real world. Once you choose the value of K you can use KNN to classify an image. Let's say you build an app to recognize cats and dogs you let k equal to three you take the photo, under the hood your app will calculate the distances find the nearest neighbours and output the class as a string. Finally, its simple to add another class, for example fish. The result would look like this in our 2d space. KNN is not usually used in practice, knn is extremely slow, and it can\u2019t deal with many of the Challenges of Image Classification so let\u2019s learn about other classifiers","title":"Hyper Parameter K"},{"location":"notes/data_science/Coursera/computer_vision/Image_Classification/#linear-classifier","text":"Linear classifiers are widely used in classification, and they are the building blocks of more advanced classification methods. In this video, we will discuss the two-class case we gave the images labels y equals zero for cat, y equals one for dog. We will concatenate the three-channel images to a vector, or you can do the same for grayscale images. In this video we will show you how a simple function can take an image as an input and output the image class using simple algebra. We will also show you how a similar function will take in images and input, and output a probability of how likely that image belongs to a class. For example, here the function gives a 92 percent chance, it's a dog. The equation of a line in one dimension is given by the following. Here w represents the weight term and b represents the bias term. These are called learnable parameters, and we will use them to classify the image. For arbitrary dimensions, this equation generalizes to a hyperplane. You can represent this equation as a dot product of row vector w and image x. This is called the decision plane. If you're not familiar with vectors, this is just a compact way to express the equation of a line in dots of dimensions. In this video, we will use two dimensions for visualization. Note when the letter x are not bold, it's just simple algebra, not a sample. We will set the weights w_1 to one w_2 to minus one and the bias to one. In two dimensions we can plot the equation as a plane. We can plot the plane, z equals zero. We can see the line where the plane intersects with a plane at z equals zero. If we look at it from above, we get the following image. This is the plane where z equals zero, the line is where the decision plane intersects with the planes z equals zero. This line is called the decision boundary. We can overlay our sample images. Anything on the left side of the line is a dog. Anything on the right side of the line is a cat. Let's see how we can use the value of z to determine if it's a dog or a cat. Let's look at the following sample. Consider the following values for x_1. We can plug those values into an equation and get a value of z equals two. We see z as positive. Similarly, for x_2 and x_3, every point on the left side of the line will give you a positive value. The label for x_4 is a cat and the point lies on the right side of the line. Plugging the value of x into the equation, we see z is negative. Similarly, for sample x_4 and x_6, every point on the right side of the line will output a negative value. If we use z to calculate the class of the points, it always returns real numbers such as negative one, three, negative two, and so on. But we need a class between zero and one, so how do we convert these numbers? We'll use something called a threshold function pictured here. If z is greater than zero, it will return a one, and if z is less than zero, it will return to zero. For the following image, the output is one corresponding to dog. Every sample on this side of the plane will be classified as a cat, every sample on this side of the plane will be classified as a dog. A plane can't always separate the data. The sample is misclassified. In this case, the data is not linearly separable. The logistic function resembles the threshold function and is given by the following expression known as the sigmoid function. It will give us a probability of how likely our estimate is. In addition, it has better performance than the threshold function, for reasons we will discuss later. If the value of z is a very large negative number, the expression is approximately zero and for a very large positive value of z, the expression is approximately one. For everything in the middle, the value is between zero and one. To determine y hat as a discrete class, we use a threshold shown by the line. If the output of the logistic function is larger than 0.5, we set the prediction value y hat to one. If it's less, we set y hat to zero. Let's try out some values of x that we used previously with the linear classification. If we set x equal to zero 0.32 into the equation, we get the value of z as two, we pass the value through the sigmoid function. Since the value of the sigmoid function is greater than 0.5, we set y hat to one. Similarly, if we plug in the value of x as minus two, minus three, the value of z is minus two. We pass the value through the sigmoid function. We see the result is less than 0.5. We set y hat to zero. We can also represent the logistic function as a probability. We can find the probability of the image being a dog, that is y hat equals one. We can find the probability of the image being a cat, y hat equals zero by using the following. We see the image is more likely a dog. Given the following image, we can find the probability of being a dog y hat equals one. We find the probability of it being cat, that is y hat equals zero. We see the image is more likely a cat. If you have the learnable parameters, you can use a linear classifier, you take the photo. Under the hood, you app will use the linear classifier to make a prediction and output the class as a string.","title":"Linear Classifier"},{"location":"notes/data_science/Coursera/computer_vision/Image_Features/","text":"Image Features In this video we will review Image Features. In practice using the image intensities for classification does not function well. Here we convert the image to a vector. We use large patches for illustrative purposes instead of individual pixel values. Classifying an image involves the relationship between pixels; a slight change in the image affects this relationship, as discussed in the first chapter. Consider how a minor shift affects the feature vector. Comparing the two vectors we see the small shift in the image makes the vectors different. Features are measurements taken from the image that help with classification. For example here we have the color histogram. Determining the histogram of the shifted image and eliminating the zeros we see the histograms are similar. But the histograms only count intensities and does not consider the relationships between pixels. Here we have a circle and a square. The histogram only quantifies the fact that they have the same number of black pixels. One way too overcome this problem, to split image into sub-images and calculate the histogram for each sub-image. Here we have the histogram for the first 5 sub images. Here we have the histogram for the second 5 sub images and so on. Color is not always the best tool for classification, consider the task of classifying a circle. In this case many of the elements of the red channel would have large values. For the blue circle only the elements of the blue or green channel would have a large magnitude. Their histograms have to be the same. Although humans use colors too classify images it's hard for people to come up with features that use colors. Consider the similar circles. Converting the image to gray scale we see they look similar, surprisingly the gradients look identical. As a result we us features based on gradients. HOG Essential HOG explaination HOG Is one of many image features we can use, it's relatively simple to understand. Histogram of oriented gradients \u2013 H.O.G. , is one of many types of features that have been developed over the years. The technique counts occurrences of gradient orientation in localized portions of an image. HOG would generate a Histogram for each of these regions separately. The histograms are created using the gradients and orientations of the pixel values, hence the name \u2018Histogram of Oriented Gradients\u2019. Let's give a basic idea of how hog works. Consider the unit circle: We expect the gradients to have the same magnitude. We can calculate the histogram where each bin index is the angle of the gradient. The value of the histogram is the magnitude. We can plot the gradians for the square. The square will have a different histogram. General (H.O.G.) is calculated as follows Assuming we have an image of a kitten. We will convert the image to grayscale, we calculate the magnitude and angles of the gradients using Sobel The images are divided in a grid fashion into cells, and for the pixels within each cell, a histogram of gradient directions is compiled. To improve imbalance to highlights and shadows in the image, cells are block normalized. The HOG feature vector is a combination of all pixel-level histograms and used with SVM to classify the image. This example is simplified. We must also consider other free parameters like number of image cells or how many angle bins in the histogram. There are other types of features for images like SURF and SIFT. We can summarize the entire machine learning process as follows. Feature extraction, Kernel i.e non-linear mapping, Linear classification.","title":"12. Image Features"},{"location":"notes/data_science/Coursera/computer_vision/Image_Features/#image-features","text":"In this video we will review Image Features. In practice using the image intensities for classification does not function well. Here we convert the image to a vector. We use large patches for illustrative purposes instead of individual pixel values. Classifying an image involves the relationship between pixels; a slight change in the image affects this relationship, as discussed in the first chapter. Consider how a minor shift affects the feature vector. Comparing the two vectors we see the small shift in the image makes the vectors different. Features are measurements taken from the image that help with classification. For example here we have the color histogram. Determining the histogram of the shifted image and eliminating the zeros we see the histograms are similar. But the histograms only count intensities and does not consider the relationships between pixels. Here we have a circle and a square. The histogram only quantifies the fact that they have the same number of black pixels. One way too overcome this problem, to split image into sub-images and calculate the histogram for each sub-image. Here we have the histogram for the first 5 sub images. Here we have the histogram for the second 5 sub images and so on. Color is not always the best tool for classification, consider the task of classifying a circle. In this case many of the elements of the red channel would have large values. For the blue circle only the elements of the blue or green channel would have a large magnitude. Their histograms have to be the same. Although humans use colors too classify images it's hard for people to come up with features that use colors. Consider the similar circles. Converting the image to gray scale we see they look similar, surprisingly the gradients look identical. As a result we us features based on gradients.","title":"Image Features"},{"location":"notes/data_science/Coursera/computer_vision/Image_Features/#hog","text":"Essential HOG explaination HOG Is one of many image features we can use, it's relatively simple to understand. Histogram of oriented gradients \u2013 H.O.G. , is one of many types of features that have been developed over the years. The technique counts occurrences of gradient orientation in localized portions of an image. HOG would generate a Histogram for each of these regions separately. The histograms are created using the gradients and orientations of the pixel values, hence the name \u2018Histogram of Oriented Gradients\u2019. Let's give a basic idea of how hog works. Consider the unit circle: We expect the gradients to have the same magnitude. We can calculate the histogram where each bin index is the angle of the gradient. The value of the histogram is the magnitude. We can plot the gradians for the square. The square will have a different histogram. General (H.O.G.) is calculated as follows Assuming we have an image of a kitten. We will convert the image to grayscale, we calculate the magnitude and angles of the gradients using Sobel The images are divided in a grid fashion into cells, and for the pixels within each cell, a histogram of gradient directions is compiled. To improve imbalance to highlights and shadows in the image, cells are block normalized. The HOG feature vector is a combination of all pixel-level histograms and used with SVM to classify the image. This example is simplified. We must also consider other free parameters like number of image cells or how many angle bins in the histogram. There are other types of features for images like SURF and SIFT. We can summarize the entire machine learning process as follows. Feature extraction, Kernel i.e non-linear mapping, Linear classification.","title":"HOG"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/","text":"Image Processing With Pillow Image Files and Paths # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png Image processing and computer vision tasks include displaying, cropping, flipping, rotating, image segmentation, classification, image restoration, image recognition, image generation. Also, working with images via the cloud requires storing, transmitting, and gathering images through the internet. import os from PIL import Image from PIL import ImageOps import matplotlib.pyplot as plt import numpy as np # First, let's define a helper function to concatenate two images side-by-side. You will not need to understand the code below at this moment, but this function will be used repeatedly in this tutorial to showcase the results. def get_concat_h(im1, im2): #https://note.nkmk.me/en/python-pillow-concat-images/ dst = Image.new('RGB', (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst my_image = \"lenna.png\" File Types: * Joint Photographic Expert Group image (or .jpg .jpeg ) * Portable Network Graphics (or .png ) cwd = os.getcwd() image_path = os.path.join(cwd, my_image) image = Image.open(my_image) type(image) # <class 'PIL.PngImagePlugin.PngImageFile'> image.show() # Matplotlib plt.figure(figsize=(10,10)) plt.imshow(image) plt.show() # Load in with PIL.Image image = Image.open(image_path) # Size print(image.size) # (512, 512) # Mode print(image.mode) The Image.open method does not load image data into the computer memory. The load method of PIL object reads the file content, decodes it, and expands the image into memory. im = image.load() # We can then check the intensity of the image at the $x$-th column and $y$-th row: x = 0 y = 1 im[y,x] # (226, 137, 125) # Save image.save(\"lenna.jpg\") Greyscale image_gray = ImageOps.grayscale(image) image_gray.show() image_gray.mode # Mode is L for greyscale. Quantization The Quantization of an image is the number of unique intensity values any given pixel of the image can take. For a grayscale image, this means the number of different shades of gray. Most images have 256 different levels. You can decrease the levels using the method quantize . Let's repeatably cut the number of levels in half and observe what happens: # Half the levels do not make a noticable difference. image_gray.quantize(256 // 2) image_gray.show() # Let\u2019s continue dividing the number of values by two and compare it to the original image. for n in range(3,8): plt.figure(figsize=(10,10)) plt.imshow(get_concat_h(image_gray, image_gray.quantize(256//2**n))) plt.title(\"256 Quantization Levels left vs {} Quantization Levels right\".format(256//2**n)) plt.show() Color Channels baboon = Image.open('baboon.png') # We can obtain the different RGB color channels and assign them to the variables red, green and blue red, green, blue = baboon.split() # color vs each channel get_concat_h(baboon, red).show() get_concat_h(baboon, green).show() get_concat_h(baboon, blue).show() array= np.asarray(image) print(type(array)) np.asarray turns the original image into a numpy array. Often, we don't want to manipulate the image directly, but instead, create a copy of the image to manipulate. The np.array method creates a new copy of the image, such that the original one will remain unmodified. array = np.array(image) # summarize shape print(array.shape) # The Intensity values are 8-bit unsigned datatype. array[0, 0] Indexing You can plot the array as an image: # Review plt.figure(figsize=(10,10)) plt.imshow(array) plt.show() # We can use numpy slicing, for example, we can return the first 256 rows corresponding to the top half of the image: rows = 256 plt.figure(figsize=(10,10)) plt.imshow(array[0:rows,:,:]) plt.show() # We can also return the first 256 columns corresponding to the first half of the image. columns = 256 plt.figure(figsize=(10,10)) plt.imshow(array[:,0:columns,:]) plt.show() # If you want to reassign an array to another variable, you should use the `copy` method (we will cover this in the next section). A = array.copy() plt.imshow(A) plt.show() # If we do not apply the method copy(), the variable will point to the same location in memory. Consider the array B. If we set all values of array A to zero, as B points to A, the values of B will be zero too: B = A A[:,:,:] = 0 plt.imshow(B) plt.show() # We can also work with the different color channels. Consider the baboon image: baboon_array = np.array(baboon) plt.figure(figsize=(10,10)) plt.imshow(baboon_array) plt.show() # We can plot the red channel as intensity values of the red channel. baboon_array = np.array(baboon) plt.figure(figsize=(10,10)) plt.imshow(baboon_array[:,:,0], cmap='gray') plt.show() # Or we can create a new array and set all but the red color channels to zero. Therefore, when we display the image it appears red: baboon_red=baboon_array.copy() baboon_red[:,:,1] = 0 baboon_red[:,:,2] = 0 plt.figure(figsize=(10,10)) plt.imshow(baboon_red) plt.show()","title":"3. Image Processing With Pillow"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#image-processing-with-pillow","text":"","title":"Image Processing With Pillow"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#image-files-and-paths","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png Image processing and computer vision tasks include displaying, cropping, flipping, rotating, image segmentation, classification, image restoration, image recognition, image generation. Also, working with images via the cloud requires storing, transmitting, and gathering images through the internet. import os from PIL import Image from PIL import ImageOps import matplotlib.pyplot as plt import numpy as np # First, let's define a helper function to concatenate two images side-by-side. You will not need to understand the code below at this moment, but this function will be used repeatedly in this tutorial to showcase the results. def get_concat_h(im1, im2): #https://note.nkmk.me/en/python-pillow-concat-images/ dst = Image.new('RGB', (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst my_image = \"lenna.png\" File Types: * Joint Photographic Expert Group image (or .jpg .jpeg ) * Portable Network Graphics (or .png ) cwd = os.getcwd() image_path = os.path.join(cwd, my_image) image = Image.open(my_image) type(image) # <class 'PIL.PngImagePlugin.PngImageFile'> image.show() # Matplotlib plt.figure(figsize=(10,10)) plt.imshow(image) plt.show() # Load in with PIL.Image image = Image.open(image_path) # Size print(image.size) # (512, 512) # Mode print(image.mode) The Image.open method does not load image data into the computer memory. The load method of PIL object reads the file content, decodes it, and expands the image into memory. im = image.load() # We can then check the intensity of the image at the $x$-th column and $y$-th row: x = 0 y = 1 im[y,x] # (226, 137, 125) # Save image.save(\"lenna.jpg\")","title":"Image Files and Paths"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#greyscale","text":"image_gray = ImageOps.grayscale(image) image_gray.show() image_gray.mode # Mode is L for greyscale.","title":"Greyscale"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#quantization","text":"The Quantization of an image is the number of unique intensity values any given pixel of the image can take. For a grayscale image, this means the number of different shades of gray. Most images have 256 different levels. You can decrease the levels using the method quantize . Let's repeatably cut the number of levels in half and observe what happens: # Half the levels do not make a noticable difference. image_gray.quantize(256 // 2) image_gray.show() # Let\u2019s continue dividing the number of values by two and compare it to the original image. for n in range(3,8): plt.figure(figsize=(10,10)) plt.imshow(get_concat_h(image_gray, image_gray.quantize(256//2**n))) plt.title(\"256 Quantization Levels left vs {} Quantization Levels right\".format(256//2**n)) plt.show()","title":"Quantization"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#color-channels","text":"baboon = Image.open('baboon.png') # We can obtain the different RGB color channels and assign them to the variables red, green and blue red, green, blue = baboon.split() # color vs each channel get_concat_h(baboon, red).show() get_concat_h(baboon, green).show() get_concat_h(baboon, blue).show() array= np.asarray(image) print(type(array)) np.asarray turns the original image into a numpy array. Often, we don't want to manipulate the image directly, but instead, create a copy of the image to manipulate. The np.array method creates a new copy of the image, such that the original one will remain unmodified. array = np.array(image) # summarize shape print(array.shape) # The Intensity values are 8-bit unsigned datatype. array[0, 0]","title":"Color Channels"},{"location":"notes/data_science/Coursera/computer_vision/Image_Processing_With_Pillow/#indexing","text":"You can plot the array as an image: # Review plt.figure(figsize=(10,10)) plt.imshow(array) plt.show() # We can use numpy slicing, for example, we can return the first 256 rows corresponding to the top half of the image: rows = 256 plt.figure(figsize=(10,10)) plt.imshow(array[0:rows,:,:]) plt.show() # We can also return the first 256 columns corresponding to the first half of the image. columns = 256 plt.figure(figsize=(10,10)) plt.imshow(array[:,0:columns,:]) plt.show() # If you want to reassign an array to another variable, you should use the `copy` method (we will cover this in the next section). A = array.copy() plt.imshow(A) plt.show() # If we do not apply the method copy(), the variable will point to the same location in memory. Consider the array B. If we set all values of array A to zero, as B points to A, the values of B will be zero too: B = A A[:,:,:] = 0 plt.imshow(B) plt.show() # We can also work with the different color channels. Consider the baboon image: baboon_array = np.array(baboon) plt.figure(figsize=(10,10)) plt.imshow(baboon_array) plt.show() # We can plot the red channel as intensity values of the red channel. baboon_array = np.array(baboon) plt.figure(figsize=(10,10)) plt.imshow(baboon_array[:,:,0], cmap='gray') plt.show() # Or we can create a new array and set all but the red color channels to zero. Therefore, when we display the image it appears red: baboon_red=baboon_array.copy() baboon_red[:,:,1] = 0 baboon_red[:,:,2] = 0 plt.figure(figsize=(10,10)) plt.imshow(baboon_red) plt.show()","title":"Indexing"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/","text":"Logistic Regression Training: Gradient Descent In the last video, we learned we could use a plane to automatically classify an image. In this video, we will learn how to determine the plane. We will use the dataset of images to train the classifier. When we have an unknown sample, we can classify the image. Cost and Loss Training is where you find the best learnable parameters of the decision boundary. In this case, we will randomly select a set of learnable parameters, w and b, the superscript is the guess number. In this case, the decision boundary does a horrible job as it classifies all the images as cats. The second decision boundary does better. Finally, this decision boundary performs the best. First, we need a way to determine how good our decision boundary is. A loss function tells you how good your prediction is. The following loss is called the classification loss. The first column will show the output of the loss function. Each time our prediction is correct, the loss function will output a zero. Each time our prediction is incorrect, the loss function will output a one. The cost is the sum of the loss. The cost tells us how good our learnable parameters are doing on the dataset. In this case, our model output y hat is incorrect, predicting a cat as a dog and a dog as a cat. In this case, our model output is correct, predicting a dog is a dog and a cat is a cat. For each incorrectly classified samples, the loss is one, increasing the cost. Correctly classified samples do not change the cost. For this decision boundary, the cost is three. For this decision boundary the cost is one. For this decision boundary the cost is zero. The cost is a function of the learnable parameters. We see a set of learnable parameters. The decision boundary misclassifies three points. Changing the learnable parameters misclassifies the following points. The final learnable parameters perform perfectly. To simplify, let's look at the cost as a function of the bias parameter b. We can plot the cost with respect to learnable parameters. In this case, we plot the cost with respect to the bias parameter b. Let's see the relationship between cost and the decision boundary. We see the first line misclassifies the following points, thus the value of the cost for this value of b is three. The second misclassifies the following two points, hence the value of the cost is two. The final lines perform perfectly. The cost is zero. Cross Entropy Loss In reality, the cost is a function of multiple parameters w and b, even our super simple 2D example has too many parameters to plot. In practice, classification error is difficult to work with. We use the cross entropy loss that uses the output of the logistic function as opposed to the prediction y hat. The cost is still the sum of the loss. The cross entropy deals with how likely the image belongs to a specific class. If the likelihood of belonging to an incorrect class is large, the cross entropy loss in turn will be large. If the likelihood of belonging to the correct class is correct, the cross entropy is small, but not zero. Gradient Descent Mosty review. However, an interesting piece. It's challenging to perform gradient descent on the threshold function. Threshold func is the line or plane that separates data and you sweep it through data to decide what is what. However that results in a non-smooth cost function as depicted below. The slope is zero in many regions. If we get stuck in these regions, the gradient will be zero and not update. The decision plane has multiple parameters. As opposed to threshold because with a plane you consider 2+ variables which will turn cost into a bowl and smooth. As a result, the gradient is a vector. We can update the parameter, it's a set of vectors. For the two dimensional case we can plot it as a surface. It's a bowl shape. When we update the parameter it will find the minimum. Usually we plot cost with respect to each iteration i. This is called the learning curve. Generally, the more parameters you have, the more images and iterations you need to make the model work. Let's look at different learning curves. We can choose the learning rate that's way too large as shown in the one dimensional example. We can choose a learning rate that's too small. We can choose a learning rate that's too large. With a good learning rate, we will reach the minimum of the cost. Mini-Batch Gradient Descent Until now we have been using every sample for gradient descent. In Mini-Batch Gradient Descent we use a few samples at a time for each iteration, it's helpful to think about it as if you are minimizing a mini cost function or the total loss. When we use all the samples in the dataset we call it an epoch. When we use all the samples it\u2019s called batch gradient descent , where one iteration equals one epoch. We use a few samples to calculate the cost, it\u2019s sometimes referred to as the total loss . Very important. Creates a saw toothed shape because of a loss calculation on each mini batch in an epoch. For the first iteration we use the first two samples. For the second iteration we use the second two samples For the 3rd iteration we use the last two samples, therefore with a batch size of three to complete one run or Epoch through the data it took 3 iterations. For the second Epoch it also takes three iterations. In this case our batch size is 2. It only takes two iterations to complete one epoch. For the second epoch it also takes 2 iterations. Let's see how we can determine the number of iterations for different batch sizes and epochs. To obtain the number of iterations we simply divide the number of training examples by the batch size. Let's verify that. For a batch size of one we get 6 iterations, we can verify this pictorially, we see for each iteration we use one sample For a batch size of 2 it takes three iterations, we can verify this pictorially. Each iteration uses two samples. Finally, for a batch size of 3 it takes two iterations. We can verify this pictorially. We calculate the total loss for each iteration. It\u2019s a noisy version of the cost. At the end of each epoch we calculate the accuracy on the validation data. We repeat the process for the next iteration. If the accuracy decreases we have trained too much. This is called overfitting we will talk about this later Code Lab Logistic Regression With Mini-Batch Gradient Descent Represent your data as a Dataset object Create a Logistic Regression Model using PyTorch Set a Criterion to calculate Loss Create a Data Loader and set the Batch Size Create an Optimizer to update Model Parameters and set Learning Rate Train a Model Preparation # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from mpl_toolkits import mplot3d import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn The class plot_error_surfaces is just to help you visualize the data space and the parameter space during training and has nothing to do with Pytorch. # Create class for plotting and the function for plotting class plot_error_surfaces(object): # Construstor def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True): W = np.linspace(-w_range, w_range, n_samples) B = np.linspace(-b_range, b_range, n_samples) w, b = np.meshgrid(W, B) Z = np.zeros((30, 30)) count1 = 0 self.y = Y.numpy() self.x = X.numpy() for w1, b1 in zip(w, b): count2 = 0 for w2, b2 in zip(w1, b1): yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2))) Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16)) count2 += 1 count1 += 1 self.Z = Z self.w = w self.b = b self.W = [] self.B = [] self.LOSS = [] self.n = 0 if go == True: plt.figure() plt.figure(figsize=(7.5, 5)) plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none') plt.title('Loss Surface') plt.xlabel('w') plt.ylabel('b') plt.show() plt.figure() plt.title('Loss Surface Contour') plt.xlabel('w') plt.ylabel('b') plt.contour(self.w, self.b, self.Z) plt.show() # Setter def set_para_loss(self, model, loss): self.n = self.n + 1 self.W.append(list(model.parameters())[0].item()) self.B.append(list(model.parameters())[1].item()) self.LOSS.append(loss) # Plot diagram def final_plot(self): ax = plt.axes(projection='3d') ax.plot_wireframe(self.w, self.b, self.Z) ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1) plt.figure() plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c='r', marker='x') plt.xlabel('w') plt.ylabel('b') plt.show() # Plot diagram def plot_ps(self): plt.subplot(121) plt.ylim plt.plot(self.x[self.y==0], self.y[self.y==0], 'ro', label=\"training points\") plt.plot(self.x[self.y==1], self.y[self.y==1]-1, 'o', label=\"training points\") plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\") plt.xlabel('x') plt.ylabel('y') plt.ylim((-0.1, 2)) plt.title('Data Space Iteration: ' + str(self.n)) plt.show() plt.subplot(122) plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c='r', marker='x') plt.title('Loss Surface Contour Iteration' + str(self.n)) plt.xlabel('w') plt.ylabel('b') # Plot the diagram def PlotStuff(X, Y, model, epoch, leg=True): plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch))) plt.plot(X.numpy(), Y.numpy(), 'r') if leg == True: plt.legend() else: pass # Setting the seed will allow us to control randomness and give us reproducibility torch.manual_seed(0) Load Data The Dataset class represents a dataset. Your custom dataset should inherit Dataset which we imported above and override the following methods: \" len \" so that len(dataset) returns the size of the dataset. \" getitem \" to support the indexing such that dataset[i] can be used to get ith sample. # Create the custom Data class which inherits Dataset class Data(Dataset): # Constructor def __init__(self): # Create X values from -1 to 1 with step .1 self.x = torch.arange(-1, 1, 0.1).view(-1, 1) # Create Y values all set to 0 self.y = torch.zeros(self.x.shape[0], 1) # Set the X values above 0.2 to 1 self.y[self.x[:, 0] > 0.2] = 1 # Set the .len attribute because we need to override the __len__ method self.len = self.x.shape[0] # Getter that returns the data at the given index def __getitem__(self, index): return self.x[index], self.y[index] # Get length of the dataset def __len__(self): return self.len # Create Data object data_set = Data() # We can see the X values of the dataset data_set.x # We can see the Y values of the dataset which correspond to the class of the X value data_set.y # We can get the length of the dataset len(data_set) # We can get the label $y$ as well as the $x$ for the first sample x,y = data_set[0] print(\"x = {}, y = {}\".format(x,y)) # We can see we can separate the one-dimensional dataset into two classes: plt.plot(data_set.x[data_set.y==0], data_set.y[data_set.y==0], 'ro', label=\"y=0\") plt.plot(data_set.x[data_set.y==1], data_set.y[data_set.y==1]-1, 'o', label=\"y=1\") plt.xlabel('x') plt.legend() plt.show() Create the Model and Total Loss Function (Cost) # Create logistic_regression class that inherits nn.Module which is the base class for all neural networks class logistic_regression(nn.Module): # Constructor def __init__(self, n_inputs): super(logistic_regression, self).__init__() # Single layer of Logistic Regression with number of inputs being n_inputs and there being 1 output self.linear = nn.Linear(n_inputs, 1) # Prediction def forward(self, x): # Using the input x value puts it through the single layer defined above then puts the output through the sigmoid function and returns the result yhat = torch.sigmoid(self.linear(x)) return yhat # Create the logistic_regression result model = logistic_regression(1) We can make a prediction sigma \\(\\sigma\\) this uses the forward function defined above x = torch.tensor([-1.0]) sigma = model(x) sigma # We can also make a prediction using our data x,y = data_set[2] sigma = model(x) sigma Create a plot_error_surfaces object to visualize the data space and the learnable parameters space during training: We can see on the Loss Surface graph, the loss value varying across w and b values with yellow being high loss and dark blue being low loss which is what we want On the Loss Surface Contour graph we can see a top-down view of the Loss Surface graph # Create the plot_error_surfaces object # 15 is the range of w # 13 is the range of b # data_set[:][0] are all the X values # data_set[:][1] are all the Y values get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1]) # We define a criterion using Binary Cross Entropy Loss. This will measure the difference/loss between the prediction and actual value. criterion = nn.BCELoss() # We have our samples: x, y = data_set[0] print(\"x = {}, y = {}\".format(x,y)) # We can make a prediction using the model: sigma = model(x) sigma # We can calculate the loss loss = criterion(sigma, y) loss Setting the Batch Size using a Data Loader You have to use data loader in PyTorch that will output a batch of data, the input is the dataset and batch_size . batch_size=10 trainloader = DataLoader(dataset = data_set, batch_size = batch_size) dataset_iter = iter(trainloader) X,y=next(dataset_iter ) # We can see here that 10 values the same as our batch size X Setting the Learning Rate learning_rate = 0.1 optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) Train the Model via Mini-Batch Gradient Descent Train the Model # First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion which will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 5 trainloader = DataLoader(dataset = data_set, batch_size = 5) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = .01) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=500 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) # Now we can get the accuracy of the training data # Getting the predictions yhat = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") # Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping. loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show() Stochastic Gradient Descent In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. The number of epochs is set to 100. Notice that in this example the batch size is decreased from 5 to 1 so there would be more iterations. Due to this, we can reduce the number of iterations by decreasing the number of epochs. Due to the reduced batch size, we are optimizing more frequently so we don't need as many epochs. First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only. get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30) Train the model # First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion which will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 1 trainloader = DataLoader(dataset = data_set, batch_size = 1) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = .01) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=100 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) # Now we can get the accuracy of the training data # Getting the predictions yhat = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") # Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping. loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show() High Learning Ratening-with-python/home/welcom In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. This time the learning rate will be set to .1 to represent a high learning rate and we will observe what will happen when we try to train. First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only. get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30) Train the Model # First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion that will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 1 trainloader = DataLoader(dataset = data_set, batch_size = 1) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = 1) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=100 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() Notice in this example the due to the high learning rate the Loss Surface Contour graph has increased movement over the previous example and also moves in multiple directions due to the minimum being overshot. We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) Now we can get the accuracy of the training data # Getting the predictions yhat- = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show()","title":"9. Logistic Regression Training"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#logistic-regression-training-gradient-descent","text":"In the last video, we learned we could use a plane to automatically classify an image. In this video, we will learn how to determine the plane. We will use the dataset of images to train the classifier. When we have an unknown sample, we can classify the image.","title":"Logistic Regression Training: Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#cost-and-loss","text":"Training is where you find the best learnable parameters of the decision boundary. In this case, we will randomly select a set of learnable parameters, w and b, the superscript is the guess number. In this case, the decision boundary does a horrible job as it classifies all the images as cats. The second decision boundary does better. Finally, this decision boundary performs the best. First, we need a way to determine how good our decision boundary is. A loss function tells you how good your prediction is. The following loss is called the classification loss. The first column will show the output of the loss function. Each time our prediction is correct, the loss function will output a zero. Each time our prediction is incorrect, the loss function will output a one. The cost is the sum of the loss. The cost tells us how good our learnable parameters are doing on the dataset. In this case, our model output y hat is incorrect, predicting a cat as a dog and a dog as a cat. In this case, our model output is correct, predicting a dog is a dog and a cat is a cat. For each incorrectly classified samples, the loss is one, increasing the cost. Correctly classified samples do not change the cost. For this decision boundary, the cost is three. For this decision boundary the cost is one. For this decision boundary the cost is zero. The cost is a function of the learnable parameters. We see a set of learnable parameters. The decision boundary misclassifies three points. Changing the learnable parameters misclassifies the following points. The final learnable parameters perform perfectly. To simplify, let's look at the cost as a function of the bias parameter b. We can plot the cost with respect to learnable parameters. In this case, we plot the cost with respect to the bias parameter b. Let's see the relationship between cost and the decision boundary. We see the first line misclassifies the following points, thus the value of the cost for this value of b is three. The second misclassifies the following two points, hence the value of the cost is two. The final lines perform perfectly. The cost is zero.","title":"Cost and Loss"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#cross-entropy-loss","text":"In reality, the cost is a function of multiple parameters w and b, even our super simple 2D example has too many parameters to plot. In practice, classification error is difficult to work with. We use the cross entropy loss that uses the output of the logistic function as opposed to the prediction y hat. The cost is still the sum of the loss. The cross entropy deals with how likely the image belongs to a specific class. If the likelihood of belonging to an incorrect class is large, the cross entropy loss in turn will be large. If the likelihood of belonging to the correct class is correct, the cross entropy is small, but not zero.","title":"Cross Entropy Loss"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#gradient-descent","text":"Mosty review. However, an interesting piece. It's challenging to perform gradient descent on the threshold function. Threshold func is the line or plane that separates data and you sweep it through data to decide what is what. However that results in a non-smooth cost function as depicted below. The slope is zero in many regions. If we get stuck in these regions, the gradient will be zero and not update. The decision plane has multiple parameters. As opposed to threshold because with a plane you consider 2+ variables which will turn cost into a bowl and smooth. As a result, the gradient is a vector. We can update the parameter, it's a set of vectors. For the two dimensional case we can plot it as a surface. It's a bowl shape. When we update the parameter it will find the minimum. Usually we plot cost with respect to each iteration i. This is called the learning curve. Generally, the more parameters you have, the more images and iterations you need to make the model work. Let's look at different learning curves. We can choose the learning rate that's way too large as shown in the one dimensional example. We can choose a learning rate that's too small. We can choose a learning rate that's too large. With a good learning rate, we will reach the minimum of the cost.","title":"Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#mini-batch-gradient-descent","text":"Until now we have been using every sample for gradient descent. In Mini-Batch Gradient Descent we use a few samples at a time for each iteration, it's helpful to think about it as if you are minimizing a mini cost function or the total loss. When we use all the samples in the dataset we call it an epoch. When we use all the samples it\u2019s called batch gradient descent , where one iteration equals one epoch. We use a few samples to calculate the cost, it\u2019s sometimes referred to as the total loss . Very important. Creates a saw toothed shape because of a loss calculation on each mini batch in an epoch. For the first iteration we use the first two samples. For the second iteration we use the second two samples For the 3rd iteration we use the last two samples, therefore with a batch size of three to complete one run or Epoch through the data it took 3 iterations. For the second Epoch it also takes three iterations. In this case our batch size is 2. It only takes two iterations to complete one epoch. For the second epoch it also takes 2 iterations. Let's see how we can determine the number of iterations for different batch sizes and epochs. To obtain the number of iterations we simply divide the number of training examples by the batch size. Let's verify that. For a batch size of one we get 6 iterations, we can verify this pictorially, we see for each iteration we use one sample For a batch size of 2 it takes three iterations, we can verify this pictorially. Each iteration uses two samples. Finally, for a batch size of 3 it takes two iterations. We can verify this pictorially. We calculate the total loss for each iteration. It\u2019s a noisy version of the cost. At the end of each epoch we calculate the accuracy on the validation data. We repeat the process for the next iteration. If the accuracy decreases we have trained too much. This is called overfitting we will talk about this later","title":"Mini-Batch Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#code-lab","text":"","title":"Code Lab"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#logistic-regression-with-mini-batch-gradient-descent","text":"Represent your data as a Dataset object Create a Logistic Regression Model using PyTorch Set a Criterion to calculate Loss Create a Data Loader and set the Batch Size Create an Optimizer to update Model Parameters and set Learning Rate Train a Model","title":"Logistic Regression With Mini-Batch Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#preparation","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from mpl_toolkits import mplot3d import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn The class plot_error_surfaces is just to help you visualize the data space and the parameter space during training and has nothing to do with Pytorch. # Create class for plotting and the function for plotting class plot_error_surfaces(object): # Construstor def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True): W = np.linspace(-w_range, w_range, n_samples) B = np.linspace(-b_range, b_range, n_samples) w, b = np.meshgrid(W, B) Z = np.zeros((30, 30)) count1 = 0 self.y = Y.numpy() self.x = X.numpy() for w1, b1 in zip(w, b): count2 = 0 for w2, b2 in zip(w1, b1): yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2))) Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16)) count2 += 1 count1 += 1 self.Z = Z self.w = w self.b = b self.W = [] self.B = [] self.LOSS = [] self.n = 0 if go == True: plt.figure() plt.figure(figsize=(7.5, 5)) plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none') plt.title('Loss Surface') plt.xlabel('w') plt.ylabel('b') plt.show() plt.figure() plt.title('Loss Surface Contour') plt.xlabel('w') plt.ylabel('b') plt.contour(self.w, self.b, self.Z) plt.show() # Setter def set_para_loss(self, model, loss): self.n = self.n + 1 self.W.append(list(model.parameters())[0].item()) self.B.append(list(model.parameters())[1].item()) self.LOSS.append(loss) # Plot diagram def final_plot(self): ax = plt.axes(projection='3d') ax.plot_wireframe(self.w, self.b, self.Z) ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1) plt.figure() plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c='r', marker='x') plt.xlabel('w') plt.ylabel('b') plt.show() # Plot diagram def plot_ps(self): plt.subplot(121) plt.ylim plt.plot(self.x[self.y==0], self.y[self.y==0], 'ro', label=\"training points\") plt.plot(self.x[self.y==1], self.y[self.y==1]-1, 'o', label=\"training points\") plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\") plt.xlabel('x') plt.ylabel('y') plt.ylim((-0.1, 2)) plt.title('Data Space Iteration: ' + str(self.n)) plt.show() plt.subplot(122) plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c='r', marker='x') plt.title('Loss Surface Contour Iteration' + str(self.n)) plt.xlabel('w') plt.ylabel('b') # Plot the diagram def PlotStuff(X, Y, model, epoch, leg=True): plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch))) plt.plot(X.numpy(), Y.numpy(), 'r') if leg == True: plt.legend() else: pass # Setting the seed will allow us to control randomness and give us reproducibility torch.manual_seed(0)","title":"Preparation"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#load-data","text":"The Dataset class represents a dataset. Your custom dataset should inherit Dataset which we imported above and override the following methods: \" len \" so that len(dataset) returns the size of the dataset. \" getitem \" to support the indexing such that dataset[i] can be used to get ith sample. # Create the custom Data class which inherits Dataset class Data(Dataset): # Constructor def __init__(self): # Create X values from -1 to 1 with step .1 self.x = torch.arange(-1, 1, 0.1).view(-1, 1) # Create Y values all set to 0 self.y = torch.zeros(self.x.shape[0], 1) # Set the X values above 0.2 to 1 self.y[self.x[:, 0] > 0.2] = 1 # Set the .len attribute because we need to override the __len__ method self.len = self.x.shape[0] # Getter that returns the data at the given index def __getitem__(self, index): return self.x[index], self.y[index] # Get length of the dataset def __len__(self): return self.len # Create Data object data_set = Data() # We can see the X values of the dataset data_set.x # We can see the Y values of the dataset which correspond to the class of the X value data_set.y # We can get the length of the dataset len(data_set) # We can get the label $y$ as well as the $x$ for the first sample x,y = data_set[0] print(\"x = {}, y = {}\".format(x,y)) # We can see we can separate the one-dimensional dataset into two classes: plt.plot(data_set.x[data_set.y==0], data_set.y[data_set.y==0], 'ro', label=\"y=0\") plt.plot(data_set.x[data_set.y==1], data_set.y[data_set.y==1]-1, 'o', label=\"y=1\") plt.xlabel('x') plt.legend() plt.show()","title":"Load Data"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#create-the-model-and-total-loss-function-cost","text":"# Create logistic_regression class that inherits nn.Module which is the base class for all neural networks class logistic_regression(nn.Module): # Constructor def __init__(self, n_inputs): super(logistic_regression, self).__init__() # Single layer of Logistic Regression with number of inputs being n_inputs and there being 1 output self.linear = nn.Linear(n_inputs, 1) # Prediction def forward(self, x): # Using the input x value puts it through the single layer defined above then puts the output through the sigmoid function and returns the result yhat = torch.sigmoid(self.linear(x)) return yhat # Create the logistic_regression result model = logistic_regression(1) We can make a prediction sigma \\(\\sigma\\) this uses the forward function defined above x = torch.tensor([-1.0]) sigma = model(x) sigma # We can also make a prediction using our data x,y = data_set[2] sigma = model(x) sigma Create a plot_error_surfaces object to visualize the data space and the learnable parameters space during training: We can see on the Loss Surface graph, the loss value varying across w and b values with yellow being high loss and dark blue being low loss which is what we want On the Loss Surface Contour graph we can see a top-down view of the Loss Surface graph # Create the plot_error_surfaces object # 15 is the range of w # 13 is the range of b # data_set[:][0] are all the X values # data_set[:][1] are all the Y values get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1]) # We define a criterion using Binary Cross Entropy Loss. This will measure the difference/loss between the prediction and actual value. criterion = nn.BCELoss() # We have our samples: x, y = data_set[0] print(\"x = {}, y = {}\".format(x,y)) # We can make a prediction using the model: sigma = model(x) sigma # We can calculate the loss loss = criterion(sigma, y) loss","title":"Create the Model and Total Loss Function (Cost)"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#setting-the-batch-size-using-a-data-loader","text":"You have to use data loader in PyTorch that will output a batch of data, the input is the dataset and batch_size . batch_size=10 trainloader = DataLoader(dataset = data_set, batch_size = batch_size) dataset_iter = iter(trainloader) X,y=next(dataset_iter ) # We can see here that 10 values the same as our batch size X","title":"Setting the Batch Size using a Data Loader"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#setting-the-learning-rate","text":"learning_rate = 0.1 optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)","title":"Setting the Learning Rate"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#train-the-model-via-mini-batch-gradient-descent","text":"","title":"Train the Model via Mini-Batch Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#train-the-model","text":"# First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion which will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 5 trainloader = DataLoader(dataset = data_set, batch_size = 5) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = .01) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=500 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) # Now we can get the accuracy of the training data # Getting the predictions yhat = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") # Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping. loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show()","title":"Train the Model"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#stochastic-gradient-descent","text":"In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. The number of epochs is set to 100. Notice that in this example the batch size is decreased from 5 to 1 so there would be more iterations. Due to this, we can reduce the number of iterations by decreasing the number of epochs. Due to the reduced batch size, we are optimizing more frequently so we don't need as many epochs. First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only. get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)","title":"Stochastic Gradient Descent"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#train-the-model_1","text":"# First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion which will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 1 trainloader = DataLoader(dataset = data_set, batch_size = 1) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = .01) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=100 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) # Now we can get the accuracy of the training data # Getting the predictions yhat = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") # Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping. loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show()","title":"Train the model"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#high-learning-ratening-with-pythonhomewelcom","text":"In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. This time the learning rate will be set to .1 to represent a high learning rate and we will observe what will happen when we try to train. First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only. get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)","title":"High Learning Ratening-with-python/home/welcom"},{"location":"notes/data_science/Coursera/computer_vision/Logistic_Regression_Training/#train-the-model_2","text":"# First we create an instance of the model we want to train model = logistic_regression(1) # We create a criterion that will measure loss criterion = nn.BCELoss() # We create a data loader with the dataset and specified batch size of 1 trainloader = DataLoader(dataset = data_set, batch_size = 1) # We create an optimizer with the model parameters and learning rate optimizer = torch.optim.SGD(model.parameters(), lr = 1) # Then we set the number of epochs which is the total number of times we will train on the entire training dataset epochs=100 # This will store the loss over iterations so we can plot it at the end loss_values = [] # Loop will execute for number of epochs for epoch in range(epochs): # For each batch in the training data for x, y in trainloader: # Make our predictions from the X values yhat = model(x) # Measure the loss between our prediction and actual Y values loss = criterion(yhat, y) # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Set the parameters for the loss surface contour graphs get_surface.set_para_loss(model, loss.tolist()) # Saves the loss of the iteration loss_values.append(loss) # Want to print the Data Space for the current iteration every 20 epochs if epoch % 20 == 0: get_surface.plot_ps() Notice in this example the due to the high learning rate the Loss Surface Contour graph has increased movement over the previous example and also moves in multiple directions due to the minimum being overshot. We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph. w = model.state_dict()['linear.weight'].data[0] b = model.state_dict()['linear.bias'].data[0] print(\"w = \", w, \"b = \", b) Now we can get the accuracy of the training data # Getting the predictions yhat- = model(data_set.x) # Rounding the prediction to the nearedt integer 0 or 1 representing the classes yhat = torch.round(yhat) # Counter to keep track of correct predictions correct = 0 # Goes through each prediction and actual y value for prediction, actual in zip(yhat, data_set.y): # Compares if the prediction and actualy y value are the same if (prediction == actual): # Adds to counter if prediction is correct correct+=1 # Outputs the accuracy by dividing the correct predictions by the length of the dataset print(\"Accuracy: \", correct/len(data_set)*100, \"%\") loss_values_np = [] for loss_tensor in loss_values: loss_detached = loss_tensor.detach() # Detach each tensor loss_np = loss_detached.numpy() # Convert to NumPy array loss_values_np.append(loss_np) # Append to the list plt.plot(loss_values_np) plt.xlabel(\"Iteration\") plt.ylabel(\"Cost\") plt.show()","title":"Train the Model"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/","text":"Manipulating Images Copying allows you to create a new image independent of the original. from PIL import Image from PIL import ImageOps from PIL import ImageDraw from PIL import ImageFont import cv2 import numpy as np import matplotlib.pyplot as plt baboon = Image.open('baboon.png'); # baboon.show() id(baboon) # 140014182059888 A = baboon id(A) # 140014182059888 B = baboon.copy() id(B) # 140014180362048 # basic flip im_flip = ImageOps.flip(baboon) im_flip.show() # mirror im_mirror = ImageOps.mirror(baboon) im_mirror.show() # transpose (has built in attributes for different transpositions) baboon.transpose(Image.FLIP_TOP_BOTTOM).show() # flip with cv2 (more control) baboon = cv2.imread('baboon.png'); # baboon.show() baboon = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB) im_flip = cv2.flip(baboon,0) im_rotate = cv2.rotate(baboon,cv2.ROTATE_90_CLOCKWISE) baboon_imshow = plt.imshow(baboon) baboon_flip_imshow = plt.imshow(im_flip) baboon_rotate_imshow = plt.imshow(im_rotate) plt.show() # show any one of those Manipulating Images One Pixel At a Time Cropping Changing Image Pixels baboon = Image.open('baboon.png'); # baboon.show() # Cropping baboon_np = np.array(baboon) baboon_np[2:5,:,0] # rows 3-5 of 1st channel baboon_np[2:5,3:7,0] # rows 3-5, cols 4-7 of 1st channel baboon_np[:,:,0] = 255 plt.imshow(baboon_np); plt.show() Drawing Rectangle baboon = Image.open('baboon.png'); # baboon.show() img_copy = baboon.copy() image_fn = ImageDraw.Draw(im=img_copy) shape = [0,0,50,50] image_fn.rectangle(xy=shape,fill='red') img_array = np.array(img_copy) plt.imshow(img_array) # either works plt.imshow(img_copy) # either works plt.show() Text baboon = Image.open('baboon.png'); # baboon.show() img_copy = baboon.copy() # Fonts: https://www.fontsquirrel.com/fonts/list/popular fnt = ImageFont.truetype('/home/bbearce/Downloads/open-sans/OpenSans-BoldItalic.ttf', 100) image_fn = ImageDraw.Draw(im=img_copy) image_fn.text(xy=(0,0),text=\"box\", font=fnt,fill=(0,0,0)) plt.imshow(img_copy) plt.show() Cropped Image Consider the image \u201ccrop_image\u201d of a cat. We also have the \u201cimage_lenna \u201d, we can superimpose the image of the cat over \u201cimage Lenna\u201d, all we need is the coordinates indicating where to paste the picture. We use the paste() method, the input is the image we would like to super-impose and the box parameter specifies the top left corner of the image you would like to paste and the box parameter specifies the top left corner of the image you would like to paste baboon = Image.open('baboon.png'); # baboon.show() baboon_c = baboon.copy() lenna = Image.open('lenna.png'); # lenna.show() lenna_cropped = lenna.crop((0,0,300,300)) left = 150 upper = 150 baboon_c.paste(lenna_cropped, box=(left,upper)) plt.imshow(baboon_c) plt.show() OpenCV Manipulations baboon = cv2.imread('baboon.png'); # baboon.show() baboon = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB) baboon_c = np.copy(baboon) left = 150 upper = 150 right = 300 lower = 300 start_point, end_point = (left, upper),(right, lower) cv2.rectangle(baboon_c, pt1=start_point, pt2=end_point, color=(0,255,0), thickness=3) plt.imshow(baboon_c) plt.show() cv2.putText(img=baboon_c, text=\"Stuff\", org=(10,500), color=(255,255,255), fontFace=4, fontScale=5, thickness=9) plt.imshow(baboon_c) plt.show()","title":"4. Manipulating Images"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#manipulating-images","text":"Copying allows you to create a new image independent of the original. from PIL import Image from PIL import ImageOps from PIL import ImageDraw from PIL import ImageFont import cv2 import numpy as np import matplotlib.pyplot as plt baboon = Image.open('baboon.png'); # baboon.show() id(baboon) # 140014182059888 A = baboon id(A) # 140014182059888 B = baboon.copy() id(B) # 140014180362048 # basic flip im_flip = ImageOps.flip(baboon) im_flip.show() # mirror im_mirror = ImageOps.mirror(baboon) im_mirror.show() # transpose (has built in attributes for different transpositions) baboon.transpose(Image.FLIP_TOP_BOTTOM).show() # flip with cv2 (more control) baboon = cv2.imread('baboon.png'); # baboon.show() baboon = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB) im_flip = cv2.flip(baboon,0) im_rotate = cv2.rotate(baboon,cv2.ROTATE_90_CLOCKWISE) baboon_imshow = plt.imshow(baboon) baboon_flip_imshow = plt.imshow(im_flip) baboon_rotate_imshow = plt.imshow(im_rotate) plt.show() # show any one of those","title":"Manipulating Images"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#manipulating-images-one-pixel-at-a-time","text":"Cropping Changing Image Pixels baboon = Image.open('baboon.png'); # baboon.show() # Cropping baboon_np = np.array(baboon) baboon_np[2:5,:,0] # rows 3-5 of 1st channel baboon_np[2:5,3:7,0] # rows 3-5, cols 4-7 of 1st channel baboon_np[:,:,0] = 255 plt.imshow(baboon_np); plt.show()","title":"Manipulating Images One Pixel At a Time"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#drawing","text":"","title":"Drawing"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#rectangle","text":"baboon = Image.open('baboon.png'); # baboon.show() img_copy = baboon.copy() image_fn = ImageDraw.Draw(im=img_copy) shape = [0,0,50,50] image_fn.rectangle(xy=shape,fill='red') img_array = np.array(img_copy) plt.imshow(img_array) # either works plt.imshow(img_copy) # either works plt.show()","title":"Rectangle"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#text","text":"baboon = Image.open('baboon.png'); # baboon.show() img_copy = baboon.copy() # Fonts: https://www.fontsquirrel.com/fonts/list/popular fnt = ImageFont.truetype('/home/bbearce/Downloads/open-sans/OpenSans-BoldItalic.ttf', 100) image_fn = ImageDraw.Draw(im=img_copy) image_fn.text(xy=(0,0),text=\"box\", font=fnt,fill=(0,0,0)) plt.imshow(img_copy) plt.show()","title":"Text"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#cropped-image","text":"Consider the image \u201ccrop_image\u201d of a cat. We also have the \u201cimage_lenna \u201d, we can superimpose the image of the cat over \u201cimage Lenna\u201d, all we need is the coordinates indicating where to paste the picture. We use the paste() method, the input is the image we would like to super-impose and the box parameter specifies the top left corner of the image you would like to paste and the box parameter specifies the top left corner of the image you would like to paste baboon = Image.open('baboon.png'); # baboon.show() baboon_c = baboon.copy() lenna = Image.open('lenna.png'); # lenna.show() lenna_cropped = lenna.crop((0,0,300,300)) left = 150 upper = 150 baboon_c.paste(lenna_cropped, box=(left,upper)) plt.imshow(baboon_c) plt.show()","title":"Cropped Image"},{"location":"notes/data_science/Coursera/computer_vision/Manipulating_Images/#opencv-manipulations","text":"baboon = cv2.imread('baboon.png'); # baboon.show() baboon = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB) baboon_c = np.copy(baboon) left = 150 upper = 150 right = 300 lower = 300 start_point, end_point = (left, upper),(right, lower) cv2.rectangle(baboon_c, pt1=start_point, pt2=end_point, color=(0,255,0), thickness=3) plt.imshow(baboon_c) plt.show() cv2.putText(img=baboon_c, text=\"Stuff\", org=(10,500), color=(255,255,255), fontFace=4, fontScale=5, thickness=9) plt.imshow(baboon_c) plt.show()","title":"OpenCV Manipulations"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/","text":"Neural Nets and Deep Learning for Image Classification Neural Nets Imagine this box is a decision function we want to learn for classification. We can also view the problem as trying to approximate the box function using logistic regression: If we attempt a linear regression turned into a logistic one, we will still get errors for the higher x values. Shown here: Reminder this logistic regression is an activation function We could draw another line though: This too perhaps: Again though not perfect: However apparently we can make a band pass filter like shape by subtracting one from another to get our prediction to match the desired square pulse from earlier (kind of): Pretty square pulse we are aiming for: As you can see with two different neurons perfomring a logistic regression with a different line, we can effectively have our square pulse. This first layer of neurons and activation functions are called the hidden layer. Each neuron is an artifician neuron. The output layer has one artificial neuron. Shown below are the activation outputs. You can see red and blue colors but that is not the output of the activation layers. Imagine the dots are black and the red and blue are just letting us know the ground truth. The final layer that fits a black line (decision boundary) is what the output layer has to figure out. Review that all these logistic regressions are a series of weights and biases that can be learned. These are free forward neural networks and are also called cully connected networks With many neurons we can create complex decision boundaries. Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # Import the libraries we need for this lab # Allows us to use arrays to manipulate and store data import numpy as np # PyTorch Library import torch # PyTorch Neural Network import torch.nn as nn # Allows us to use activation functions import torch.nn.functional as F # Used to graph data and loss curves import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap # Used to help create the dataset and perform mini-batch from torch.utils.data import Dataset, DataLoader # Plot the data def plot_decision_regions_2class(model,data_set): cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF']) X = data_set.x.numpy() y = data_set.y.numpy() h = .02 x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h)) XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()]) yhat = np.logical_not((model(XX)[:, 0] > 0.5).numpy()).reshape(xx.shape) plt.pcolormesh(xx, yy, yhat, cmap=cmap_light, shading='auto') plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0') plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1') plt.title(\"decision region\") plt.legend() # Calculate the accuracy def accuracy(model, data_set): # Rounds prediction to nearest integer 0 or 1 # Checks if prediction matches the actual values and returns accuracy rate return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] > 0.5).numpy()) # Define the class Net with one hidden layer class Net(nn.Module): # Constructor def __init__(self, D_in, H, D_out): super(Net, self).__init__() # D_in is the input size of the first layer (size of input layer) # H is the outpout size of the first layer and the input size of the second layer (size of hidden layer) # D_out is the output size of the second layer (size of output layer) self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) # Prediction def forward(self, x): # Puts x through first layer then sigmoid function x = torch.sigmoid(self.linear1(x)) # Puts result of previous line through second layer then sigmoid function x = torch.sigmoid(self.linear2(x)) # Output is a number between 0 and 1 due to the sigmoid function. Whichever the output is closer to, 0 or 1, is the class prediction return x # Function to Train the Model def train(data_set, model, criterion, train_loader, optimizer, epochs=5): # Lists to keep track of cost and accuracy COST = [] ACC = [] # Number of times we train on the entire dataset for epoch in range(epochs): # Total loss over epoch total=0 # For batch in train laoder for x, y in train_loader: # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Makes a prediction based on X value yhat = model(x) # Measures the loss between prediction and acutal Y value loss = criterion(yhat, y) # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Cumulates loss total+=loss.item() # Saves cost and accuracy ACC.append(accuracy(model, data_set)) COST.append(total) # Prints Cost vs Epoch graph fig, ax1 = plt.subplots() color = 'tab:red' ax1.plot(COST, color=color) ax1.set_xlabel('epoch', color=color) ax1.set_ylabel('total loss', color=color) ax1.tick_params(axis='y', color=color) # Prints Accuracy vs Epoch graph ax2 = ax1.twinx() color = 'tab:blue' ax2.set_ylabel('accuracy', color=color) # we already handled the x-label with ax1 ax2.plot(ACC, color=color) ax2.tick_params(axis='y', color=color) fig.tight_layout() # otherwise the right y-label is slightly clipped plt.show() return COST # Make some data # Define the class XOR_Data class XOR_Data(Dataset): # Constructor # N_s is the size of the dataset def __init__(self, N_s=100): # Create a N_s by 2 array for the X values representing the coordinates self.x = torch.zeros((N_s, 2)) # Create a N_s by 1 array for the class the X value belongs to self.y = torch.zeros((N_s, 1)) # Split the dataset into 4 sections for i in range(N_s // 4): # Create data centered around (0,0) of class 0 self.x[i, :] = torch.Tensor([0.0, 0.0]) self.y[i, 0] = torch.Tensor([0.0]) # Create data centered around (0,1) of class 1 self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0]) self.y[i + N_s // 4, 0] = torch.Tensor([1.0]) # Create data centered around (1,0) of class 1 self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0]) self.y[i + N_s // 2, 0] = torch.Tensor([1.0]) # Create data centered around (1,1) of class 0 self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0]) self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0]) # Add some noise to the X values to make them different self.x = self.x + 0.01 * torch.randn((N_s, 2)) self.len = N_s # Getter def __getitem__(self, index): return self.x[index],self.y[index] # Get Length def __len__(self): return self.len # Plot the data def plot_stuff(self): plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\") plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\") plt.legend() # Create dataset object data_set = XOR_Data() data_set.plot_stuff() plt.show() Quiz question: Create a neural network model with one neuron in the hidden layer. Then, use the following code to train it: # y in XOR is eithe in class 0 or 1 so D_in is 2. model = Net(D_in=2, H=1, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set) Two Neurons model = Net(D_in=2, H=2, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set) Three Neurons model = Net(D_in=2, H=3, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set)","title":"13. Neural Nets"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/#neural-nets-and-deep-learning-for-image-classification","text":"","title":"Neural Nets and Deep Learning for Image Classification"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/#neural-nets","text":"Imagine this box is a decision function we want to learn for classification. We can also view the problem as trying to approximate the box function using logistic regression: If we attempt a linear regression turned into a logistic one, we will still get errors for the higher x values. Shown here: Reminder this logistic regression is an activation function We could draw another line though: This too perhaps: Again though not perfect: However apparently we can make a band pass filter like shape by subtracting one from another to get our prediction to match the desired square pulse from earlier (kind of): Pretty square pulse we are aiming for: As you can see with two different neurons perfomring a logistic regression with a different line, we can effectively have our square pulse. This first layer of neurons and activation functions are called the hidden layer. Each neuron is an artifician neuron. The output layer has one artificial neuron. Shown below are the activation outputs. You can see red and blue colors but that is not the output of the activation layers. Imagine the dots are black and the red and blue are just letting us know the ground truth. The final layer that fits a black line (decision boundary) is what the output layer has to figure out. Review that all these logistic regressions are a series of weights and biases that can be learned. These are free forward neural networks and are also called cully connected networks With many neurons we can create complex decision boundaries.","title":"Neural Nets"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/#lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # Import the libraries we need for this lab # Allows us to use arrays to manipulate and store data import numpy as np # PyTorch Library import torch # PyTorch Neural Network import torch.nn as nn # Allows us to use activation functions import torch.nn.functional as F # Used to graph data and loss curves import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap # Used to help create the dataset and perform mini-batch from torch.utils.data import Dataset, DataLoader # Plot the data def plot_decision_regions_2class(model,data_set): cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF']) X = data_set.x.numpy() y = data_set.y.numpy() h = .02 x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h)) XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()]) yhat = np.logical_not((model(XX)[:, 0] > 0.5).numpy()).reshape(xx.shape) plt.pcolormesh(xx, yy, yhat, cmap=cmap_light, shading='auto') plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0') plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1') plt.title(\"decision region\") plt.legend() # Calculate the accuracy def accuracy(model, data_set): # Rounds prediction to nearest integer 0 or 1 # Checks if prediction matches the actual values and returns accuracy rate return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] > 0.5).numpy()) # Define the class Net with one hidden layer class Net(nn.Module): # Constructor def __init__(self, D_in, H, D_out): super(Net, self).__init__() # D_in is the input size of the first layer (size of input layer) # H is the outpout size of the first layer and the input size of the second layer (size of hidden layer) # D_out is the output size of the second layer (size of output layer) self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) # Prediction def forward(self, x): # Puts x through first layer then sigmoid function x = torch.sigmoid(self.linear1(x)) # Puts result of previous line through second layer then sigmoid function x = torch.sigmoid(self.linear2(x)) # Output is a number between 0 and 1 due to the sigmoid function. Whichever the output is closer to, 0 or 1, is the class prediction return x # Function to Train the Model def train(data_set, model, criterion, train_loader, optimizer, epochs=5): # Lists to keep track of cost and accuracy COST = [] ACC = [] # Number of times we train on the entire dataset for epoch in range(epochs): # Total loss over epoch total=0 # For batch in train laoder for x, y in train_loader: # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset optimizer.zero_grad() # Makes a prediction based on X value yhat = model(x) # Measures the loss between prediction and acutal Y value loss = criterion(yhat, y) # Calculates the gradient value with respect to each weight and bias loss.backward() # Updates the weight and bias according to calculated gradient value optimizer.step() # Cumulates loss total+=loss.item() # Saves cost and accuracy ACC.append(accuracy(model, data_set)) COST.append(total) # Prints Cost vs Epoch graph fig, ax1 = plt.subplots() color = 'tab:red' ax1.plot(COST, color=color) ax1.set_xlabel('epoch', color=color) ax1.set_ylabel('total loss', color=color) ax1.tick_params(axis='y', color=color) # Prints Accuracy vs Epoch graph ax2 = ax1.twinx() color = 'tab:blue' ax2.set_ylabel('accuracy', color=color) # we already handled the x-label with ax1 ax2.plot(ACC, color=color) ax2.tick_params(axis='y', color=color) fig.tight_layout() # otherwise the right y-label is slightly clipped plt.show() return COST # Make some data # Define the class XOR_Data class XOR_Data(Dataset): # Constructor # N_s is the size of the dataset def __init__(self, N_s=100): # Create a N_s by 2 array for the X values representing the coordinates self.x = torch.zeros((N_s, 2)) # Create a N_s by 1 array for the class the X value belongs to self.y = torch.zeros((N_s, 1)) # Split the dataset into 4 sections for i in range(N_s // 4): # Create data centered around (0,0) of class 0 self.x[i, :] = torch.Tensor([0.0, 0.0]) self.y[i, 0] = torch.Tensor([0.0]) # Create data centered around (0,1) of class 1 self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0]) self.y[i + N_s // 4, 0] = torch.Tensor([1.0]) # Create data centered around (1,0) of class 1 self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0]) self.y[i + N_s // 2, 0] = torch.Tensor([1.0]) # Create data centered around (1,1) of class 0 self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0]) self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0]) # Add some noise to the X values to make them different self.x = self.x + 0.01 * torch.randn((N_s, 2)) self.len = N_s # Getter def __getitem__(self, index): return self.x[index],self.y[index] # Get Length def __len__(self): return self.len # Plot the data def plot_stuff(self): plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\") plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\") plt.legend() # Create dataset object data_set = XOR_Data() data_set.plot_stuff() plt.show() Quiz question: Create a neural network model with one neuron in the hidden layer. Then, use the following code to train it: # y in XOR is eithe in class 0 or 1 so D_in is 2. model = Net(D_in=2, H=1, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set)","title":"Lab"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/#two-neurons","text":"model = Net(D_in=2, H=2, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set)","title":"Two Neurons"},{"location":"notes/data_science/Coursera/computer_vision/Neural_Nets/#three-neurons","text":"model = Net(D_in=2, H=3, D_out=1) # Train the model learning_rate = 0.1 # We create a criterion which will measure loss criterion = nn.BCELoss() # Create an optimizer that updates model parameters using the learning rate and gradient optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create a Data Loader for the training data with a batch size of 1 train_loader = DataLoader(dataset=data_set, batch_size=1) # Using the training function train the model on 500 epochs LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) # Plot the data with decision boundaries plot_decision_regions_2class(model, data_set)","title":"Three Neurons"},{"location":"notes/data_science/Coursera/computer_vision/Overview/","text":"Computer Vision What Is Computer Vision Provide computers the ability to see and understand images. Why use Computer Vision To automate many tasks in many industries based on images: Automotive Food and Beverage Medical Etc. Applications of Computer Vision What about sifting through videos? Have you ever tried looking for particular scene of a movie and had to to fast forward to skip around until you could find it? What about your own videos? What if they could be searchable? IBM created an example of how you could do this by tagging videos with keywords based on the objects that appear in each scene. Basically examples of many images of things like powerlines and house damage to identify rust and or hail damage for insurance companies. Recent Research in Computer Vision Object detection To make meaningful inferences in videos, this needs to be completed as a first step. Image to image translation Summer to winter horse to zebra Motion transfer between subject like dance moves Brainstorming Your Own Applications Medicine: * Training doctors to accurately detect cancer takes years of education and experience. Driving: * Driving requires constant focused visual attention and accidents can be life-threatening. * Drivers are prone to distraction and long-distance drivers are prone to falling asleep. Security: * Looking for suspect across hours of footage consumes lots of time. * Detecting dangerous goods in x-rays at the airport is difficult and requires significant training and focused attention. Manufacturing: * Defect detection in manufactured goods. * Employees are not using safetly helmets or equipment despite strict regulations.","title":"1. Overview"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#computer-vision","text":"","title":"Computer Vision"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#what-is-computer-vision","text":"Provide computers the ability to see and understand images.","title":"What Is Computer Vision"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#why-use-computer-vision","text":"To automate many tasks in many industries based on images: Automotive Food and Beverage Medical Etc.","title":"Why use Computer Vision"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#applications-of-computer-vision","text":"What about sifting through videos? Have you ever tried looking for particular scene of a movie and had to to fast forward to skip around until you could find it? What about your own videos? What if they could be searchable? IBM created an example of how you could do this by tagging videos with keywords based on the objects that appear in each scene. Basically examples of many images of things like powerlines and house damage to identify rust and or hail damage for insurance companies.","title":"Applications of Computer Vision"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#recent-research-in-computer-vision","text":"Object detection To make meaningful inferences in videos, this needs to be completed as a first step. Image to image translation Summer to winter horse to zebra Motion transfer between subject like dance moves","title":"Recent Research in Computer Vision"},{"location":"notes/data_science/Coursera/computer_vision/Overview/#brainstorming-your-own-applications","text":"Medicine: * Training doctors to accurately detect cancer takes years of education and experience. Driving: * Driving requires constant focused visual attention and accidents can be life-threatening. * Drivers are prone to distraction and long-distance drivers are prone to falling asleep. Security: * Looking for suspect across hours of footage consumes lots of time. * Detecting dangerous goods in x-rays at the airport is difficult and requires significant training and focused attention. Manufacturing: * Defect detection in manufactured goods. * Employees are not using safetly helmets or equipment despite strict regulations.","title":"Brainstorming Your Own Applications"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/","text":"Pixel Transformations Histograms Intensity Transformations Thresholding and Simple Segmentation Histograms A histogram counts the number of occurrences of a pixel, and it's a useful tool for understanding and manipulating images. Consider the following toy image with three intensity values, we can plot the occurrence of each pixel we have one black pixel, 5 gray pixels and two white pixels instead of counting the pixels, we can count the intensity values. Consider the following example as numbers ranging from 0 to 2, where 0 is black, 1 gray and 2 is white The histogram counts the pixel intensities. We represent these intensities as an array, the index of the array is the intensity level r in most images we have 256 levels, representing the count of the different intensity of gray levels consider the following image. We can calculate the histogram as follows. We have the image, the channel we would like to calculate, in this video we use the gray scale, the number of channels. Finally we have the range of intensity values in yellow We can plot the histogram as a bar graph. cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) Intensity Transformations An Intensity Transformation T depends on only one single point i,j, in the image array \u201cf\u201d, the image array \u201df\u201d is converted to array g. The transform also operates on the intensity of gray levels \u201cr\u201d mapping them them to \u201cs\u201d This changes the histogram, lets apply a simple transform to a toy image. We apply the following linear transform to the image array f, returning the array g, we apply the transform at the pixel located at the first row first column we get the following result at the same pixel location at array g We apply the transform at the pixel located at the second row first column We get the following result at the same index in array g We repeat the process for the entire array. We have the histogram each pixel intensity is a function of \u201cr\u201d or histogram r The transform maps each intensity value at r to s we have the new intensity values s and it\u2019s histogram s, let\u2019s see the relationship between histogram \u201cs\u201d and \u201cr\u201d the value of histogram r has one intensity value at zero(click 1) the value of histogram r has one intensity value at zero when applying the transform to r we see the intensity value of zero is mapped to 1 the histogram s now has one intensity value at one we see the intensity value r =1 is mapped to 3 the histogram r has a value of 5 at r=1 In histogram s the value of 5 is mapped to s=3, and so on. If we plot the histogram after applying the transform, we see the histogram is shifted and scaled. Image Negatives Reverse intensity levels of an image. Consider the following image: The following is the transformed image: We can apply the transform as an array operation as follows The new image looks like this; we see the details are much more evident. Consider the following histogram: * We overlay transform as function of intensity (axis on right) * This flips graph about the 125 intensity point on x-axis Brightness and Contrast Adjustments A linear transform can be seen as applying Brightness and Contrast Adjustments. We can use the following linear model where alpha is Simple contrast control and beta is Simple brightness control We can use the following values, just changing beta we can adjust the brightness Rather than using array operations we use the function \u201cconvertScaleAbs\u201d after applying the transformation. The function scales, calculates absolute values, so the intensity values fall in the 0 to 255 value range We see the image is much brighter. We see the histogram of the bright image has shifted to the right. A lot of the intensity values have been mapped to the point value of 255. We can adjust alpha to change the contrast. We see the contrast in the darker areas have improved but the lighter areas the image appears washed out Comparing the original image histogram to the histogram of the new image We see the lower values exhibit more spread, but many of the larger values have been mapped to 255 explaining why the image appeared washed out. Historgram Equalization We can use nonlinear functions, but let\u2019s explore some algorithms that optimize contrast. Consider, the following image with following histogram the function \u201cequalizeHist\u201d improves contrast, by using the histogram to determine a transform that flattens the histogram, the resulting image has improved contrast. Thresholding and Simple Segmentation A threshold function applies a threshold to every pixel, it can be used in extracting objects from an image this is called segmentation The following function applies a threshold to the input array input image array imput_out and outputs the result the following portion of the code will cycle through each pixel (\ud835\udc56,\ud835\udc57). If the pixel is greater then that threshold It will set a pixel in the array \u201cimage_out\u201d pixel to some value, usually 1 or 255, Otherwise it will set it to another value, usually zero. Consider the following array In this case the threshold value is 1 and the min value is 0 and the max value 255 the first loop increments through the rows, for each column in \u201cinput_img\u201d we check the condition if the values greater than 1 we set the corresponding element in \u201cimage_out\u201d to 255. Otherwise we set it to zeros. We repeat the process for the second row As none of the columns are less than one all the corresponding rows in \u201dImage_out\u201d is set to zero. We repeat the process for the final row. The result is all the values in the new image array are white or black. Consider the following image, it\u2019s histogram bimodal We see the camera man corresponds to the first mode i.e intensities under 87, the second mode is the back round. By setting the threshold accordingly we can segment the camera man. We use the function threshold to perform thresholding, the output new_image is the image after thresholding has been applied. Along with the image ld the input is the threshold and the max value is set to 255. The final parameter is the threshold type, in this case THRESH_BINARY. This means the output will be 0 or 255. In the output we see the camera has zero values and most of the background has values of 255. We see the histograms intensities are mapped to black and wight after thresholding has been applied. Sometimes its difficult to select a threshold, therefore we can use OTSU method to select the value automatically. ret is the threshold value determined, We see the image looks identical to the one using the manual threshold. Code Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/goldhill.bmp -O goldhill.bmp wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg -O cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/zelda.png -O zelda.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/mammogram.png -O mammogram.png import matplotlib.pyplot as plt import cv2 import numpy as np # First, lets define a helper function to plot two images side-by-side. You will not need to understand this code at this moment, but this function will be used repeatedly in this tutorial to showcase the results. def plot_image(image_1, image_2,title_1=\"Orignal\", title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1,cmap=\"gray\") plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2,cmap=\"gray\") plt.title(title_2) plt.show() # Lets define another helper function. The following one will plot two histograms side-by-side. Again, you do not need to understand the body of this function at this moment. def plot_hist(old_image, new_image,title_old=\"Orignal\", title_new=\"New Image\"): intensity_values=np.array([x for x in range(256)]) plt.subplot(1, 2, 1) plt.bar(intensity_values, cv2.calcHist([old_image],[0],None,[256],[0,256])[:,0],width = 5) plt.title(title_old) plt.xlabel('intensity') plt.subplot(1, 2, 2) plt.bar(intensity_values, cv2.calcHist([new_image],[0],None,[256],[0,256])[:,0],width = 5) plt.title(title_new) plt.xlabel('intensity') plt.show() Histograms # Toy Example toy_image = np.array([[0,2,2],[1,1,1],[1,1,2]],dtype=np.uint8) plt.imshow(toy_image, cmap=\"gray\") plt.show() print(\"toy_image:\",toy_image) # toy_image: [[0 2 2] # [1 1 1] # [1 1 2]] # cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) # We can use the caclHist function. # In this case, we use only three bins as there are only three values, and the index of the bins are from 1 to 3. hist_counts = cv2.calcHist([toy_image], [0], None, [3], [0,3]) hist_counts_list = [i[0] for i in hist_counts] # array([[1.], # [5.], # [3.]], dtype=float32) plt.bar([x for x in range(3)],hist_counts_list) plt.show() The histogram is a function where \\(h[r]\\) where \\(r \\in {0,1,2}\\) . In the above example \\(h[0]=1\\) , \\(h[1]=5\\) and \\(h[2]=3\\) Gray Scale Histograms Histograms are used in grayscale images. Grayscale images are used in many applications, including medical and industrial. Color images are split into luminance and chrominance. The luminance is the grayscale portion and is usually processed in many applications. Consider the following \"Gold Hill\" image: goldhill = cv2.imread(\"goldhill.bmp\",cv2.IMREAD_GRAYSCALE) plt.figure(figsize=(10,10)) plt.imshow(goldhill,cmap=\"gray\") plt.show() # We can calculate the histogram using the `calcHist` function from the `cv2` module as follows, the shape is 256. hist = cv2.calcHist([goldhill],[0], None, [256], [0,256]) We can plot it as a bar graph, the \\(x\\) -axis are the pixel intensities and the \\(y\\) -axis is the number of times of occurrences that the corresponding pixel intensity value on \\(x\\) -axis occurred. intensity_values = np.array([x for x in range(hist.shape[0])]) plt.bar(intensity_values, hist[:,0], width = 5) plt.title(\"Bar histogram\") plt.show() The histogram is a function where \\(h[r]\\) where \\(r \\in {0,1,..,255}\\) . PMF = hist / (goldhill.shape[0] * goldhill.shape[1]) plt.plot(intensity_values,PMF) plt.title(\"PMF\") plt.show() We can also apply a histogram to each image color channel: baboon = cv2.imread(\"baboon.png\") plt.imshow(cv2.cvtColor(baboon,cv2.COLOR_BGR2RGB)) plt.show() # In the loop, the value for i specifies what color channel calcHist is going to calculate the histogram for. color = ('blue','green','red') for i,col in enumerate(color): histr = cv2.calcHist([baboon],[i],None,[256],[0,256]) plt.plot(intensity_values,histr,color = col,label=col+\" channel\") plt.xlim([0,256]) plt.legend() plt.title(\"Histogram Channels\") plt.show() Intensity Transformations It's helpful to think of an image as a function \\(f(x,y)\\) instead of an array at this point, where x is the row index and y is the column index. You can apply a transformation \\(T\\) to the image and get a new image: $$ g(x,y)=T(f(x,y)) $$ An Intensity Transformation depends on only one single point \\((x,y)\\) . For example, you can apply a linear transform \\(g(x,y) = 2f(x,y) + 1\\) ; this will multiply each image pixel by two and add one. As the Intensity transforms only depend on one value; as a result, it is sometimes referred to as a gray-level mapping. The variable if \\(r\\) is the gray level intensity, similar to the histogram values. The new output s is given by: \\[ s=T(r) \\] Image Negatives Consider an image with \\(L\\) intensity values ranging from \\([0,L-1]\\) . We can reverse the intensity levels by applying the following: \\[ g(x,y)=L-1-f(x,y) \\] Using the intensity transformation function notation $$ s = L - 1 - r $$ This is called the image negative. For \\(L= 256\\) the formulas simplifys to: \\[ g(x,y)=255-f(x,y) \\qquad and \\qquad s=255-r \\] We can perform intensity transformation on the toy image where \\(L=3\\) . L = 3 neg_toy_image = (L - 1) - toy_image print(\"toy image\\n\", neg_toy_image) print(\"image negatives\\n\", neg_toy_image) # We see darker intensity\u2019s become brighter and brighter become darker, middle intensity\u2019s remain the same. plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(toy_image,cmap=\"gray\") plt.subplot(1, 2, 2) plt.imshow(neg_toy_image,cmap=\"gray\") plt.show() print(\"toy_image:\",toy_image) # Reversing image intensity has many applications, including making it simpler to analyze medical images. Consider the mammogram with micro-calcifications on the upper quadrant: image = cv2.imread(\"mammogram.png\", cv2.IMREAD_GRAYSCALE) cv2.rectangle(image, pt1=(160, 212), pt2=(250, 289), color = (255), thickness=2) plt.figure(figsize = (10,10)) plt.imshow(image, cmap=\"gray\") plt.show() # Transform img_neg = -1 * image + 255 # We see the micro-calcifications in the image negatives is easier it is to analyze: plt.figure(figsize=(10,10)) plt.imshow(img_neg, cmap = \"gray\") plt.show() Brightness and Color Adjustments We can use multiplication by \\(\\alpha\\) for contrast control and addition by \\(\\beta\\) to improve brightness control. This applies the Intensity Transformation as well. The image is \\(f(x,y)\\) and the transformed image is \\(g(x,y)\\) , where \\(g(x,y) = \\alpha f(x,y) + \\beta\\) . Rather than implementing via array operations, we use the function convertScaleAbs. It scales, calculates absolute values, and converts the result to 8-bit so the values fall between \\([0,255]\\) . For brightness control, we can set \\(\\alpha\\) to 1 and \\(\\beta\\) to 100: Remember the Good Hill image, it\u2019s dark and hazy so let's see if we can improve it. alpha = 1 # Simple contrast control beta = 100 # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill, new_image, title_1 = \"Orignal\", title_2 = \"brightness control\") # We see the brighter image's histogram is shifted: plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image, \"Orignal\", \"brightness control\") We can increase the contrast by increasing \\(\\alpha\\) : plt.figure(figsize=(10,5)) alpha = 2# Simple contrast control beta = 0 # Simple brightness control # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill,new_image,\"Orignal\",\"contrast control\") plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image,\"Orignal\",\"contrast control\") # When plotting the image we see it's too bright. We can adapt the brightness by making the image darker and increasing the contrast at the same time. plt.figure(figsize=(10,5)) alpha = 3 # Simple contrast control beta = -200 # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill, new_image, \"Orignal\", \"brightness & contrast control\") plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image, \"Orignal\", \"brightness & contrast control\") Histogram Equalization Histogram Equalization increases the contrast of images, by stretching out the range of the grayscale pixels; It does this by flatting the histogram. We simply apply the function cv2.equalizeHist. zelda = cv2.imread(\"zelda.png\",cv2.IMREAD_GRAYSCALE) new_image = cv2.equalizeHist(zelda) # We can compare the image before and after Histogram Equalization, we see the contrast is improved. We see after the Histogram Equalization is applied, the histogram is stretched out: plot_image(zelda,new_image,\"Orignal\",\"Histogram Equalization\") plt.figure(figsize=(10,5)) plot_hist(zelda, new_image,\"Orignal\",\"Histogram Equalization\") Thresholding and Simple Segmentation Thresholding is used in image segmentation this means extracting objects from an image. Image segmentation is used in many applications including extracting text, medical imaging, and industrial imaging. Thresholding an image takes a threshold; If a particular pixel (i,j) is greater than that threshold it will set that pixel to some value usually 1 or 255, otherwise, it will set it to another value, usually zero. We can write a Python function that will perform thresholding and output a new image given some input grayscale image: def thresholding(input_img,threshold,max_value=255, min_value=0): N,M=input_img.shape image_out=np.zeros((N,M),dtype=np.uint8) for i in range(N): for j in range(M): if input_img[i,j]> threshold: image_out[i,j]=max_value else: image_out[i,j]=min_value return image_out # Consider the following toy image: toy_image # We can apply thresholding, by setting all the values less than two to zero. threshold = 1 max_value = 2 min_value = 0 thresholding_toy = thresholding(toy_image, threshold=threshold, max_value=max_value, min_value=min_value) thresholding_toy # We can compare the two images. In the new image we see all the gray values are now black: plt.figure(figsize=(10, 10)) plt.subplot(1, 2, 1) plt.imshow(toy_image, cmap=\"gray\") plt.title(\"Original Image\") plt.subplot(1, 2, 2) plt.imshow(thresholding_toy, cmap=\"gray\") plt.title(\"Image After Thresholding\") plt.show() # Consider the cameraman image: image = cv2.imread(\"cameraman.jpeg\", cv2.IMREAD_GRAYSCALE) plt.figure(figsize=(10, 10)) plt.imshow(image, cmap=\"gray\") plt.show() # We can see the histogram as two peeks, this means that there is a large proportion of pixels in those two ranges: hist = cv2.calcHist([image], [0], None, [256], [0, 256]) intensity_values = np.array([x for x in range(hist.shape[0])]) plt.bar(intensity_values, hist[:, 0], width=5) plt.title(\"Bar histogram\") plt.show() # The cameraman corresponds to the darker pixels, therefore we can set the Threshold in such a way as to segment the cameraman. In this case, it looks to be slightly less than 90, let\u2019s give it a try: threshold = 87 max_value = 255 min_value = 0 new_image = thresholding(image, threshold=threshold, max_value=max_value, min_value=min_value) # We see the pixels corresponding to the cameraman are now zero and the rest are set to 255: plot_image(image, new_image, \"Orignal\", \"Image After Thresholding\") plt.figure(figsize=(10,5)) plot_hist(image, new_image, \"Orignal\", \"Image After Thresholding\") The function cv2.threshold applies a threshold to the gray image, with the following parameters: cv2.THRESH_BINARY # 0 # We can apply thresholding to the image as follows: ret, new_image = cv2.threshold(image,threshold,max_value,cv2.THRESH_BINARY) plot_image(image,new_image,\"Orignal\",\"Image After Thresholding\") plot_hist(image, new_image,\"Orignal\",\"Image After Thresholding\") ret is the threshold value and new_image is the image after thresholding has been applied. There are different threshold types, for example, cv2.THRESH_TRUNC will not change the values if the pixels are less than the threshold value: ret, new_image = cv2.threshold(image,86,255,cv2.THRESH_TRUNC) plot_image(image,new_image,\"Orignal\",\"Image After Thresholding\") plot_hist(image, new_image,\"Orignal\",\"Image After Thresholding\") # We see that the darker elements have not changed and the lighter values are set to 255. Otsu's method cv2.THRESH_OTSU avoids having to choose a value and determines it automatically, using the histogram. ret, otsu = cv2.threshold(image,0,255,cv2.THRESH_OTSU) plot_image(image,otsu,\"Orignal\",\"Otsu\") plot_hist(image, otsu,\"Orignal\",\" Otsu's method\")","title":"5. Pixel Transformations"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#pixel-transformations","text":"Histograms Intensity Transformations Thresholding and Simple Segmentation","title":"Pixel Transformations"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#histograms","text":"A histogram counts the number of occurrences of a pixel, and it's a useful tool for understanding and manipulating images. Consider the following toy image with three intensity values, we can plot the occurrence of each pixel we have one black pixel, 5 gray pixels and two white pixels instead of counting the pixels, we can count the intensity values. Consider the following example as numbers ranging from 0 to 2, where 0 is black, 1 gray and 2 is white The histogram counts the pixel intensities. We represent these intensities as an array, the index of the array is the intensity level r in most images we have 256 levels, representing the count of the different intensity of gray levels consider the following image. We can calculate the histogram as follows. We have the image, the channel we would like to calculate, in this video we use the gray scale, the number of channels. Finally we have the range of intensity values in yellow We can plot the histogram as a bar graph. cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])","title":"Histograms"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#intensity-transformations","text":"An Intensity Transformation T depends on only one single point i,j, in the image array \u201cf\u201d, the image array \u201df\u201d is converted to array g. The transform also operates on the intensity of gray levels \u201cr\u201d mapping them them to \u201cs\u201d This changes the histogram, lets apply a simple transform to a toy image. We apply the following linear transform to the image array f, returning the array g, we apply the transform at the pixel located at the first row first column we get the following result at the same pixel location at array g We apply the transform at the pixel located at the second row first column We get the following result at the same index in array g We repeat the process for the entire array. We have the histogram each pixel intensity is a function of \u201cr\u201d or histogram r The transform maps each intensity value at r to s we have the new intensity values s and it\u2019s histogram s, let\u2019s see the relationship between histogram \u201cs\u201d and \u201cr\u201d the value of histogram r has one intensity value at zero(click 1) the value of histogram r has one intensity value at zero when applying the transform to r we see the intensity value of zero is mapped to 1 the histogram s now has one intensity value at one we see the intensity value r =1 is mapped to 3 the histogram r has a value of 5 at r=1 In histogram s the value of 5 is mapped to s=3, and so on. If we plot the histogram after applying the transform, we see the histogram is shifted and scaled.","title":"Intensity Transformations"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#image-negatives","text":"Reverse intensity levels of an image. Consider the following image: The following is the transformed image: We can apply the transform as an array operation as follows The new image looks like this; we see the details are much more evident. Consider the following histogram: * We overlay transform as function of intensity (axis on right) * This flips graph about the 125 intensity point on x-axis","title":"Image Negatives"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#brightness-and-contrast-adjustments","text":"A linear transform can be seen as applying Brightness and Contrast Adjustments. We can use the following linear model where alpha is Simple contrast control and beta is Simple brightness control We can use the following values, just changing beta we can adjust the brightness Rather than using array operations we use the function \u201cconvertScaleAbs\u201d after applying the transformation. The function scales, calculates absolute values, so the intensity values fall in the 0 to 255 value range We see the image is much brighter. We see the histogram of the bright image has shifted to the right. A lot of the intensity values have been mapped to the point value of 255. We can adjust alpha to change the contrast. We see the contrast in the darker areas have improved but the lighter areas the image appears washed out Comparing the original image histogram to the histogram of the new image We see the lower values exhibit more spread, but many of the larger values have been mapped to 255 explaining why the image appeared washed out.","title":"Brightness and Contrast Adjustments"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#historgram-equalization","text":"We can use nonlinear functions, but let\u2019s explore some algorithms that optimize contrast. Consider, the following image with following histogram the function \u201cequalizeHist\u201d improves contrast, by using the histogram to determine a transform that flattens the histogram, the resulting image has improved contrast.","title":"Historgram Equalization"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#thresholding-and-simple-segmentation","text":"A threshold function applies a threshold to every pixel, it can be used in extracting objects from an image this is called segmentation The following function applies a threshold to the input array input image array imput_out and outputs the result the following portion of the code will cycle through each pixel (\ud835\udc56,\ud835\udc57). If the pixel is greater then that threshold It will set a pixel in the array \u201cimage_out\u201d pixel to some value, usually 1 or 255, Otherwise it will set it to another value, usually zero. Consider the following array In this case the threshold value is 1 and the min value is 0 and the max value 255 the first loop increments through the rows, for each column in \u201cinput_img\u201d we check the condition if the values greater than 1 we set the corresponding element in \u201cimage_out\u201d to 255. Otherwise we set it to zeros. We repeat the process for the second row As none of the columns are less than one all the corresponding rows in \u201dImage_out\u201d is set to zero. We repeat the process for the final row. The result is all the values in the new image array are white or black. Consider the following image, it\u2019s histogram bimodal We see the camera man corresponds to the first mode i.e intensities under 87, the second mode is the back round. By setting the threshold accordingly we can segment the camera man. We use the function threshold to perform thresholding, the output new_image is the image after thresholding has been applied. Along with the image ld the input is the threshold and the max value is set to 255. The final parameter is the threshold type, in this case THRESH_BINARY. This means the output will be 0 or 255. In the output we see the camera has zero values and most of the background has values of 255. We see the histograms intensities are mapped to black and wight after thresholding has been applied. Sometimes its difficult to select a threshold, therefore we can use OTSU method to select the value automatically. ret is the threshold value determined, We see the image looks identical to the one using the manual threshold.","title":"Thresholding and Simple Segmentation"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#code-lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/goldhill.bmp -O goldhill.bmp wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg -O cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/zelda.png -O zelda.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/mammogram.png -O mammogram.png import matplotlib.pyplot as plt import cv2 import numpy as np # First, lets define a helper function to plot two images side-by-side. You will not need to understand this code at this moment, but this function will be used repeatedly in this tutorial to showcase the results. def plot_image(image_1, image_2,title_1=\"Orignal\", title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1,cmap=\"gray\") plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2,cmap=\"gray\") plt.title(title_2) plt.show() # Lets define another helper function. The following one will plot two histograms side-by-side. Again, you do not need to understand the body of this function at this moment. def plot_hist(old_image, new_image,title_old=\"Orignal\", title_new=\"New Image\"): intensity_values=np.array([x for x in range(256)]) plt.subplot(1, 2, 1) plt.bar(intensity_values, cv2.calcHist([old_image],[0],None,[256],[0,256])[:,0],width = 5) plt.title(title_old) plt.xlabel('intensity') plt.subplot(1, 2, 2) plt.bar(intensity_values, cv2.calcHist([new_image],[0],None,[256],[0,256])[:,0],width = 5) plt.title(title_new) plt.xlabel('intensity') plt.show()","title":"Code Lab"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#histograms_1","text":"# Toy Example toy_image = np.array([[0,2,2],[1,1,1],[1,1,2]],dtype=np.uint8) plt.imshow(toy_image, cmap=\"gray\") plt.show() print(\"toy_image:\",toy_image) # toy_image: [[0 2 2] # [1 1 1] # [1 1 2]] # cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) # We can use the caclHist function. # In this case, we use only three bins as there are only three values, and the index of the bins are from 1 to 3. hist_counts = cv2.calcHist([toy_image], [0], None, [3], [0,3]) hist_counts_list = [i[0] for i in hist_counts] # array([[1.], # [5.], # [3.]], dtype=float32) plt.bar([x for x in range(3)],hist_counts_list) plt.show() The histogram is a function where \\(h[r]\\) where \\(r \\in {0,1,2}\\) . In the above example \\(h[0]=1\\) , \\(h[1]=5\\) and \\(h[2]=3\\)","title":"Histograms"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#gray-scale-histograms","text":"Histograms are used in grayscale images. Grayscale images are used in many applications, including medical and industrial. Color images are split into luminance and chrominance. The luminance is the grayscale portion and is usually processed in many applications. Consider the following \"Gold Hill\" image: goldhill = cv2.imread(\"goldhill.bmp\",cv2.IMREAD_GRAYSCALE) plt.figure(figsize=(10,10)) plt.imshow(goldhill,cmap=\"gray\") plt.show() # We can calculate the histogram using the `calcHist` function from the `cv2` module as follows, the shape is 256. hist = cv2.calcHist([goldhill],[0], None, [256], [0,256]) We can plot it as a bar graph, the \\(x\\) -axis are the pixel intensities and the \\(y\\) -axis is the number of times of occurrences that the corresponding pixel intensity value on \\(x\\) -axis occurred. intensity_values = np.array([x for x in range(hist.shape[0])]) plt.bar(intensity_values, hist[:,0], width = 5) plt.title(\"Bar histogram\") plt.show() The histogram is a function where \\(h[r]\\) where \\(r \\in {0,1,..,255}\\) . PMF = hist / (goldhill.shape[0] * goldhill.shape[1]) plt.plot(intensity_values,PMF) plt.title(\"PMF\") plt.show() We can also apply a histogram to each image color channel: baboon = cv2.imread(\"baboon.png\") plt.imshow(cv2.cvtColor(baboon,cv2.COLOR_BGR2RGB)) plt.show() # In the loop, the value for i specifies what color channel calcHist is going to calculate the histogram for. color = ('blue','green','red') for i,col in enumerate(color): histr = cv2.calcHist([baboon],[i],None,[256],[0,256]) plt.plot(intensity_values,histr,color = col,label=col+\" channel\") plt.xlim([0,256]) plt.legend() plt.title(\"Histogram Channels\") plt.show()","title":"Gray Scale Histograms"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#intensity-transformations_1","text":"It's helpful to think of an image as a function \\(f(x,y)\\) instead of an array at this point, where x is the row index and y is the column index. You can apply a transformation \\(T\\) to the image and get a new image: $$ g(x,y)=T(f(x,y)) $$ An Intensity Transformation depends on only one single point \\((x,y)\\) . For example, you can apply a linear transform \\(g(x,y) = 2f(x,y) + 1\\) ; this will multiply each image pixel by two and add one. As the Intensity transforms only depend on one value; as a result, it is sometimes referred to as a gray-level mapping. The variable if \\(r\\) is the gray level intensity, similar to the histogram values. The new output s is given by: \\[ s=T(r) \\]","title":"Intensity Transformations"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#image-negatives_1","text":"Consider an image with \\(L\\) intensity values ranging from \\([0,L-1]\\) . We can reverse the intensity levels by applying the following: \\[ g(x,y)=L-1-f(x,y) \\] Using the intensity transformation function notation $$ s = L - 1 - r $$ This is called the image negative. For \\(L= 256\\) the formulas simplifys to: \\[ g(x,y)=255-f(x,y) \\qquad and \\qquad s=255-r \\] We can perform intensity transformation on the toy image where \\(L=3\\) . L = 3 neg_toy_image = (L - 1) - toy_image print(\"toy image\\n\", neg_toy_image) print(\"image negatives\\n\", neg_toy_image) # We see darker intensity\u2019s become brighter and brighter become darker, middle intensity\u2019s remain the same. plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(toy_image,cmap=\"gray\") plt.subplot(1, 2, 2) plt.imshow(neg_toy_image,cmap=\"gray\") plt.show() print(\"toy_image:\",toy_image) # Reversing image intensity has many applications, including making it simpler to analyze medical images. Consider the mammogram with micro-calcifications on the upper quadrant: image = cv2.imread(\"mammogram.png\", cv2.IMREAD_GRAYSCALE) cv2.rectangle(image, pt1=(160, 212), pt2=(250, 289), color = (255), thickness=2) plt.figure(figsize = (10,10)) plt.imshow(image, cmap=\"gray\") plt.show() # Transform img_neg = -1 * image + 255 # We see the micro-calcifications in the image negatives is easier it is to analyze: plt.figure(figsize=(10,10)) plt.imshow(img_neg, cmap = \"gray\") plt.show()","title":"Image Negatives"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#brightness-and-color-adjustments","text":"We can use multiplication by \\(\\alpha\\) for contrast control and addition by \\(\\beta\\) to improve brightness control. This applies the Intensity Transformation as well. The image is \\(f(x,y)\\) and the transformed image is \\(g(x,y)\\) , where \\(g(x,y) = \\alpha f(x,y) + \\beta\\) . Rather than implementing via array operations, we use the function convertScaleAbs. It scales, calculates absolute values, and converts the result to 8-bit so the values fall between \\([0,255]\\) . For brightness control, we can set \\(\\alpha\\) to 1 and \\(\\beta\\) to 100: Remember the Good Hill image, it\u2019s dark and hazy so let's see if we can improve it. alpha = 1 # Simple contrast control beta = 100 # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill, new_image, title_1 = \"Orignal\", title_2 = \"brightness control\") # We see the brighter image's histogram is shifted: plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image, \"Orignal\", \"brightness control\") We can increase the contrast by increasing \\(\\alpha\\) : plt.figure(figsize=(10,5)) alpha = 2# Simple contrast control beta = 0 # Simple brightness control # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill,new_image,\"Orignal\",\"contrast control\") plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image,\"Orignal\",\"contrast control\") # When plotting the image we see it's too bright. We can adapt the brightness by making the image darker and increasing the contrast at the same time. plt.figure(figsize=(10,5)) alpha = 3 # Simple contrast control beta = -200 # Simple brightness control new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) plot_image(goldhill, new_image, \"Orignal\", \"brightness & contrast control\") plt.figure(figsize=(10,5)) plot_hist(goldhill, new_image, \"Orignal\", \"brightness & contrast control\")","title":"Brightness and Color Adjustments"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#histogram-equalization","text":"Histogram Equalization increases the contrast of images, by stretching out the range of the grayscale pixels; It does this by flatting the histogram. We simply apply the function cv2.equalizeHist. zelda = cv2.imread(\"zelda.png\",cv2.IMREAD_GRAYSCALE) new_image = cv2.equalizeHist(zelda) # We can compare the image before and after Histogram Equalization, we see the contrast is improved. We see after the Histogram Equalization is applied, the histogram is stretched out: plot_image(zelda,new_image,\"Orignal\",\"Histogram Equalization\") plt.figure(figsize=(10,5)) plot_hist(zelda, new_image,\"Orignal\",\"Histogram Equalization\")","title":"Histogram Equalization"},{"location":"notes/data_science/Coursera/computer_vision/Pixel_Transformations/#thresholding-and-simple-segmentation_1","text":"Thresholding is used in image segmentation this means extracting objects from an image. Image segmentation is used in many applications including extracting text, medical imaging, and industrial imaging. Thresholding an image takes a threshold; If a particular pixel (i,j) is greater than that threshold it will set that pixel to some value usually 1 or 255, otherwise, it will set it to another value, usually zero. We can write a Python function that will perform thresholding and output a new image given some input grayscale image: def thresholding(input_img,threshold,max_value=255, min_value=0): N,M=input_img.shape image_out=np.zeros((N,M),dtype=np.uint8) for i in range(N): for j in range(M): if input_img[i,j]> threshold: image_out[i,j]=max_value else: image_out[i,j]=min_value return image_out # Consider the following toy image: toy_image # We can apply thresholding, by setting all the values less than two to zero. threshold = 1 max_value = 2 min_value = 0 thresholding_toy = thresholding(toy_image, threshold=threshold, max_value=max_value, min_value=min_value) thresholding_toy # We can compare the two images. In the new image we see all the gray values are now black: plt.figure(figsize=(10, 10)) plt.subplot(1, 2, 1) plt.imshow(toy_image, cmap=\"gray\") plt.title(\"Original Image\") plt.subplot(1, 2, 2) plt.imshow(thresholding_toy, cmap=\"gray\") plt.title(\"Image After Thresholding\") plt.show() # Consider the cameraman image: image = cv2.imread(\"cameraman.jpeg\", cv2.IMREAD_GRAYSCALE) plt.figure(figsize=(10, 10)) plt.imshow(image, cmap=\"gray\") plt.show() # We can see the histogram as two peeks, this means that there is a large proportion of pixels in those two ranges: hist = cv2.calcHist([image], [0], None, [256], [0, 256]) intensity_values = np.array([x for x in range(hist.shape[0])]) plt.bar(intensity_values, hist[:, 0], width=5) plt.title(\"Bar histogram\") plt.show() # The cameraman corresponds to the darker pixels, therefore we can set the Threshold in such a way as to segment the cameraman. In this case, it looks to be slightly less than 90, let\u2019s give it a try: threshold = 87 max_value = 255 min_value = 0 new_image = thresholding(image, threshold=threshold, max_value=max_value, min_value=min_value) # We see the pixels corresponding to the cameraman are now zero and the rest are set to 255: plot_image(image, new_image, \"Orignal\", \"Image After Thresholding\") plt.figure(figsize=(10,5)) plot_hist(image, new_image, \"Orignal\", \"Image After Thresholding\") The function cv2.threshold applies a threshold to the gray image, with the following parameters: cv2.THRESH_BINARY # 0 # We can apply thresholding to the image as follows: ret, new_image = cv2.threshold(image,threshold,max_value,cv2.THRESH_BINARY) plot_image(image,new_image,\"Orignal\",\"Image After Thresholding\") plot_hist(image, new_image,\"Orignal\",\"Image After Thresholding\") ret is the threshold value and new_image is the image after thresholding has been applied. There are different threshold types, for example, cv2.THRESH_TRUNC will not change the values if the pixels are less than the threshold value: ret, new_image = cv2.threshold(image,86,255,cv2.THRESH_TRUNC) plot_image(image,new_image,\"Orignal\",\"Image After Thresholding\") plot_hist(image, new_image,\"Orignal\",\"Image After Thresholding\") # We see that the darker elements have not changed and the lighter values are set to 255. Otsu's method cv2.THRESH_OTSU avoids having to choose a value and determines it automatically, using the histogram. ret, otsu = cv2.threshold(image,0,255,cv2.THRESH_OTSU) plot_image(image,otsu,\"Orignal\",\"Otsu\") plot_hist(image, otsu,\"Orignal\",\" Otsu's method\")","title":"Thresholding and Simple Segmentation"},{"location":"notes/data_science/Coursera/computer_vision/Softmax_and_Multi-Class_Classification/","text":"Softmax and Multi-Class Classification Argmax review The argmax function returns the index corresponding to the largest value in a sequence of numbers. Here the largest value in Z is 100 and the corresponding index is zero, thus the argmax function will return zero. Multiclass Instead of using one plane to classify the data, we will use one plane for each class. In this case, we have three equations representing three classes, but we can generalize to any number of classes. We can also use the graph to represent equations. In this case, nodes are representing the different components of X. We add nodes for each output Z. The edges represent the different learnable parameters with subscripts indicating the dimension. This is the plane where Z equals zero, the line is where the decision plane intersects with the plane Z equals zero. We can overlay our sample images, we see the lines split the classes. If the input is in the blue region, the value of z0 corresponding to the equation zero is the largest. This is where the blue plane has a higher value than the other regions. Therefore, anything in this region will be in class zero. If the input is in the red region, the value of z1 corresponding to equation one is the largest. Therefore, anything in this region will be in class one. If the input is in the yellow region, the value of z2 corresponding to equation Z is the largest. Therefore, anything in this region will be in class two. Just to note, the yellow line we see is where the plane is greater than zero. The yellow region is where the yellow plane is larger than the blue and red region. We can now use the planes to classify this unknown point.","title":"10. Softmax and Multi-Class Classification"},{"location":"notes/data_science/Coursera/computer_vision/Softmax_and_Multi-Class_Classification/#softmax-and-multi-class-classification","text":"","title":"Softmax and Multi-Class Classification"},{"location":"notes/data_science/Coursera/computer_vision/Softmax_and_Multi-Class_Classification/#argmax-review","text":"The argmax function returns the index corresponding to the largest value in a sequence of numbers. Here the largest value in Z is 100 and the corresponding index is zero, thus the argmax function will return zero.","title":"Argmax review"},{"location":"notes/data_science/Coursera/computer_vision/Softmax_and_Multi-Class_Classification/#multiclass","text":"Instead of using one plane to classify the data, we will use one plane for each class. In this case, we have three equations representing three classes, but we can generalize to any number of classes. We can also use the graph to represent equations. In this case, nodes are representing the different components of X. We add nodes for each output Z. The edges represent the different learnable parameters with subscripts indicating the dimension. This is the plane where Z equals zero, the line is where the decision plane intersects with the plane Z equals zero. We can overlay our sample images, we see the lines split the classes. If the input is in the blue region, the value of z0 corresponding to the equation zero is the largest. This is where the blue plane has a higher value than the other regions. Therefore, anything in this region will be in class zero. If the input is in the red region, the value of z1 corresponding to equation one is the largest. Therefore, anything in this region will be in class one. If the input is in the yellow region, the value of z2 corresponding to equation Z is the largest. Therefore, anything in this region will be in class two. Just to note, the yellow line we see is where the plane is greater than zero. The yellow region is where the yellow plane is larger than the blue and red region. We can now use the planes to classify this unknown point.","title":"Multiclass"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/","text":"Spatial Operations In Image Processing In this video we will discuss Spatial Operations in Image Processing. In this video we will review: Convolution sometimes called Linear Filtering Edge Detection Median Filters In this video we will show the one channel representation, but unless specified we can apply these operations to each channel independently. Convolution Spatial Operations consists of a neighbourhood, in this example we take a neighbourhood of 2 by 2 pixels we apply a function that involves each pixel in the neighbourhood and output the result we then shift the neighbourhood and repeat the process for each pixel in the image. The result is a new image that has enhanced characteristics, these spatial operations take advantage is spatial patterns in the image. Convolution or linear filtering is a standard way to Filter an image. The filter is called the kernel. The different kernels perform different tasks. Convolution is analogous to this linear equation. The input is the image X . Output will be another image Z . We have the parameter W this is known as a kernel or filter. \\(z = wx\\) \\(Z = W * Xx\\) The star this operation is the convolution operation. There is actually an analytical expression for it, but it's a little confusing. So let's just go over an example to understand what's going on. Here's our image X, here's our kernel w, it\u2019s just a smaller array. The array is made up of values that have been pre determined for some operation. Let's go over the operation of convolution. We start off in the top right corner of the image, and we'll overlay the kernel with that region of the image. We see the kernel values in red, we'll multiply every element of the image by the corresponding element of the kernel. For the first row we'll get the following operation, multiplying the intensity value and summing the results this process is repeated for the second row, multiplying the intensity value and summing the results. Finally for the final row we multiply the intensity values and sum the results. We sum those elements together. The result is the first element of the image z. We shift the kernel to the left represented by the different colors in red. We multiply all the elements of the kernel with the image. This gives us the second element of the output image z. We'll shift the kernel one more column and perform the exact same operation, multiplying every element of the kernel by the corresponding intensity value of the image, adding them all together. This gives us the next value. We then shift the kernel down and repeat the process until we get to the end of the column. Repeating the process until we get a new image. One problem is the images are different sizes. We can fix this by changing the size of the image x. We change the size of the image by padding. In zero padding we add two additional rows of zeros and two additional columns of zeros. We can also replicate the values of the border. Low Pass Filters Low Pass Filters are used to smooth the image getting rid of noise. Consider the following image of a square. We have the following noise pixel. We can reduce this noise by applying a smoothing filter or kernel. Smoothing filters average out the pixels within a neighbourhood. They are sometimes called low pass filters. For mean filtering the kernel simply averages out the kernels in a neighbourhood. Let's see what happens to the pixel intensities. To explore how the kernel effects different areas we can plot the output image and its relationship to specific regions of this input. When we apply the kernel to this area of the image, the intensity values remain unchanged around the edge of the box. The values change as the values of 255 are averaged out with the zeros. Finally, when we get to the region with the noise pixel, we see the noise value is smaller. Comparing the original image and the output image we see the intensity of the noise has reduced but the edges appear less sharp. In general there is a trade off between sharpness and smoothness, in the lab we will review several other kernels that explore this trade off. Edge Detection Edge Detection is an important first step in many computer vision algorithms. Edges in a digital image are where the image brightness changes sharply. Edge detection uses methods to approximate derivatives and gradients for identifying these areas. Consider the following image with a vertical and horizontal edge. Let's take a look at the intensity values. Let\u2019s plot the first row of the image, the horizontal axis is the column index, and the vertical axis is the intensity value. If we move in the right direction from pixel 1 to pixel 2, the intensity value increases. In the image, we can represent this as a vector, in the same manner, when moving from pixel 2 to pixel 3, the intensity value decreases. We can represent this as a vector pointing in the opposite direction. The direction of the vector represents if the adjacent pixel is increasing. We can represent this change by applying the following difference equation, and this works by subtracting the intensity value of the adjacent columns \\(j\\) and \\(j+1\\) in row \\(I\\) . This computes an approximation of the gradient in the x-direction. In the following table, each row applies the equation to the intensity values for column 1 and 2. The final column is the result of using the equation. We can overly the result over the image as vectors. We can apply the same logic to the next column, this time the values are negative. It turns out we can perform similar Horizontal Derivative Approximations using convolution. The Horizontal changes or gradient are computed by convolving the image with a kernel. These kernels are called Sobel operators. We can represent the output in the array of intensities \\(G_x\\) . All the values are zero except the elements corresponding to the horizontal edges. In this image the gray values have different ranges where black is negative, gray is zero, and white is positive. These represent vectors we discussed earlier. We can use the same process to find vertical changes. This is computed by convolving the image with a kernel to get the image \\(G_y\\) . This time the vectors point in the vertical direction. We can combine the output of each filter into a vector. If you are familiar with calculus this approximates the gradient. Taking the magnitude of the vectors we get the intensity values. We can plot it as an image this represents the edges. We can also calculate the angle representing the direction. Mediam Filtering Median Filters are another popular filter. They are better at removing some types of noise but may distort the image. The median filter outputs the median value of the neighbourhood. Consider the yellow pixel. Consider the region in this three by three neighbourhood. The resultant value of the output image is the median value of the 9 pixels. Depending on the padding we see the median is identical to the image in most regions. In the region with the noise sample we see the output is as follows. Overlaying the image values unlike the median filter, we see the noise is no longer there and the edges are straight, but the square is distorted as its missing a pixel. OpenCV Let\u2019s apply some Spatial Operations in open cv. PIL is relatively simple, so check out labs for PIL examples. We will create an image \u201cnew image\u201d that is a noisy version of the original first. We create a kernel for mean filtering. The function filter2D performs 2D convolution between the image and the kernel on each color channel independently. The parameter values are in the lab. We see the new image has less noise, but the image is blurry. Image Sharpening involves smoothing the image and enhancing the edges. We can accomplish image sharpening by applying the following Kernel. We then apply fillters2D, comparing the image before and after we see it is much crisper. We can perform edge detection consider the following image. We smooth the image using GaussianBlur , a low pass filter we will be discussing in the lab; this decreases changes that may be caused by noise that would affect the gradient. We can approximate the derivative in the X or Y direction using the Sobel function . \u201cddepthis\u201d is the output image depth. dx, dy is the order of the derivative in each direction; in this case it will be one. Finally, \u201ck-size\u201d size of the extended Sobel kernel; we can output the gradients as images. We can approximate the magnitude of the gradient. We calculate absolute values, and convert the result to 8-bit using convertScaleAbs Then apply the function \u201daddWeighted\u201d to calculate the sum of two arrays and assign it to grad we plot Grad as an image the areas with high intensity values represent edges. Code Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png PIL # Used to view the images import matplotlib.pyplot as plt # Used to load an image from PIL import Image # Used to create kernels for filtering import numpy as np # This function will plot two images side by side def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1) plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2) plt.title(title_2) plt.show() # Spatial operations use the neighboring pixels to determine the present pixel value Linear Filtering Filtering involves enhancing an image, for example, removing the noise from an image. Noise can be caused by a bad camera or bad image compression. The same factors that cause noise may lead to blurry images. We can apply filters to sharpen these images. Convolution is a standard way to filter an image. The filter is called the kernel and different kernels perform different tasks. In addition, Convolution is used for many of the most advanced artificial intelligence algorithms. We simply take the dot product of the kernel and an equally-sized portion of the image. We then shift the kernel and repeat. # Loads the image from the specified file image = Image.open(\"lenna.png\") # Renders the image plt.figure(figsize=(5,5)) plt.imshow(image) plt.show() # The images we are working with are comprised of RGB values, which are values from 0 to 255. Zero means white noise, this makes the image look grainy: # Get the number of rows and columns in the image rows, cols = image.size # Creates values using a normal distribution with a mean of 0 (loc) and standard deviation (size) of 15, the values are converted to unit8 which means the values are between 0 and 255 noise = np.random.normal(loc=0,scale=15,size=(rows,cols,3)).astype(np.uint8) # Add the noise to the image noisy_image = image + noise # Creates a PIL Image from an array noisy_image = Image.fromarray(noisy_image) # Plots the original image and the image with noise using the function defined at the top plot_image(image, noisy_image, title_1=\"Orignal\", title_2=\"Image Plus Noise\") When adding noise to an image sometimes the value might be greater than 255, in this case 256, is subtracted from the value to wrap the number around keeping it between 0 and 255. For example, consider an image with an RGB value of 137 and we add noise with an RGB value of 215 to get an RGB value of 352. We then subtract 256, the total number of possible values between 0 and 255, to get a number between 0 and 255. Filtering Noise To be able to create customer kernels and use predefined filters we must import the following library from PIL import ImageFilter Smoothing filters average out the Pixels within a neighborhood, they are sometimes called low pass filters. For mean filtering, the kernel simply averages out the kernels in a neighborhood. # Create a kernel which is a 5 by 5 array where each value is 1/36 kernel = np.ones((5,5))/36 # Create a ImageFilter Kernel by providing the kernel size and a flattened kernel kernel_filter = ImageFilter.Kernel((5,5), kernel.flatten()) # The function filter performs a convolution between the image and the kernel on each color channel independently. # Filters the images using the kernel image_filtered = noisy_image.filter(kernel_filter) # We can plot the image before and after the filtering. We see the noise is reduced, but the image is blurry: plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # A smaller kernel keeps the image sharp, but filters less noise, here we try a 3x3 kernel. You can see her shoulders are sharper in this image but the green noise is brighter than the filtered image above. # Create a kernel which is a 3 by 3 array where each value is 1/36 kernel = np.ones((3,3))/36 # Create a ImageFilter Kernel by providing the kernel size and a flattened kernel kernel_filter = ImageFilter.Kernel((3,3), kernel.flatten()) # Filters the images using the kernel image_filtered = noisy_image.filter(kernel_filter) # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") Gaussian Blur To perform Gaussian Blur we use the filter function on an image using the predefined filter ImageFilter.GaussianBlur Parameters: * radius: blur kernel radius, default 2 # Filters the images using GaussianBlur image_filtered = noisy_image.filter(ImageFilter.GaussianBlur) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # Lets try using a 4 by 4 kernel # Filters the images using GaussianBlur on the image with noise using a 4 by 4 kernel image_filtered = noisy_image.filter(ImageFilter.GaussianBlur(4)) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") Image Sharpening Image Sharpening involves smoothing the image and calculating the derivatives. We can accomplish image sharpening by applying the following Kernel. # Common Kernel for image sharpening kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]]) kernel = ImageFilter.Kernel((3,3), kernel.flatten()) # Applys the sharpening filter using kernel on the original image without noise sharpened = image.filter(kernel) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\") # We can also sharpen using a predefined filter # Sharpends image using predefined image filter from PIL sharpened = image.filter(ImageFilter.SHARPEN) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\") Edges Edges are where pixel intensities change. The Gradient of a function outputs the rate of change; we can approximate the gradient of a grayscale image with convolution. Consider the following image: # Loads the image from the specified file img_gray = Image.open('barbara.png') # Renders the image from the array of data, notice how it is 2 diemensional instead of 3 diemensional because it has no color plt.imshow(img_gray ,cmap='gray') plt.show() # Filters the images using FIND_EDGES filter img_gray = img_gray.filter(ImageFilter.FIND_EDGES) # Renders the filtered image plt.figure(figsize=(10,10)) plt.imshow(img_gray ,cmap='gray') plt.show() Median Median filters find the median of all the pixels under the kernel area and the central element is replaced with this median value. We can apply median filters to regular images but let\u2019s see how we can use a median filter to improve segmentation. Consider the cameraman example: # Load the camera man image image = Image.open(\"cameraman.jpeg\") # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show() # Median filtering blurs the background, increasing the segmentation between the cameraman and the background image = image.filter(ImageFilter.MedianFilter) plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show() OpenCV # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png # Used to view the images import matplotlib.pyplot as plt # Used to perform filtering on an image import cv2 # Used to create kernels for filtering import numpy as np def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)) plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)) plt.title(title_2) plt.show() Linear Filtering # Loads the image from the specified file image = cv2.imread(\"lenna.png\") print(image) # Converts the order of the color from BGR (Blue Green Red) to RGB (Red Green Blue) then renders the image from the array of data plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.show() # The images we are working with are comprised of RGB values which are values from 0 to 255. Zero means white noise, this makes the image look grainy: # Get the number of rows and columns in the image rows, cols,_= image.shape # Creates values using a normal distribution with a mean of 0 and standard deviation of 15, the values are converted to unit8 which means the values are between 0 and 255 noise = np.random.normal(0,15,(rows,cols,3)).astype(np.uint8) # Add the noise to the image noisy_image = image + noise # Plots the original image and the image with noise using the function defined at the top plot_image(image, noisy_image, title_1=\"Orignal\",title_2=\"Image Plus Noise\") When adding noise to an image sometimes the value might be greater than 255, in this case, 256, is subtracted from the value to wrap the number around keeping it between 0 and 255. For example, consider an image with an RGB value of 137 and we add noise with an RGB value of 215 we get an RGB value of 352. We then subtract 256, the total number of possible values between 0 and 255, to get a number between 0 and 255. Filtering Noise Smoothing filters average out the Pixels within a neighborhood, they are sometimes called low pass filters. For mean filtering, the kernel simply averages out the kernels in a neighborhood. # Create a kernel which is a 6 by 6 array where each value is 1/36 kernel = np.ones((6,6))/36 The function filter2D performs 2D convolution between the image src and the kernel on each color channel independently. The parameter ddepth has to do with the size of the output image, we will set it to -1 so the input and output are the same size. # Filters the images using the kernel image_filtered = cv2.filter2D(src=noisy_image, ddepth=-1, kernel=kernel) # We can plot the image before and after the filtering; we see the noise is reduced, but the image is blurry: # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # A smaller kernel keeps the image sharp, but filters less noise, here we try a 4x4 kernel # Creates a kernel which is a 4 by 4 array where each value is 1/16 kernel = np.ones((4,4))/16 # Filters the images using the kernel image_filtered=cv2.filter2D(src=noisy_image,ddepth=-1,kernel=kernel) # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered , noisy_image,title_1=\"filtered image\",title_2=\"Image Plus Noise\") Gaussian Blur The function GaussianBlur convolves the source image with the specified Gaussian kernel It filters noise but does a better job of preserving the edges. It has the following parameters: Parameters: src : input image; the image can have any number of channels, which are processed independently. ksize : Gaussian kernel size. sigmaX : Gaussian kernel standard deviation in the X direction. sigmaY : Gaussian kernel standard deviation in the Y direction; if sigmaY is zero, it is set to be equal to sigmaX . # Filters the images using GaussianBlur on the image with noise using a 4 by 4 kernel image_filtered = cv2.GaussianBlur(noisy_image,(5,5),sigmaX=4,sigmaY=4) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # Sigma behaves like the size of the mean filter, a larger value of sigma will make the image blurry, but you are still constrained by the size of the filter, there we set sigma to 10 # Filters the images using GaussianBlur on the image with noise using a 11 by 11 kernel image_filtered = cv2.GaussianBlur(noisy_image,(11,11),sigmaX=10,sigmaY=10) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"filtered image\",title_2=\"Image Plus Noise\") Image Sharpening Image Sharpening involves smoothing the image and calculating the derivatives. We can accomplish image sharpening by applying the following Kernel. # Common Kernel for image sharpening kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]]) # Applys the sharpening filter using kernel on the original image without noise sharpened = cv2.filter2D(image, -1, kernel) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\") Edges Edges are where pixel intensities change. The Gradient of a function outputs the rate of change; we can approximate the gradient of a grayscale image with convolution. There are several methods to approximate the gradient, let\u2019s use the Sobel edge detector. This combines several convolutions and finding the magnitude of the result. Consider the following image: # Loads the image from the specified file img_gray = cv2.imread('barbara.png', cv2.IMREAD_GRAYSCALE) print(img_gray) # Renders the image from the array of data, notice how it is 2 diemensional instead of 3 diemensional because it has no color plt.imshow(img_gray ,cmap='gray') plt.show() # We smooth the image, this decreases changes that may be caused by noise that would affect the gradient. # Filters the images using GaussianBlur on the image with noise using a 3 by 3 kernel img_gray = cv2.GaussianBlur(img_gray,(3,3),sigmaX=0.1,sigmaY=0.1) # Renders the filtered image plt.imshow(img_gray ,cmap='gray') plt.show() We can approximate the derivative in the X or Y direction using the Sobel function, here are the parameters: src : input image ddepth : output image depth, see combinations; in the case of 8-bit input images it will result in truncated derivatives dx : order of the derivative x dx : order of the derivative y ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7 dx = 1 represents the derivative in the x-direction. The function approximates the derivative by convolving the image with the following kernel \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\\\\\ 2 & 0 & -2 \\\\\\\\ 1 & 0 & -1 \\end{bmatrix} \\] # We can apply the function: ddepth = cv2.CV_16S # Applys the filter on the image in the X direction grad_x = cv2.Sobel(src=img_gray, ddepth=ddepth, dx=1, dy=0, ksize=3) # We can plot the result plt.imshow(grad_x,cmap='gray') plt.show() dy=1 represents the derivative in the y-direction. The function approximates the derivative by convolving the image with the following kernel $$ \\begin{bmatrix} \\ \\ 1 & \\ \\ 2 & \\ \\ 1 \\\\ \\ \\ 0 & \\ \\ 0 & \\ \\ 0 \\\\ -1 & -2 & -1 \\end{bmatrix} $$ # We can apply the function and plot the result # Applys the filter on the image in the X direction grad_y = cv2.Sobel(src=img_gray, ddepth=ddepth, dx=0, dy=1, ksize=3) plt.imshow(grad_y,cmap='gray') plt.show() # We can approximate the gradient by calculating absolute values, and converts the result to 8-bit: # Converts the values back to a number between 0 and 255 abs_grad_x = cv2.convertScaleAbs(grad_x) abs_grad_y = cv2.convertScaleAbs(grad_y) Then apply the function addWeighted to calculates the sum of two arrays as follows: # Adds the derivative in the X and Y direction grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0) # We then plot the results, we see the image with lines have high-intensity values representing a large gradient # Make the figure bigger and renders the image plt.figure(figsize=(10,10)) plt.imshow(grad,cmap='gray') plt.show() Median Median filters find the median of all the pixels under the kernel area and the central element is replaced with this median value. We can apply median filters to regular images but let\u2019s see how we can use a median filter to improve segmentation. Consider the cameraman example # Load the camera man image image = cv2.imread(\"cameraman.jpeg\",cv2.IMREAD_GRAYSCALE) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show() Now let's apply a Median Filter by using the medianBlur function. The parameters for this function are src : The image and ksize : Kernel size # Filter the image using Median Blur with a kernel of size 5 filtered_image = cv2.medianBlur(image, 5) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(filtered_image,cmap=\"gray\") plt.show() We would like to find the cameraman, but median filtering captures some of the background. Threshold Function Parameters src : The image to use thresh : The threshold maxval : The maxval to use type : Type of filtering The threshold function works by looking at each pixel's grayscale value and assigning a value if it is below the threshold and another value if it is above the threshold. In our example the threshold is 0 (black) and the type is binary inverse so if a value is above the threshold the assigned value is 0 (black) and if it is below or equals the threshold the maxval 255 (white) is used. So if the pixel is 0 black it is assigned 255 (white) and if the pixel is not black then it is assigned black which is what THRESH_BINARY_INV tells OpenCV to do. This is how it would work without THRESH_OTSU. Since we are using THRESH_OTSU it means that OpenCV will decide an optimal threshold. In our example below the threshold, we provide does not get used in the filter OpenCV will use an optimal one. # Returns ret which is the threshold used and outs which is the image ret, outs = cv2.threshold(src = image, thresh = 0, maxval = 255, type = cv2.THRESH_OTSU+cv2.THRESH_BINARY_INV) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Render the image plt.imshow(outs, cmap='gray') plt.show()","title":"7. Spatial Operations In Image Processing"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#spatial-operations-in-image-processing","text":"In this video we will discuss Spatial Operations in Image Processing. In this video we will review: Convolution sometimes called Linear Filtering Edge Detection Median Filters In this video we will show the one channel representation, but unless specified we can apply these operations to each channel independently.","title":"Spatial Operations In Image Processing"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#convolution","text":"Spatial Operations consists of a neighbourhood, in this example we take a neighbourhood of 2 by 2 pixels we apply a function that involves each pixel in the neighbourhood and output the result we then shift the neighbourhood and repeat the process for each pixel in the image. The result is a new image that has enhanced characteristics, these spatial operations take advantage is spatial patterns in the image. Convolution or linear filtering is a standard way to Filter an image. The filter is called the kernel. The different kernels perform different tasks. Convolution is analogous to this linear equation. The input is the image X . Output will be another image Z . We have the parameter W this is known as a kernel or filter. \\(z = wx\\) \\(Z = W * Xx\\) The star this operation is the convolution operation. There is actually an analytical expression for it, but it's a little confusing. So let's just go over an example to understand what's going on. Here's our image X, here's our kernel w, it\u2019s just a smaller array. The array is made up of values that have been pre determined for some operation. Let's go over the operation of convolution. We start off in the top right corner of the image, and we'll overlay the kernel with that region of the image. We see the kernel values in red, we'll multiply every element of the image by the corresponding element of the kernel. For the first row we'll get the following operation, multiplying the intensity value and summing the results this process is repeated for the second row, multiplying the intensity value and summing the results. Finally for the final row we multiply the intensity values and sum the results. We sum those elements together. The result is the first element of the image z. We shift the kernel to the left represented by the different colors in red. We multiply all the elements of the kernel with the image. This gives us the second element of the output image z. We'll shift the kernel one more column and perform the exact same operation, multiplying every element of the kernel by the corresponding intensity value of the image, adding them all together. This gives us the next value. We then shift the kernel down and repeat the process until we get to the end of the column. Repeating the process until we get a new image. One problem is the images are different sizes. We can fix this by changing the size of the image x. We change the size of the image by padding. In zero padding we add two additional rows of zeros and two additional columns of zeros. We can also replicate the values of the border.","title":"Convolution"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#low-pass-filters","text":"Low Pass Filters are used to smooth the image getting rid of noise. Consider the following image of a square. We have the following noise pixel. We can reduce this noise by applying a smoothing filter or kernel. Smoothing filters average out the pixels within a neighbourhood. They are sometimes called low pass filters. For mean filtering the kernel simply averages out the kernels in a neighbourhood. Let's see what happens to the pixel intensities. To explore how the kernel effects different areas we can plot the output image and its relationship to specific regions of this input. When we apply the kernel to this area of the image, the intensity values remain unchanged around the edge of the box. The values change as the values of 255 are averaged out with the zeros. Finally, when we get to the region with the noise pixel, we see the noise value is smaller. Comparing the original image and the output image we see the intensity of the noise has reduced but the edges appear less sharp. In general there is a trade off between sharpness and smoothness, in the lab we will review several other kernels that explore this trade off.","title":"Low Pass Filters"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#edge-detection","text":"Edge Detection is an important first step in many computer vision algorithms. Edges in a digital image are where the image brightness changes sharply. Edge detection uses methods to approximate derivatives and gradients for identifying these areas. Consider the following image with a vertical and horizontal edge. Let's take a look at the intensity values. Let\u2019s plot the first row of the image, the horizontal axis is the column index, and the vertical axis is the intensity value. If we move in the right direction from pixel 1 to pixel 2, the intensity value increases. In the image, we can represent this as a vector, in the same manner, when moving from pixel 2 to pixel 3, the intensity value decreases. We can represent this as a vector pointing in the opposite direction. The direction of the vector represents if the adjacent pixel is increasing. We can represent this change by applying the following difference equation, and this works by subtracting the intensity value of the adjacent columns \\(j\\) and \\(j+1\\) in row \\(I\\) . This computes an approximation of the gradient in the x-direction. In the following table, each row applies the equation to the intensity values for column 1 and 2. The final column is the result of using the equation. We can overly the result over the image as vectors. We can apply the same logic to the next column, this time the values are negative. It turns out we can perform similar Horizontal Derivative Approximations using convolution. The Horizontal changes or gradient are computed by convolving the image with a kernel. These kernels are called Sobel operators. We can represent the output in the array of intensities \\(G_x\\) . All the values are zero except the elements corresponding to the horizontal edges. In this image the gray values have different ranges where black is negative, gray is zero, and white is positive. These represent vectors we discussed earlier. We can use the same process to find vertical changes. This is computed by convolving the image with a kernel to get the image \\(G_y\\) . This time the vectors point in the vertical direction. We can combine the output of each filter into a vector. If you are familiar with calculus this approximates the gradient. Taking the magnitude of the vectors we get the intensity values. We can plot it as an image this represents the edges. We can also calculate the angle representing the direction.","title":"Edge Detection"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#mediam-filtering","text":"Median Filters are another popular filter. They are better at removing some types of noise but may distort the image. The median filter outputs the median value of the neighbourhood. Consider the yellow pixel. Consider the region in this three by three neighbourhood. The resultant value of the output image is the median value of the 9 pixels. Depending on the padding we see the median is identical to the image in most regions. In the region with the noise sample we see the output is as follows. Overlaying the image values unlike the median filter, we see the noise is no longer there and the edges are straight, but the square is distorted as its missing a pixel.","title":"Mediam Filtering"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#opencv","text":"Let\u2019s apply some Spatial Operations in open cv. PIL is relatively simple, so check out labs for PIL examples. We will create an image \u201cnew image\u201d that is a noisy version of the original first. We create a kernel for mean filtering. The function filter2D performs 2D convolution between the image and the kernel on each color channel independently. The parameter values are in the lab. We see the new image has less noise, but the image is blurry. Image Sharpening involves smoothing the image and enhancing the edges. We can accomplish image sharpening by applying the following Kernel. We then apply fillters2D, comparing the image before and after we see it is much crisper. We can perform edge detection consider the following image. We smooth the image using GaussianBlur , a low pass filter we will be discussing in the lab; this decreases changes that may be caused by noise that would affect the gradient. We can approximate the derivative in the X or Y direction using the Sobel function . \u201cddepthis\u201d is the output image depth. dx, dy is the order of the derivative in each direction; in this case it will be one. Finally, \u201ck-size\u201d size of the extended Sobel kernel; we can output the gradients as images. We can approximate the magnitude of the gradient. We calculate absolute values, and convert the result to 8-bit using convertScaleAbs Then apply the function \u201daddWeighted\u201d to calculate the sum of two arrays and assign it to grad we plot Grad as an image the areas with high intensity values represent edges.","title":"OpenCV"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#code-lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png","title":"Code Lab"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#pil","text":"# Used to view the images import matplotlib.pyplot as plt # Used to load an image from PIL import Image # Used to create kernels for filtering import numpy as np # This function will plot two images side by side def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(image_1) plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(image_2) plt.title(title_2) plt.show() # Spatial operations use the neighboring pixels to determine the present pixel value","title":"PIL"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#linear-filtering","text":"Filtering involves enhancing an image, for example, removing the noise from an image. Noise can be caused by a bad camera or bad image compression. The same factors that cause noise may lead to blurry images. We can apply filters to sharpen these images. Convolution is a standard way to filter an image. The filter is called the kernel and different kernels perform different tasks. In addition, Convolution is used for many of the most advanced artificial intelligence algorithms. We simply take the dot product of the kernel and an equally-sized portion of the image. We then shift the kernel and repeat. # Loads the image from the specified file image = Image.open(\"lenna.png\") # Renders the image plt.figure(figsize=(5,5)) plt.imshow(image) plt.show() # The images we are working with are comprised of RGB values, which are values from 0 to 255. Zero means white noise, this makes the image look grainy: # Get the number of rows and columns in the image rows, cols = image.size # Creates values using a normal distribution with a mean of 0 (loc) and standard deviation (size) of 15, the values are converted to unit8 which means the values are between 0 and 255 noise = np.random.normal(loc=0,scale=15,size=(rows,cols,3)).astype(np.uint8) # Add the noise to the image noisy_image = image + noise # Creates a PIL Image from an array noisy_image = Image.fromarray(noisy_image) # Plots the original image and the image with noise using the function defined at the top plot_image(image, noisy_image, title_1=\"Orignal\", title_2=\"Image Plus Noise\") When adding noise to an image sometimes the value might be greater than 255, in this case 256, is subtracted from the value to wrap the number around keeping it between 0 and 255. For example, consider an image with an RGB value of 137 and we add noise with an RGB value of 215 to get an RGB value of 352. We then subtract 256, the total number of possible values between 0 and 255, to get a number between 0 and 255.","title":"Linear Filtering"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#filtering-noise","text":"To be able to create customer kernels and use predefined filters we must import the following library from PIL import ImageFilter Smoothing filters average out the Pixels within a neighborhood, they are sometimes called low pass filters. For mean filtering, the kernel simply averages out the kernels in a neighborhood. # Create a kernel which is a 5 by 5 array where each value is 1/36 kernel = np.ones((5,5))/36 # Create a ImageFilter Kernel by providing the kernel size and a flattened kernel kernel_filter = ImageFilter.Kernel((5,5), kernel.flatten()) # The function filter performs a convolution between the image and the kernel on each color channel independently. # Filters the images using the kernel image_filtered = noisy_image.filter(kernel_filter) # We can plot the image before and after the filtering. We see the noise is reduced, but the image is blurry: plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # A smaller kernel keeps the image sharp, but filters less noise, here we try a 3x3 kernel. You can see her shoulders are sharper in this image but the green noise is brighter than the filtered image above. # Create a kernel which is a 3 by 3 array where each value is 1/36 kernel = np.ones((3,3))/36 # Create a ImageFilter Kernel by providing the kernel size and a flattened kernel kernel_filter = ImageFilter.Kernel((3,3), kernel.flatten()) # Filters the images using the kernel image_filtered = noisy_image.filter(kernel_filter) # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\")","title":"Filtering Noise"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#gaussian-blur","text":"To perform Gaussian Blur we use the filter function on an image using the predefined filter ImageFilter.GaussianBlur Parameters: * radius: blur kernel radius, default 2 # Filters the images using GaussianBlur image_filtered = noisy_image.filter(ImageFilter.GaussianBlur) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # Lets try using a 4 by 4 kernel # Filters the images using GaussianBlur on the image with noise using a 4 by 4 kernel image_filtered = noisy_image.filter(ImageFilter.GaussianBlur(4)) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\")","title":"Gaussian Blur"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#image-sharpening","text":"Image Sharpening involves smoothing the image and calculating the derivatives. We can accomplish image sharpening by applying the following Kernel. # Common Kernel for image sharpening kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]]) kernel = ImageFilter.Kernel((3,3), kernel.flatten()) # Applys the sharpening filter using kernel on the original image without noise sharpened = image.filter(kernel) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\") # We can also sharpen using a predefined filter # Sharpends image using predefined image filter from PIL sharpened = image.filter(ImageFilter.SHARPEN) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\")","title":"Image Sharpening"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#edges","text":"Edges are where pixel intensities change. The Gradient of a function outputs the rate of change; we can approximate the gradient of a grayscale image with convolution. Consider the following image: # Loads the image from the specified file img_gray = Image.open('barbara.png') # Renders the image from the array of data, notice how it is 2 diemensional instead of 3 diemensional because it has no color plt.imshow(img_gray ,cmap='gray') plt.show() # Filters the images using FIND_EDGES filter img_gray = img_gray.filter(ImageFilter.FIND_EDGES) # Renders the filtered image plt.figure(figsize=(10,10)) plt.imshow(img_gray ,cmap='gray') plt.show()","title":"Edges"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#median","text":"Median filters find the median of all the pixels under the kernel area and the central element is replaced with this median value. We can apply median filters to regular images but let\u2019s see how we can use a median filter to improve segmentation. Consider the cameraman example: # Load the camera man image image = Image.open(\"cameraman.jpeg\") # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show() # Median filtering blurs the background, increasing the segmentation between the cameraman and the background image = image.filter(ImageFilter.MedianFilter) plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show()","title":"Median"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#opencv_1","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/cameraman.jpeg wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png # Used to view the images import matplotlib.pyplot as plt # Used to perform filtering on an image import cv2 # Used to create kernels for filtering import numpy as np def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"): plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)) plt.title(title_1) plt.subplot(1, 2, 2) plt.imshow(cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)) plt.title(title_2) plt.show()","title":"OpenCV"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#linear-filtering_1","text":"# Loads the image from the specified file image = cv2.imread(\"lenna.png\") print(image) # Converts the order of the color from BGR (Blue Green Red) to RGB (Red Green Blue) then renders the image from the array of data plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.show() # The images we are working with are comprised of RGB values which are values from 0 to 255. Zero means white noise, this makes the image look grainy: # Get the number of rows and columns in the image rows, cols,_= image.shape # Creates values using a normal distribution with a mean of 0 and standard deviation of 15, the values are converted to unit8 which means the values are between 0 and 255 noise = np.random.normal(0,15,(rows,cols,3)).astype(np.uint8) # Add the noise to the image noisy_image = image + noise # Plots the original image and the image with noise using the function defined at the top plot_image(image, noisy_image, title_1=\"Orignal\",title_2=\"Image Plus Noise\") When adding noise to an image sometimes the value might be greater than 255, in this case, 256, is subtracted from the value to wrap the number around keeping it between 0 and 255. For example, consider an image with an RGB value of 137 and we add noise with an RGB value of 215 we get an RGB value of 352. We then subtract 256, the total number of possible values between 0 and 255, to get a number between 0 and 255.","title":"Linear Filtering"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#filtering-noise_1","text":"Smoothing filters average out the Pixels within a neighborhood, they are sometimes called low pass filters. For mean filtering, the kernel simply averages out the kernels in a neighborhood. # Create a kernel which is a 6 by 6 array where each value is 1/36 kernel = np.ones((6,6))/36 The function filter2D performs 2D convolution between the image src and the kernel on each color channel independently. The parameter ddepth has to do with the size of the output image, we will set it to -1 so the input and output are the same size. # Filters the images using the kernel image_filtered = cv2.filter2D(src=noisy_image, ddepth=-1, kernel=kernel) # We can plot the image before and after the filtering; we see the noise is reduced, but the image is blurry: # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered, noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # A smaller kernel keeps the image sharp, but filters less noise, here we try a 4x4 kernel # Creates a kernel which is a 4 by 4 array where each value is 1/16 kernel = np.ones((4,4))/16 # Filters the images using the kernel image_filtered=cv2.filter2D(src=noisy_image,ddepth=-1,kernel=kernel) # Plots the Filtered and Image with Noise using the function defined at the top plot_image(image_filtered , noisy_image,title_1=\"filtered image\",title_2=\"Image Plus Noise\")","title":"Filtering Noise"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#gaussian-blur_1","text":"The function GaussianBlur convolves the source image with the specified Gaussian kernel It filters noise but does a better job of preserving the edges. It has the following parameters: Parameters: src : input image; the image can have any number of channels, which are processed independently. ksize : Gaussian kernel size. sigmaX : Gaussian kernel standard deviation in the X direction. sigmaY : Gaussian kernel standard deviation in the Y direction; if sigmaY is zero, it is set to be equal to sigmaX . # Filters the images using GaussianBlur on the image with noise using a 4 by 4 kernel image_filtered = cv2.GaussianBlur(noisy_image,(5,5),sigmaX=4,sigmaY=4) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"Filtered image\",title_2=\"Image Plus Noise\") # Sigma behaves like the size of the mean filter, a larger value of sigma will make the image blurry, but you are still constrained by the size of the filter, there we set sigma to 10 # Filters the images using GaussianBlur on the image with noise using a 11 by 11 kernel image_filtered = cv2.GaussianBlur(noisy_image,(11,11),sigmaX=10,sigmaY=10) # Plots the Filtered Image then the Unfiltered Image with Noise plot_image(image_filtered , noisy_image,title_1=\"filtered image\",title_2=\"Image Plus Noise\")","title":"Gaussian Blur"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#image-sharpening_1","text":"Image Sharpening involves smoothing the image and calculating the derivatives. We can accomplish image sharpening by applying the following Kernel. # Common Kernel for image sharpening kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]]) # Applys the sharpening filter using kernel on the original image without noise sharpened = cv2.filter2D(image, -1, kernel) # Plots the sharpened image and the original image without noise plot_image(sharpened , image, title_1=\"Sharpened image\",title_2=\"Image\")","title":"Image Sharpening"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#edges_1","text":"Edges are where pixel intensities change. The Gradient of a function outputs the rate of change; we can approximate the gradient of a grayscale image with convolution. There are several methods to approximate the gradient, let\u2019s use the Sobel edge detector. This combines several convolutions and finding the magnitude of the result. Consider the following image: # Loads the image from the specified file img_gray = cv2.imread('barbara.png', cv2.IMREAD_GRAYSCALE) print(img_gray) # Renders the image from the array of data, notice how it is 2 diemensional instead of 3 diemensional because it has no color plt.imshow(img_gray ,cmap='gray') plt.show() # We smooth the image, this decreases changes that may be caused by noise that would affect the gradient. # Filters the images using GaussianBlur on the image with noise using a 3 by 3 kernel img_gray = cv2.GaussianBlur(img_gray,(3,3),sigmaX=0.1,sigmaY=0.1) # Renders the filtered image plt.imshow(img_gray ,cmap='gray') plt.show() We can approximate the derivative in the X or Y direction using the Sobel function, here are the parameters: src : input image ddepth : output image depth, see combinations; in the case of 8-bit input images it will result in truncated derivatives dx : order of the derivative x dx : order of the derivative y ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7 dx = 1 represents the derivative in the x-direction. The function approximates the derivative by convolving the image with the following kernel \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\\\\\ 2 & 0 & -2 \\\\\\\\ 1 & 0 & -1 \\end{bmatrix} \\] # We can apply the function: ddepth = cv2.CV_16S # Applys the filter on the image in the X direction grad_x = cv2.Sobel(src=img_gray, ddepth=ddepth, dx=1, dy=0, ksize=3) # We can plot the result plt.imshow(grad_x,cmap='gray') plt.show() dy=1 represents the derivative in the y-direction. The function approximates the derivative by convolving the image with the following kernel $$ \\begin{bmatrix} \\ \\ 1 & \\ \\ 2 & \\ \\ 1 \\\\ \\ \\ 0 & \\ \\ 0 & \\ \\ 0 \\\\ -1 & -2 & -1 \\end{bmatrix} $$ # We can apply the function and plot the result # Applys the filter on the image in the X direction grad_y = cv2.Sobel(src=img_gray, ddepth=ddepth, dx=0, dy=1, ksize=3) plt.imshow(grad_y,cmap='gray') plt.show() # We can approximate the gradient by calculating absolute values, and converts the result to 8-bit: # Converts the values back to a number between 0 and 255 abs_grad_x = cv2.convertScaleAbs(grad_x) abs_grad_y = cv2.convertScaleAbs(grad_y) Then apply the function addWeighted to calculates the sum of two arrays as follows: # Adds the derivative in the X and Y direction grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0) # We then plot the results, we see the image with lines have high-intensity values representing a large gradient # Make the figure bigger and renders the image plt.figure(figsize=(10,10)) plt.imshow(grad,cmap='gray') plt.show()","title":"Edges"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#median_1","text":"Median filters find the median of all the pixels under the kernel area and the central element is replaced with this median value. We can apply median filters to regular images but let\u2019s see how we can use a median filter to improve segmentation. Consider the cameraman example # Load the camera man image image = cv2.imread(\"cameraman.jpeg\",cv2.IMREAD_GRAYSCALE) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(image,cmap=\"gray\") plt.show() Now let's apply a Median Filter by using the medianBlur function. The parameters for this function are src : The image and ksize : Kernel size # Filter the image using Median Blur with a kernel of size 5 filtered_image = cv2.medianBlur(image, 5) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Renders the image plt.imshow(filtered_image,cmap=\"gray\") plt.show() We would like to find the cameraman, but median filtering captures some of the background.","title":"Median"},{"location":"notes/data_science/Coursera/computer_vision/Spatial_Operations_In_Image_Processing/#threshold-function-parameters","text":"src : The image to use thresh : The threshold maxval : The maxval to use type : Type of filtering The threshold function works by looking at each pixel's grayscale value and assigning a value if it is below the threshold and another value if it is above the threshold. In our example the threshold is 0 (black) and the type is binary inverse so if a value is above the threshold the assigned value is 0 (black) and if it is below or equals the threshold the maxval 255 (white) is used. So if the pixel is 0 black it is assigned 255 (white) and if the pixel is not black then it is assigned black which is what THRESH_BINARY_INV tells OpenCV to do. This is how it would work without THRESH_OTSU. Since we are using THRESH_OTSU it means that OpenCV will decide an optimal threshold. In our example below the threshold, we provide does not get used in the filter OpenCV will use an optimal one. # Returns ret which is the threshold used and outs which is the image ret, outs = cv2.threshold(src = image, thresh = 0, maxval = 255, type = cv2.THRESH_OTSU+cv2.THRESH_BINARY_INV) # Make the image larger when it renders plt.figure(figsize=(10,10)) # Render the image plt.imshow(outs, cmap='gray') plt.show()","title":"Threshold Function Parameters"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/","text":"Support Vector Machines Topics Kernels Maximum Margin Kernels Sometimes data isn't linearly separable: We can transform it to separate it: Sometimes it is hard to calculate mapping: We use a short cut called a kernel, there are different types, such as: - Linear , - Polynomial , - Radial basis function (or RBF) , The RBF is most widely used. Each of these functions has its own characteristics, its pros and cons. The RBF kernel finds the difference between two inputs X and X\u2019 that is called a support vector. The RBF kernel has the parameter Gamma, lets see how we select Gamma Consider the following dataset of cats and dogs, anything in this region is a dog, anything in this region is a cat, anything in this region is a dog, anything in this region is a cat Therefore any sample in the red or blue region should be classified accordingly. Unfortunately you can\u2019t find a plane that separates the data. Here we use a plane to separate a similar dataset, it does not separate the data, lets try the RBF kernel. Using a value of gamma of 0.01 increases the flexibility of the classifier. Using a value of gamma of 0.1 seems to be classifying more points. The value of 0.5 classified almost all the points correctly but it does not seem to match the regions in the original slide. This is known as overfitting, where the classifier fits the data points not the actual pattern, higher gamma the more likely we will over fit, let's clarify this with an example. The following images of cats look like dogs, they could be mislabeled or the photo could be taken at a bad angle or the cat could just look like a dog. As a result the image points will appear in the incorrect region. Fitting the model with a high value of gamma we get the following results, the performs almost perfect on the training points We can represent the classifier with the following decision region, where every point is classified by the color accordingly this does not match our Decision regions; this is called overfitting where we do well on the training samples but we may do poorly when we encounter new Data. To avoid this we find the best value of gamma by using validation data. We split the data into training and validation sets. We use the validation samples to find the Hyperparameters. We test the model for a gamma of 0.5. We get the following misclassified samples. We see a value for gamma of 0.1 performs better. As a result, we use a value of 0.1. In practice we try several different values of Gamma and select the value that does the best on the validation data. Maximum Margin SVM\u2019s work by finding the Maximum Margin. Witch of the three planes do you think perform better in classifying the data? Intuitively we choose green and even with some noise, would do well. How to find best line? Maximize margine between line and data points with closest ones being support vectors . We can ignore other data samples when we have the support vectors. Hyperplane and boundary decision lines have their own equations. The math is involved. That said, the hyperplane is learned from training data using an optimization procedure that maximizes the margin; and like many other problems, this optimization problem can also be solved by gradient descent. When the classes are not separable the Soft Margin SVM can be used: This is usually controlled by the regularization parameter. This allows some samples to be misclassified We select gamma and the regulation parameter C by using the values that do best on the validation data. Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, svm, metrics, model_selection from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import confusion_matrix import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score # Load Important Libraries and Digit Dataset digits = datasets.load_digits() target = digits.target flatten_digits = digits.images.reshape((len(digits.images), -1)) # (1797, 64) # Visualize Some Handwritten Images in the Dataset _, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 4)) for ax, image, label in zip(axes, digits.images, target): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') ax.set_title('%i' % label) plt.show() # Divide Images into Training and Test Set X_train, X_test, y_train, y_test = train_test_split(flatten_digits, target, test_size=0.2) # Hand-written classification with Logistic Regression ## Standardize the dataset to put all the features of the variables on the same scale scaler = StandardScaler() X_train_logistic = scaler.fit_transform(X_train) X_test_logistic = scaler.transform(X_test) Logistic Regression Create the logistic regression and fit the logistic regression and use the l1 penalty. Note here that since this is a multiclass problem the Logistic Regression parameter multi_class is set to multinomial . logit = LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1, multi_class='multinomial') logit.fit(X_train_logistic, y_train) y_pred_logistic = logit.predict(X_test_logistic) # Get the accuracy of the logistic regression print(\"Accuracy: \"+str(logit.score(X_test_logistic, y_test))) # Lets plot out the confusion matrix, each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class. label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] cmx = confusion_matrix(y_test, y_pred_logistic, labels=label_names) Accuracy is fine and above 80% but we can see some heavily misclassified values, The classifier had a hard time classifying 8 . df_cm = pd.DataFrame(cmx) # plt.figure(figsize=(10,7)) sns.set(font_scale=1.4) # for label size sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size title = \"Confusion Matrix for Logistic Regression results\" plt.title(title) plt.show() SVM svm_classifier = svm.SVC(gamma='scale') svm_classifier.fit(X_train, y_train) y_pred_svm = svm_classifier.predict(X_test) print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred_svm))) label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] cmx = confusion_matrix(y_test, y_pred_svm, labels=label_names) df_cm = pd.DataFrame(cmx) # plt.figure(figsize=(10,7)) sns.set(font_scale=1.4) # for label size sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size title = \"Confusion Matrix for SVM results\" plt.title(title) plt.show() Comparing both SVM and Logistic Regression with K-Fold Cross Validation k-fold Cross validation is used when there are limited samples , the handwritten dataset contains about 1800 samples, this will give an opportunity for all the data to be in the training and test set at different given times. We will add l2 regularization to visualize how well they both do against SVM. This is a comparing SVM to two different logistic regressions \"using\" k-fold cross validation algorithm = [] algorithm.append(('SVM', svm_classifier)) algorithm.append(('Logistic_L1', logit)) algorithm.append(('Logistic_L2', LogisticRegression(C=0.01, penalty='l2', solver='saga', tol=0.1, multi_class='multinomial'))) results = [] names = [] y = digits.target for name, algo in algorithm: k_fold = model_selection.KFold(n_splits=10) #, random_state=10) # Got an error - BB if name == 'SVM': X = flatten_digits cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy') else: scaler = StandardScaler() X = scaler.fit_transform(flatten_digits) cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy') results.append(cv_results) names.append(name) fig = plt.figure() fig.suptitle('Compare Logistic and SVM results') ax = fig.add_subplot() plt.boxplot(results) plt.ylabel('Accuracy') ax.set_xticklabels(names) plt.show()","title":"11. Support Vector Machines"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#support-vector-machines","text":"","title":"Support Vector Machines"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#topics","text":"Kernels Maximum Margin","title":"Topics"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#kernels","text":"Sometimes data isn't linearly separable: We can transform it to separate it: Sometimes it is hard to calculate mapping: We use a short cut called a kernel, there are different types, such as: - Linear , - Polynomial , - Radial basis function (or RBF) , The RBF is most widely used. Each of these functions has its own characteristics, its pros and cons. The RBF kernel finds the difference between two inputs X and X\u2019 that is called a support vector. The RBF kernel has the parameter Gamma, lets see how we select Gamma Consider the following dataset of cats and dogs, anything in this region is a dog, anything in this region is a cat, anything in this region is a dog, anything in this region is a cat Therefore any sample in the red or blue region should be classified accordingly. Unfortunately you can\u2019t find a plane that separates the data. Here we use a plane to separate a similar dataset, it does not separate the data, lets try the RBF kernel. Using a value of gamma of 0.01 increases the flexibility of the classifier. Using a value of gamma of 0.1 seems to be classifying more points. The value of 0.5 classified almost all the points correctly but it does not seem to match the regions in the original slide. This is known as overfitting, where the classifier fits the data points not the actual pattern, higher gamma the more likely we will over fit, let's clarify this with an example. The following images of cats look like dogs, they could be mislabeled or the photo could be taken at a bad angle or the cat could just look like a dog. As a result the image points will appear in the incorrect region. Fitting the model with a high value of gamma we get the following results, the performs almost perfect on the training points We can represent the classifier with the following decision region, where every point is classified by the color accordingly this does not match our Decision regions; this is called overfitting where we do well on the training samples but we may do poorly when we encounter new Data. To avoid this we find the best value of gamma by using validation data. We split the data into training and validation sets. We use the validation samples to find the Hyperparameters. We test the model for a gamma of 0.5. We get the following misclassified samples. We see a value for gamma of 0.1 performs better. As a result, we use a value of 0.1. In practice we try several different values of Gamma and select the value that does the best on the validation data.","title":"Kernels"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#maximum-margin","text":"SVM\u2019s work by finding the Maximum Margin. Witch of the three planes do you think perform better in classifying the data? Intuitively we choose green and even with some noise, would do well. How to find best line? Maximize margine between line and data points with closest ones being support vectors . We can ignore other data samples when we have the support vectors. Hyperplane and boundary decision lines have their own equations. The math is involved. That said, the hyperplane is learned from training data using an optimization procedure that maximizes the margin; and like many other problems, this optimization problem can also be solved by gradient descent. When the classes are not separable the Soft Margin SVM can be used: This is usually controlled by the regularization parameter. This allows some samples to be misclassified We select gamma and the regulation parameter C by using the values that do best on the validation data.","title":"Maximum Margin"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, svm, metrics, model_selection from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import confusion_matrix import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score # Load Important Libraries and Digit Dataset digits = datasets.load_digits() target = digits.target flatten_digits = digits.images.reshape((len(digits.images), -1)) # (1797, 64) # Visualize Some Handwritten Images in the Dataset _, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 4)) for ax, image, label in zip(axes, digits.images, target): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') ax.set_title('%i' % label) plt.show() # Divide Images into Training and Test Set X_train, X_test, y_train, y_test = train_test_split(flatten_digits, target, test_size=0.2) # Hand-written classification with Logistic Regression ## Standardize the dataset to put all the features of the variables on the same scale scaler = StandardScaler() X_train_logistic = scaler.fit_transform(X_train) X_test_logistic = scaler.transform(X_test)","title":"Lab"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#logistic-regression","text":"Create the logistic regression and fit the logistic regression and use the l1 penalty. Note here that since this is a multiclass problem the Logistic Regression parameter multi_class is set to multinomial . logit = LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1, multi_class='multinomial') logit.fit(X_train_logistic, y_train) y_pred_logistic = logit.predict(X_test_logistic) # Get the accuracy of the logistic regression print(\"Accuracy: \"+str(logit.score(X_test_logistic, y_test))) # Lets plot out the confusion matrix, each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class. label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] cmx = confusion_matrix(y_test, y_pred_logistic, labels=label_names) Accuracy is fine and above 80% but we can see some heavily misclassified values, The classifier had a hard time classifying 8 . df_cm = pd.DataFrame(cmx) # plt.figure(figsize=(10,7)) sns.set(font_scale=1.4) # for label size sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size title = \"Confusion Matrix for Logistic Regression results\" plt.title(title) plt.show()","title":"Logistic Regression"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#svm","text":"svm_classifier = svm.SVC(gamma='scale') svm_classifier.fit(X_train, y_train) y_pred_svm = svm_classifier.predict(X_test) print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred_svm))) label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] cmx = confusion_matrix(y_test, y_pred_svm, labels=label_names) df_cm = pd.DataFrame(cmx) # plt.figure(figsize=(10,7)) sns.set(font_scale=1.4) # for label size sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size title = \"Confusion Matrix for SVM results\" plt.title(title) plt.show()","title":"SVM"},{"location":"notes/data_science/Coursera/computer_vision/Support_Vector_Machines/#comparing-both-svm-and-logistic-regression-with-k-fold-cross-validation","text":"k-fold Cross validation is used when there are limited samples , the handwritten dataset contains about 1800 samples, this will give an opportunity for all the data to be in the training and test set at different given times. We will add l2 regularization to visualize how well they both do against SVM. This is a comparing SVM to two different logistic regressions \"using\" k-fold cross validation algorithm = [] algorithm.append(('SVM', svm_classifier)) algorithm.append(('Logistic_L1', logit)) algorithm.append(('Logistic_L2', LogisticRegression(C=0.01, penalty='l2', solver='saga', tol=0.1, multi_class='multinomial'))) results = [] names = [] y = digits.target for name, algo in algorithm: k_fold = model_selection.KFold(n_splits=10) #, random_state=10) # Got an error - BB if name == 'SVM': X = flatten_digits cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy') else: scaler = StandardScaler() X = scaler.fit_transform(flatten_digits) cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy') results.append(cv_results) names.append(name) fig = plt.figure() fig.suptitle('Compare Logistic and SVM results') ax = fig.add_subplot() plt.boxplot(results) plt.ylabel('Accuracy') ax.set_xticklabels(names) plt.show()","title":"Comparing both SVM and Logistic Regression with K-Fold Cross Validation"},{"location":"notes/data_science/Coursera/computer_vision/What_Is_A_Digital_Image/","text":"What is a Digital Image A rectangular array of numbers. In the real world, an image can take on an almost unlimited number of values, but digital images have intensity values between zero (black) and 255 (white). It turns out that's all we need, 256 different intensity values to represent an image. If we use less than 256 values, things will look cartoonish. RGB: Image Masks: Basic Image Types: An image is a file on your computer. Two popular image formats, Joint Photographic Expert Group image or JPEG, and Portable Network Graphics or PNG, these formats reduce file size and have other features. No matter what Python library you use, you're going to have to load the image PIL Intro ImageOps: Quantizing: Numpy Intro OpenCV Intro OpenCV is a library used for computer vision. It has more functionality than the PIL library, but is more difficult to use. We can plot the image using Imshow, but the colors appear off. This is because the order of each channel is different in OpenCV unlike PIL that is RGB. OpenCV is BGR. This is the main difference between the arrays and PIL versus OpenCV. We can change the color space with conversion code, this changes the color space. We use the function cvtColor, the input is the color image and the color code BGR to RGB or blue, green, red to red, green, blue. We can now plot the image. You can also convert the image to gray-scale using cvtColor. The input is the original image and the BGR to gray color code. We can plot the image. We can save the image using imright, the input is the path and the image array. We can change the color space with conversion code, this changes the color space. We use the function cvtColor, the input is the color image and the color code BGR to RGB or blue, green, red to red, green, blue. We can now plot the image. You can also convert the image to gray-scale using cvtColor. The input is the original image and the BGR to gray color code. We can plot the image.","title":"2. What Is A Digital Image"},{"location":"notes/data_science/Coursera/computer_vision/What_Is_A_Digital_Image/#what-is-a-digital-image","text":"A rectangular array of numbers. In the real world, an image can take on an almost unlimited number of values, but digital images have intensity values between zero (black) and 255 (white). It turns out that's all we need, 256 different intensity values to represent an image. If we use less than 256 values, things will look cartoonish. RGB: Image Masks: Basic Image Types: An image is a file on your computer. Two popular image formats, Joint Photographic Expert Group image or JPEG, and Portable Network Graphics or PNG, these formats reduce file size and have other features. No matter what Python library you use, you're going to have to load the image","title":"What is a Digital Image"},{"location":"notes/data_science/Coursera/computer_vision/What_Is_A_Digital_Image/#pil-intro","text":"ImageOps: Quantizing:","title":"PIL Intro"},{"location":"notes/data_science/Coursera/computer_vision/What_Is_A_Digital_Image/#numpy-intro","text":"","title":"Numpy Intro"},{"location":"notes/data_science/Coursera/computer_vision/What_Is_A_Digital_Image/#opencv-intro","text":"OpenCV is a library used for computer vision. It has more functionality than the PIL library, but is more difficult to use. We can plot the image using Imshow, but the colors appear off. This is because the order of each channel is different in OpenCV unlike PIL that is RGB. OpenCV is BGR. This is the main difference between the arrays and PIL versus OpenCV. We can change the color space with conversion code, this changes the color space. We use the function cvtColor, the input is the color image and the color code BGR to RGB or blue, green, red to red, green, blue. We can now plot the image. You can also convert the image to gray-scale using cvtColor. The input is the original image and the BGR to gray color code. We can plot the image. We can save the image using imright, the input is the path and the image array. We can change the color space with conversion code, this changes the color space. We use the function cvtColor, the input is the color image and the color code BGR to RGB or blue, green, red to red, green, blue. We can now plot the image. You can also convert the image to gray-scale using cvtColor. The input is the original image and the BGR to gray color code. We can plot the image.","title":"OpenCV Intro"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/","text":"Artificial_Neural_Networks Gradient Descent Cost Function We fit a line to data. If we nail the weight \\(w\\) then we have 0 loss. There is much loss if \\(w\\) is not ideal and the loss increases exponentially the further we get from the origin with the loss function we defined. We need derivatives or \"gradients\" so we know if we change \\(w\\) that the loss will be lower. We do this iteravely adjusting learning rate \\(\\eta\\) . Backpropogation How do we train and optimize weights and biases. Training is done in a supervised setting with a ground truth. Calculus Calculate error between GT and estimated output. Denote this \\(E\\) . Propogate this error back into the network and update each weight and bias as per the following equation: \\(w_i \\rightarrow w_i - \\eta * \\frac{\\delta E}{\\delta w_i}\\) \\(b_i \\rightarrow b_i - \\eta * \\frac{\\delta E}{\\delta b_i}\\) Backpropogation For a two neuron network we can calculate \\(E\\) for an observation. For multiple observations the equation is as follows: \\(E = \\frac{1}{2m} \\sum_{i=1}^{m} (T_i - a_{2,i})^2\\) This is a total aggregate error. Chain Rule Notice how \\(E\\) is not directly a function of \\(w_2\\) . Through a proof we can show that updating weights and biases is as follows (noticed small exponent errors...fyi): Below \\(\\frac{\\delta E a_2}{\\delta z_2}\\) becomes \\(a_2*(1-a_2)\\) . This isn't immediately obvious and here is the derivation . Example: Assume ground truth is \\(T=0.25\\) . Compute error between \\(a_2\\) and \\(T\\) . Update weights and biases for predefined number of \\(epochs\\) like 1000 for example or until error is under a certain threshold \\(\\epsilon = 0.001\\) . Update w2 Update b2 Update w1 Update b1 Summary Initialize the weights and biases to random values Then, we iteratively repeat the following steps: Calculate the network output using forward propagation. Calculate the error between the ground truth and the estimated or predicted output of the network. Update the weights and the biases through backpropagation. Repeat the above three steps until the number of iterations or epochs is reached or the error between the ground truth and the predicted output is below a predefined threshold. Vanishing Gradient There is a problem with the sigmoid activation function. Recall from the previous video, with a very simple network of two neurons only, the derivatives of the error with respect to the weights were as follows: \\(\\frac{\\delta E}{\\delta w_1} = 0.001021\\) \\(\\frac{\\delta E}{\\delta w_2} = 0.05706\\) Well it turns out that because we are using the sigmoid function as the activation function, then all the intermediate values in the network are between 0 and 1. So when we do backpropagation, we keep multiplying factors that are less than 1 by each other, and so their gradients tend to get smaller and smaller as we keep on moving backward in the network. This means that the neurons in the earlier layers learn very slowly as compared to the neurons in the later layers in the network. The earlier layers in the network, are the slowest to train. The result is a training process that takes too long and a prediction accuracy that is compromised. Accordingly, this is the reason why we do not use the sigmoid function or similar functions as activation functions, since they are prone to the vanishing gradient problem. Activation Functions These play a major role in the learning process of a neural network. 7 Types of activation functions you can use: 1. Binary step function 2. Linear Function 3. Sigmoid Function - most popular 4. Hyperbolic Tangent Function - most popular 5. ReLU (Rectified Linear Unit) - most popular 6. Leaky ReLU 7. Softmax Function - most popular Sigmoid Function Sigmoid functions used to be widely used as activation functions in the hidden layers of a neural network. However, as you can see, the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region, the gradients become very small. Another problem with the sigmoid function is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin. The values received are all positive. Well, not all the times would we desire that values going to the next neuron be all of the same sign. This can be addressed by scaling the sigmoid function, and this brings us to the next activation function: the hyperbolic tangent function. Hyperbolic Tangent Function It is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function, but unlike the sigmoid function, it's symmetric over the origin. It ranges from -1 to +1. However, although it overcomes the lack of symmetry of the sigmoid function, it also leads to the vanishing gradient problem in very deep neural networks. ReLU Function The rectified linear unit, or ReLU, function is the most widely used activation function when designing networks today. In addition to it being nonlinear, the main advantage of using the ReLU, function over the other activation functions is that it does not activate all the neurons at the same time. According to the plot here, if the input is negative it will be converted to 0, and the neuron does not get activated. This means that at a time, only a few neurons are activated, making the network sparse and very efficient. Also, the ReLU function was one of the main advancements in the field of deep learning that led to overcoming the vanishing gradient problem. Softmax Function Sigmoid type function that is handy for classification problems. Maps outputs to between 0 and 1. This way, it is easier for us to classify a given data point and determine to which category it belongs.","title":"2. Artificial Neural Networks"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#artificial_neural_networks","text":"","title":"Artificial_Neural_Networks"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#gradient-descent","text":"","title":"Gradient Descent"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#cost-function","text":"We fit a line to data. If we nail the weight \\(w\\) then we have 0 loss. There is much loss if \\(w\\) is not ideal and the loss increases exponentially the further we get from the origin with the loss function we defined. We need derivatives or \"gradients\" so we know if we change \\(w\\) that the loss will be lower. We do this iteravely adjusting learning rate \\(\\eta\\) .","title":"Cost Function"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#backpropogation","text":"How do we train and optimize weights and biases. Training is done in a supervised setting with a ground truth.","title":"Backpropogation"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#calculus","text":"Calculate error between GT and estimated output. Denote this \\(E\\) . Propogate this error back into the network and update each weight and bias as per the following equation: \\(w_i \\rightarrow w_i - \\eta * \\frac{\\delta E}{\\delta w_i}\\) \\(b_i \\rightarrow b_i - \\eta * \\frac{\\delta E}{\\delta b_i}\\) Backpropogation For a two neuron network we can calculate \\(E\\) for an observation. For multiple observations the equation is as follows: \\(E = \\frac{1}{2m} \\sum_{i=1}^{m} (T_i - a_{2,i})^2\\) This is a total aggregate error. Chain Rule Notice how \\(E\\) is not directly a function of \\(w_2\\) . Through a proof we can show that updating weights and biases is as follows (noticed small exponent errors...fyi): Below \\(\\frac{\\delta E a_2}{\\delta z_2}\\) becomes \\(a_2*(1-a_2)\\) . This isn't immediately obvious and here is the derivation . Example: Assume ground truth is \\(T=0.25\\) . Compute error between \\(a_2\\) and \\(T\\) . Update weights and biases for predefined number of \\(epochs\\) like 1000 for example or until error is under a certain threshold \\(\\epsilon = 0.001\\) . Update w2 Update b2 Update w1 Update b1","title":"Calculus"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#summary","text":"Initialize the weights and biases to random values Then, we iteratively repeat the following steps: Calculate the network output using forward propagation. Calculate the error between the ground truth and the estimated or predicted output of the network. Update the weights and the biases through backpropagation. Repeat the above three steps until the number of iterations or epochs is reached or the error between the ground truth and the predicted output is below a predefined threshold.","title":"Summary"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#vanishing-gradient","text":"There is a problem with the sigmoid activation function. Recall from the previous video, with a very simple network of two neurons only, the derivatives of the error with respect to the weights were as follows: \\(\\frac{\\delta E}{\\delta w_1} = 0.001021\\) \\(\\frac{\\delta E}{\\delta w_2} = 0.05706\\) Well it turns out that because we are using the sigmoid function as the activation function, then all the intermediate values in the network are between 0 and 1. So when we do backpropagation, we keep multiplying factors that are less than 1 by each other, and so their gradients tend to get smaller and smaller as we keep on moving backward in the network. This means that the neurons in the earlier layers learn very slowly as compared to the neurons in the later layers in the network. The earlier layers in the network, are the slowest to train. The result is a training process that takes too long and a prediction accuracy that is compromised. Accordingly, this is the reason why we do not use the sigmoid function or similar functions as activation functions, since they are prone to the vanishing gradient problem.","title":"Vanishing Gradient"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#activation-functions","text":"These play a major role in the learning process of a neural network. 7 Types of activation functions you can use: 1. Binary step function 2. Linear Function 3. Sigmoid Function - most popular 4. Hyperbolic Tangent Function - most popular 5. ReLU (Rectified Linear Unit) - most popular 6. Leaky ReLU 7. Softmax Function - most popular","title":"Activation Functions"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#sigmoid-function","text":"Sigmoid functions used to be widely used as activation functions in the hidden layers of a neural network. However, as you can see, the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region, the gradients become very small. Another problem with the sigmoid function is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin. The values received are all positive. Well, not all the times would we desire that values going to the next neuron be all of the same sign. This can be addressed by scaling the sigmoid function, and this brings us to the next activation function: the hyperbolic tangent function.","title":"Sigmoid Function"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#hyperbolic-tangent-function","text":"It is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function, but unlike the sigmoid function, it's symmetric over the origin. It ranges from -1 to +1. However, although it overcomes the lack of symmetry of the sigmoid function, it also leads to the vanishing gradient problem in very deep neural networks.","title":"Hyperbolic Tangent Function"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#relu-function","text":"The rectified linear unit, or ReLU, function is the most widely used activation function when designing networks today. In addition to it being nonlinear, the main advantage of using the ReLU, function over the other activation functions is that it does not activate all the neurons at the same time. According to the plot here, if the input is negative it will be converted to 0, and the neuron does not get activated. This means that at a time, only a few neurons are activated, making the network sparse and very efficient. Also, the ReLU function was one of the main advancements in the field of deep learning that led to overcoming the vanishing gradient problem.","title":"ReLU Function"},{"location":"notes/data_science/Coursera/keras/Artificial_Neural_Networks/#softmax-function","text":"Sigmoid type function that is handy for classification problems. Maps outputs to between 0 and 1. This way, it is easier for us to classify a given data point and determine to which category it belongs.","title":"Softmax Function"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/","text":"Deep Learning Networks Shallow Versus Deep Neural Networks.md Shallow neural network consists of one hidden layer. Deep neural network consists of more hidden layers and has a large number of neurons in each layer. Why did deep learning take off just recently? Why Deep Learning Took Off Advancement in field itself Data is readily available GPUs and compute power Supervised Learning Convolutional Neural Networks CNNs are similar to the typical NN. Made up of neurons Neurons combines inputs it receives Fits output into activation function CNNs take inputs as images. (ALWAYS IMAGES) This allows us to incorporate properties that make the training process much more efficient. Solve problems involving image reconstruction, object detection, and other computer vision applications. Typical CNN: CNN: We see different layers: * Input Layer: * Usually n inputs * CNNs will have * \\(m\\) x \\(n\\) for grey scale * 3 x \\(m\\) x \\(n\\) for color images * Convolution Layer * We define filters * We slide a kernel over an image and perform a dot product between the input image and kernel mask as it slides * Why not just flatten image in to a (1 x \\(m\\) x \\(n\\) )? Why convolve? * We will end up with a massive number of parameters that will need to be optimized, and it will be super computationally expensive. * Also, decreasing the number of parameters would definitely help in preventing the model from overfitting the training data. * It is worth mentioning that a convolutional layer also consists of ReLU's which filter the output of the convolutional step passing only positive values and turning any negative values to 0. * Pooling Layer * Main objective is to reduce spatial dimensions of data. * 2 types: * Max Pooling * For each kernel, we keep the max cell * Average Pooling * For each kernel, we keep the average of kernel cells * Pooling provides spatial variance which enables the NN to recognize objects in the image even if the object does not exactly resemble the original object. * Fully Connected Layer * We flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. * This layer takes output of previous layer (pooling, conv, relu, etc) and connects to nodes representing each class we want to predict. Keras Code Conv2D(16...) means 16 filters. So kernel of (2,2) strides 1 pixel at a time and slides acros whole image. That is one filter pass. 15 more are completed. What kind of filter? MaxPool, and assumes max pooling . Conv2D(32...) twice as many filters as first layer. MaxPool, max pooling again. Flatten output from layers so it can proceed to output layers. We add a 100 neuron FCL and then create an output layer with the num_classes we are searching for. It has a softmax activation function as well. Recurrent Neural Networks (RNNs) So far, we have seen neural networks and deep learning models that see datapoints as independent instances. However, let's say you want to build a model that can analyze scenes in a movie. Well, you cannot assume that scenes in a movie are independent, and therefore, traditional deep learning models are not suitable for this application. Networks with loops that feed into itself They don't just take a new input at a time, but also take in as input the output from the previous data point that was fed into the network. Essentially, we can start with a normal neural network. At time t = 0, the network takes in input x0 and outputs a0. Then, at time t = 1, in addition to the input x1, the network also takes a0 as input, weighted with weight w0,1, and so on and so forth. As a result, recurrent neural networks are very good at modeling patterns and sequences of data, such as texts, genomes, handwriting, and stock markets. These algorithms take time and sequence into account, which means that they have a temporal dimension. Long Short-Term Memory Model Popular example of an RNN is Long Short-Term Memory Model (LSTM) Applications include: Image Generation Handwriting Generation Automatic Captioning of Images Automatic Description of Videos Unsupervised Learning Autoencoders Autoencoding is a data compression algorithm where the compression and the decompression functions are learned automatically from data instead of being engineered by a human. Autoencoders are data specific, which means that they will only be able to compress data similar to what they have been trained on. Therefore, an autoencoder trained on pictures of cars would do a rather poor job of compressing pictures of buildings, because the features it would learn would be vehicle or car specific. Some interesting applications of autoencoders are data denoising and dimensionality reduction for data visualization. It takes an image, for example, as an input and uses an encoder to find the optimal compressed representation of the input image. Then, using a decoder the original image is restored. So an autoencoder is an unsupervised neural network model. It uses backpropagation by setting the target variable to be the same as the input. Restricted Boltzman Machines A very popular type of autoencoders is the Restricted Boltzmann Machines or (RBMs) Applications include: Fixing imbalanced datasets Estimating missing values Automatic feature extraction Convolutional Neural Networks with Keras Convolutional Layer with one set of convolutional and pooling layers # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import keras from keras.models import Sequential from keras.layers import Dense from keras.utils import to_categorical from keras.layers import Convolution2D # to add convolutional layers from keras.layers import MaxPooling2D # to add pooling layers from keras.layers import Flatten # to flatten data for fully connected layers from keras.datasets import mnist # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples]][width][height][pixels] X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') X_train.shape # (60000, 28, 28, 1) X_train = X_train / 255 # normalize training data X_test = X_test / 255 # normalize test data # Next, let's convert the target variable into binary categories. y_train = to_categorical(y_train) y_test = to_categorical(y_test) num_classes = y_test.shape[1] # number of categories # Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers. def convolutional_model(): # create model model = Sequential() model.add(Convolution2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Finally, let's call the function to create the model, and then let's train it and evaluate it. ## build the model model = convolutional_model() ## fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) ## evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100)) Convolutional Layer with two sets of convolutional and pooling layers def convolutional_model(): # create model model = Sequential() model.add(Convolution2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Convolution2D(8, (2, 2), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it. ## build the model model = convolutional_model() ## fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) ## evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))","title":"4. Deep Learning Models"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#deep-learning-networks","text":"","title":"Deep Learning Networks"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#shallow-versus-deep-neural-networksmd","text":"Shallow neural network consists of one hidden layer. Deep neural network consists of more hidden layers and has a large number of neurons in each layer. Why did deep learning take off just recently?","title":"Shallow Versus Deep Neural Networks.md"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#why-deep-learning-took-off","text":"Advancement in field itself Data is readily available GPUs and compute power","title":"Why Deep Learning Took Off"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#convolutional-neural-networks","text":"CNNs are similar to the typical NN. Made up of neurons Neurons combines inputs it receives Fits output into activation function CNNs take inputs as images. (ALWAYS IMAGES) This allows us to incorporate properties that make the training process much more efficient. Solve problems involving image reconstruction, object detection, and other computer vision applications. Typical CNN: CNN: We see different layers: * Input Layer: * Usually n inputs * CNNs will have * \\(m\\) x \\(n\\) for grey scale * 3 x \\(m\\) x \\(n\\) for color images * Convolution Layer * We define filters * We slide a kernel over an image and perform a dot product between the input image and kernel mask as it slides * Why not just flatten image in to a (1 x \\(m\\) x \\(n\\) )? Why convolve? * We will end up with a massive number of parameters that will need to be optimized, and it will be super computationally expensive. * Also, decreasing the number of parameters would definitely help in preventing the model from overfitting the training data. * It is worth mentioning that a convolutional layer also consists of ReLU's which filter the output of the convolutional step passing only positive values and turning any negative values to 0. * Pooling Layer * Main objective is to reduce spatial dimensions of data. * 2 types: * Max Pooling * For each kernel, we keep the max cell * Average Pooling * For each kernel, we keep the average of kernel cells * Pooling provides spatial variance which enables the NN to recognize objects in the image even if the object does not exactly resemble the original object. * Fully Connected Layer * We flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. * This layer takes output of previous layer (pooling, conv, relu, etc) and connects to nodes representing each class we want to predict.","title":"Convolutional Neural Networks"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#keras-code","text":"Conv2D(16...) means 16 filters. So kernel of (2,2) strides 1 pixel at a time and slides acros whole image. That is one filter pass. 15 more are completed. What kind of filter? MaxPool, and assumes max pooling . Conv2D(32...) twice as many filters as first layer. MaxPool, max pooling again. Flatten output from layers so it can proceed to output layers. We add a 100 neuron FCL and then create an output layer with the num_classes we are searching for. It has a softmax activation function as well.","title":"Keras Code"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#recurrent-neural-networks-rnns","text":"So far, we have seen neural networks and deep learning models that see datapoints as independent instances. However, let's say you want to build a model that can analyze scenes in a movie. Well, you cannot assume that scenes in a movie are independent, and therefore, traditional deep learning models are not suitable for this application. Networks with loops that feed into itself They don't just take a new input at a time, but also take in as input the output from the previous data point that was fed into the network. Essentially, we can start with a normal neural network. At time t = 0, the network takes in input x0 and outputs a0. Then, at time t = 1, in addition to the input x1, the network also takes a0 as input, weighted with weight w0,1, and so on and so forth. As a result, recurrent neural networks are very good at modeling patterns and sequences of data, such as texts, genomes, handwriting, and stock markets. These algorithms take time and sequence into account, which means that they have a temporal dimension.","title":"Recurrent Neural Networks (RNNs)"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#long-short-term-memory-model","text":"Popular example of an RNN is Long Short-Term Memory Model (LSTM) Applications include: Image Generation Handwriting Generation Automatic Captioning of Images Automatic Description of Videos","title":"Long Short-Term Memory Model"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#unsupervised-learning","text":"","title":"Unsupervised Learning"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#autoencoders","text":"Autoencoding is a data compression algorithm where the compression and the decompression functions are learned automatically from data instead of being engineered by a human. Autoencoders are data specific, which means that they will only be able to compress data similar to what they have been trained on. Therefore, an autoencoder trained on pictures of cars would do a rather poor job of compressing pictures of buildings, because the features it would learn would be vehicle or car specific. Some interesting applications of autoencoders are data denoising and dimensionality reduction for data visualization. It takes an image, for example, as an input and uses an encoder to find the optimal compressed representation of the input image. Then, using a decoder the original image is restored. So an autoencoder is an unsupervised neural network model. It uses backpropagation by setting the target variable to be the same as the input.","title":"Autoencoders"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#restricted-boltzman-machines","text":"A very popular type of autoencoders is the Restricted Boltzmann Machines or (RBMs) Applications include: Fixing imbalanced datasets Estimating missing values Automatic feature extraction","title":"Restricted Boltzman Machines"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#convolutional-neural-networks-with-keras","text":"","title":"Convolutional Neural Networks with Keras"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#convolutional-layer-with-one-set-of-convolutional-and-pooling-layers","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import keras from keras.models import Sequential from keras.layers import Dense from keras.utils import to_categorical from keras.layers import Convolution2D # to add convolutional layers from keras.layers import MaxPooling2D # to add pooling layers from keras.layers import Flatten # to flatten data for fully connected layers from keras.datasets import mnist # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples]][width][height][pixels] X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') X_train.shape # (60000, 28, 28, 1) X_train = X_train / 255 # normalize training data X_test = X_test / 255 # normalize test data # Next, let's convert the target variable into binary categories. y_train = to_categorical(y_train) y_test = to_categorical(y_test) num_classes = y_test.shape[1] # number of categories # Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers. def convolutional_model(): # create model model = Sequential() model.add(Convolution2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Finally, let's call the function to create the model, and then let's train it and evaluate it. ## build the model model = convolutional_model() ## fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) ## evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))","title":"Convolutional Layer with one set of convolutional and pooling layers"},{"location":"notes/data_science/Coursera/keras/Deep_Learning_Models/#convolutional-layer-with-two-sets-of-convolutional-and-pooling-layers","text":"def convolutional_model(): # create model model = Sequential() model.add(Convolution2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Convolution2D(8, (2, 2), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it. ## build the model model = convolutional_model() ## fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) ## evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))","title":"Convolutional Layer with two sets of convolutional and pooling layers"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/","text":"Intro to Deep Neural Nets Applications Color Restoration Speech Reenactment Synching lip movements in a video with an audio clip Synthesizing a video to go along with an audio clip You could pull audio from one video clip and sync with a synthesized video. Automatic hand writing generation Other Applications: Automatic machine translation Automatically adding sounds to silent movies Object classification and detection in images Self Driving Cars Neurons and Neural Networks The main body of the neuron is called the soma , which contains the nucleus of the neuron. The big network of arms sticking out of the body is called the dendrites , and then the long arm that sticks out of the soma in the other direction is called the axon . The whiskers at the end of the axon are called the terminal buttons or synapses. So the dendrites receive electrical impulses which carry information, or data, from sensors or terminal buttons of other adjoining neurons. The dendrites then carry the impulses or data to the soma. In the nucleus, electrical impulses, or the data, are processed by combining them together, and then they are passed on to the axon. The axon then carries the processed information to the terminal button or synapse, and the output of this neuron becomes the input to thousands of other neurons. Learning in the brain occurs by repeatedly activating certain neural connections over others, and this reinforces those connections. This makes them more likely to produce a desired outcome given a specified input. Once the desired outcome occurs, the neural connections causing that outcome become strengthened. An artificial neuron behaves in the same way as a biological neuron. So it consists of a soma, dendrites, and an axon to pass on the output of this neuron to other neurons. The end of the axon can branch off to connect to many other neurons, but for simplicity we are just showing one branch here. The learning process also very much resembles the way learning occurs in the brain as you will see in the next couple of videos. Now that we understand the different parts of an artificial neuron, let's learn how we formulate the way artificial neural networks process information. Artificial Neural Nets For a network of neurons, we normally divide it into different layers: the first layer that feeds the input into the network is obviously called the input layer. The set of nodes that provide the output of the network is called the output layer. And any sets of nodes in between the input and the output layers are called the hidden layers. Three main topics are associated with artifical neural networks: * Forward Propogation * Backpropagation * Activation Functions Forward Propagation Forward propagation is the process through which data passes through layers of neurons in a neural network from the input layer all the way to the output layer. As shown here, the data flows through each neuron by connections or the dendrites. Every connection has a specific weight by which the flow of data is regulated. Here x1 and x2 are the two inputs, they could be an integer or float. When these inputs pass through the connections, they're adjusted depending on the connection weights, w1 and w2 . The neuron then processes this information by outputting a weighted sum of these inputs. It also adds a constant to the sum which is referred to as the bias (b) . So, z here is the linear combination of the inputs and weights along with the bias, and a is the output of the network. For consistency, we will stick to these letters throughout the course, so z will always represent the linear combination of the inputs, and a will always represent the output of a neuron. \\(a = f(z) = \\frac{1}{1 + e^-z}\\) Therefore, a better processing of the data would be to map the weighted sum to a nonlinear space. A popular function is the sigmoid function, where if the weighted sum is a very large positive number, then the output of the neuron is close to 1, and if the weighted sum is a very large negative number, then the output of the neuron is close to zero. Non-linear transformations like the sigmoid function are called activation functions. Activation functions are another extremely important feature of artificial neural networks. They basically decide whether a neuron should be activated or not. In other words, whether the information that the neuron is receiving is relevant or should be ignored. The takeaway message here is that a neural network without an activation function is essentially just a linear regression model. The activation function performs non-linear transformation to the input enabling the neural network of learning and performing more complex tasks, such as image classifications and language translations. For further simplification, I am going to proceed with a neural network of one neuron and one input. Let's go over an example of how to compute the output. Let's say that the value of x1 is 0.1, and we want to predict the output for this input. The network has optimized weight and bias where w1 is 0.15 and b1 is 0.4. The first step is to calculate z, which is the dot product of the inputs and the corresponding weights plus the bias. So we find that z is 0.415. The neuron then uses the sigmoid function to apply non-linear transformation to z. Therefore, the output of the neuron is 0.6023. For a network with two neurons, the output from the first neuron will be the input to the second neuron. The rest is then exactly the same. The second neuron takes the input and computes the dot product of the input, which is a1 in this case, and the weight which is w2, and adds the bias which is b2. Using a sigmoid function as the activation function, the output of the network would be 0.7153. And this would be the predicted value for the input 0.1. This is in essence how a neural network predicts the output for any given input. No matter how complicated the network gets, it is the same exact process. To summarize, given a neural network with a set of weights and biases, you should be able to compute the output of the network for any given input. Forward Prop Code Ex # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node. Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer. First Network: import numpy as np # import Numpy library to generate import pprint as pp from random import seed weights = np.around(np.random.uniform(size=6), decimals=2) # initialize the weights biases = np.around(np.random.uniform(size=3), decimals=2) # initialize the biases Now that we have the weights and the biases defined for the network, let's compute the output for a given input, \\(x_1\\) and \\(x_2\\) . x_1 = 0.5 # input 1 x_2 = 0.85 # input 2 print('x1 is {} and x2 is {}'.format(x_1, x_2)) # x1 is 0.5 and x2 is 0.85 Let's start by computing the wighted sum of the inputs, \\(z_{1, 1}\\) , at the first node of the hidden layer. z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0] print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11)) # The weighted sum of the inputs at the first node in the hidden layer is 1.577 # Second node z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1] print('The weighted sum of the inputs at the second node in the hidden layer is {}'.format(np.around(z_12, decimals=4))) # The weighted sum of the inputs at the second node in the hidden layer is 1.074 Next, assuming a sigmoid activation function, let's compute the activation of the first node, \\(a_{1, 1}\\) , in the hidden layer. a_11 = 1.0 / (1.0 + np.exp(-z_11)) print('The activation of the first node in the hidden layer is {}'.format(np.around(a_11, decimals=4))) # Second node a_12 = 1.0 / (1.0 + np.exp(-z_12)) print('The activation of the second node in the hidden layer is {}'.format(np.around(a_12, decimals=4))) # The activation of the second node in the hidden layer is 0.7454 Now compute \\(z_2\\) : z_2 = z_11 * weights[4] + z_12 * weights[5] + biases[2] print('The weighted sum of the inputs at the node in the output layer is {}'.format(np.around(z_2, decimals=4))) # The weighted sum of the inputs at the node in the output layer is 2.7104 Finally, activation: a_2 = 1.0 / (1.0 + np.exp(-z_2)) print('The output of the network for x1 = 0.5 and x2 = 0.85 is {}'.format(np.around(a_2, decimals=4))) # The output of the network for x1 = 0.5 and x2 = 0.85 is 0.9376 In order to code an automatic way of making predictions, let's generalize our network. A general network would take \\(n\\) inputs, would have many hidden layers, each hidden layer having \\(m\\) nodes, and would have an output layer. Although the network is showing one hidden layer, we will code the network to have many hidden layers . Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer. Initialize a network: n = 2 # number of inputs num_hidden_layers = 2 # number of hidden layers m = [2, 2] # number of nodes in each hidden layer num_nodes_output = 1 # number of nodes in the output layer num_nodes_previous = n # number of nodes in the previous layer network = {} # initialize network an an empty dictionary # loop through each layer and randomly initialize the weights and biases associated with each node # notice how we are adding 1 to the number of hidden layers in order to include the output layer for layer in range(num_hidden_layers + 1): # determine name of layer if layer == num_hidden_layers: layer_name = 'output' num_nodes = num_nodes_output else: layer_name = 'layer_{}'.format(layer + 1) num_nodes = m[layer] # initialize weights and biases associated with each node in the current layer network[layer_name] = {} for node in range(num_nodes): node_name = 'node_{}'.format(node+1) network[layer_name][node_name] = { 'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2), 'bias': np.around(np.random.uniform(size=1), decimals=2), } num_nodes_previous = num_nodes pp.pprint(network) # print network Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network. def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output): num_nodes_previous = num_inputs # number of nodes in the previous layer network = {} # loop through each layer and randomly initialize the weights and biases associated with each layer for layer in range(num_hidden_layers + 1): if layer == num_hidden_layers: layer_name = 'output' # name last layer in the network output num_nodes = num_nodes_output else: layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number num_nodes = num_nodes_hidden[layer] # initialize weights and bias for each node network[layer_name] = {} for node in range(num_nodes): node_name = 'node_{}'.format(node+1) network[layer_name][node_name] = { 'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2), 'bias': np.around(np.random.uniform(size=1), decimals=2), } num_nodes_previous = num_nodes return network # return the network small_network = initialize_network(num_inputs=5, \\ num_hidden_layers=3, \\ num_nodes_hidden=[3,2,3], \\ num_nodes_output=1) Compute Weighted Sum At Each Node: def compute_weighted_sum(inputs, weights, bias): return np.sum(inputs * weights) + bias np.random.seed(12) inputs = np.around(np.random.uniform(size=5), decimals=2) print('The inputs to the network are {}'.format(inputs)) # The inputs to the network are [0.15 0.74 0.26 0.53 0.01] weighted_sum = compute_weighted_sum(inputs, \\ small_network['layer_1']['node_1']['weights'], \\ small_network['layer_1']['node_1']['bias'] ) Compute Node Activation: def node_activation(weighted_sum): return 1.0 / (1.0 + np.exp(-1 * weighted_sum)) node_activation(weighted_sum) Forward Propagation Put together for all nodes: The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the compute_weighted_sum and node_activation functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer. def forward_propagate(network, inputs): layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer for layer in network: layer_data = network[layer] layer_outputs = [] for layer_node in layer_data: node_data = layer_data[layer_node] # compute the weighted sum and the output of each node at the same time node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias'])) layer_outputs.append(np.around(node_output[0], decimals=4)) if layer != 'output': print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs)) layer_inputs = layer_outputs # set the output of this layer to be the input to next layer network_predictions = layer_outputs return network_predictions forward_propagate(small_network, inputs)","title":"1. Intro to Neural Networks and Deep Learning"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#intro-to-deep-neural-nets","text":"","title":"Intro to Deep Neural Nets"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#applications","text":"Color Restoration Speech Reenactment Synching lip movements in a video with an audio clip Synthesizing a video to go along with an audio clip You could pull audio from one video clip and sync with a synthesized video. Automatic hand writing generation Other Applications: Automatic machine translation Automatically adding sounds to silent movies Object classification and detection in images Self Driving Cars","title":"Applications"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#neurons-and-neural-networks","text":"The main body of the neuron is called the soma , which contains the nucleus of the neuron. The big network of arms sticking out of the body is called the dendrites , and then the long arm that sticks out of the soma in the other direction is called the axon . The whiskers at the end of the axon are called the terminal buttons or synapses. So the dendrites receive electrical impulses which carry information, or data, from sensors or terminal buttons of other adjoining neurons. The dendrites then carry the impulses or data to the soma. In the nucleus, electrical impulses, or the data, are processed by combining them together, and then they are passed on to the axon. The axon then carries the processed information to the terminal button or synapse, and the output of this neuron becomes the input to thousands of other neurons. Learning in the brain occurs by repeatedly activating certain neural connections over others, and this reinforces those connections. This makes them more likely to produce a desired outcome given a specified input. Once the desired outcome occurs, the neural connections causing that outcome become strengthened. An artificial neuron behaves in the same way as a biological neuron. So it consists of a soma, dendrites, and an axon to pass on the output of this neuron to other neurons. The end of the axon can branch off to connect to many other neurons, but for simplicity we are just showing one branch here. The learning process also very much resembles the way learning occurs in the brain as you will see in the next couple of videos. Now that we understand the different parts of an artificial neuron, let's learn how we formulate the way artificial neural networks process information.","title":"Neurons and Neural Networks"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#artificial-neural-nets","text":"For a network of neurons, we normally divide it into different layers: the first layer that feeds the input into the network is obviously called the input layer. The set of nodes that provide the output of the network is called the output layer. And any sets of nodes in between the input and the output layers are called the hidden layers. Three main topics are associated with artifical neural networks: * Forward Propogation * Backpropagation * Activation Functions","title":"Artificial Neural Nets"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#forward-propagation","text":"Forward propagation is the process through which data passes through layers of neurons in a neural network from the input layer all the way to the output layer. As shown here, the data flows through each neuron by connections or the dendrites. Every connection has a specific weight by which the flow of data is regulated. Here x1 and x2 are the two inputs, they could be an integer or float. When these inputs pass through the connections, they're adjusted depending on the connection weights, w1 and w2 . The neuron then processes this information by outputting a weighted sum of these inputs. It also adds a constant to the sum which is referred to as the bias (b) . So, z here is the linear combination of the inputs and weights along with the bias, and a is the output of the network. For consistency, we will stick to these letters throughout the course, so z will always represent the linear combination of the inputs, and a will always represent the output of a neuron. \\(a = f(z) = \\frac{1}{1 + e^-z}\\) Therefore, a better processing of the data would be to map the weighted sum to a nonlinear space. A popular function is the sigmoid function, where if the weighted sum is a very large positive number, then the output of the neuron is close to 1, and if the weighted sum is a very large negative number, then the output of the neuron is close to zero. Non-linear transformations like the sigmoid function are called activation functions. Activation functions are another extremely important feature of artificial neural networks. They basically decide whether a neuron should be activated or not. In other words, whether the information that the neuron is receiving is relevant or should be ignored. The takeaway message here is that a neural network without an activation function is essentially just a linear regression model. The activation function performs non-linear transformation to the input enabling the neural network of learning and performing more complex tasks, such as image classifications and language translations. For further simplification, I am going to proceed with a neural network of one neuron and one input. Let's go over an example of how to compute the output. Let's say that the value of x1 is 0.1, and we want to predict the output for this input. The network has optimized weight and bias where w1 is 0.15 and b1 is 0.4. The first step is to calculate z, which is the dot product of the inputs and the corresponding weights plus the bias. So we find that z is 0.415. The neuron then uses the sigmoid function to apply non-linear transformation to z. Therefore, the output of the neuron is 0.6023. For a network with two neurons, the output from the first neuron will be the input to the second neuron. The rest is then exactly the same. The second neuron takes the input and computes the dot product of the input, which is a1 in this case, and the weight which is w2, and adds the bias which is b2. Using a sigmoid function as the activation function, the output of the network would be 0.7153. And this would be the predicted value for the input 0.1. This is in essence how a neural network predicts the output for any given input. No matter how complicated the network gets, it is the same exact process. To summarize, given a neural network with a set of weights and biases, you should be able to compute the output of the network for any given input.","title":"Forward Propagation"},{"location":"notes/data_science/Coursera/keras/Intro_Deep_Neural_Nets/#forward-prop-code-ex","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node. Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer. First Network: import numpy as np # import Numpy library to generate import pprint as pp from random import seed weights = np.around(np.random.uniform(size=6), decimals=2) # initialize the weights biases = np.around(np.random.uniform(size=3), decimals=2) # initialize the biases Now that we have the weights and the biases defined for the network, let's compute the output for a given input, \\(x_1\\) and \\(x_2\\) . x_1 = 0.5 # input 1 x_2 = 0.85 # input 2 print('x1 is {} and x2 is {}'.format(x_1, x_2)) # x1 is 0.5 and x2 is 0.85 Let's start by computing the wighted sum of the inputs, \\(z_{1, 1}\\) , at the first node of the hidden layer. z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0] print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11)) # The weighted sum of the inputs at the first node in the hidden layer is 1.577 # Second node z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1] print('The weighted sum of the inputs at the second node in the hidden layer is {}'.format(np.around(z_12, decimals=4))) # The weighted sum of the inputs at the second node in the hidden layer is 1.074 Next, assuming a sigmoid activation function, let's compute the activation of the first node, \\(a_{1, 1}\\) , in the hidden layer. a_11 = 1.0 / (1.0 + np.exp(-z_11)) print('The activation of the first node in the hidden layer is {}'.format(np.around(a_11, decimals=4))) # Second node a_12 = 1.0 / (1.0 + np.exp(-z_12)) print('The activation of the second node in the hidden layer is {}'.format(np.around(a_12, decimals=4))) # The activation of the second node in the hidden layer is 0.7454 Now compute \\(z_2\\) : z_2 = z_11 * weights[4] + z_12 * weights[5] + biases[2] print('The weighted sum of the inputs at the node in the output layer is {}'.format(np.around(z_2, decimals=4))) # The weighted sum of the inputs at the node in the output layer is 2.7104 Finally, activation: a_2 = 1.0 / (1.0 + np.exp(-z_2)) print('The output of the network for x1 = 0.5 and x2 = 0.85 is {}'.format(np.around(a_2, decimals=4))) # The output of the network for x1 = 0.5 and x2 = 0.85 is 0.9376 In order to code an automatic way of making predictions, let's generalize our network. A general network would take \\(n\\) inputs, would have many hidden layers, each hidden layer having \\(m\\) nodes, and would have an output layer. Although the network is showing one hidden layer, we will code the network to have many hidden layers . Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer. Initialize a network: n = 2 # number of inputs num_hidden_layers = 2 # number of hidden layers m = [2, 2] # number of nodes in each hidden layer num_nodes_output = 1 # number of nodes in the output layer num_nodes_previous = n # number of nodes in the previous layer network = {} # initialize network an an empty dictionary # loop through each layer and randomly initialize the weights and biases associated with each node # notice how we are adding 1 to the number of hidden layers in order to include the output layer for layer in range(num_hidden_layers + 1): # determine name of layer if layer == num_hidden_layers: layer_name = 'output' num_nodes = num_nodes_output else: layer_name = 'layer_{}'.format(layer + 1) num_nodes = m[layer] # initialize weights and biases associated with each node in the current layer network[layer_name] = {} for node in range(num_nodes): node_name = 'node_{}'.format(node+1) network[layer_name][node_name] = { 'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2), 'bias': np.around(np.random.uniform(size=1), decimals=2), } num_nodes_previous = num_nodes pp.pprint(network) # print network Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network. def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output): num_nodes_previous = num_inputs # number of nodes in the previous layer network = {} # loop through each layer and randomly initialize the weights and biases associated with each layer for layer in range(num_hidden_layers + 1): if layer == num_hidden_layers: layer_name = 'output' # name last layer in the network output num_nodes = num_nodes_output else: layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number num_nodes = num_nodes_hidden[layer] # initialize weights and bias for each node network[layer_name] = {} for node in range(num_nodes): node_name = 'node_{}'.format(node+1) network[layer_name][node_name] = { 'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2), 'bias': np.around(np.random.uniform(size=1), decimals=2), } num_nodes_previous = num_nodes return network # return the network small_network = initialize_network(num_inputs=5, \\ num_hidden_layers=3, \\ num_nodes_hidden=[3,2,3], \\ num_nodes_output=1) Compute Weighted Sum At Each Node: def compute_weighted_sum(inputs, weights, bias): return np.sum(inputs * weights) + bias np.random.seed(12) inputs = np.around(np.random.uniform(size=5), decimals=2) print('The inputs to the network are {}'.format(inputs)) # The inputs to the network are [0.15 0.74 0.26 0.53 0.01] weighted_sum = compute_weighted_sum(inputs, \\ small_network['layer_1']['node_1']['weights'], \\ small_network['layer_1']['node_1']['bias'] ) Compute Node Activation: def node_activation(weighted_sum): return 1.0 / (1.0 + np.exp(-1 * weighted_sum)) node_activation(weighted_sum) Forward Propagation Put together for all nodes: The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the compute_weighted_sum and node_activation functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer. def forward_propagate(network, inputs): layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer for layer in network: layer_data = network[layer] layer_outputs = [] for layer_node in layer_data: node_data = layer_data[layer_node] # compute the weighted sum and the output of each node at the same time node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias'])) layer_outputs.append(np.around(node_output[0], decimals=4)) if layer != 'output': print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs)) layer_inputs = layer_outputs # set the output of this layer to be the input to next layer network_predictions = layer_outputs return network_predictions forward_propagate(small_network, inputs)","title":"Forward Prop Code Ex"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/","text":"Keras and Deep Learning Libraries Deep Learning Libraries Tensorflow - most popular IBM says...but this is pretty debatable...more github activity and enterprise use and can be debugged easier at large scales due to static graphs. Keras - Easiest API to use and the go-to library for quick prototyping and fast development. Pytorch - Cousin of Lua-based Torch framework. Strong Tensorflow competitor. Released 2016 and is becoming more popular than Tensorflow in academic settings generally. Regression Models with Keras See Jupyter Notebook Example Let's take a look at a regression example. Here is a data set of the compressive strength of different samples of concrete based on the volumes of the different materials that were used to make them. So let's say we would like to use the Keras library to quickly build a deep neural network to model this dataset, and so we can automatically determine the compressive strength of a given concrete sample based on its ingredients. So let's say that the deep neural network that we would like to create takes all the eight features as input, feeds them into a hidden layer of five nodes, which is connected to another hidden layer of five nodes, which is then connected to the output layer with one node that is supposed to output the compressive strength of a given concrete sample. Note that usually you would go with a much larger number of neurons in each hidden layer like 50 or even 100, but we're just using a small network for simplicity. Notice how all the nodes in one layer are connected to all the other nodes in the next layer. Such a network is called a dense network . Before we begin using the Keras library, let's prepare our data and have it in the right format. The only thing we would need to do is to split the dataframe into two dataframes, one that has the predictors columns and another one that has the target column. We will also name them predictors and target. Keras Code # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; There are two models in the Keras library. One of them is the Sequential model and the other one is the model class used with the functional API. So to create your model, you simply call the Sequential constructor. import keras from keras.models import Sequential from keras.layers import Dense # Because our network consists of a linear stack of layers, then the Sequential model is what you would want to use. This is the case most of the time unless you are building something out of the ordinary. model = Sequential() # n_cols = concrete_data.shape[1] # Assuming you have the csv # Now building your layers is pretty straightforward as well. For that, we would need to import the \"Dense\" type of layers from \"keras.layers\". Then we use the add method to add each dense layer. We specify the number of neurons in each layer and the activation function that we want to use. model.add(Dense(5, activation='relu', input_Shape=(n_cols,))) model.add(Dense(5, activation='relu')) model.add(Dense(1)) # Now for training, we need to define an optimizer and the error metric. # As for the minimization algorithm (optimizer), there are actually other more efficient algorithms than the gradient descent for deep learning applications. One of them is \"adam\". One of the main advantages of the \"adam\" optimizer is that you don't need to specify the learning rate that we saw in the gradient descent video. model.compile(optimizer='adam', loss='mean_squared_error') model.fit(predictors, target) predictors = model.predict(test_data) Classification Models with Keras Let's say that we would like to build a model that would inform someone whether purchasing a certain car would be a good choice based on the price of the car, the cost to maintain it, and whether it can accommodate two or more people. So, here is a dataset that we are calling \"car_data\". One Hot Encoding The data already has one-hot encoding to transform each category of price, maintenance, and how many people the car can accommodate, into separate columns. So the price of the car can be either high, medium, or low. Similarly, the cost of maintaining the car can also be high, medium, or low, and the car can either fit two people or more. If you take the first car in the dataset, it is considered an expensive car, has high maintenance cost, and can fit only two people. The decision is 0, meaning that buying this car would be a bad choice . A decision of 1 means that buying the car is acceptable , a decision of 2 means that buying the car would be a good decision , and a decision of 3 means that buying the car would be a very good decision . We will use the same neural network. Predictors and Target However, with Keras, for classification problems, we can't use the target column as is; we actually need to transform the column into an array with binary values similar to one-hot encoding like the output shown here. We easily achieve that using the \"to_categorical\" function from the Keras utilities package. In other words, our model instead of having just one neuron in the output layer, it would have four neurons, since our target variable consists of four categories. In terms of code, the structure of our code is pretty similar to the one we use to build the model for our regression problem. We start by importing the Keras library and the Sequential model and we use it to construct our model. We also import the \"Dense\" layer since we will be using it to build our network. The additional import statement here is the \"to_categorical\" function in order to transform our target column into an array of binary numbers for classification. Then, we proceed to constructing our layers. We use the add method to create two hidden layers, each with five neurons and the neurons are activated using the ReLU activation function. Notice how here we also specify the softmax function as the activation function for the output layer, so that the sum of the predicted values from all the neurons in the output layer sum nicely to 1. Then in defining our compiler, here we will use the categorical cross-entropy as our loss measure instead of the mean squared error that we use for regression, and we will specify the evaluation metric to be \"accuracy\". \"accuracy\" is a built-in evaluation metric in Keras but you can actually define your own evaluation metric and pass it in the metrics parameter. Then we fit the model. Notice how this time we're specifying the number of epochs for training the model. Although we didn't specify the number of epochs when we built a regression model, but we could have done that. Finally, we use the predict method to make predictions. Now the output of the Keras predict method would be something like what's shown here. Now the output of the Keras predict method would be something like what's shown here. For each data point, the output is the probability that the decision of purchasing a given car belongs to one of the four classes. For each data point, the probabilities should sum to 1, and the higher the probability the more confident is the algorithm that a datapoint belongs to the respective class. So for the first data point or the first car in the test set, the decision would be 0 meaning not acceptable, since the first probability is the highest, with a value of 0.99 or close to 1, in this case. Similarly, for the second datapoint, the decision is also 0 or not acceptable, since the probability for this class is the highest, again with a value of 0.99 or almost 1. For the first three datapoints, the model is very confident that purchasing these cars is not acceptable. As for the last three datapoints, the decision would be 1 or acceptable, since the probabilities for the second class are higher than the rest of the classes. But notice how the probabilities for decision 0 and decision 1 are very close. Therefore, the model is not very confident but it would lean towards accepting purchasing these cars. In the lab part, you will get the chance to build your own regression and classification models using the Keras library, so make sure to complete this module's lab components. Code Examples Regressions Models with Keras # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; We will be playing around with the same dataset that we used in the videos. The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include: Cement Blast Furnace Slag Fly Ash Water Superplasticizer Coarse Aggregate Fine Aggregate The target variable in this problem is the concrete sample strength . import pandas as pd import numpy as np import keras from keras.models import Sequential from keras.layers import Dense concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv') concrete_data.head(1) concrete_data.describe() # Split into Predictors and Targets concrete_data_columns = concrete_data.columns predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength target = concrete_data['Strength'] # Strength column predictors.head() target.head() # Normalize Data (Scott always says this is really important) predictors_norm = (predictors - predictors.mean()) / predictors.std() predictors_norm.head() # Save predictors to n_cols since we need this for building the network n_cols = predictors_norm.shape[1] # number of predictors # define regression model def regression_model(): # create model model = Sequential() model.add(Dense(50, activation='relu', input_shape=(n_cols,))) model.add(Dense(50, activation='relu')) model.add(Dense(1)) # compile model model.compile(optimizer='adam', loss='mean_squared_error') return model # build the model model = regression_model() # Next, we will train and test the model at the same time using the *fit* method. We will leave out 30% of the data for validation and we will train the model for 100 epochs. # fit the model model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2) Classifications Models with Keras The MNIST database, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau. # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import keras from keras.models import Sequential from keras.layers import Dense from keras.utils import to_categorical from keras.models import load_model import pandas as pd import numpy as np import matplotlib.pyplot as plt # import the data from keras.datasets import mnist # read the data (X_train, y_train), (X_test, y_test) = mnist.load_data() X_train.shape # (60000, 28, 28) X_test.shape # (10000, 28, 28) plt.imshow(X_train[0]) plt.show() # flatten images into one-dimensional vector num_pixels = X_train.shape[1] * X_train.shape[2] # find size of one-dimensional vector X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # flatten training images X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # flatten test images # Normalize the vectors # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = to_categorical(y_train) y_test = to_categorical(y_test) num_classes = y_test.shape[1] print(num_classes) # Build a Neural Network # define classification model def classification_model(): # create model model = Sequential() model.add(Dense(num_pixels, activation='relu', input_shape=(num_pixels,))) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Train and Test the Network # build the model model = classification_model() # fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2) # evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1])) model.save('classification_model.h5') # or model.save('classification_model.keras') # Load Pre-trained Model pretrained_model = load_model('classification_model.h5') scores = pretrained_model.evaluate(X_test, y_test, verbose=0) print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))","title":"3. Keras and Deep Learning Libraries"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#keras-and-deep-learning-libraries","text":"","title":"Keras and Deep Learning Libraries"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#deep-learning-libraries","text":"Tensorflow - most popular IBM says...but this is pretty debatable...more github activity and enterprise use and can be debugged easier at large scales due to static graphs. Keras - Easiest API to use and the go-to library for quick prototyping and fast development. Pytorch - Cousin of Lua-based Torch framework. Strong Tensorflow competitor. Released 2016 and is becoming more popular than Tensorflow in academic settings generally.","title":"Deep Learning Libraries"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#regression-models-with-keras","text":"See Jupyter Notebook Example Let's take a look at a regression example. Here is a data set of the compressive strength of different samples of concrete based on the volumes of the different materials that were used to make them. So let's say we would like to use the Keras library to quickly build a deep neural network to model this dataset, and so we can automatically determine the compressive strength of a given concrete sample based on its ingredients. So let's say that the deep neural network that we would like to create takes all the eight features as input, feeds them into a hidden layer of five nodes, which is connected to another hidden layer of five nodes, which is then connected to the output layer with one node that is supposed to output the compressive strength of a given concrete sample. Note that usually you would go with a much larger number of neurons in each hidden layer like 50 or even 100, but we're just using a small network for simplicity. Notice how all the nodes in one layer are connected to all the other nodes in the next layer. Such a network is called a dense network . Before we begin using the Keras library, let's prepare our data and have it in the right format. The only thing we would need to do is to split the dataframe into two dataframes, one that has the predictors columns and another one that has the target column. We will also name them predictors and target.","title":"Regression Models with Keras"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#keras-code","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; There are two models in the Keras library. One of them is the Sequential model and the other one is the model class used with the functional API. So to create your model, you simply call the Sequential constructor. import keras from keras.models import Sequential from keras.layers import Dense # Because our network consists of a linear stack of layers, then the Sequential model is what you would want to use. This is the case most of the time unless you are building something out of the ordinary. model = Sequential() # n_cols = concrete_data.shape[1] # Assuming you have the csv # Now building your layers is pretty straightforward as well. For that, we would need to import the \"Dense\" type of layers from \"keras.layers\". Then we use the add method to add each dense layer. We specify the number of neurons in each layer and the activation function that we want to use. model.add(Dense(5, activation='relu', input_Shape=(n_cols,))) model.add(Dense(5, activation='relu')) model.add(Dense(1)) # Now for training, we need to define an optimizer and the error metric. # As for the minimization algorithm (optimizer), there are actually other more efficient algorithms than the gradient descent for deep learning applications. One of them is \"adam\". One of the main advantages of the \"adam\" optimizer is that you don't need to specify the learning rate that we saw in the gradient descent video. model.compile(optimizer='adam', loss='mean_squared_error') model.fit(predictors, target) predictors = model.predict(test_data)","title":"Keras Code"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#classification-models-with-keras","text":"Let's say that we would like to build a model that would inform someone whether purchasing a certain car would be a good choice based on the price of the car, the cost to maintain it, and whether it can accommodate two or more people. So, here is a dataset that we are calling \"car_data\".","title":"Classification Models with Keras"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#one-hot-encoding","text":"The data already has one-hot encoding to transform each category of price, maintenance, and how many people the car can accommodate, into separate columns. So the price of the car can be either high, medium, or low. Similarly, the cost of maintaining the car can also be high, medium, or low, and the car can either fit two people or more. If you take the first car in the dataset, it is considered an expensive car, has high maintenance cost, and can fit only two people. The decision is 0, meaning that buying this car would be a bad choice . A decision of 1 means that buying the car is acceptable , a decision of 2 means that buying the car would be a good decision , and a decision of 3 means that buying the car would be a very good decision . We will use the same neural network.","title":"One Hot Encoding"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#predictors-and-target","text":"However, with Keras, for classification problems, we can't use the target column as is; we actually need to transform the column into an array with binary values similar to one-hot encoding like the output shown here. We easily achieve that using the \"to_categorical\" function from the Keras utilities package. In other words, our model instead of having just one neuron in the output layer, it would have four neurons, since our target variable consists of four categories. In terms of code, the structure of our code is pretty similar to the one we use to build the model for our regression problem. We start by importing the Keras library and the Sequential model and we use it to construct our model. We also import the \"Dense\" layer since we will be using it to build our network. The additional import statement here is the \"to_categorical\" function in order to transform our target column into an array of binary numbers for classification. Then, we proceed to constructing our layers. We use the add method to create two hidden layers, each with five neurons and the neurons are activated using the ReLU activation function. Notice how here we also specify the softmax function as the activation function for the output layer, so that the sum of the predicted values from all the neurons in the output layer sum nicely to 1. Then in defining our compiler, here we will use the categorical cross-entropy as our loss measure instead of the mean squared error that we use for regression, and we will specify the evaluation metric to be \"accuracy\". \"accuracy\" is a built-in evaluation metric in Keras but you can actually define your own evaluation metric and pass it in the metrics parameter. Then we fit the model. Notice how this time we're specifying the number of epochs for training the model. Although we didn't specify the number of epochs when we built a regression model, but we could have done that. Finally, we use the predict method to make predictions. Now the output of the Keras predict method would be something like what's shown here. Now the output of the Keras predict method would be something like what's shown here. For each data point, the output is the probability that the decision of purchasing a given car belongs to one of the four classes. For each data point, the probabilities should sum to 1, and the higher the probability the more confident is the algorithm that a datapoint belongs to the respective class. So for the first data point or the first car in the test set, the decision would be 0 meaning not acceptable, since the first probability is the highest, with a value of 0.99 or close to 1, in this case. Similarly, for the second datapoint, the decision is also 0 or not acceptable, since the probability for this class is the highest, again with a value of 0.99 or almost 1. For the first three datapoints, the model is very confident that purchasing these cars is not acceptable. As for the last three datapoints, the decision would be 1 or acceptable, since the probabilities for the second class are higher than the rest of the classes. But notice how the probabilities for decision 0 and decision 1 are very close. Therefore, the model is not very confident but it would lean towards accepting purchasing these cars. In the lab part, you will get the chance to build your own regression and classification models using the Keras library, so make sure to complete this module's lab components.","title":"Predictors and Target"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#code-examples","text":"","title":"Code Examples"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#regressions-models-with-keras","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; We will be playing around with the same dataset that we used in the videos. The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include: Cement Blast Furnace Slag Fly Ash Water Superplasticizer Coarse Aggregate Fine Aggregate The target variable in this problem is the concrete sample strength . import pandas as pd import numpy as np import keras from keras.models import Sequential from keras.layers import Dense concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv') concrete_data.head(1) concrete_data.describe() # Split into Predictors and Targets concrete_data_columns = concrete_data.columns predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength target = concrete_data['Strength'] # Strength column predictors.head() target.head() # Normalize Data (Scott always says this is really important) predictors_norm = (predictors - predictors.mean()) / predictors.std() predictors_norm.head() # Save predictors to n_cols since we need this for building the network n_cols = predictors_norm.shape[1] # number of predictors # define regression model def regression_model(): # create model model = Sequential() model.add(Dense(50, activation='relu', input_shape=(n_cols,))) model.add(Dense(50, activation='relu')) model.add(Dense(1)) # compile model model.compile(optimizer='adam', loss='mean_squared_error') return model # build the model model = regression_model() # Next, we will train and test the model at the same time using the *fit* method. We will leave out 30% of the data for validation and we will train the model for 100 epochs. # fit the model model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)","title":"Regressions Models with Keras"},{"location":"notes/data_science/Coursera/keras/Keras_And_Deep_Learning_Libraries/#classifications-models-with-keras","text":"The MNIST database, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau. # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import keras from keras.models import Sequential from keras.layers import Dense from keras.utils import to_categorical from keras.models import load_model import pandas as pd import numpy as np import matplotlib.pyplot as plt # import the data from keras.datasets import mnist # read the data (X_train, y_train), (X_test, y_test) = mnist.load_data() X_train.shape # (60000, 28, 28) X_test.shape # (10000, 28, 28) plt.imshow(X_train[0]) plt.show() # flatten images into one-dimensional vector num_pixels = X_train.shape[1] * X_train.shape[2] # find size of one-dimensional vector X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # flatten training images X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # flatten test images # Normalize the vectors # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = to_categorical(y_train) y_test = to_categorical(y_test) num_classes = y_test.shape[1] print(num_classes) # Build a Neural Network # define classification model def classification_model(): # create model model = Sequential() model.add(Dense(num_pixels, activation='relu', input_shape=(num_pixels,))) model.add(Dense(100, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # compile model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model # Train and Test the Network # build the model model = classification_model() # fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2) # evaluate the model scores = model.evaluate(X_test, y_test, verbose=0) print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1])) model.save('classification_model.h5') # or model.save('classification_model.keras') # Load Pre-trained Model pretrained_model = load_model('classification_model.h5') scores = pretrained_model.evaluate(X_test, y_test, verbose=0) print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))","title":"Classifications Models with Keras"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/","text":"Classification Source: Machine_Learning_With_Python_IBM Supervised learning Unknown items into discrete categories Multiple inputs (values) to predict a single field (output column Decisions trees Naive bayes Linear discriminant analysis k-Nearsest neighbors Logistic regression Neural networks Support vector machines (SVM) KNN (K-Nearest Neighbors) independent variables: ( \\(x_n\\) ) target field: dependent var (y) - might have n classes Ex: \\(x_n\\) 's = age and income Literally cluster a bit...maybe 5 nearest neighbors. k is 5. Classify cases based on their similarity to other cases Cases that are near each other are said to be neighbors Based on similar cases with same class labels are near each other Pick a value for k. Calc distance to unknown case from allcases Select the k-obserservations ,............ Predict the response of the unknown data point using the most popular value response value from the K-nearest neighbors Ex: Customer1 = {age: 34} Customer2 = {age: 30} \\(Dis(x_1, x_2) = \\sqrt{\\sum_{i=0}^{n}(x_{1i}-x_{2i})^2}\\) \\(Dis(x_1, x_2) = \\sqrt{(34-30)^2}\\) n here is attributes and the 1 and 2 are neighbors 1 and 2. 2 independent variables (age and income): Customer1 = {age: 34, income: 190} Customer2 = {age: 30, income: 200} \\(Dis(x_1, x_2) = \\sqrt{(34-30)^2+(190-200)^2}\\) So, how do we choose the right K? A low value of K causes a highly complex model and causes overfitting. A high value is too general. We're not sure yet... loop over k and test accuracy Classification Accuracy Compare predictions \\(y\\) and actual \\(\\hat{y}\\) of test data set. Jaccard index \\(J(y, \\hat{y}) = \\frac{|y\\cap\\hat{y}|}{|y\\cup\\hat{y}|} = \\frac{|y\\cap\\hat{y}|}{|y|+|\\hat{y}|-|y\\cap\\hat{y}|}\\) F1-score Precision = TP / (TP + FP) Recall = TP / (TP + FN) F1-score = 2x(prc x rec)/(prc + rec) Log Loss Probability of label versus label itself. Only for binary output. This is related to logistic regression. \\(LogLoss = \\frac{1}{n}\\sum_{i=1}^{i}(y\\log(\\hat{y}) +(1-y)*\\log(1-\\hat{y}))\\) Looks like infinity towards 0 for x and 0 for correct prediction with 100% confidence. (y) 00|. | . | . | . | . | . | . | . 0| . ------------------------------(x) K-NN Code Example ML0101EN-Clas-K-Nearest-neighbors-CustCat.ipynb # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove import numpy as np import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn import preprocessing, metrics from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/teleCust1000t.csv') df.head() # classes in dataset (customer category) key = {'1':'Basic Service','2':'E Service','3':'Plus Service','4':'Total Service'} df['custcat'].value_counts() # normalize the data X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values #.astype(float) y = df['custcat'].values; y[0:5] X = preprocessing.StandardScaler().fit(X).transform(X.astype(float)); X[0:5] # train/test/split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape) # k-nearest neighbor k = 4 # train model and predict neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train); neigh # predicting yhat = neigh.predict(X_test); yhat[0:5] print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train))) print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat)) # practice k=6 # do over with this k neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train); neigh yhat = neigh.predict(X_test); yhat[0:5] print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train))) print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat)) metrics.accuracy_score is jaccard_score (jaccard index) # try even more Ks Ks = 10 mean_acc = np.zeros((Ks-1)) std_acc = np.zeros((Ks-1)) for n in range(1,Ks): #Train Model and Predict neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train) yhat=neigh.predict(X_test) mean_acc[n-1] = metrics.accuracy_score(y_test, yhat) # jaccards std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0]) # standard error # https://en.wikipedia.org/wiki/Standard_error mean_acc std_acc # plot plt.plot(range(1,Ks),mean_acc,'g') plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10) plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\") plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd')) plt.ylabel('Accuracy ') plt.xlabel('Number of Neighbors (K)') plt.tight_layout() plt.show() # best accuracy print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","title":"2. Classification"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#classification","text":"Source: Machine_Learning_With_Python_IBM Supervised learning Unknown items into discrete categories Multiple inputs (values) to predict a single field (output column Decisions trees Naive bayes Linear discriminant analysis k-Nearsest neighbors Logistic regression Neural networks Support vector machines (SVM)","title":"Classification"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#knn-k-nearest-neighbors","text":"independent variables: ( \\(x_n\\) ) target field: dependent var (y) - might have n classes Ex: \\(x_n\\) 's = age and income Literally cluster a bit...maybe 5 nearest neighbors. k is 5. Classify cases based on their similarity to other cases Cases that are near each other are said to be neighbors Based on similar cases with same class labels are near each other Pick a value for k. Calc distance to unknown case from allcases Select the k-obserservations ,............ Predict the response of the unknown data point using the most popular value response value from the K-nearest neighbors Ex: Customer1 = {age: 34} Customer2 = {age: 30} \\(Dis(x_1, x_2) = \\sqrt{\\sum_{i=0}^{n}(x_{1i}-x_{2i})^2}\\) \\(Dis(x_1, x_2) = \\sqrt{(34-30)^2}\\) n here is attributes and the 1 and 2 are neighbors 1 and 2. 2 independent variables (age and income): Customer1 = {age: 34, income: 190} Customer2 = {age: 30, income: 200} \\(Dis(x_1, x_2) = \\sqrt{(34-30)^2+(190-200)^2}\\)","title":"KNN (K-Nearest Neighbors)"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#so-how-do-we-choose-the-right-k","text":"A low value of K causes a highly complex model and causes overfitting. A high value is too general. We're not sure yet... loop over k and test accuracy","title":"So, how do we choose the right K?"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#classification-accuracy","text":"Compare predictions \\(y\\) and actual \\(\\hat{y}\\) of test data set.","title":"Classification Accuracy"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#jaccard-index","text":"\\(J(y, \\hat{y}) = \\frac{|y\\cap\\hat{y}|}{|y\\cup\\hat{y}|} = \\frac{|y\\cap\\hat{y}|}{|y|+|\\hat{y}|-|y\\cap\\hat{y}|}\\)","title":"Jaccard index"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#f1-score","text":"Precision = TP / (TP + FP) Recall = TP / (TP + FN) F1-score = 2x(prc x rec)/(prc + rec)","title":"F1-score"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#log-loss","text":"Probability of label versus label itself. Only for binary output. This is related to logistic regression. \\(LogLoss = \\frac{1}{n}\\sum_{i=1}^{i}(y\\log(\\hat{y}) +(1-y)*\\log(1-\\hat{y}))\\) Looks like infinity towards 0 for x and 0 for correct prediction with 100% confidence. (y) 00|. | . | . | . | . | . | . | . 0| . ------------------------------(x)","title":"Log Loss"},{"location":"notes/data_science/Coursera/machine_learning/Classification_IBM/#k-nn-code-example","text":"ML0101EN-Clas-K-Nearest-neighbors-CustCat.ipynb # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove import numpy as np import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn import preprocessing, metrics from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/teleCust1000t.csv') df.head() # classes in dataset (customer category) key = {'1':'Basic Service','2':'E Service','3':'Plus Service','4':'Total Service'} df['custcat'].value_counts() # normalize the data X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values #.astype(float) y = df['custcat'].values; y[0:5] X = preprocessing.StandardScaler().fit(X).transform(X.astype(float)); X[0:5] # train/test/split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape) # k-nearest neighbor k = 4 # train model and predict neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train); neigh # predicting yhat = neigh.predict(X_test); yhat[0:5] print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train))) print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat)) # practice k=6 # do over with this k neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train); neigh yhat = neigh.predict(X_test); yhat[0:5] print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train))) print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat)) metrics.accuracy_score is jaccard_score (jaccard index) # try even more Ks Ks = 10 mean_acc = np.zeros((Ks-1)) std_acc = np.zeros((Ks-1)) for n in range(1,Ks): #Train Model and Predict neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train) yhat=neigh.predict(X_test) mean_acc[n-1] = metrics.accuracy_score(y_test, yhat) # jaccards std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0]) # standard error # https://en.wikipedia.org/wiki/Standard_error mean_acc std_acc # plot plt.plot(range(1,Ks),mean_acc,'g') plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10) plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\") plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd')) plt.ylabel('Accuracy ') plt.xlabel('Number of Neighbors (K)') plt.tight_layout() plt.show() # best accuracy print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","title":"K-NN Code Example"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/","text":"Clustering Intro to Clustering Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy, as it allows the business to target specific groups of customers, so as to more effectively allocate marketing resources. For example, one group might contain customers who are high profit and low risk. That is, more likely to purchase products or subscribe for a service. Knowing this information allows a business to devote more time and attention to retaining these customers. Another group might include customers from nonprofit organizations and so on. Clustering means finding clusters in a dataset, unsupervised. So what is a cluster? A cluster is a group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to datapoints in other clusters. Now the question is,\" What is different between clustering and classification?\" Let's look at our customer dataset again. Classification algorithms predict categorical classed labels. This means assigning instances to predefined classes such as defaulted or not defaulted. For example, if an analyst wants to analyze customer data in order to know which customers might default on their payments, she uses a labeled dataset as training data and uses classification approaches such as a decision tree, Support Vector Machines or SVM, or logistic regression, to predict the default value for a new or unknown customer. Generally speaking, classification is a supervised learning where each training data instance belongs to a particular class. In clustering however, the data is unlabeled and the process is unsupervised. For example, we can use a clustering algorithm such as k-means to group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as; age, education, and so on. In the insurance industry, clustering is used for fraud detection in claims analysis, or to evaluate the insurance risk of certain customers based on their segments. In publication media, clustering is used to auto categorize news based on his content or to tag news, then cluster it so as to recommend similar news articles to readers. In medicine, it can be used to characterize patient behavior, based on their similar characteristics. So as to identify successful medical therapies for different illnesses or in biology, clustering is used to group genes with similar expression patterns or to cluster genetic markers to identify family ties. K-Means Clustering Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. One of the algorithms that can be used for customer segmentation is K-Means clustering. K-Means can group data only unsupervised based on the similarity of customers to each other. There are various types of clustering algorithms such as: Partitioning, hierarchical or density-based clustering. K-Means is a type of partitioning clustering, that is, it divides the data into K non-overlapping subsets or clusters without any cluster internal structure or labels. This means, it's an unsupervised algorithm. Objects within a cluster are very similar Objects across different clusters are very different or dissimilar. As you can see, for using K-Means we have to find similar samples: for example, similar customers. Now, we face a couple of key questions. First, how can we find the similarity of samples in clustering, and then how do we measure how similar two customers are with regard to their demographics? Though the objective of K-Means is to form clusters in such a way that similar samples go into a cluster, and dissimilar samples fall into different clusters, it can be shown that instead of a similarity metric, we can use dissimilarity metrics. In other words, conventionally the distance of samples from each other is used to shape the clusters. So we can say K-Means tries to minimize the intra-cluster distances and maximize the inter-cluster distances. Now, the question is, how can we calculate the dissimilarity or distance of two cases such as two customers? Assume that we have two customers, we will call them Customer one and two. Let's also assume that we have only one feature for each of these two customers and that feature is age. We can easily use a specific type of Minkowski distance to calculate the distance of these two customers. Indeed, it is the Euclidean distance. What about if we have more than one feature, for example, age and income? For example, if we have income and age for each customer, we can still use the same formula but this time in a two-dimensional space. Also, we can use the same distance matrix for multidimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used for this purpose, but it is highly dependent on datatype and also the domain that clustering is done for it. For example you may use Euclidean distance, cosine similarity, average distance, and so on. Indeed, the similarity measure highly controls how the clusters are formed, so it is recommended to understand the domain knowledge of your dataset and datatype of features and then choose the meaningful distance measurement. Now, let's see how K-Means clustering works. For the sake of simplicity, let's assume that our dataset has only two features: the age and income of customers. This means, it's a two-dimensional space. We can show the distribution of customers using a scatter plot: The Y-axis indicates age and the X-axis shows income of customers. We try to cluster the customer dataset into distinct groups or clusters based on these two dimensions. In the first step, we should determine the number of clusters. The key concept of the K-Means algorithm is that it randomly picks a center point for each cluster. It means we must initialize K which represents number of clusters. Essentially, determining the number of clusters in a dataset or K is a hard problem in K-Means, that we will discuss later. For now, let's put K equals three here for our sample dataset. It is like we have three representative points for our clusters. These three data points are called centroids of clusters and should be of same feature size of our customer feature set. There are two approaches to choose these centroids. One, we can randomly choose three observations out of the dataset and use these observations as the initial means. Or two, we can create three random points as centroids of the clusters which is our choice that is shown in the plot with red color. After the initialization step which was defining the centroid of each cluster, we have to assign each customer to the closest center. For this purpose, we have to calculate the distance of each data point or in our case each customer from the centroid points. As mentioned before, depending on the nature of the data and the purpose for which clustering is being used, different measures of distance may be used to place items into clusters. Therefore, you will form a matrix where each row represents the distance of a customer from each centroid. It is called the Distance Matrix. The main objective of K-Means clustering is to minimize the distance of data points from the centroid of this cluster and maximize the distance from other cluster centroids. So, in this step, we have to find the closest centroid to each data point. We can use the distance matrix to find the nearest centroid to datapoints. Finding the closest centroids for each data point, we assign each data point to that cluster . In other words, all the customers will fall to a cluster based on their distance from centroids. We can easily say that it does not result in good clusters because the centroids were chosen randomly from the first. Indeed, the model would have a high error. Here, error is the total distance of each point from its centroid. It can be shown as within-cluster sum of squares error. In case that was confusing, every dot is closest to one of the clusters mathematically. So assign every dot the category of the cluster center it is closest to. Redundant but hopefully helps. Intuitively, we try to reduce this error. It means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized. Now, the question is, how can we turn it into better clusters with less error? Okay, we move centroids. In the next step, each cluster center will be updated to be the mean for datapoints in its cluster. Indeed, each centroid moves according to their cluster members. In other words the centroid of each of the three clusters becomes the new mean. For example, if point A coordination is 7.4 and 3.6, and B point features are 7.8 and 3.8, the new centroid of this cluster with two points would be the average of them, which is 7.6 and 3.7. Now, we have new centroids. As you can guess, once again we will have to calculate the distance of all points from the new centroids. The points are reclustered and the centroids move again. This continues until the centroids no longer move. Please note that whenever a centroid moves, each point's distance to the centroid needs to be measured again. Yes, K-Means is an iterative algorithm and we have to repeat steps two to four until the algorithm converges. In each iteration, it will move the centroids, calculate the distances from new centroids and assign data points to the nearest centroid. It results in the clusters with minimum error or the most dense clusters. However, as it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum and the result may depend on the initial clusters. It means, this algorithm is guaranteed to converge to a result, but the result may be a local optimum i.e. not necessarily the best possible outcome. To solve this problem, it is common to run the whole process multiple times with different starting conditions. This means with randomized starting centroids, it may give a better outcome. As the algorithm is usually very fast, it wouldn't be any problem to run it multiple times. Thanks for watching this video. More On K-Means (Evaluate Accuracy) Euclidean distance is used to measure the distance from the object to the centroid. Please note, however, that you can also use different types of distance measurements, not just Euclidean distance. Euclidean distance is used because it's the most popular. Now, the questions is, how can we evaluate the goodness of the clusters formed by k-Means? In other words, how do we calculate the accuracy of k-Means clustering? One way is to compare the clusters with the ground truth, if it's available. However, because k-Means is an unsupervised algorithm we usually don't have ground truth in real world problems to be used. But there is still a way to say how bad each cluster is, based on the objective of the k-Means. This value is the average distance between data points within a cluster. Also, average of the distances of data points from their cluster centroids can be used as a metric of error for the clustering algorithm. Essentially, determining the number of clusters in a data set, or k as in the k-Means algorithm, is a frequent problem in data clustering. The correct choice of K is often ambiguous because it's very dependent on the shape and scale of the distribution of points in a dataset. There are some approaches to address this problem, but one of the techniques that is commonly used is to run the clustering across the different values of K and looking at a metric of accuracy for clustering. This metric can be mean, distance between data points and their cluster's centroid, which indicate how dense our clusters are or, to what extent we minimize the error of clustering. Then, looking at the change of this metric, we can find the best value for K. Play video starting at :2:49 and follow transcript2:49 But the problem is that with increasing the number of clusters, the distance of centroids to data points will always reduce. This means increasing K will always decrease the error. So, the value of the metric as a function of K is plotted and the elbow point is determined where the rate of decrease sharply shifts. It is the right K for clustering. This method is called the elbow method. So let's recap k-Means clustering: k-Means is a partition-based clustering which is A, relatively efficient on medium and large sized data sets; B, produces sphere-like clusters because the clusters are shaped around the centroids; and C, its drawback is that we should pre-specify the number of clusters, and this is not an easy task. Thanks for watching. Lab Generated Data (Blobs) # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # Data path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/Cust_Segmentation.csv' wget $path import random import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt np.random.seed(0) # Data path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/Cust_Segmentation.csv' Next we will be making random clusters of points by using the make_blobs class. The make_blobs class can take in many inputs, but we will be using these specific ones. Input: n_samples: 5000 - total num of points equally divided among clusters. center: [[4,4],[-2,-1],[2,-3],[1,1]] num of centers to generate, or the fixed center locations. * cluster_std: 0.9 the standard deviation of the clusters. Output: * X: Array of shape [n_samples, n_features]. (Feature Matrix) - generated samples. * y: Arayy of shape [n_samples] (Response Vector) - the integer labels for cluster membership of each sample. X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9) # plot plt.scatter(X[:, 0], X[:, 1], marker='.') plt.show() The KMeans class has many parameters that can be used, but we will be using these three: init : Initialization method of the centroids. Value will be: \"k-means++\" k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. n_clusters : The number of clusters to form as well as the number of centroids to generate. Value will be: 4 (since we have 4 centers) n_init : Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. Value will be: 12 k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12) # fit k_means.fit(X) Now let's grab the labels for each point in the model using KMeans' .labels_ attribute and save it as k_means_labels . k_means_labels = k_means.labels_ k_means_labels # 5000 points labeled We will also get the coordinates of the cluster centers using KMeans' .cluster_centers_ and save it as k_means_cluster_centers . k_means_cluster_centers = k_means.cluster_centers_ k_means_cluster_centers # Creating the visual plot def plot(): # Initialize the plot with the specified dimensions. fig = plt.figure(figsize=(6, 4)) # Colors uses a color map, which will produce an array of colors based on # the number of labels there are. We use set(k_means_labels) to get the # unique labels. colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels)))) # Create a plot ax = fig.add_subplot(1, 1, 1) # For loop that plots the data points and centroids. # k will range from 0-3, which will match the possible clusters that each # data point is in. for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors): # Create a list of all data points, where the data points that are # in the cluster (ex. cluster 0) are labeled as true, else they are # labeled as false. my_members = (k_means_labels == k) # Define the centroid, or cluster center. cluster_center = k_means_cluster_centers[k] # Plots the datapoints with color col. ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.') # Plots the centroids with specified color, but with a darker outline ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6) # Title of the plot ax.set_title('KMeans') # Remove x-axis ticks ax.set_xticks(()) # Remove y-axis ticks ax.set_yticks(()) # Show the plot plt.show() plot() Customer Segmentation Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data. Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retain those customers. Another group might include customers from non-profit organizations and so on. cust_df = pd.read_csv(\"Cust_Segmentation.csv\") cust_df.head() # Pre-processing (address is categorical; not applicable for k-means) df = cust_df.drop('Address', axis=1) df.head() # Normalizing over standard deviation ## Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use StandardScaler() to normalize our dataset. X = df.values[:,1:] # age and onwards...don't use CustomerId; not a feature X = np.nan_to_num(X) Clus_dataSet = StandardScaler().fit_transform(X) Clus_dataSet # Modeling clusterNum = 3 k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12) k_means.fit(X) labels = k_means.labels_ print(labels) # Insights (We assign the labels to each row in the dataframe.) df[\"Clus_km\"] = labels df.head(5) # Check centroids by averaging the features in each cluster ## Note Customer ID meaningless here... df.groupby('Clus_km').mean() # Look at distribution of customers based on their age and income: area = np.pi * ( X[:, 1])**2 plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(float), alpha=0.5) plt.xlabel('Age', fontsize=18) plt.ylabel('Income', fontsize=16) plt.show() # What is this? from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(1, figsize=(8, 6)) plt.clf() ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134) plt.cla() # plt.ylabel('Age', fontsize=18) # plt.xlabel('Income', fontsize=16) # plt.zlabel('Education', fontsize=16) ax.set_xlabel('Education') ax.set_ylabel('Age') ax.set_zlabel('Income') ax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(float)) plt.show() k-means will partition your customers into mutually exclusive groups, for example, into 3 clusters. The customers in each cluster are similar to each other demographically. Now we can create a profile for each group, considering the common characteristics of each cluster. For example, the 3 clusters can be: AFFLUENT, EDUCATED AND OLD AGED MIDDLE AGED AND MIDDLE INCOME YOUNG AND LOW INCOME","title":"8. Clustering"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#clustering","text":"","title":"Clustering"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#intro-to-clustering","text":"Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy, as it allows the business to target specific groups of customers, so as to more effectively allocate marketing resources. For example, one group might contain customers who are high profit and low risk. That is, more likely to purchase products or subscribe for a service. Knowing this information allows a business to devote more time and attention to retaining these customers. Another group might include customers from nonprofit organizations and so on. Clustering means finding clusters in a dataset, unsupervised. So what is a cluster? A cluster is a group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to datapoints in other clusters. Now the question is,\" What is different between clustering and classification?\" Let's look at our customer dataset again. Classification algorithms predict categorical classed labels. This means assigning instances to predefined classes such as defaulted or not defaulted. For example, if an analyst wants to analyze customer data in order to know which customers might default on their payments, she uses a labeled dataset as training data and uses classification approaches such as a decision tree, Support Vector Machines or SVM, or logistic regression, to predict the default value for a new or unknown customer. Generally speaking, classification is a supervised learning where each training data instance belongs to a particular class. In clustering however, the data is unlabeled and the process is unsupervised. For example, we can use a clustering algorithm such as k-means to group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as; age, education, and so on. In the insurance industry, clustering is used for fraud detection in claims analysis, or to evaluate the insurance risk of certain customers based on their segments. In publication media, clustering is used to auto categorize news based on his content or to tag news, then cluster it so as to recommend similar news articles to readers. In medicine, it can be used to characterize patient behavior, based on their similar characteristics. So as to identify successful medical therapies for different illnesses or in biology, clustering is used to group genes with similar expression patterns or to cluster genetic markers to identify family ties.","title":"Intro to Clustering"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#k-means-clustering","text":"Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. One of the algorithms that can be used for customer segmentation is K-Means clustering. K-Means can group data only unsupervised based on the similarity of customers to each other. There are various types of clustering algorithms such as: Partitioning, hierarchical or density-based clustering. K-Means is a type of partitioning clustering, that is, it divides the data into K non-overlapping subsets or clusters without any cluster internal structure or labels. This means, it's an unsupervised algorithm. Objects within a cluster are very similar Objects across different clusters are very different or dissimilar. As you can see, for using K-Means we have to find similar samples: for example, similar customers. Now, we face a couple of key questions. First, how can we find the similarity of samples in clustering, and then how do we measure how similar two customers are with regard to their demographics? Though the objective of K-Means is to form clusters in such a way that similar samples go into a cluster, and dissimilar samples fall into different clusters, it can be shown that instead of a similarity metric, we can use dissimilarity metrics. In other words, conventionally the distance of samples from each other is used to shape the clusters. So we can say K-Means tries to minimize the intra-cluster distances and maximize the inter-cluster distances. Now, the question is, how can we calculate the dissimilarity or distance of two cases such as two customers? Assume that we have two customers, we will call them Customer one and two. Let's also assume that we have only one feature for each of these two customers and that feature is age. We can easily use a specific type of Minkowski distance to calculate the distance of these two customers. Indeed, it is the Euclidean distance. What about if we have more than one feature, for example, age and income? For example, if we have income and age for each customer, we can still use the same formula but this time in a two-dimensional space. Also, we can use the same distance matrix for multidimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used for this purpose, but it is highly dependent on datatype and also the domain that clustering is done for it. For example you may use Euclidean distance, cosine similarity, average distance, and so on. Indeed, the similarity measure highly controls how the clusters are formed, so it is recommended to understand the domain knowledge of your dataset and datatype of features and then choose the meaningful distance measurement. Now, let's see how K-Means clustering works. For the sake of simplicity, let's assume that our dataset has only two features: the age and income of customers. This means, it's a two-dimensional space. We can show the distribution of customers using a scatter plot: The Y-axis indicates age and the X-axis shows income of customers. We try to cluster the customer dataset into distinct groups or clusters based on these two dimensions. In the first step, we should determine the number of clusters. The key concept of the K-Means algorithm is that it randomly picks a center point for each cluster. It means we must initialize K which represents number of clusters. Essentially, determining the number of clusters in a dataset or K is a hard problem in K-Means, that we will discuss later. For now, let's put K equals three here for our sample dataset. It is like we have three representative points for our clusters. These three data points are called centroids of clusters and should be of same feature size of our customer feature set. There are two approaches to choose these centroids. One, we can randomly choose three observations out of the dataset and use these observations as the initial means. Or two, we can create three random points as centroids of the clusters which is our choice that is shown in the plot with red color. After the initialization step which was defining the centroid of each cluster, we have to assign each customer to the closest center. For this purpose, we have to calculate the distance of each data point or in our case each customer from the centroid points. As mentioned before, depending on the nature of the data and the purpose for which clustering is being used, different measures of distance may be used to place items into clusters. Therefore, you will form a matrix where each row represents the distance of a customer from each centroid. It is called the Distance Matrix. The main objective of K-Means clustering is to minimize the distance of data points from the centroid of this cluster and maximize the distance from other cluster centroids. So, in this step, we have to find the closest centroid to each data point. We can use the distance matrix to find the nearest centroid to datapoints. Finding the closest centroids for each data point, we assign each data point to that cluster . In other words, all the customers will fall to a cluster based on their distance from centroids. We can easily say that it does not result in good clusters because the centroids were chosen randomly from the first. Indeed, the model would have a high error. Here, error is the total distance of each point from its centroid. It can be shown as within-cluster sum of squares error. In case that was confusing, every dot is closest to one of the clusters mathematically. So assign every dot the category of the cluster center it is closest to. Redundant but hopefully helps. Intuitively, we try to reduce this error. It means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized. Now, the question is, how can we turn it into better clusters with less error? Okay, we move centroids. In the next step, each cluster center will be updated to be the mean for datapoints in its cluster. Indeed, each centroid moves according to their cluster members. In other words the centroid of each of the three clusters becomes the new mean. For example, if point A coordination is 7.4 and 3.6, and B point features are 7.8 and 3.8, the new centroid of this cluster with two points would be the average of them, which is 7.6 and 3.7. Now, we have new centroids. As you can guess, once again we will have to calculate the distance of all points from the new centroids. The points are reclustered and the centroids move again. This continues until the centroids no longer move. Please note that whenever a centroid moves, each point's distance to the centroid needs to be measured again. Yes, K-Means is an iterative algorithm and we have to repeat steps two to four until the algorithm converges. In each iteration, it will move the centroids, calculate the distances from new centroids and assign data points to the nearest centroid. It results in the clusters with minimum error or the most dense clusters. However, as it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum and the result may depend on the initial clusters. It means, this algorithm is guaranteed to converge to a result, but the result may be a local optimum i.e. not necessarily the best possible outcome. To solve this problem, it is common to run the whole process multiple times with different starting conditions. This means with randomized starting centroids, it may give a better outcome. As the algorithm is usually very fast, it wouldn't be any problem to run it multiple times. Thanks for watching this video.","title":"K-Means Clustering"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#more-on-k-means-evaluate-accuracy","text":"Euclidean distance is used to measure the distance from the object to the centroid. Please note, however, that you can also use different types of distance measurements, not just Euclidean distance. Euclidean distance is used because it's the most popular. Now, the questions is, how can we evaluate the goodness of the clusters formed by k-Means? In other words, how do we calculate the accuracy of k-Means clustering? One way is to compare the clusters with the ground truth, if it's available. However, because k-Means is an unsupervised algorithm we usually don't have ground truth in real world problems to be used. But there is still a way to say how bad each cluster is, based on the objective of the k-Means. This value is the average distance between data points within a cluster. Also, average of the distances of data points from their cluster centroids can be used as a metric of error for the clustering algorithm. Essentially, determining the number of clusters in a data set, or k as in the k-Means algorithm, is a frequent problem in data clustering. The correct choice of K is often ambiguous because it's very dependent on the shape and scale of the distribution of points in a dataset. There are some approaches to address this problem, but one of the techniques that is commonly used is to run the clustering across the different values of K and looking at a metric of accuracy for clustering. This metric can be mean, distance between data points and their cluster's centroid, which indicate how dense our clusters are or, to what extent we minimize the error of clustering. Then, looking at the change of this metric, we can find the best value for K. Play video starting at :2:49 and follow transcript2:49 But the problem is that with increasing the number of clusters, the distance of centroids to data points will always reduce. This means increasing K will always decrease the error. So, the value of the metric as a function of K is plotted and the elbow point is determined where the rate of decrease sharply shifts. It is the right K for clustering. This method is called the elbow method. So let's recap k-Means clustering: k-Means is a partition-based clustering which is A, relatively efficient on medium and large sized data sets; B, produces sphere-like clusters because the clusters are shaped around the centroids; and C, its drawback is that we should pre-specify the number of clusters, and this is not an easy task. Thanks for watching.","title":"More On K-Means (Evaluate Accuracy)"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#lab","text":"","title":"Lab"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#generated-data-blobs","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # Data path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/Cust_Segmentation.csv' wget $path import random import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt np.random.seed(0) # Data path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/Cust_Segmentation.csv' Next we will be making random clusters of points by using the make_blobs class. The make_blobs class can take in many inputs, but we will be using these specific ones. Input: n_samples: 5000 - total num of points equally divided among clusters. center: [[4,4],[-2,-1],[2,-3],[1,1]] num of centers to generate, or the fixed center locations. * cluster_std: 0.9 the standard deviation of the clusters. Output: * X: Array of shape [n_samples, n_features]. (Feature Matrix) - generated samples. * y: Arayy of shape [n_samples] (Response Vector) - the integer labels for cluster membership of each sample. X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9) # plot plt.scatter(X[:, 0], X[:, 1], marker='.') plt.show() The KMeans class has many parameters that can be used, but we will be using these three: init : Initialization method of the centroids. Value will be: \"k-means++\" k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. n_clusters : The number of clusters to form as well as the number of centroids to generate. Value will be: 4 (since we have 4 centers) n_init : Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. Value will be: 12 k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12) # fit k_means.fit(X) Now let's grab the labels for each point in the model using KMeans' .labels_ attribute and save it as k_means_labels . k_means_labels = k_means.labels_ k_means_labels # 5000 points labeled We will also get the coordinates of the cluster centers using KMeans' .cluster_centers_ and save it as k_means_cluster_centers . k_means_cluster_centers = k_means.cluster_centers_ k_means_cluster_centers # Creating the visual plot def plot(): # Initialize the plot with the specified dimensions. fig = plt.figure(figsize=(6, 4)) # Colors uses a color map, which will produce an array of colors based on # the number of labels there are. We use set(k_means_labels) to get the # unique labels. colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels)))) # Create a plot ax = fig.add_subplot(1, 1, 1) # For loop that plots the data points and centroids. # k will range from 0-3, which will match the possible clusters that each # data point is in. for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors): # Create a list of all data points, where the data points that are # in the cluster (ex. cluster 0) are labeled as true, else they are # labeled as false. my_members = (k_means_labels == k) # Define the centroid, or cluster center. cluster_center = k_means_cluster_centers[k] # Plots the datapoints with color col. ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.') # Plots the centroids with specified color, but with a darker outline ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6) # Title of the plot ax.set_title('KMeans') # Remove x-axis ticks ax.set_xticks(()) # Remove y-axis ticks ax.set_yticks(()) # Show the plot plt.show() plot()","title":"Generated Data (Blobs)"},{"location":"notes/data_science/Coursera/machine_learning/Clustering/#customer-segmentation","text":"Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data. Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retain those customers. Another group might include customers from non-profit organizations and so on. cust_df = pd.read_csv(\"Cust_Segmentation.csv\") cust_df.head() # Pre-processing (address is categorical; not applicable for k-means) df = cust_df.drop('Address', axis=1) df.head() # Normalizing over standard deviation ## Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use StandardScaler() to normalize our dataset. X = df.values[:,1:] # age and onwards...don't use CustomerId; not a feature X = np.nan_to_num(X) Clus_dataSet = StandardScaler().fit_transform(X) Clus_dataSet # Modeling clusterNum = 3 k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12) k_means.fit(X) labels = k_means.labels_ print(labels) # Insights (We assign the labels to each row in the dataframe.) df[\"Clus_km\"] = labels df.head(5) # Check centroids by averaging the features in each cluster ## Note Customer ID meaningless here... df.groupby('Clus_km').mean() # Look at distribution of customers based on their age and income: area = np.pi * ( X[:, 1])**2 plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(float), alpha=0.5) plt.xlabel('Age', fontsize=18) plt.ylabel('Income', fontsize=16) plt.show() # What is this? from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(1, figsize=(8, 6)) plt.clf() ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134) plt.cla() # plt.ylabel('Age', fontsize=18) # plt.xlabel('Income', fontsize=16) # plt.zlabel('Education', fontsize=16) ax.set_xlabel('Education') ax.set_ylabel('Age') ax.set_zlabel('Income') ax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(float)) plt.show() k-means will partition your customers into mutually exclusive groups, for example, into 3 clusters. The customers in each cluster are similar to each other demographically. Now we can create a profile for each group, considering the common characteristics of each cluster. For example, the 3 clusters can be: AFFLUENT, EDUCATED AND OLD AGED MIDDLE AGED AND MIDDLE INCOME YOUNG AND LOW INCOME","title":"Customer Segmentation"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/","text":"Decision Trees Source: Machine_Learning_With_Python_IBM What Is A Decision Tree Image a csv with patients and demographics and the target is the drug that each patient responded to. Part of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. patient_id age sex BP cholesterol drug p1 Young F High Normal Drug A p2 Young F High High Drug A p3 Middle-age F High Normal Drug B p4 Senior F Young Normal Drug B Note \"B\" is drug B and \"A\" is drug A. If Middle-age recommend drug B If either Young or Senior proceed to that branch and ask another relevant question...and so on Building Decision Trees Refer to above dataset again. Use recursive partitioning to classify the data. We need to determine which attribute is the best or more predictive to split data based on the feature. Let's choose Cholesterol. If patient has high cholesterol we can't say with high confidence that drugB might be suitable. Same for normal. Let's try sex. It is seems much better but not perfect. Let's got a step further For the male patient branch, we again test other attributes to split the sub-tree. We test cholesterol again here, as you can see it results in even more pure leaves. So we can easily make a decision here. For example, if a patient is male and his cholesterol is high, we can certainly prescribe drug A, but if it is normal, we can prescribe drug B with high confidence. As you might notice, the choice of attribute to split data is very important and it is all about purity of the leaves after the split. A node in the tree is considered pure if in 100 percent of the cases, the nodes fall into a specific category of the target field. Entropy Impurity of nodes is calculated by entropy of data in the node. What is entropy? Entropy is the amount of information disorder or the amount of randomness in the data. The entropy in the node depends on how much random data is in that node and is calculated for each node. In decision trees, we're looking for trees that have the smallest entropy in their nodes. {\"DrugA\": 0, \"DrugB\": 8 } -> Entropy is 0 {\"DrugA\": 1, \"DrugB\": 7 } -> Entropy is Low {\"DrugA\": 3, \"DrugB\": 5 } -> Entropy is High {\"DrugA\": 4, \"DrugB\": 4 } -> Entropy is 1 The lower the entropy , the less uniform the distribution. \\(Entropy = -p(A)*log_2(p(A)) - p(B)*log_2(p(B))\\) \\(p(A)\\) is the probability of drug A and is also x on the graph below For reference: \\(y(x)=log_2(x)\\) | | |----0.1----0.2----0.3----0.4----0.5----0.6----0.7----0.8----0.9---- . ---- | . -1| . | . | . -2| . | . | . -3| . Calculate entropy before splitting it After split Another node, which has the least entropy? The answer is the tree with the higher information gain after splitting. Information Gain Information gain is the information that can increase the level of certainty after splitting. It is the entropy of a tree before the split minus the weighted entropy after the split by an attribute. We can think of information gain and entropy as opposites. As entropy or the amount of randomness decreases, the information gain or amount of certainty increases and vice versa. \\(Information Gain = (EntropyBeforeSplit) - (WeightedEntropyAfterSplit)\\) Ex: Weights come from each node. For each decision, count total observations of that decision over total observations from both decisions What we learn is that we should choose sex first when choosing between sex and cholesterol. How do we choose the next branch? Well, as you can guess, we should repeat the process for each branch and test each of the other attributes to continue to reach the most pure leaves. Decision Tree Example 1 (Drug) # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove wget -o drug200.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv import numpy as np import pandas as pd from sklearn.tree import DecisionTreeClassifier import sklearn.tree as tree from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn import metrics import matplotlib.pyplot as plt path=\"drug200.csv\" my_data = pd.read_csv(\"drug200.csv\", delimiter=\",\") my_data[0:5] # Remove the column containing the target name since it doesn't contain numeric values. X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values X[0:5] # As you may figure out, some features in this dataset are categorical, such as __Sex__ or __BP__. Unfortunately, Sklearn Decision Trees does not handle categorical variables. We can still convert these features to numerical values using __LabelEncoder__ to convert the categorical variable into numerical variables. le_sex = preprocessing.LabelEncoder() le_sex.fit(['F','M']) X[:,1] = le_sex.transform(X[:,1]) le_BP = preprocessing.LabelEncoder() le_BP.fit([ 'LOW', 'NORMAL', 'HIGH']) X[:,2] = le_BP.transform(X[:,2]) le_Chol = preprocessing.LabelEncoder() le_Chol.fit([ 'NORMAL', 'HIGH']) X[:,3] = le_Chol.transform(X[:,3]) X[0:5] # Now we can fill the target variable. y = my_data[\"Drug\"] y[0:5] # We will be using train\\test split on our decision tree. # train_test_split will return 4 different parameters. We will name them X_trainset, X_testset, y_trainset, y_testset X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3) X_trainset.shape y_trainset.shape X_testset.shape y_testset.shape # We will first create an instance of the DecisionTreeClassifier called drugTree. # Inside of the classifier, specify criterion='entropy' so we can see the information gain of each node. drugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) drugTree # it shows the default parameters # Next, we will fit the data with the training feature matrix <b> X_trainset </b> and training response vector <b> y_trainset </b> drugTree.fit(X_trainset,y_trainset) # Let's make some predictions on the testing dataset and store it into a variable called predTree. predTree = drugTree.predict(X_testset) print (predTree [0:5]) print (y_testset [0:5]) # Evaluation: # use metrics and check accuracy of our model print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_testset, predTree)) # Visualization tree.plot_tree(drugTree) plt.show() Decision Tree Example 2 (Credit Card Fraud) classification_tree_svm.ipynb We will be exploring a decision tree and a support vector machine (SVM) to recognize fraudulent credit card transactions. You will use the trained model to assess if a credit card transaction is legitimate or not. After completing this lab you will be able to: * Perform basic data preprocessing in Python * Model a classification task using the Scikit-Learn and Snap ML Python APIs * Train Suppport Vector Machine and Decision Tree models using Scikit-Learn and Snap ML * Run inference and assess the quality of the trained models A transaction belongs to the positive class (1) if it is a fraud, otherwise it belongs to the negative class (0). You have access to transactions that occured over a certain period of time. The majority of the transactions are normally legitimate and only a small fraction are non-legitimate. Thus, typically you have access to a dataset that is highly unbalanced. This is also the case of the current dataset: only 492 transactions out of 284,807 are fraudulent (the positive class - the frauds - accounts for 0.172% of all transactions). To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset. # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove import opendatasets as od # download the dataset (this is a Kaggle dataset) # during download you will be required to input your Kaggle username and password od.download(\"https://www.kaggle.com/mlg-ulb/creditcardfraud\") # Import the libraries we need to use in this lab from __future__ import print_function import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import normalize, StandardScaler from sklearn.utils.class_weight import compute_sample_weight from sklearn.metrics import roc_auc_score import time import warnings # import the Decision Tree Classifier Model from scikit-learn from sklearn.tree import DecisionTreeClassifier # import the linear Support Vector Machine (SVM) model from Scikit-Learn from sklearn.svm import LinearSVC # import the hinge_loss metric from scikit-learn from sklearn.metrics import hinge_loss warnings.filterwarnings('ignore') # read the input data raw_data = pd.read_csv('creditcardfraud/creditcard.csv') print(\"There are \" + str(len(raw_data)) + \" observations in the credit card fraud dataset.\") print(\"There are \" + str(len(raw_data.columns)) + \" variables in the dataset.\") # display the first rows in the dataset raw_data.head() # In practice, a financial institution may have access to a much larger dataset of transactions. To simulate such a case, we will inflate the original one 10 times. n_replicas = 10 # inflate the original dataset big_raw_data = pd.DataFrame(np.repeat(raw_data.values, n_replicas, axis=0), columns=raw_data.columns) print(\"There are \" + str(len(big_raw_data)) + \" observations in the inflated credit card fraud dataset.\") print(\"There are \" + str(len(big_raw_data.columns)) + \" variables in the dataset.\") # display first rows in the new dataset big_raw_data.head() big_raw_data.shape # 'Class' is the target variable (1-fraud 0-otherwise) # get the set of distinct classes labels = big_raw_data.Class.unique() # get the count of each class sizes = big_raw_data.Class.value_counts().values # plot the class value counts fig, ax = plt.subplots() ax.pie(sizes, labels=labels, autopct='%1.3f%%') ax.set_title('Target Variable Value Counts') plt.show() # generate historgram of amounts spent per transaction big_raw_data['Amount'].hist(bins=75) plt.show() big_raw_data['Amount'].describe() # Data preprocessing such as scaling/normalization is typically useful for # linear models to accelerate the training convergence. # Standardize features by removing the mean and scaling to unit variance. big_raw_data.iloc[:, 1:30] = StandardScaler().fit_transform(big_raw_data.iloc[:, 1:30]) data_matrix = big_raw_data.values big_raw_data.head() # X: feature matrix (for this analysis, we exclude the Time variable from the dataset) X = data_matrix[:, 1:30] # y: labels vector y = data_matrix[:, 30] # data normalization X = normalize(X, norm=\"l1\") # print the shape of the features matrix and the labels vector print('X.shape=', X.shape, 'y.shape=', y.shape) # train\\test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape) print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape) # Compute the sample weights to be used as input to the train routine so that # it takes into account the class imbalance present in this dataset. w_train = compute_sample_weight('balanced', y_train) # for reproducible output across multiple function calls, set random_state to a given integer value sklearn_dt = DecisionTreeClassifier(max_depth=4, random_state=35) # train a Decision Tree Classifier using scikit-learn t0 = time.time() sklearn_dt.fit(X_train, y_train, sample_weight=w_train) sklearn_time = time.time()-t0 print(\"[Scikit-Learn] Training time (s): {0:.5f}\".format(sklearn_time)) # run inference and compute the probabilities of the test samples # to belong to the class of fraudulent transactions sklearn_pred = sklearn_dt.predict_proba(X_test)[:,1] # evaluate the Compute Area Under the Receiver Operating Characteristic # Curve (ROC-AUC) score from the predictions sklearn_roc_auc = roc_auc_score(y_test, sklearn_pred) print('[Scikit-Learn] ROC-AUC score : {0:.3f}'.format(sklearn_roc_auc)) # instatiate a scikit-learn SVM model # to indicate the class imbalance at fit time, set class_weight='balanced' # for reproducible output across multiple function calls, set random_state to a given integer value sklearn_svm = LinearSVC(class_weight='balanced', random_state=31, loss=\"hinge\", fit_intercept=False) # train a linear Support Vector Machine model using Scikit-Learn t0 = time.time() sklearn_svm.fit(X_train, y_train) sklearn_time = time.time() - t0 print(\"[Scikit-Learn] Training time (s): {0:.2f}\".format(sklearn_time)) # run inference using the Scikit-Learn model # get the confidence scores for the test samples sklearn_pred = sklearn_svm.decision_function(X_test) # evaluate accuracy on test set acc_sklearn = roc_auc_score(y_test, sklearn_pred) print(\"[Scikit-Learn] ROC-AUC score: {0:.3f}\".format(acc_sklearn)) Note: Decision function is a method present in classifier{ SVC, Logistic Regression } class of sklearn machine learning framework. This method basically returns a Numpy array, In which each element represents whether a predicted sample for x_test by the classifier lies to the right or left side of the Hyperplane and also how far from the HyperPlane. Decision Tree Versus Support Vector Machine Chat GPT Prompt: What are the primary similarities and differences between decision trees and support vector machines? Decision trees and support vector machines (SVMs) are both machine learning algorithms used for classification and regression tasks, but they have different approaches and characteristics. Here are the primary similarities and differences between decision trees and support vector machines: Similarities: Supervised Learning: Both decision trees and SVMs are supervised learning algorithms, which means they require labeled training data to make predictions. Classification and Regression: Both algorithms can be used for both classification and regression tasks. Decision trees are versatile in this regard, while SVMs are often used for classification but can also be adapted for regression. Differences: Algorithmic Approach: Decision Trees: Decision trees use a tree-like structure to make decisions. They recursively split the data into subsets based on the features that provide the most information gain or reduction in impurity (e.g., Gini impurity or entropy). Support Vector Machines: SVMs aim to find a hyperplane that best separates data points into different classes while maximizing the margin (distance) between the hyperplane and the nearest data points (support vectors). Linearity: Decision Trees: Decision trees can handle both linear and non-linear relationships between features and the target variable. They can model complex decision boundaries. Support Vector Machines: SVMs are inherently designed for linear separation, but they can be extended to handle non-linear data by using kernel tricks (e.g., polynomial kernels or radial basis function kernels). Interpretability: Decision Trees: Decision trees are highly interpretable. You can easily visualize the decision-making process as a tree, making it easier to understand how the model arrived at a particular prediction. Support Vector Machines: SVMs are less interpretable by nature, as the decision boundary is defined by a hyperplane in a high-dimensional space, which is not as intuitive as a decision tree. Overfitting: Decision Trees: Decision trees are prone to overfitting when they become too deep and complex. Techniques like pruning and setting maximum depth can help mitigate this issue. Support Vector Machines: SVMs are less prone to overfitting, especially when using a large margin (C parameter) and appropriate kernel functions. They tend to generalize well to new data. Handling Imbalanced Data: Decision Trees: Decision trees can be biased towards the majority class in imbalanced datasets, as they tend to choose the splits that minimize impurity, which may not be ideal for the minority class. Support Vector Machines: SVMs can handle imbalanced data better because they aim to maximize the margin and are less influenced by class distribution. Computation and Memory Requirements: Decision trees are generally computationally less expensive and require less memory compared to SVMs, especially when dealing with large datasets. In summary, the choice between decision trees and SVMs depends on the specific characteristics of your dataset and the interpretability requirements. Decision trees are often favored for their simplicity and interpretability, while SVMs are powerful for cases where finding a well-defined margin is essential or when dealing with high-dimensional data. Practice In this section you will evaluate the quality of the SVM models trained above using the hinge loss metric (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html). Run inference on the test set using both Scikit-Learn and Snap ML models. Compute the hinge loss metric for both sets of predictions. Print the hinge losses of Scikit-Learn and Snap ML. # get the confidence scores for the test samples sklearn_pred = sklearn_svm.decision_function(X_test) # evaluate the hinge loss metric from the predictions loss_sklearn = hinge_loss(y_test, sklearn_pred) print(\"[Scikit-Learn] Hinge loss: {0:.3f}\".format(loss_sklearn)) Hinge Loss Hinge loss is a function popularly used in support vector machine algorithms to measure the distance of data points from the decision boundary. This helps approximate the possibility of incorrect predictions and evaluate the model's performance.","title":"3. Decision Trees"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#decision-trees","text":"Source: Machine_Learning_With_Python_IBM","title":"Decision Trees"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#what-is-a-decision-tree","text":"Image a csv with patients and demographics and the target is the drug that each patient responded to. Part of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. patient_id age sex BP cholesterol drug p1 Young F High Normal Drug A p2 Young F High High Drug A p3 Middle-age F High Normal Drug B p4 Senior F Young Normal Drug B Note \"B\" is drug B and \"A\" is drug A. If Middle-age recommend drug B If either Young or Senior proceed to that branch and ask another relevant question...and so on","title":"What Is A Decision Tree"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#building-decision-trees","text":"Refer to above dataset again. Use recursive partitioning to classify the data. We need to determine which attribute is the best or more predictive to split data based on the feature. Let's choose Cholesterol. If patient has high cholesterol we can't say with high confidence that drugB might be suitable. Same for normal. Let's try sex. It is seems much better but not perfect. Let's got a step further For the male patient branch, we again test other attributes to split the sub-tree. We test cholesterol again here, as you can see it results in even more pure leaves. So we can easily make a decision here. For example, if a patient is male and his cholesterol is high, we can certainly prescribe drug A, but if it is normal, we can prescribe drug B with high confidence. As you might notice, the choice of attribute to split data is very important and it is all about purity of the leaves after the split. A node in the tree is considered pure if in 100 percent of the cases, the nodes fall into a specific category of the target field.","title":"Building Decision Trees"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#entropy","text":"Impurity of nodes is calculated by entropy of data in the node. What is entropy? Entropy is the amount of information disorder or the amount of randomness in the data. The entropy in the node depends on how much random data is in that node and is calculated for each node. In decision trees, we're looking for trees that have the smallest entropy in their nodes. {\"DrugA\": 0, \"DrugB\": 8 } -> Entropy is 0 {\"DrugA\": 1, \"DrugB\": 7 } -> Entropy is Low {\"DrugA\": 3, \"DrugB\": 5 } -> Entropy is High {\"DrugA\": 4, \"DrugB\": 4 } -> Entropy is 1 The lower the entropy , the less uniform the distribution. \\(Entropy = -p(A)*log_2(p(A)) - p(B)*log_2(p(B))\\) \\(p(A)\\) is the probability of drug A and is also x on the graph below For reference: \\(y(x)=log_2(x)\\) | | |----0.1----0.2----0.3----0.4----0.5----0.6----0.7----0.8----0.9---- . ---- | . -1| . | . | . -2| . | . | . -3| . Calculate entropy before splitting it After split Another node, which has the least entropy? The answer is the tree with the higher information gain after splitting.","title":"Entropy"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#information-gain","text":"Information gain is the information that can increase the level of certainty after splitting. It is the entropy of a tree before the split minus the weighted entropy after the split by an attribute. We can think of information gain and entropy as opposites. As entropy or the amount of randomness decreases, the information gain or amount of certainty increases and vice versa. \\(Information Gain = (EntropyBeforeSplit) - (WeightedEntropyAfterSplit)\\) Ex: Weights come from each node. For each decision, count total observations of that decision over total observations from both decisions What we learn is that we should choose sex first when choosing between sex and cholesterol. How do we choose the next branch? Well, as you can guess, we should repeat the process for each branch and test each of the other attributes to continue to reach the most pure leaves.","title":"Information Gain"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#decision-tree-example-1-drug","text":"# Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove wget -o drug200.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv import numpy as np import pandas as pd from sklearn.tree import DecisionTreeClassifier import sklearn.tree as tree from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn import metrics import matplotlib.pyplot as plt path=\"drug200.csv\" my_data = pd.read_csv(\"drug200.csv\", delimiter=\",\") my_data[0:5] # Remove the column containing the target name since it doesn't contain numeric values. X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values X[0:5] # As you may figure out, some features in this dataset are categorical, such as __Sex__ or __BP__. Unfortunately, Sklearn Decision Trees does not handle categorical variables. We can still convert these features to numerical values using __LabelEncoder__ to convert the categorical variable into numerical variables. le_sex = preprocessing.LabelEncoder() le_sex.fit(['F','M']) X[:,1] = le_sex.transform(X[:,1]) le_BP = preprocessing.LabelEncoder() le_BP.fit([ 'LOW', 'NORMAL', 'HIGH']) X[:,2] = le_BP.transform(X[:,2]) le_Chol = preprocessing.LabelEncoder() le_Chol.fit([ 'NORMAL', 'HIGH']) X[:,3] = le_Chol.transform(X[:,3]) X[0:5] # Now we can fill the target variable. y = my_data[\"Drug\"] y[0:5] # We will be using train\\test split on our decision tree. # train_test_split will return 4 different parameters. We will name them X_trainset, X_testset, y_trainset, y_testset X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3) X_trainset.shape y_trainset.shape X_testset.shape y_testset.shape # We will first create an instance of the DecisionTreeClassifier called drugTree. # Inside of the classifier, specify criterion='entropy' so we can see the information gain of each node. drugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) drugTree # it shows the default parameters # Next, we will fit the data with the training feature matrix <b> X_trainset </b> and training response vector <b> y_trainset </b> drugTree.fit(X_trainset,y_trainset) # Let's make some predictions on the testing dataset and store it into a variable called predTree. predTree = drugTree.predict(X_testset) print (predTree [0:5]) print (y_testset [0:5]) # Evaluation: # use metrics and check accuracy of our model print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_testset, predTree)) # Visualization tree.plot_tree(drugTree) plt.show()","title":"Decision Tree Example 1 (Drug)"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#decision-tree-example-2-credit-card-fraud","text":"classification_tree_svm.ipynb We will be exploring a decision tree and a support vector machine (SVM) to recognize fraudulent credit card transactions. You will use the trained model to assess if a credit card transaction is legitimate or not. After completing this lab you will be able to: * Perform basic data preprocessing in Python * Model a classification task using the Scikit-Learn and Snap ML Python APIs * Train Suppport Vector Machine and Decision Tree models using Scikit-Learn and Snap ML * Run inference and assess the quality of the trained models A transaction belongs to the positive class (1) if it is a fraud, otherwise it belongs to the negative class (0). You have access to transactions that occured over a certain period of time. The majority of the transactions are normally legitimate and only a small fraction are non-legitimate. Thus, typically you have access to a dataset that is highly unbalanced. This is also the case of the current dataset: only 492 transactions out of 284,807 are fraudulent (the positive class - the frauds - accounts for 0.172% of all transactions). To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset. # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; # cd ~/Desktop; rm -r temp; # To remove import opendatasets as od # download the dataset (this is a Kaggle dataset) # during download you will be required to input your Kaggle username and password od.download(\"https://www.kaggle.com/mlg-ulb/creditcardfraud\") # Import the libraries we need to use in this lab from __future__ import print_function import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import normalize, StandardScaler from sklearn.utils.class_weight import compute_sample_weight from sklearn.metrics import roc_auc_score import time import warnings # import the Decision Tree Classifier Model from scikit-learn from sklearn.tree import DecisionTreeClassifier # import the linear Support Vector Machine (SVM) model from Scikit-Learn from sklearn.svm import LinearSVC # import the hinge_loss metric from scikit-learn from sklearn.metrics import hinge_loss warnings.filterwarnings('ignore') # read the input data raw_data = pd.read_csv('creditcardfraud/creditcard.csv') print(\"There are \" + str(len(raw_data)) + \" observations in the credit card fraud dataset.\") print(\"There are \" + str(len(raw_data.columns)) + \" variables in the dataset.\") # display the first rows in the dataset raw_data.head() # In practice, a financial institution may have access to a much larger dataset of transactions. To simulate such a case, we will inflate the original one 10 times. n_replicas = 10 # inflate the original dataset big_raw_data = pd.DataFrame(np.repeat(raw_data.values, n_replicas, axis=0), columns=raw_data.columns) print(\"There are \" + str(len(big_raw_data)) + \" observations in the inflated credit card fraud dataset.\") print(\"There are \" + str(len(big_raw_data.columns)) + \" variables in the dataset.\") # display first rows in the new dataset big_raw_data.head() big_raw_data.shape # 'Class' is the target variable (1-fraud 0-otherwise) # get the set of distinct classes labels = big_raw_data.Class.unique() # get the count of each class sizes = big_raw_data.Class.value_counts().values # plot the class value counts fig, ax = plt.subplots() ax.pie(sizes, labels=labels, autopct='%1.3f%%') ax.set_title('Target Variable Value Counts') plt.show() # generate historgram of amounts spent per transaction big_raw_data['Amount'].hist(bins=75) plt.show() big_raw_data['Amount'].describe() # Data preprocessing such as scaling/normalization is typically useful for # linear models to accelerate the training convergence. # Standardize features by removing the mean and scaling to unit variance. big_raw_data.iloc[:, 1:30] = StandardScaler().fit_transform(big_raw_data.iloc[:, 1:30]) data_matrix = big_raw_data.values big_raw_data.head() # X: feature matrix (for this analysis, we exclude the Time variable from the dataset) X = data_matrix[:, 1:30] # y: labels vector y = data_matrix[:, 30] # data normalization X = normalize(X, norm=\"l1\") # print the shape of the features matrix and the labels vector print('X.shape=', X.shape, 'y.shape=', y.shape) # train\\test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape) print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape) # Compute the sample weights to be used as input to the train routine so that # it takes into account the class imbalance present in this dataset. w_train = compute_sample_weight('balanced', y_train) # for reproducible output across multiple function calls, set random_state to a given integer value sklearn_dt = DecisionTreeClassifier(max_depth=4, random_state=35) # train a Decision Tree Classifier using scikit-learn t0 = time.time() sklearn_dt.fit(X_train, y_train, sample_weight=w_train) sklearn_time = time.time()-t0 print(\"[Scikit-Learn] Training time (s): {0:.5f}\".format(sklearn_time)) # run inference and compute the probabilities of the test samples # to belong to the class of fraudulent transactions sklearn_pred = sklearn_dt.predict_proba(X_test)[:,1] # evaluate the Compute Area Under the Receiver Operating Characteristic # Curve (ROC-AUC) score from the predictions sklearn_roc_auc = roc_auc_score(y_test, sklearn_pred) print('[Scikit-Learn] ROC-AUC score : {0:.3f}'.format(sklearn_roc_auc)) # instatiate a scikit-learn SVM model # to indicate the class imbalance at fit time, set class_weight='balanced' # for reproducible output across multiple function calls, set random_state to a given integer value sklearn_svm = LinearSVC(class_weight='balanced', random_state=31, loss=\"hinge\", fit_intercept=False) # train a linear Support Vector Machine model using Scikit-Learn t0 = time.time() sklearn_svm.fit(X_train, y_train) sklearn_time = time.time() - t0 print(\"[Scikit-Learn] Training time (s): {0:.2f}\".format(sklearn_time)) # run inference using the Scikit-Learn model # get the confidence scores for the test samples sklearn_pred = sklearn_svm.decision_function(X_test) # evaluate accuracy on test set acc_sklearn = roc_auc_score(y_test, sklearn_pred) print(\"[Scikit-Learn] ROC-AUC score: {0:.3f}\".format(acc_sklearn)) Note: Decision function is a method present in classifier{ SVC, Logistic Regression } class of sklearn machine learning framework. This method basically returns a Numpy array, In which each element represents whether a predicted sample for x_test by the classifier lies to the right or left side of the Hyperplane and also how far from the HyperPlane.","title":"Decision Tree Example 2 (Credit Card Fraud)"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#decision-tree-versus-support-vector-machine","text":"Chat GPT Prompt: What are the primary similarities and differences between decision trees and support vector machines? Decision trees and support vector machines (SVMs) are both machine learning algorithms used for classification and regression tasks, but they have different approaches and characteristics. Here are the primary similarities and differences between decision trees and support vector machines: Similarities: Supervised Learning: Both decision trees and SVMs are supervised learning algorithms, which means they require labeled training data to make predictions. Classification and Regression: Both algorithms can be used for both classification and regression tasks. Decision trees are versatile in this regard, while SVMs are often used for classification but can also be adapted for regression. Differences: Algorithmic Approach: Decision Trees: Decision trees use a tree-like structure to make decisions. They recursively split the data into subsets based on the features that provide the most information gain or reduction in impurity (e.g., Gini impurity or entropy). Support Vector Machines: SVMs aim to find a hyperplane that best separates data points into different classes while maximizing the margin (distance) between the hyperplane and the nearest data points (support vectors). Linearity: Decision Trees: Decision trees can handle both linear and non-linear relationships between features and the target variable. They can model complex decision boundaries. Support Vector Machines: SVMs are inherently designed for linear separation, but they can be extended to handle non-linear data by using kernel tricks (e.g., polynomial kernels or radial basis function kernels). Interpretability: Decision Trees: Decision trees are highly interpretable. You can easily visualize the decision-making process as a tree, making it easier to understand how the model arrived at a particular prediction. Support Vector Machines: SVMs are less interpretable by nature, as the decision boundary is defined by a hyperplane in a high-dimensional space, which is not as intuitive as a decision tree. Overfitting: Decision Trees: Decision trees are prone to overfitting when they become too deep and complex. Techniques like pruning and setting maximum depth can help mitigate this issue. Support Vector Machines: SVMs are less prone to overfitting, especially when using a large margin (C parameter) and appropriate kernel functions. They tend to generalize well to new data. Handling Imbalanced Data: Decision Trees: Decision trees can be biased towards the majority class in imbalanced datasets, as they tend to choose the splits that minimize impurity, which may not be ideal for the minority class. Support Vector Machines: SVMs can handle imbalanced data better because they aim to maximize the margin and are less influenced by class distribution. Computation and Memory Requirements: Decision trees are generally computationally less expensive and require less memory compared to SVMs, especially when dealing with large datasets. In summary, the choice between decision trees and SVMs depends on the specific characteristics of your dataset and the interpretability requirements. Decision trees are often favored for their simplicity and interpretability, while SVMs are powerful for cases where finding a well-defined margin is essential or when dealing with high-dimensional data.","title":"Decision Tree Versus Support Vector Machine"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#practice","text":"In this section you will evaluate the quality of the SVM models trained above using the hinge loss metric (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html). Run inference on the test set using both Scikit-Learn and Snap ML models. Compute the hinge loss metric for both sets of predictions. Print the hinge losses of Scikit-Learn and Snap ML. # get the confidence scores for the test samples sklearn_pred = sklearn_svm.decision_function(X_test) # evaluate the hinge loss metric from the predictions loss_sklearn = hinge_loss(y_test, sklearn_pred) print(\"[Scikit-Learn] Hinge loss: {0:.3f}\".format(loss_sklearn))","title":"Practice"},{"location":"notes/data_science/Coursera/machine_learning/Decision_Trees_IBM/#hinge-loss","text":"Hinge loss is a function popularly used in support vector machine algorithms to measure the distance of data points from the decision boundary. This helps approximate the possibility of incorrect predictions and evaluate the model's performance.","title":"Hinge Loss"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/","text":"Machine Learning With Python Source: Machine_Learning_With_Python_IBM Supervised and Unsupervised Learning Supervised Overall this is labeled data. Data from a csv is labeled. However be careful. Supervised learning is predicting a column for which you already have data but for a new observation or row that you haven't seen yet. There are two types of supervised techniques. Classification Predict discrete class label or category So with a csv like: animal_id|eye_size|num_legs|weight|animal ... We could predict the animal col as we have that label. Regression Predict continuous value. So with a csv like: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... If you have many observations, you could attempt to predict co2_emissions for a new observation Unsupervised Overall this is unlabeled data. This could still be from the same csv of data but you would be predicting a new column you don't know about yet. So with a csv like: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... Dimension Reduction Feature selection and dimension reduction reduces redundant features to make the clasification easier. Density Estimation Simple concept to explore the data to find structue within it. Market Basket Analysis If you buy a certain group of items, you are likely to buy another group of items. Clustering Grouping data points and objects that are somehow similar. Discover structure Summarization Anomaly detection Summary supervised learning has labeled data and unsupervised does not. Regression Take this dataset: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... Can we predict the co2 emissions of the car given all features but co2? Dependent (y) variables are the goal we study. The the dependent variables have to be continuous. Independent (x) variables are the causes of the dependent variable states. Independent variables can be continuous or categorical. Simple Regression One independent variable is used to predict a dependent variable. Multiple Regression Multiple independent variables are used to predict a dependent variable. Applications of Regression Sales forecasting Satisfaction analysis Price estimation Employment income prediction Regression Algorithms Ordinal regression Poisson regression Fast forest quantile regression Linear, Polynomial, Lasson, Stepwise, Ridge regression Bayesian linear regression Neural network regression Decision forest regression Boosted decision tree regression KNN (K-nearest neighbors) Simple Linear Regression Return to this this dataset: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... Remember dependent variables have to be continuous \\(\\hat{y} = \\theta_0 + \\theta_1 x_1\\) A residual error is the difference between y which is the data itself (x,y) from the csv and y(hat), the regression line evaluated at x (x, \\(\\hat{y}\\) ). The mean of all residual errors shows how poorly the estimation approximates the best regression. This is called the Mean Squared Error: \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\\) It can be shown that \\(\\theta_0\\) and \\(\\theta_1\\) are: \\(\\theta_1 = \\frac{\\sum_{i=1}^{S} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^{S} (x_i - \\bar{x})^2}\\) \\(\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}\\) where: \\(\\bar{x}\\) is the mean of all x values \\(\\bar{y}\\) is the mean of all y values Let's use code: import pandas as pd engine_size = [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7] cylinders = [4,4,4,6,6,6,6,6,6] fuel_consumption_comb = [8.5,9.6,5.9,11.1,10.6,10.0,10.1,11.1,11.6] co2_emissions = [196,221,136,255,244,230,232,255,267] engines = pd.DataFrame({'engine_size':engine_size, 'cylinders':cylinders, 'fuel_consumption_comb':fuel_consumption_comb, 'co2_emissions':co2_emissions}) engines theta_0 = 100 theta_1 = 30 x1 = 4 y = theta_0 + theta_1 * 4 y Pros of regressions: * Very fast * No parameter tuning * East to understand, and highly interpretable Model Evaluation Approaches Train and Test on the Same Dataset Train/Test Split Train and Test on the Same Dataset Identify a testing subset of all data having observations for a given dependent variable. We train on all the data then predict on the testing subset and compare predictions of the dependent variable from the test set to the observations of the test set for the same variable. Training set is all data. Testing set is subset of training. Let's evaluate the error ( \\(y_i\\) being the actual observation and \\(\\hat{y}_j\\) being the prediction): Error = \\(\\frac{1}{n}\\sum_{j=1}^{n}|y_i - \\hat{y}_j|\\) Above is the mean absolute error across all values. This approach has a high training accuracy but a low out-of-sample accuracy. High training accuracy isn't necessarily a good thing. It results in overfitting. It's important that our models have a high, out-of-sample accuracy. The quiz made a point that I want to record. If a model is overly trained to the dataset, it may capture noise and produce a non-generalized model. Train/Test Split Let's select a train/test split. Training set is subset of data. Testing set is also a subset of data but with no overlap with the training set. Training and Test data are mutually exclusive , which is a statistical term describing two or more events that cannot happen simultaneously. This is more realistic for real world problems. One problem with this approach is the model is highly dependent on which datasets the data is trained and tested. K-Fold Cross-Validation Another evaluation model called K-fold cross-validation can resolve most of these issues. How do you fix a high variation that results from a dataset dependency? Well you average it. Example: Take a dataset and make a train\\test split where 75% of the data is training and 25% is test. If we have K equals four folds , then we make 4 train/test splits. In the first fold for example, we use the first 25 percent of the dataset for testing and the rest for training. The model is built using the training set and is evaluated using the test set. Then, in the next round or in the second fold, the second 25 percent of the dataset is used for testing and the rest for training the model. Again, the accuracy of the model is calculated. We continue for all folds. Finally, the result of all four evaluations are averaged. That is, the accuracy of each fold is then averaged, keeping in mind that each fold is distinct, where no training data in one fold is used in another . K-fold cross-validation in its simplest form performs multiple train/test splits, using the same dataset where each split is different. Then, the result is average to produce a more consistent out-of-sample accuracy. However, going in depth with K-fold cross-validation model is out of the scope for this course. Evaluation Metrics in Regression Models M ean A bsolute E rror - Average error. \\(MAE =\\frac{1}{n}\\sum_{j=1}^{n}|y_j-\\hat{y}_j|\\) M ean S quared E rror - Focus is geared towards large errors. It accentuates worse errors. Also a normalized residual sum of square (which is the same equation without dividing by n ). \\(MSE = \\frac{1}{n}\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2\\) R oot M ean S quared E rror - Square root of MSE. It is interpretable in the same units as response vector or Y units. \\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2}\\) R elative A bsolute E rror - Takes the MAE and normalizes it. \\(RAE = \\frac{\\sum_{j=1}^{n}|y_j-\\hat{y}_j|}{\\sum_{j=1}^{n}|y_j-\\bar{y}_j|}\\) R elative S quared E rror - Similar to RAE, but is widely adopted by the data science community as it is used for calculating R-squared . R-squared is not an error per say but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the R-squared , the better the model fits your data. \\(RSE = \\frac{\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2}{\\sum_{j=1}^{n}(y_j-\\bar{y}_j)^2}\\) \\(R^2 = 1 - RSE\\) Error is difference betwee data points and the trend generated by the algorithm. Lab: Simple Linear Regression ML0101EN-Reg-Simple-Linear-Regression-Co2.ipynb Sklearn Intro # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv # cd ~/Desktop; rm -r temp; # To remove import pandas as pd, numpy as np from sklearn import linear_model import matplotlib.pyplot as plt df = pd.read_csv(\"FuelConsumption.csv\") cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] # summarize the data df.describe() # matplotlib plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color='blue') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # train\\test msk = np.random.rand(len(df)) < 0.8 train = cdf[msk] test = cdf[~msk] # train data distribution plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # modeling with sklearn regr = linear_model.LinearRegression() train_x = np.asanyarray(train[['ENGINESIZE']]) train_y = np.asanyarray(train[['CO2EMISSIONS']]) regr.fit(train_x, train_y) # The coefficients print ('Coefficients: (theta1)', regr.coef_) print ('Intercept: (theta0)',regr.intercept_) # fit that regression! plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue') plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # MSE calculations test_x = np.asanyarray(test[['ENGINESIZE']]) test_y = np.asanyarray(test[['CO2EMISSIONS']]) test_y_ = regr.predict(test_x) print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y))) print(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2)) print(\"R2-score: %.2f\" % r2_score(test_y , test_y_) ) Multiple Linear Regression ML0101EN-Reg-Mulitple-Linear-Regression-Co2.ipynb Review: * Simple Linear Regression - predict co2 emission vs engine ize * Multiple Linear Regression - predict co2 emission vs engine size and cyclinders Applications: Independent variables effectiveness on prediction. Does revision time, test anxiety, lecture attendance and gender have any effect on the exam performance of students. Predicting impacts of change. How mch does blood pressure go up (or down) for every unit increase (or decease) in the BMI index of patient. Ex: \\(Co2 = \\theta_0 + \\theta_1EngineSize + \\theta2Cylinders + ...\\) Multiple Linear Regression: \\(\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n\\) Vector Form (more concise): \\(\\hat{y} = \\theta^TX\\) \"Theta Transpose X, it's really just missing \\(\\theta_0\\) from above expanded form. That is bias .\" \\(\\theta^T = [\\theta^0,\\theta^1,\\theta^2,...]\\) \\(X = \\left[ \\begin{array}{c} 1 \\\\ x_1 \\\\ x_2 \\\\ \\end{array} \\right]\\) Estimating Multiple Linear Regression Parameters How to estimate \\(\\theta^T\\) ? * Ordinary Least Squares * Linear algebra operations * Takes a long time for large datasets (10K+rows) * An optimized algorithm * Gradient Descent * Proper approach if you have a very large dataset. Questions to consider: * How to determine whether to use simple or multiple linear regression? * How many independent variables should you use? * Too many causes overfit. It's too specific. * Should the indendent variable be continuous? * What are the linear relationships between the dependent variable and the independent variables? Sklearn Multiple Lenear Regression # Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv # cd ~/Desktop; rm -r temp; # To remove import matplotlib.pyplot as plt import pandas as pd import pylab as pl import numpy as np from sklearn import linear_model # train\\teset df = pd.read_csv(\"FuelConsumption.csv\") cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']] msk = np.random.rand(len(df)) < 0.8 train = cdf[msk] test = cdf[~msk] # multiple regression model regr = linear_model.LinearRegression() x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) y = np.asanyarray(train[['CO2EMISSIONS']]) regr.fit (x, y) # The coefficients print ('Coefficients: ', regr.coef_) As mentioned before, Coefficient and Intercept are the parameters of the fitted line. Given that it is a multiple linear regression model with 3 parameters and that the parameters are the intercept and coefficients of the hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain Ordinary Least Squares method to solve this problem. It tries to minimize the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output ( \\(\\hat{y}\\) ) over all samples in the dataset. OLS can find the best parameters using of the following methods: * Solving the model parameters analytically using closed-form equations * Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton\u2019s Method, etc.) Prediction: y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) y = np.asanyarray(test[['CO2EMISSIONS']]) print(\"Mean Squared Error (MSE) : %.2f\" % np.mean((y_hat - y) ** 2)) # Explained variance score: 1 is perfect prediction print('Variance score: %.2f' % regr.score(x, y)) Explained variance regression score ( \\(R^2\\) ): Let \\(\\hat{y}\\) be the estimated target output, y the corresponding (correct) target output, and Var be the Variance (the square of the standard deviation). Then the explained variance is estimated as follows: \\(\\texttt{explainedVariance}(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}\\) The best possible score is 1.0, the lower values are worse.","title":"1. Intro and Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#machine-learning-with-python","text":"Source: Machine_Learning_With_Python_IBM","title":"Machine Learning With Python"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#supervised-and-unsupervised-learning","text":"","title":"Supervised and Unsupervised Learning"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#supervised","text":"Overall this is labeled data. Data from a csv is labeled. However be careful. Supervised learning is predicting a column for which you already have data but for a new observation or row that you haven't seen yet. There are two types of supervised techniques.","title":"Supervised"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#classification","text":"Predict discrete class label or category So with a csv like: animal_id|eye_size|num_legs|weight|animal ... We could predict the animal col as we have that label.","title":"Classification"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#regression","text":"Predict continuous value. So with a csv like: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... If you have many observations, you could attempt to predict co2_emissions for a new observation","title":"Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#unsupervised","text":"Overall this is unlabeled data. This could still be from the same csv of data but you would be predicting a new column you don't know about yet. So with a csv like: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ...","title":"Unsupervised"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#dimension-reduction","text":"Feature selection and dimension reduction reduces redundant features to make the clasification easier.","title":"Dimension Reduction"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#density-estimation","text":"Simple concept to explore the data to find structue within it.","title":"Density Estimation"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#market-basket-analysis","text":"If you buy a certain group of items, you are likely to buy another group of items.","title":"Market Basket Analysis"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#clustering","text":"Grouping data points and objects that are somehow similar. Discover structure Summarization Anomaly detection Summary supervised learning has labeled data and unsupervised does not.","title":"Clustering"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#regression_1","text":"Take this dataset: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... Can we predict the co2 emissions of the car given all features but co2? Dependent (y) variables are the goal we study. The the dependent variables have to be continuous. Independent (x) variables are the causes of the dependent variable states. Independent variables can be continuous or categorical.","title":"Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#simple-regression","text":"One independent variable is used to predict a dependent variable.","title":"Simple Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#multiple-regression","text":"Multiple independent variables are used to predict a dependent variable.","title":"Multiple Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#applications-of-regression","text":"Sales forecasting Satisfaction analysis Price estimation Employment income prediction","title":"Applications of Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#regression-algorithms","text":"Ordinal regression Poisson regression Fast forest quantile regression Linear, Polynomial, Lasson, Stepwise, Ridge regression Bayesian linear regression Neural network regression Decision forest regression Boosted decision tree regression KNN (K-nearest neighbors)","title":"Regression Algorithms"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#simple-linear-regression","text":"Return to this this dataset: car_id|engine_size|cylinders|fuel_consumption|co2_emissions ... Remember dependent variables have to be continuous \\(\\hat{y} = \\theta_0 + \\theta_1 x_1\\) A residual error is the difference between y which is the data itself (x,y) from the csv and y(hat), the regression line evaluated at x (x, \\(\\hat{y}\\) ). The mean of all residual errors shows how poorly the estimation approximates the best regression. This is called the Mean Squared Error: \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\\) It can be shown that \\(\\theta_0\\) and \\(\\theta_1\\) are: \\(\\theta_1 = \\frac{\\sum_{i=1}^{S} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^{S} (x_i - \\bar{x})^2}\\) \\(\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}\\) where: \\(\\bar{x}\\) is the mean of all x values \\(\\bar{y}\\) is the mean of all y values Let's use code: import pandas as pd engine_size = [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7] cylinders = [4,4,4,6,6,6,6,6,6] fuel_consumption_comb = [8.5,9.6,5.9,11.1,10.6,10.0,10.1,11.1,11.6] co2_emissions = [196,221,136,255,244,230,232,255,267] engines = pd.DataFrame({'engine_size':engine_size, 'cylinders':cylinders, 'fuel_consumption_comb':fuel_consumption_comb, 'co2_emissions':co2_emissions}) engines theta_0 = 100 theta_1 = 30 x1 = 4 y = theta_0 + theta_1 * 4 y Pros of regressions: * Very fast * No parameter tuning * East to understand, and highly interpretable","title":"Simple Linear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#model-evaluation-approaches","text":"Train and Test on the Same Dataset Train/Test Split","title":"Model Evaluation Approaches"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#train-and-test-on-the-same-dataset","text":"Identify a testing subset of all data having observations for a given dependent variable. We train on all the data then predict on the testing subset and compare predictions of the dependent variable from the test set to the observations of the test set for the same variable. Training set is all data. Testing set is subset of training. Let's evaluate the error ( \\(y_i\\) being the actual observation and \\(\\hat{y}_j\\) being the prediction): Error = \\(\\frac{1}{n}\\sum_{j=1}^{n}|y_i - \\hat{y}_j|\\) Above is the mean absolute error across all values. This approach has a high training accuracy but a low out-of-sample accuracy. High training accuracy isn't necessarily a good thing. It results in overfitting. It's important that our models have a high, out-of-sample accuracy. The quiz made a point that I want to record. If a model is overly trained to the dataset, it may capture noise and produce a non-generalized model.","title":"Train and Test on the Same Dataset"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#traintest-split","text":"Let's select a train/test split. Training set is subset of data. Testing set is also a subset of data but with no overlap with the training set. Training and Test data are mutually exclusive , which is a statistical term describing two or more events that cannot happen simultaneously. This is more realistic for real world problems. One problem with this approach is the model is highly dependent on which datasets the data is trained and tested.","title":"Train/Test Split"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#k-fold-cross-validation","text":"Another evaluation model called K-fold cross-validation can resolve most of these issues. How do you fix a high variation that results from a dataset dependency? Well you average it. Example: Take a dataset and make a train\\test split where 75% of the data is training and 25% is test. If we have K equals four folds , then we make 4 train/test splits. In the first fold for example, we use the first 25 percent of the dataset for testing and the rest for training. The model is built using the training set and is evaluated using the test set. Then, in the next round or in the second fold, the second 25 percent of the dataset is used for testing and the rest for training the model. Again, the accuracy of the model is calculated. We continue for all folds. Finally, the result of all four evaluations are averaged. That is, the accuracy of each fold is then averaged, keeping in mind that each fold is distinct, where no training data in one fold is used in another . K-fold cross-validation in its simplest form performs multiple train/test splits, using the same dataset where each split is different. Then, the result is average to produce a more consistent out-of-sample accuracy. However, going in depth with K-fold cross-validation model is out of the scope for this course.","title":"K-Fold Cross-Validation"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#evaluation-metrics-in-regression-models","text":"M ean A bsolute E rror - Average error. \\(MAE =\\frac{1}{n}\\sum_{j=1}^{n}|y_j-\\hat{y}_j|\\) M ean S quared E rror - Focus is geared towards large errors. It accentuates worse errors. Also a normalized residual sum of square (which is the same equation without dividing by n ). \\(MSE = \\frac{1}{n}\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2\\) R oot M ean S quared E rror - Square root of MSE. It is interpretable in the same units as response vector or Y units. \\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2}\\) R elative A bsolute E rror - Takes the MAE and normalizes it. \\(RAE = \\frac{\\sum_{j=1}^{n}|y_j-\\hat{y}_j|}{\\sum_{j=1}^{n}|y_j-\\bar{y}_j|}\\) R elative S quared E rror - Similar to RAE, but is widely adopted by the data science community as it is used for calculating R-squared . R-squared is not an error per say but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the R-squared , the better the model fits your data. \\(RSE = \\frac{\\sum_{j=1}^{n}(y_j-\\hat{y}_j)^2}{\\sum_{j=1}^{n}(y_j-\\bar{y}_j)^2}\\) \\(R^2 = 1 - RSE\\) Error is difference betwee data points and the trend generated by the algorithm.","title":"Evaluation Metrics in Regression Models"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#lab-simple-linear-regression","text":"ML0101EN-Reg-Simple-Linear-Regression-Co2.ipynb","title":"Lab: Simple Linear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#sklearn-intro","text":"# Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv # cd ~/Desktop; rm -r temp; # To remove import pandas as pd, numpy as np from sklearn import linear_model import matplotlib.pyplot as plt df = pd.read_csv(\"FuelConsumption.csv\") cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] # summarize the data df.describe() # matplotlib plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color='blue') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # train\\test msk = np.random.rand(len(df)) < 0.8 train = cdf[msk] test = cdf[~msk] # train data distribution plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # modeling with sklearn regr = linear_model.LinearRegression() train_x = np.asanyarray(train[['ENGINESIZE']]) train_y = np.asanyarray(train[['CO2EMISSIONS']]) regr.fit(train_x, train_y) # The coefficients print ('Coefficients: (theta1)', regr.coef_) print ('Intercept: (theta0)',regr.intercept_) # fit that regression! plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue') plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r') plt.xlabel(\"Engine size\") plt.ylabel(\"Emission\") plt.show() # MSE calculations test_x = np.asanyarray(test[['ENGINESIZE']]) test_y = np.asanyarray(test[['CO2EMISSIONS']]) test_y_ = regr.predict(test_x) print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y))) print(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2)) print(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )","title":"Sklearn Intro"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#multiple-linear-regression","text":"ML0101EN-Reg-Mulitple-Linear-Regression-Co2.ipynb Review: * Simple Linear Regression - predict co2 emission vs engine ize * Multiple Linear Regression - predict co2 emission vs engine size and cyclinders","title":"Multiple Linear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#applications","text":"Independent variables effectiveness on prediction. Does revision time, test anxiety, lecture attendance and gender have any effect on the exam performance of students. Predicting impacts of change. How mch does blood pressure go up (or down) for every unit increase (or decease) in the BMI index of patient. Ex: \\(Co2 = \\theta_0 + \\theta_1EngineSize + \\theta2Cylinders + ...\\) Multiple Linear Regression: \\(\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n\\) Vector Form (more concise): \\(\\hat{y} = \\theta^TX\\) \"Theta Transpose X, it's really just missing \\(\\theta_0\\) from above expanded form. That is bias .\" \\(\\theta^T = [\\theta^0,\\theta^1,\\theta^2,...]\\) \\(X = \\left[ \\begin{array}{c} 1 \\\\ x_1 \\\\ x_2 \\\\ \\end{array} \\right]\\)","title":"Applications:"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#estimating-multiple-linear-regression-parameters","text":"How to estimate \\(\\theta^T\\) ? * Ordinary Least Squares * Linear algebra operations * Takes a long time for large datasets (10K+rows) * An optimized algorithm * Gradient Descent * Proper approach if you have a very large dataset. Questions to consider: * How to determine whether to use simple or multiple linear regression? * How many independent variables should you use? * Too many causes overfit. It's too specific. * Should the indendent variable be continuous? * What are the linear relationships between the dependent variable and the independent variables?","title":"Estimating Multiple Linear Regression Parameters"},{"location":"notes/data_science/Coursera/machine_learning/Intro_and_Regression/#sklearn-multiple-lenear-regression","text":"# Setup Environment cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv # cd ~/Desktop; rm -r temp; # To remove import matplotlib.pyplot as plt import pandas as pd import pylab as pl import numpy as np from sklearn import linear_model # train\\teset df = pd.read_csv(\"FuelConsumption.csv\") cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']] msk = np.random.rand(len(df)) < 0.8 train = cdf[msk] test = cdf[~msk] # multiple regression model regr = linear_model.LinearRegression() x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) y = np.asanyarray(train[['CO2EMISSIONS']]) regr.fit (x, y) # The coefficients print ('Coefficients: ', regr.coef_) As mentioned before, Coefficient and Intercept are the parameters of the fitted line. Given that it is a multiple linear regression model with 3 parameters and that the parameters are the intercept and coefficients of the hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain Ordinary Least Squares method to solve this problem. It tries to minimize the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output ( \\(\\hat{y}\\) ) over all samples in the dataset. OLS can find the best parameters using of the following methods: * Solving the model parameters analytically using closed-form equations * Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton\u2019s Method, etc.) Prediction: y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']]) y = np.asanyarray(test[['CO2EMISSIONS']]) print(\"Mean Squared Error (MSE) : %.2f\" % np.mean((y_hat - y) ** 2)) # Explained variance score: 1 is perfect prediction print('Variance score: %.2f' % regr.score(x, y)) Explained variance regression score ( \\(R^2\\) ): Let \\(\\hat{y}\\) be the estimated target output, y the corresponding (correct) target output, and Var be the Variance (the square of the standard deviation). Then the explained variance is estimated as follows: \\(\\texttt{explainedVariance}(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}\\) The best possible score is 1.0, the lower values are worse.","title":"Sklearn Multiple Lenear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/","text":"Logistic Regression Intro to Logistic Regression Overview Logistic regression is a statistical and machine learning technique for classifying records of a datastr based on the values of the input fields. Let's say we have a telecommunication dataset that we'd like to analyze in order to understand which customers might leave us next month. This is historical customer data where each row represents one customer. Imagine that you're an analyst at this company and you have to find out who is leaving and why? You'll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group. In logistic regression, we use one or more independent variables such as tenure, age, and income to predict an outcome, such as churn, which we call the dependent variable representing whether or not customers will stop using the service. Logistic regression is analogous to linear regression but tries to predict a categorical or discrete target field instead of a numeric one. In linear regression, we might try to predict a continuous value of variables such as the price of a house, blood pressure of a patient, or fuel consumption of a car. But in logistic regression, we predict a variable which is binary such as yes/no, true/false, successful or not successful, pregnant/not pregnant, and so on, all of which can be coded as zero or one. In logistic regression independent variables should be continuous. If categorical, they should be dummy or indicator coded. This means we have to transform them to some continuous value. Please note that logistic regression can be used for both binary classification and multi-class classification. Applications To predict the probability of a person having a heart attack within a specified time period Predict the chance of mortality in an injured patient Predict whether a patient has a given disease such as diabetes Predict the likelihood of a customer purchasing a product or halting a subscription as we've done in our churn example Probability of failure of a given process, system or product Predict the likelihood of a homeowner defaulting on a mortgage Notice that in all these examples not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class. one inner 1 inner 2 two The question is, when should we use logistic regression? If your data is binary: 0/1, YES/NO, True/False If you need probabalistic results: When you need a linear decision boundary: The decision boundary of a logistic regression is a line or a plane or a hyper plane A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features and are not applying any polynomial processing we can obtain an inequality like \\(\\theta_0 + \\theta_1x_1 + \\theta_2x2 > 0\\) , which is a half-plane easily plottable. You need to understand the impact of a feature. You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters. That is, after finding the optimum parameters, a feature X with the weight \\(\\theta_1\\) close to 0 has a smaller effect on the prediction than features with large absolute values of \\(\\theta_1\\) . X is our dataset in the space of real numbers of m by n: \\(X\\epsilon \\mathbb{R}^{m \\times n}\\) Y is the class that we want to predict: \\(y\\epsilon\\{0,1\\}\\) , which can be either 0 or 1 Ideally, a logistic regression model, so-called Y hat, can predict that the class of the customer is one, given its features X: \\(\\hat{y} = P(y=1|x)\\) It can also be shown quite easily that the probability of a customer being in class zero can be calculated as one minus the probability that the class of the customer is one: \\(P(y=0|x)= 1 - P(y=1|x)\\) Logistic Regression vs Linear Regression We will learn the difference between linear regression and logistic regression. We go over linear regression and see why it cannot be used properly for some binary classification problems. We also look at the sigmoid function, which is the main part of logistic regression. Recall we want to predict the class of each customer \\(\\hat{y} = P(y=1|x)\\) and also the probability of each sample belonging to a class. y is the label's vector, also called actual values , that we would like to predict, and \\(\\hat{y}\\) is the vector of the predicted values by our model. Let's Try Linear Regression Let's use age to predict chrun (categorical value). We need to plot churn versus age which will have two horizontal areas of points for churn=0 and churn=1. Prediction can be represented as \\(\\hat{y}=\\theta_0 + \\theta_1x_1\\) Where \\(\\theta^T=[\\theta_0,\\theta_1,...,\\theta_n]\\) Formally the line is \\(\\theta^TX=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\) X (feature set) can be represented as: \\(X = \\left[ \\begin{array}{c} 1 \\\\ x_1 \\\\ x_2 \\\\ \\end{array} \\right]\\) Seen here we can estimate the regression line and predict churn with \\(\\theta^TX\\) . Let's try with 13 for \\(x_1\\) . Then set a threshold of 0.5 for determining 0 or 1. Finally \\(\\hat{y}\\) can be represented as a piecewise function to force it to be categorical. In this example \\(P_1=[13]\\) is Class 0. There is one problem here. What is the probability that this customer belongs to class zero? As you can see, it's not the best model to solve this problem. Also, there are some other issues which verify that linear regression is not the proper method for classification problems. So, as mentioned, if we use the regression line to calculate the class of a point, it always returns a number such as three or negative two, and so on. Then, we should use a threshold, for example, 0.5, to assign that point to either class of zero or one. This threshold works as a step function that outputs zero or one regardless of how big or small, positive or negative the input is. So, using the threshold, we can find the class of a record. Notice that in the step function, no matter how big the value is, as long as it's greater than 0.5, it simply equals one and vice versa. Regardless of how small the value y is, the output would be zero if it is less than 0.5. In other words, there is no difference between a customer who has a value of one or 1,000. The outcome would be one. Instead of having this step function, wouldn't it be nice if we had a smoother line, one that would project these values between zero and one? Let's use the Sigmoid Function . It gives a non-step function approach that gives us the probabilty that a point belongs to a certain class instead of the value of y directly. Instead of calculating the value of Theta transpose x directly, it returns the probability that a Theta transpose x is very big or very small. It always returns a value between 0 and 1, depending on how large the Theta transpose x actually is. Now, our model is \\(\\hat{y}=\\sigma(\\theta^TX)\\) , which represents the probability that the output is 1 given x. Sigmoid Function \\(\\sigma(\\theta^TX)=\\frac{1}{1+e^{-\\theta^TX}}\\) When \\(\\theta^T\\) goes up \\(\\sigma(\\theta^TX)\\) tends towards 1 and when small it tends to 0. This is the probability not the class itself. What is the output of our model? \\(P(Y=1|X)\\) - probability of Y=1 given X \\(P(Y=0|X) = 1-P(Y=1|X)\\) - probability of Y=0 given X Ex: * \\(P(Churn=1|income,age)\\) = 0.8 * \\(P(Churn=0|income,age)\\) = 1- 0.8 = 0.2 Now we can train our model to set its parameter values in such a way that our model is a good estimate of the probablity of Y given X: \\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) . \\(\\sigma\\) is model. \\(P(y=1|x)\\) and \\(P(y=0|x)\\) are actual probabilities. How to train this model? \\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) Initialize \\(\\theta\\) with random values: \\(\\theta\\) = [-1, 2] Calculate model output \\(\\hat{y} = \\sigma(\\theta^TX)\\) for a sample customer. \\(\\hat{y} = \\sigma([-1,2]] x [2,5])\\) = 0.7 - \"2 and 5 are age and income\" Compare the output of \\(\\hat{y}\\) with the actual output of custom, y, and record it as error. Example: \\(Error = 1 - 0.7 = 0.3\\) . 1 Here was Churn and 0.7 was the probability of Chrun from model Calculate the error for all customers. Add up these errors. Total error is the cost of your model and is calculated by your models cost function. The cost function, by the way, basically represents how to calculate the error of the model which is the difference between the actual and the models predicted values. You want low cost (preferrably 0). Cost is high right now. We need to change to get lower cost. We change \\(\\theta\\) in such a way to hopefully reduce the total cost. Go back to step 2. and iterate to get a new cost. We do this until the cost is low enough. One of the most popular ways is gradient descent. Also, there are various ways to stop iterations, but essentially you stop training by calculating the accuracy of your model and stop it when it's satisfactory. Logistic Regression Training Objective - \\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) Let's look at the cost function. Cost( \\(\\hat{y}, y\\) ) = \\(\\sigma(\\theta^TX) - y\\) Usually the square of this equation is used because of the possibility of the negative result and for the sake of simplicity, half of this value is considered as the cost function through the derivative process. Cost( \\(\\hat{y}, y\\) ) = \\(\\frac{(\\sigma(\\theta^TX) - y)^2}{2}\\) Note this is for a particular sample. If we want for all samples we need a new equation: \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^mCost(\\hat{y},y)\\) This confused me for a second as \\(\\theta\\) refers to the models parameters. It's the cost across all samples divided by the number of samples. This is also called mean squared error . The reason it's a function of \\(\\theta\\) is because all this \"cost\" we are measuring is for a specific set of \\(\\theta\\) parameters. Now, how do we find or set the best weights or parameters that minimize this cost function? The answer is, we should calculate the minimum point of this cost function and it will show us the best parameters for our model. Although we can find the minimum point of a function using the derivative of a function, there's not an easy way to find the global minimum point for such an equation. Given this complexity, describing how to reach the global minimum for this equation is outside the scope of this video. So, what is the solution? Well we should find another cost function instead, one which has the same behavior but is easier to find its minimum point. Let's plot the desirable cost function for our model. Let's plot the desirable cost function for our model. Recall that our model is y hat. Our actual value is y which equals zero or one, and our model tries to estimate it as we want to find a simple cost function for our model. This means our model is best if it estimates y equals one. In this case, we need a cost function that returns zero if the outcome of our model is one, which is the same as the actual label. And the cost should keep increasing as the outcome of our model gets farther from one. And cost should be very large if the outcome of our model is close to zero. We can see that a function \\(-log(\\hat{y})\\) looks similar to how we want to model this: Recall the derivative of the cost function was hard to calculate. So instead we define as: \\[ Cost(\\hat{y},y) = \\begin{cases} -log(\\hat{y}) & \\text{if } y = 1 \\\\ -log(1-\\hat{y}) & \\text{if } y = 0 \\end{cases} \\] and redefine \\(J(\\theta)\\) as: \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^m y^i log(\\hat{y}^i) + (1-y^i) log(1-\\hat{y}^i)\\) Gradient Descent To Minimize the Cost Let's assume \\(\\hat{y} = \\sigma(\\theta_1x_i + \\theta_2x_2)\\) and \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^m y^i log(\\hat{y}^i) + (1-y^i) log(1-\\hat{y}^i)\\) Imagine that it makes a bowl as shown: We need to find the minimum of this cost function. We change the parameters of \\((\\Delta\\theta_1,\\Delta\\theta_2)\\) . We keep taking steps. First while slope is large we take larger steps. As slope gets smaller we take smaller steps. How do we calculate slope? \\(\\frac{\\delta J}{\\delta \\theta_1}\\) = \\(-\\frac{1}{m}\\sum_i^m(y^i-\\hat{y})x_1^i\\) It's technically some math proof we should look up if we are interested. If the derivative of J is positive with respect to \\(\\theta_1\\) then J increases as \\(\\theta1\\) increases. This tells us to move in the opposite direction. We can take smaller steps as the slop decreases. That is just for \\(\\theta_1\\) . We can represent the vector of all the slopes as the gradient vector: \\(\\nabla J = \\left[ \\begin{array}{c} \\frac{\\delta J}{\\delta \\theta_1} \\\\ \\frac{\\delta J}{\\delta \\theta_2} \\\\ \\frac{\\delta J}{\\delta \\theta_3} \\\\ ... \\\\ \\frac{\\delta J}{\\delta \\theta_k} \\\\ \\end{array} \\right]\\) We can use this vector to change or update all parameters. We take the previous values of the parametes and subtract the error derivative. This results in the new parameters for \\(\\theta\\) that we know decrease the cost. \\(New\\theta = old\\theta - \\nabla J\\) Note: Units of gradient vector are \\(\\delta loss/ \\delta \\theta\\) so that it matches \\(\\theta\\) 's units. We can change all weights a little bit to move towards lower loss. Last we add a learning rate \\(\\mu\\) (dimensionless number | hyperparameter) to change the step we take towards lowering the cost. \\(New\\theta = old\\theta - \\mu \\nabla J\\) Logistic Regression Code Example # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why. The dataset includes information about: * Customers who left within the last month \u2013 the column is called Churn * Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies * Customer account information \u2013 how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges * Demographic info about customers \u2013 gender, age range, and if they have partners and dependents Download the data: wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\" import pandas as pd import pylab as pl import numpy as np import scipy.optimize as opt from sklearn import preprocessing import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix from sklearn.metrics import jaccard_score from sklearn.metrics import classification_report from sklearn.metrics import log_loss import itertools churn_df = pd.read_csv(\"ChurnData.csv\") churn_df.head() # Let's select some features for the modeling. churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'wireless','churn']] churn_df['churn'] = churn_df['churn'].astype('int') churn_df.head() Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm: # Define X and Y for the dataset X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) X[0:5] y = np.asarray(churn_df['churn']) y [0:5] # Normalize the dataset X = preprocessing.StandardScaler().fit(X).transform(X) X[0:5] # Train\\Test X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape) Let's build our model using LogisticRegression from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019 solvers. You can find extensive information about the pros and cons of these optimizers if you search it in the internet. The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem of machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. Now let's fit our model with train set: LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train) LR # Predict on test set yhat = LR.predict(X_test) yhat predict_proba returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X): yhat_prob = LR.predict_proba(X_test) yhat_prob # Evaluation (Jaccard Index) jaccard_score(y_test, yhat,pos_label=0) # Confusion Matrix def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') print(confusion_matrix(y_test, yhat, labels=[1,0])) # Plot Confusion Matrix # Compute confusion matrix cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0]) np.set_printoptions(precision=2) # Plot non-normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False, title='Confusion matrix') plt.show() # Print Classification Report print (classification_report(y_test, yhat)) log_loss(y_test, yhat_prob)","title":"5. Logistic Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#intro-to-logistic-regression","text":"","title":"Intro to Logistic Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#overview","text":"Logistic regression is a statistical and machine learning technique for classifying records of a datastr based on the values of the input fields. Let's say we have a telecommunication dataset that we'd like to analyze in order to understand which customers might leave us next month. This is historical customer data where each row represents one customer. Imagine that you're an analyst at this company and you have to find out who is leaving and why? You'll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group. In logistic regression, we use one or more independent variables such as tenure, age, and income to predict an outcome, such as churn, which we call the dependent variable representing whether or not customers will stop using the service. Logistic regression is analogous to linear regression but tries to predict a categorical or discrete target field instead of a numeric one. In linear regression, we might try to predict a continuous value of variables such as the price of a house, blood pressure of a patient, or fuel consumption of a car. But in logistic regression, we predict a variable which is binary such as yes/no, true/false, successful or not successful, pregnant/not pregnant, and so on, all of which can be coded as zero or one. In logistic regression independent variables should be continuous. If categorical, they should be dummy or indicator coded. This means we have to transform them to some continuous value. Please note that logistic regression can be used for both binary classification and multi-class classification.","title":"Overview"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#applications","text":"To predict the probability of a person having a heart attack within a specified time period Predict the chance of mortality in an injured patient Predict whether a patient has a given disease such as diabetes Predict the likelihood of a customer purchasing a product or halting a subscription as we've done in our churn example Probability of failure of a given process, system or product Predict the likelihood of a homeowner defaulting on a mortgage Notice that in all these examples not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class. one inner 1 inner 2 two The question is, when should we use logistic regression? If your data is binary: 0/1, YES/NO, True/False If you need probabalistic results: When you need a linear decision boundary: The decision boundary of a logistic regression is a line or a plane or a hyper plane A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features and are not applying any polynomial processing we can obtain an inequality like \\(\\theta_0 + \\theta_1x_1 + \\theta_2x2 > 0\\) , which is a half-plane easily plottable. You need to understand the impact of a feature. You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters. That is, after finding the optimum parameters, a feature X with the weight \\(\\theta_1\\) close to 0 has a smaller effect on the prediction than features with large absolute values of \\(\\theta_1\\) . X is our dataset in the space of real numbers of m by n: \\(X\\epsilon \\mathbb{R}^{m \\times n}\\) Y is the class that we want to predict: \\(y\\epsilon\\{0,1\\}\\) , which can be either 0 or 1 Ideally, a logistic regression model, so-called Y hat, can predict that the class of the customer is one, given its features X: \\(\\hat{y} = P(y=1|x)\\) It can also be shown quite easily that the probability of a customer being in class zero can be calculated as one minus the probability that the class of the customer is one: \\(P(y=0|x)= 1 - P(y=1|x)\\)","title":"Applications"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#logistic-regression-vs-linear-regression","text":"We will learn the difference between linear regression and logistic regression. We go over linear regression and see why it cannot be used properly for some binary classification problems. We also look at the sigmoid function, which is the main part of logistic regression. Recall we want to predict the class of each customer \\(\\hat{y} = P(y=1|x)\\) and also the probability of each sample belonging to a class. y is the label's vector, also called actual values , that we would like to predict, and \\(\\hat{y}\\) is the vector of the predicted values by our model.","title":"Logistic Regression vs Linear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#lets-try-linear-regression","text":"Let's use age to predict chrun (categorical value). We need to plot churn versus age which will have two horizontal areas of points for churn=0 and churn=1. Prediction can be represented as \\(\\hat{y}=\\theta_0 + \\theta_1x_1\\) Where \\(\\theta^T=[\\theta_0,\\theta_1,...,\\theta_n]\\) Formally the line is \\(\\theta^TX=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\) X (feature set) can be represented as: \\(X = \\left[ \\begin{array}{c} 1 \\\\ x_1 \\\\ x_2 \\\\ \\end{array} \\right]\\) Seen here we can estimate the regression line and predict churn with \\(\\theta^TX\\) . Let's try with 13 for \\(x_1\\) . Then set a threshold of 0.5 for determining 0 or 1. Finally \\(\\hat{y}\\) can be represented as a piecewise function to force it to be categorical. In this example \\(P_1=[13]\\) is Class 0. There is one problem here. What is the probability that this customer belongs to class zero? As you can see, it's not the best model to solve this problem. Also, there are some other issues which verify that linear regression is not the proper method for classification problems. So, as mentioned, if we use the regression line to calculate the class of a point, it always returns a number such as three or negative two, and so on. Then, we should use a threshold, for example, 0.5, to assign that point to either class of zero or one. This threshold works as a step function that outputs zero or one regardless of how big or small, positive or negative the input is. So, using the threshold, we can find the class of a record. Notice that in the step function, no matter how big the value is, as long as it's greater than 0.5, it simply equals one and vice versa. Regardless of how small the value y is, the output would be zero if it is less than 0.5. In other words, there is no difference between a customer who has a value of one or 1,000. The outcome would be one. Instead of having this step function, wouldn't it be nice if we had a smoother line, one that would project these values between zero and one? Let's use the Sigmoid Function . It gives a non-step function approach that gives us the probabilty that a point belongs to a certain class instead of the value of y directly. Instead of calculating the value of Theta transpose x directly, it returns the probability that a Theta transpose x is very big or very small. It always returns a value between 0 and 1, depending on how large the Theta transpose x actually is. Now, our model is \\(\\hat{y}=\\sigma(\\theta^TX)\\) , which represents the probability that the output is 1 given x.","title":"Let's Try Linear Regression"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#sigmoid-function","text":"\\(\\sigma(\\theta^TX)=\\frac{1}{1+e^{-\\theta^TX}}\\) When \\(\\theta^T\\) goes up \\(\\sigma(\\theta^TX)\\) tends towards 1 and when small it tends to 0. This is the probability not the class itself.","title":"Sigmoid Function"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#what-is-the-output-of-our-model","text":"\\(P(Y=1|X)\\) - probability of Y=1 given X \\(P(Y=0|X) = 1-P(Y=1|X)\\) - probability of Y=0 given X Ex: * \\(P(Churn=1|income,age)\\) = 0.8 * \\(P(Churn=0|income,age)\\) = 1- 0.8 = 0.2 Now we can train our model to set its parameter values in such a way that our model is a good estimate of the probablity of Y given X: \\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) . \\(\\sigma\\) is model. \\(P(y=1|x)\\) and \\(P(y=0|x)\\) are actual probabilities.","title":"What is the output of our model?"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#how-to-train-this-model","text":"\\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) Initialize \\(\\theta\\) with random values: \\(\\theta\\) = [-1, 2] Calculate model output \\(\\hat{y} = \\sigma(\\theta^TX)\\) for a sample customer. \\(\\hat{y} = \\sigma([-1,2]] x [2,5])\\) = 0.7 - \"2 and 5 are age and income\" Compare the output of \\(\\hat{y}\\) with the actual output of custom, y, and record it as error. Example: \\(Error = 1 - 0.7 = 0.3\\) . 1 Here was Churn and 0.7 was the probability of Chrun from model Calculate the error for all customers. Add up these errors. Total error is the cost of your model and is calculated by your models cost function. The cost function, by the way, basically represents how to calculate the error of the model which is the difference between the actual and the models predicted values. You want low cost (preferrably 0). Cost is high right now. We need to change to get lower cost. We change \\(\\theta\\) in such a way to hopefully reduce the total cost. Go back to step 2. and iterate to get a new cost. We do this until the cost is low enough. One of the most popular ways is gradient descent. Also, there are various ways to stop iterations, but essentially you stop training by calculating the accuracy of your model and stop it when it's satisfactory.","title":"How to train this model?"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#logistic-regression-training","text":"Objective - \\(\\sigma(\\theta^TX)\\) -> \\(P(y=1|x)\\) Let's look at the cost function. Cost( \\(\\hat{y}, y\\) ) = \\(\\sigma(\\theta^TX) - y\\) Usually the square of this equation is used because of the possibility of the negative result and for the sake of simplicity, half of this value is considered as the cost function through the derivative process. Cost( \\(\\hat{y}, y\\) ) = \\(\\frac{(\\sigma(\\theta^TX) - y)^2}{2}\\) Note this is for a particular sample. If we want for all samples we need a new equation: \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^mCost(\\hat{y},y)\\) This confused me for a second as \\(\\theta\\) refers to the models parameters. It's the cost across all samples divided by the number of samples. This is also called mean squared error . The reason it's a function of \\(\\theta\\) is because all this \"cost\" we are measuring is for a specific set of \\(\\theta\\) parameters. Now, how do we find or set the best weights or parameters that minimize this cost function? The answer is, we should calculate the minimum point of this cost function and it will show us the best parameters for our model. Although we can find the minimum point of a function using the derivative of a function, there's not an easy way to find the global minimum point for such an equation. Given this complexity, describing how to reach the global minimum for this equation is outside the scope of this video. So, what is the solution? Well we should find another cost function instead, one which has the same behavior but is easier to find its minimum point. Let's plot the desirable cost function for our model. Let's plot the desirable cost function for our model. Recall that our model is y hat. Our actual value is y which equals zero or one, and our model tries to estimate it as we want to find a simple cost function for our model. This means our model is best if it estimates y equals one. In this case, we need a cost function that returns zero if the outcome of our model is one, which is the same as the actual label. And the cost should keep increasing as the outcome of our model gets farther from one. And cost should be very large if the outcome of our model is close to zero. We can see that a function \\(-log(\\hat{y})\\) looks similar to how we want to model this: Recall the derivative of the cost function was hard to calculate. So instead we define as: \\[ Cost(\\hat{y},y) = \\begin{cases} -log(\\hat{y}) & \\text{if } y = 1 \\\\ -log(1-\\hat{y}) & \\text{if } y = 0 \\end{cases} \\] and redefine \\(J(\\theta)\\) as: \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^m y^i log(\\hat{y}^i) + (1-y^i) log(1-\\hat{y}^i)\\)","title":"Logistic Regression Training"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#gradient-descent-to-minimize-the-cost","text":"Let's assume \\(\\hat{y} = \\sigma(\\theta_1x_i + \\theta_2x_2)\\) and \\(J(\\theta) = \\frac{1}{m} \\sum_{i-1}^m y^i log(\\hat{y}^i) + (1-y^i) log(1-\\hat{y}^i)\\) Imagine that it makes a bowl as shown: We need to find the minimum of this cost function. We change the parameters of \\((\\Delta\\theta_1,\\Delta\\theta_2)\\) . We keep taking steps. First while slope is large we take larger steps. As slope gets smaller we take smaller steps.","title":"Gradient Descent To Minimize the Cost"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#how-do-we-calculate-slope","text":"\\(\\frac{\\delta J}{\\delta \\theta_1}\\) = \\(-\\frac{1}{m}\\sum_i^m(y^i-\\hat{y})x_1^i\\) It's technically some math proof we should look up if we are interested. If the derivative of J is positive with respect to \\(\\theta_1\\) then J increases as \\(\\theta1\\) increases. This tells us to move in the opposite direction. We can take smaller steps as the slop decreases. That is just for \\(\\theta_1\\) . We can represent the vector of all the slopes as the gradient vector: \\(\\nabla J = \\left[ \\begin{array}{c} \\frac{\\delta J}{\\delta \\theta_1} \\\\ \\frac{\\delta J}{\\delta \\theta_2} \\\\ \\frac{\\delta J}{\\delta \\theta_3} \\\\ ... \\\\ \\frac{\\delta J}{\\delta \\theta_k} \\\\ \\end{array} \\right]\\) We can use this vector to change or update all parameters. We take the previous values of the parametes and subtract the error derivative. This results in the new parameters for \\(\\theta\\) that we know decrease the cost. \\(New\\theta = old\\theta - \\nabla J\\) Note: Units of gradient vector are \\(\\delta loss/ \\delta \\theta\\) so that it matches \\(\\theta\\) 's units. We can change all weights a little bit to move towards lower loss. Last we add a learning rate \\(\\mu\\) (dimensionless number | hyperparameter) to change the step we take towards lowering the cost. \\(New\\theta = old\\theta - \\mu \\nabla J\\)","title":"How do we calculate slope?"},{"location":"notes/data_science/Coursera/machine_learning/Logistic_Regression_IBM/#logistic-regression-code-example","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why. The dataset includes information about: * Customers who left within the last month \u2013 the column is called Churn * Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies * Customer account information \u2013 how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges * Demographic info about customers \u2013 gender, age range, and if they have partners and dependents Download the data: wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\" import pandas as pd import pylab as pl import numpy as np import scipy.optimize as opt from sklearn import preprocessing import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix from sklearn.metrics import jaccard_score from sklearn.metrics import classification_report from sklearn.metrics import log_loss import itertools churn_df = pd.read_csv(\"ChurnData.csv\") churn_df.head() # Let's select some features for the modeling. churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'wireless','churn']] churn_df['churn'] = churn_df['churn'].astype('int') churn_df.head() Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm: # Define X and Y for the dataset X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) X[0:5] y = np.asarray(churn_df['churn']) y [0:5] # Normalize the dataset X = preprocessing.StandardScaler().fit(X).transform(X) X[0:5] # Train\\Test X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape) Let's build our model using LogisticRegression from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019 solvers. You can find extensive information about the pros and cons of these optimizers if you search it in the internet. The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem of machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. Now let's fit our model with train set: LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train) LR # Predict on test set yhat = LR.predict(X_test) yhat predict_proba returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X): yhat_prob = LR.predict_proba(X_test) yhat_prob # Evaluation (Jaccard Index) jaccard_score(y_test, yhat,pos_label=0) # Confusion Matrix def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') print(confusion_matrix(y_test, yhat, labels=[1,0])) # Plot Confusion Matrix # Compute confusion matrix cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0]) np.set_printoptions(precision=2) # Plot non-normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False, title='Confusion matrix') plt.show() # Print Classification Report print (classification_report(y_test, yhat)) log_loss(y_test, yhat_prob)","title":"Logistic Regression Code Example"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/","text":"Multiclass Prediction SoftMax Regression, One-vs-All & One-vs-One for Multi-class Classification Unlike classification trees and nearest neighbors, the concept of Multi-class classification for linear classifiers is not as straightforward. We can convert logistic regression to Multi-class classification using multinomial logistic regression or SoftMax regression; this is a generalization of logistic regression. SoftMax regression will not work for Support Vector Machines (SVM); One vs. All (One-vs-Rest) and One vs One are two other multi-class classification techniques that can convert most two-class classifiers to a multi-class classifier. SoftMax Regression SoftMax regression is similar to logistic regression, the SoftMax function converts the actual distances i.e. dot products of \\(x\\) with each of the parameters \\(\\theta_i\\) for \\(K\\) classes in the range from 0 to \\(K-1\\) . This is converted to probabilities using the following formula. \\(softmax(x,i) = \\frac{e^{-\\theta_i^Tx}}{\\sum_{j=1}^{K}e^{-\\theta_j^Tx}}\\) The training procedure is almost identical to logistic regression using cross-entropy, but the prediction is different. Consider the three-class example where: \\(y\\) \\(\\epsilon\\) { \\(0,1,2\\) } i.e. \\(y\\) can equal 0,1,2. We would like to classify \\(x\\) . We can use the SoftMax function to generate a probability of how likely the sample belongs to each class. We then make a prediction using the argmax function: \\(\\hat{y} = argmax_i(softmax(x,i))\\) Let\u2019s do an example, consider sample \\(x_i\\) , we will start by creating a table where each column will be the \\(i\u2212th\\) values of the SoftMax function. The index of each column is the same as the class. probability of \\(\\hat{y} = 0\\) probability of \\(\\hat{y} = 1\\) probability of \\(\\hat{y} = 2\\) softmax( \\(x_1\\) ,0) softmax( \\(x_1\\) ,1) softmax( \\(x_1\\) ,2) i=0 i=1 i=2 Sample output probabilities (notice they add to 1): | | | | |-------------------------------|-------------------------------|-------------------------------| | 0.97 | 0.02 | 0.01 | | i = 0 | i = 1 | i = 2 | We can represent the probability as a vector [0.97, 0.02, 0.01]. To get the class we simply apply the argmax function, this returns the index of the largest value. \\(\\hat{y} = argmax_i([0.97, 0.02, 0.01])\\) Geometric Inerpretation Each \\(\\theta_i^Tx\\) is the equation of a hyperplane, we plot the intersection of the three hyperplanes with 0 (I don't know what this means -BB-) in fig 1 as colored lines, in addition, we can overlay several training samples. We also shade the regions where the value of \\(\\theta_i^Tx\\) is largest, this also corresponds to the largest probability. This color corresponds to where a sample \\(x\\) would be classified. For example if the input is in the blue region, the sample would be classified \\(\\hat{y}=0\\) , If the input is in the red region it would be classified as \\(\\hat{y}=1\\) , and in the yellow region \\(\\hat{y}=2\\) . We will use this convention going forward. One problem with SoftMax regression with cross-entropy is it cannot be used for SVM and other types of two-class classifiers. One-vs-All For one-vs-All classification, if we have \\(K\\) classes, we use \\(K\\) two-class classifier models, the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class. We will call this \"dummy\" class. For each classifier, we split the data into two classes. We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a classification, we can use majority vote or use the classifier with the highest probability, disregarding the probabilities generated for the dummy class. Multi-class Classification Although classifiers such as logistic regression and SVM class values are {0,1} and {\u22121,1} respectively, we will use arbitrary class values. Consider the following samples colored according to class y=0 for blue, y=1 for red, and y=2 for yellow: For each class we take the class samples we would like to classify, and the rest will be labeled as a \u201cdummy\u201d class. For example, to build a classifier for the blue class we simply assign all other labels that are not in the blue class to the Dummy class, we then train the classifier accordingly. The result is shown in fig 3 where the classifier predicts blue \\(\\hat{y} = 0\\) and in the purple region where we have our \u201cdummy class\" \\(\\hat{y} = dummy\\) . We repeat the process for each class as shown in Fig 4, the actual class is shown with the same color and the corresponding dummy class is shown in purple. The color of the space is the actual classifier predictions shown in the same manner as above. One issue with one vs all is the ambiguous regions as shown in Fig 5 in purple. In these regions you may get multiple classes for example \\(\\hat{y}_0=0\\) and \\(\\hat{y}_1=1\\) or all the outputs will equal \u201ddummy.\u201d There are several ways to reduce this ambiguous region, you can use the output based on the output of the linear function this is called the fusion rule. We can also use the probability of a sample belonging to the actual class as shown in Fig 6, where we select the class with the largest probability in this case \\(\\hat{y}_0=0\\) ; we disregard the dummy values. These probabilities are scores, as the probabilities are between the dummy class and the actual class not between classes. Just a note packages like Scikit-learn can output probabilities for SVM. I think this chart is misleading as it makes it see like the model is confident that its prediction is either \\(\\hat{y}_0\\) , \\(\\hat{y}_1\\) or \\(\\hat{y}_2\\) . In reality I think the likely scenarios is one where your data was always a \"dummy\" and you get likelihoods that look like: \\(P(\\hat{y}_0 = 0 | x) = 0.45\\) \\(P(\\hat{y}_1 = 0 | x) = 0.39\\) \\(P(\\hat{y}_2 = 0 | x) = 0.28\\) Not confident anywhere but at least mostly confident in \\(\\hat{y}_0\\) . One-vs-One Classification In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1, and 2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2, and a final classifier on samples of class 1 and class 2. Fig 7 is an example of class 0 vs class 1, where we drop training samples of class 2. Using the same convention as above where the color of the training samples are based on their class. The separating plane of the classifier is in black. The color represents the output of the classifier for that particular point in space. We repeat the process for each pair of classes, in Fig 8. For \\(K\\) classes, we have to train \\(K \\frac{K\u22121}{2}\\) classifiers. So if \\(K=3\\) , we have \\((3\u00d72)/2=3\\) classes. To perform Classification on a sample, we perform a majority vote where we select the class with the most predictions. This is shown in Fig 9 where the black point represents a new sample and the output of each classifier is shown in the table. In this case, the final output is one as selected by two of the three classifiers. There is also an ambiguous region but it\u2019s smaller, we can also use similar schemes as in One vs all like the fusion rule or using the probability. Check out the labs for more. Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report from sklearn.metrics import f1_score import pandas as pd plot_colors = \"ryb\" plot_step = 0.02 # This functions Plots different decision boundary def decision_boundary (X,y,model,iris, two=None): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu) if two: cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu) for i, color in zip(np.unique(y), plot_colors): idx = np.where( y== i) plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, s=15) plt.show() else: set_={0,1,2} print(set_) for i, color in zip(range(3), plot_colors): idx = np.where( y== i) if np.any(idx): set_.remove(i) plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, edgecolor='black', s=15) for i in set_: idx = np.where( iris.target== i) plt.scatter(X[idx, 0], X[idx, 1], marker='x',color='black') plt.show() # This function will plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number. def plot_probability_array(X,probability_array): plot_array=np.zeros((X.shape[0],30)) col_start=0 ones=np.ones((X.shape[0],30)) for class_,col_end in enumerate([10,20,30]): plot_array[:,col_start:col_end]= np.repeat(probability_array[:,class_].reshape(-1,1), 10,axis=1) col_start=col_end plt.imshow(plot_array) plt.xticks([]) plt.ylabel(\"samples\") plt.xlabel(\"probability of 3 classes\") plt.colorbar() plt.show() # In ths lab we will use the iris dataset, it consists of 3 different types of irises\u2019 (Setosa y=0, Versicolour y=1, and Virginica y=2) or targets and petal and sepal length and width (data), stored in a 150x4 numpy.ndarray. pair=[1, 3] iris = datasets.load_iris() X = iris.data[:, pair] y = iris.target np.unique(y) # plot plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu) plt.xlabel(\"sepal width (cm)\") plt.ylabel(\"petal width\") plt.show() SoftMax regression is similar to logistic regression, the softmax function converts the actual distances i.e. dot products of \\(x\\) with each of the parameters \\(\\theta_i\\) for the \\(K\\) classes. This is converted to probabilities using the following: \\(softmax(x,i) = \\frac{e^{ \\theta_i^T \\bf x}}{\\sum_{j=1}^K e^{\\theta_j^T x}}\\) # Softmax Regression # sklearn does this automatically, but we can verify the prediction step, we fit the model: lr = LogisticRegression(random_state=0).fit(X, y) # Generate probability uing predict_proba probability=lr.predict_proba(X) # We can plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number. plot_probability_array(X,probability) # First sample probability[0,:] # We see it sums to 1 probability[0,:].sum() # Apply argmax for first prediction np.argmax(probability[0,:]) # 0 # We can apply to each sample softmax_prediction=np.argmax(probability,axis=1) softmax_prediction # we took lr.predict_proba, applied argmax across all probabilities of all classes to get prediction # can have lr do all this with lr.predict. Here we test they are the same: yhat =lr.predict(X) accuracy_score(yhat,softmax_prediction) # 1 We can't use Softmax regression for SVMs let explore two methods of Multi-class Classification. that we can apply to SVM. Sklean performs Multi-class Classification automatically, we can apply the method and calculate the accuracy. Train a SVM classifier with the kernel set to linear , gamma set to 0.5 , and the probability paramter set to True , then train the model using the X and y data. SVM # linear kernel linear_svm = SVC(kernel='linear', gamma=0.5, probability=True) linear_svm.fit(X, y) yhat = linear_svm.predict(X) print (classification_report(y, yhat)) # f1_score f1_score(y, yhat, average='weighted') # notice accuracy_score accuracy_score(y, yhat) # plot decision boundary decision_boundary (X,y,linear_svm,iris) One vs All For one-vs-All classification, if we have K classes, we use K two-class classifier models\u2014the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class we will call this \"dummy\" class. For each classifier, we split the data into two classes. We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a classification, we use the classifier with the highest probability, disregarding the dummy class. #dummy class dummy_class=y.max()+1 #list used for classifiers my_models=[] #iterate through each class for class_ in np.unique(y): #select the index of our class select=(y==class_) temp_y=np.zeros(y.shape) #class, we are trying to classify temp_y[y==class_]=class_ #set other samples to a dummy class temp_y[y!=class_]=dummy_class #Train model and add to list model=SVC(kernel='linear', gamma=.5, probability=True) my_models.append(model.fit(X,temp_y)) #plot decision boundary decision_boundary (X,temp_y,model,iris) For each sample we calculate the probability of belonging to each class, not including the dummy class. probability_array=np.zeros((X.shape[0],3)) for j,model in enumerate(my_models): real_class=np.where(np.array(model.classes_)!=3)[0] probability_array[:,j]=model.predict_proba(X)[:,real_class][:,0] # first sample probability_array[0,:] # As each is the probability of belonging to the actual class and not the dummy class is does not sum to one. probability_array[0,:].sum() # We can plot the probability of belonging to the class. The row number is the sample number. plot_probability_array(X,probability_array) # We can apply the argmax function to each sample to find the class one_vs_all=np.argmax(probability_array,axis=1) one_vs_all # we can calculate the accuracy accuracy_score(y,one_vs_all) # We see the accuracy is less than the one obtained by sklearn, and this is because for SVM sklearn uses one vs one; let's verify it by comparing the outputs. accuracy_score(one_vs_all,yhat) # we see that the output are different, now lets implement one vs one One vs One In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1,2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2 and a final classifier on samples of class 1 and class 2. For \\(K\\) classes, we have to train \\(K(K-1)/2\\) classifiers. So if \\(K=3\\) , we have \\((3x2)/2=3\\) classes. To perform classification on a sample, we perform a majority vote and select the class with the most predictions. # list each class classes_=set(np.unique(y)) classes_ # determine the number of classifiers: K=len(classes_) K*(K-1)/2 # We then train a two-class classifier on each pair of classes. We plot the different training points for each of the two classes pairs=[] left_overs=classes_.copy() #list used for classifiers my_models=[] #iterate through each class for class_ in classes_: #remove class we have seen before left_overs.remove(class_) #the second class in the pair for second_class in left_overs: pairs.append(str(class_)+' and '+str(second_class)) print(\"class {} vs class {} \".format(class_,second_class) ) temp_y=np.zeros(y.shape) #find classes in pair select=np.logical_or(y==class_ , y==second_class) #train model model=SVC(kernel='linear', gamma=.5, probability=True) model.fit(X[select,:],y[select]) my_models.append(model) #Plot decision boundary for each pair and corresponding Training samples. decision_boundary (X[select,:],y[select],model,iris,two=True) # pairs pairs # ['0 and 1', '0 and 2', '1 and 2'] # As we can see, our data is left-skewed, containing greater number of '5' star reviews. # (This means the blue regions have more data points) # Here, we are ploting the disribution of ext length pairs majority_vote_array=np.zeros((X.shape[0],3)) majority_vote_dict={} for j,(model,pair) in enumerate(zip(my_models,pairs)): majority_vote_dict[pair]=model.predict(X) majority_vote_array[:,j]=model.predict(X) pd.DataFrame(majority_vote_dict).head(10) pd.DataFrame(majority_vote_dict).iloc[60:70] # To perform classification on a sample, we perform a majority vote i.e. select the class with the most predictions. We repeat the process for each sample. one_vs_one=np.array([np.bincount(sample.astype(int)).argmax() for sample in majority_vote_array]) one_vs_one # We calculate the accuracy: accuracy_score(y,one_vs_one) # we compare it to sklearn , it's the same! accuracy_score(yhat,one_vs_one)","title":"7. Multiclass Prediction"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#multiclass-prediction","text":"","title":"Multiclass Prediction"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#softmax-regression-one-vs-all-one-vs-one-for-multi-class-classification","text":"Unlike classification trees and nearest neighbors, the concept of Multi-class classification for linear classifiers is not as straightforward. We can convert logistic regression to Multi-class classification using multinomial logistic regression or SoftMax regression; this is a generalization of logistic regression. SoftMax regression will not work for Support Vector Machines (SVM); One vs. All (One-vs-Rest) and One vs One are two other multi-class classification techniques that can convert most two-class classifiers to a multi-class classifier.","title":"SoftMax Regression, One-vs-All &amp; One-vs-One for Multi-class Classification"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#softmax-regression","text":"SoftMax regression is similar to logistic regression, the SoftMax function converts the actual distances i.e. dot products of \\(x\\) with each of the parameters \\(\\theta_i\\) for \\(K\\) classes in the range from 0 to \\(K-1\\) . This is converted to probabilities using the following formula. \\(softmax(x,i) = \\frac{e^{-\\theta_i^Tx}}{\\sum_{j=1}^{K}e^{-\\theta_j^Tx}}\\) The training procedure is almost identical to logistic regression using cross-entropy, but the prediction is different. Consider the three-class example where: \\(y\\) \\(\\epsilon\\) { \\(0,1,2\\) } i.e. \\(y\\) can equal 0,1,2. We would like to classify \\(x\\) . We can use the SoftMax function to generate a probability of how likely the sample belongs to each class. We then make a prediction using the argmax function: \\(\\hat{y} = argmax_i(softmax(x,i))\\) Let\u2019s do an example, consider sample \\(x_i\\) , we will start by creating a table where each column will be the \\(i\u2212th\\) values of the SoftMax function. The index of each column is the same as the class. probability of \\(\\hat{y} = 0\\) probability of \\(\\hat{y} = 1\\) probability of \\(\\hat{y} = 2\\) softmax( \\(x_1\\) ,0) softmax( \\(x_1\\) ,1) softmax( \\(x_1\\) ,2) i=0 i=1 i=2 Sample output probabilities (notice they add to 1): | | | | |-------------------------------|-------------------------------|-------------------------------| | 0.97 | 0.02 | 0.01 | | i = 0 | i = 1 | i = 2 | We can represent the probability as a vector [0.97, 0.02, 0.01]. To get the class we simply apply the argmax function, this returns the index of the largest value. \\(\\hat{y} = argmax_i([0.97, 0.02, 0.01])\\)","title":"SoftMax Regression"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#geometric-inerpretation","text":"Each \\(\\theta_i^Tx\\) is the equation of a hyperplane, we plot the intersection of the three hyperplanes with 0 (I don't know what this means -BB-) in fig 1 as colored lines, in addition, we can overlay several training samples. We also shade the regions where the value of \\(\\theta_i^Tx\\) is largest, this also corresponds to the largest probability. This color corresponds to where a sample \\(x\\) would be classified. For example if the input is in the blue region, the sample would be classified \\(\\hat{y}=0\\) , If the input is in the red region it would be classified as \\(\\hat{y}=1\\) , and in the yellow region \\(\\hat{y}=2\\) . We will use this convention going forward. One problem with SoftMax regression with cross-entropy is it cannot be used for SVM and other types of two-class classifiers.","title":"Geometric Inerpretation"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#one-vs-all","text":"For one-vs-All classification, if we have \\(K\\) classes, we use \\(K\\) two-class classifier models, the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class. We will call this \"dummy\" class. For each classifier, we split the data into two classes. We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a classification, we can use majority vote or use the classifier with the highest probability, disregarding the probabilities generated for the dummy class.","title":"One-vs-All"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#multi-class-classification","text":"Although classifiers such as logistic regression and SVM class values are {0,1} and {\u22121,1} respectively, we will use arbitrary class values. Consider the following samples colored according to class y=0 for blue, y=1 for red, and y=2 for yellow: For each class we take the class samples we would like to classify, and the rest will be labeled as a \u201cdummy\u201d class. For example, to build a classifier for the blue class we simply assign all other labels that are not in the blue class to the Dummy class, we then train the classifier accordingly. The result is shown in fig 3 where the classifier predicts blue \\(\\hat{y} = 0\\) and in the purple region where we have our \u201cdummy class\" \\(\\hat{y} = dummy\\) . We repeat the process for each class as shown in Fig 4, the actual class is shown with the same color and the corresponding dummy class is shown in purple. The color of the space is the actual classifier predictions shown in the same manner as above. One issue with one vs all is the ambiguous regions as shown in Fig 5 in purple. In these regions you may get multiple classes for example \\(\\hat{y}_0=0\\) and \\(\\hat{y}_1=1\\) or all the outputs will equal \u201ddummy.\u201d There are several ways to reduce this ambiguous region, you can use the output based on the output of the linear function this is called the fusion rule. We can also use the probability of a sample belonging to the actual class as shown in Fig 6, where we select the class with the largest probability in this case \\(\\hat{y}_0=0\\) ; we disregard the dummy values. These probabilities are scores, as the probabilities are between the dummy class and the actual class not between classes. Just a note packages like Scikit-learn can output probabilities for SVM. I think this chart is misleading as it makes it see like the model is confident that its prediction is either \\(\\hat{y}_0\\) , \\(\\hat{y}_1\\) or \\(\\hat{y}_2\\) . In reality I think the likely scenarios is one where your data was always a \"dummy\" and you get likelihoods that look like: \\(P(\\hat{y}_0 = 0 | x) = 0.45\\) \\(P(\\hat{y}_1 = 0 | x) = 0.39\\) \\(P(\\hat{y}_2 = 0 | x) = 0.28\\) Not confident anywhere but at least mostly confident in \\(\\hat{y}_0\\) .","title":"Multi-class Classification"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#one-vs-one-classification","text":"In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1, and 2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2, and a final classifier on samples of class 1 and class 2. Fig 7 is an example of class 0 vs class 1, where we drop training samples of class 2. Using the same convention as above where the color of the training samples are based on their class. The separating plane of the classifier is in black. The color represents the output of the classifier for that particular point in space. We repeat the process for each pair of classes, in Fig 8. For \\(K\\) classes, we have to train \\(K \\frac{K\u22121}{2}\\) classifiers. So if \\(K=3\\) , we have \\((3\u00d72)/2=3\\) classes. To perform Classification on a sample, we perform a majority vote where we select the class with the most predictions. This is shown in Fig 9 where the black point represents a new sample and the output of each classifier is shown in the table. In this case, the final output is one as selected by two of the three classifiers. There is also an ambiguous region but it\u2019s smaller, we can also use similar schemes as in One vs all like the fusion rule or using the probability. Check out the labs for more.","title":"One-vs-One Classification"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report from sklearn.metrics import f1_score import pandas as pd plot_colors = \"ryb\" plot_step = 0.02 # This functions Plots different decision boundary def decision_boundary (X,y,model,iris, two=None): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu) if two: cs = plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu) for i, color in zip(np.unique(y), plot_colors): idx = np.where( y== i) plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, s=15) plt.show() else: set_={0,1,2} print(set_) for i, color in zip(range(3), plot_colors): idx = np.where( y== i) if np.any(idx): set_.remove(i) plt.scatter(X[idx, 0], X[idx, 1], label=y,cmap=plt.cm.RdYlBu, edgecolor='black', s=15) for i in set_: idx = np.where( iris.target== i) plt.scatter(X[idx, 0], X[idx, 1], marker='x',color='black') plt.show() # This function will plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number. def plot_probability_array(X,probability_array): plot_array=np.zeros((X.shape[0],30)) col_start=0 ones=np.ones((X.shape[0],30)) for class_,col_end in enumerate([10,20,30]): plot_array[:,col_start:col_end]= np.repeat(probability_array[:,class_].reshape(-1,1), 10,axis=1) col_start=col_end plt.imshow(plot_array) plt.xticks([]) plt.ylabel(\"samples\") plt.xlabel(\"probability of 3 classes\") plt.colorbar() plt.show() # In ths lab we will use the iris dataset, it consists of 3 different types of irises\u2019 (Setosa y=0, Versicolour y=1, and Virginica y=2) or targets and petal and sepal length and width (data), stored in a 150x4 numpy.ndarray. pair=[1, 3] iris = datasets.load_iris() X = iris.data[:, pair] y = iris.target np.unique(y) # plot plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu) plt.xlabel(\"sepal width (cm)\") plt.ylabel(\"petal width\") plt.show() SoftMax regression is similar to logistic regression, the softmax function converts the actual distances i.e. dot products of \\(x\\) with each of the parameters \\(\\theta_i\\) for the \\(K\\) classes. This is converted to probabilities using the following: \\(softmax(x,i) = \\frac{e^{ \\theta_i^T \\bf x}}{\\sum_{j=1}^K e^{\\theta_j^T x}}\\) # Softmax Regression # sklearn does this automatically, but we can verify the prediction step, we fit the model: lr = LogisticRegression(random_state=0).fit(X, y) # Generate probability uing predict_proba probability=lr.predict_proba(X) # We can plot the probability of belonging to each class; each column is the probability of belonging to a class the row number is the sample number. plot_probability_array(X,probability) # First sample probability[0,:] # We see it sums to 1 probability[0,:].sum() # Apply argmax for first prediction np.argmax(probability[0,:]) # 0 # We can apply to each sample softmax_prediction=np.argmax(probability,axis=1) softmax_prediction # we took lr.predict_proba, applied argmax across all probabilities of all classes to get prediction # can have lr do all this with lr.predict. Here we test they are the same: yhat =lr.predict(X) accuracy_score(yhat,softmax_prediction) # 1 We can't use Softmax regression for SVMs let explore two methods of Multi-class Classification. that we can apply to SVM. Sklean performs Multi-class Classification automatically, we can apply the method and calculate the accuracy. Train a SVM classifier with the kernel set to linear , gamma set to 0.5 , and the probability paramter set to True , then train the model using the X and y data.","title":"Lab"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#svm","text":"# linear kernel linear_svm = SVC(kernel='linear', gamma=0.5, probability=True) linear_svm.fit(X, y) yhat = linear_svm.predict(X) print (classification_report(y, yhat)) # f1_score f1_score(y, yhat, average='weighted') # notice accuracy_score accuracy_score(y, yhat) # plot decision boundary decision_boundary (X,y,linear_svm,iris)","title":"SVM"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#one-vs-all_1","text":"For one-vs-All classification, if we have K classes, we use K two-class classifier models\u2014the number of class labels present in the dataset is equal to the number of generated classifiers. First, we create an artificial class we will call this \"dummy\" class. For each classifier, we split the data into two classes. We take the class samples we would like to classify; the rest of the samples will be labelled as a dummy class. We repeat the process for each class. To make a classification, we use the classifier with the highest probability, disregarding the dummy class. #dummy class dummy_class=y.max()+1 #list used for classifiers my_models=[] #iterate through each class for class_ in np.unique(y): #select the index of our class select=(y==class_) temp_y=np.zeros(y.shape) #class, we are trying to classify temp_y[y==class_]=class_ #set other samples to a dummy class temp_y[y!=class_]=dummy_class #Train model and add to list model=SVC(kernel='linear', gamma=.5, probability=True) my_models.append(model.fit(X,temp_y)) #plot decision boundary decision_boundary (X,temp_y,model,iris) For each sample we calculate the probability of belonging to each class, not including the dummy class. probability_array=np.zeros((X.shape[0],3)) for j,model in enumerate(my_models): real_class=np.where(np.array(model.classes_)!=3)[0] probability_array[:,j]=model.predict_proba(X)[:,real_class][:,0] # first sample probability_array[0,:] # As each is the probability of belonging to the actual class and not the dummy class is does not sum to one. probability_array[0,:].sum() # We can plot the probability of belonging to the class. The row number is the sample number. plot_probability_array(X,probability_array) # We can apply the argmax function to each sample to find the class one_vs_all=np.argmax(probability_array,axis=1) one_vs_all # we can calculate the accuracy accuracy_score(y,one_vs_all) # We see the accuracy is less than the one obtained by sklearn, and this is because for SVM sklearn uses one vs one; let's verify it by comparing the outputs. accuracy_score(one_vs_all,yhat) # we see that the output are different, now lets implement one vs one","title":"One vs All"},{"location":"notes/data_science/Coursera/machine_learning/Multiclass_Prediction/#one-vs-one","text":"In One-vs-One classification, we split up the data into each class; we then train a two-class classifier on each pair of classes. For example, if we have class 0,1,2, we would train one classifier on the samples that are class 0 and class 1, a second classifier on samples that are of class 0 and class 2 and a final classifier on samples of class 1 and class 2. For \\(K\\) classes, we have to train \\(K(K-1)/2\\) classifiers. So if \\(K=3\\) , we have \\((3x2)/2=3\\) classes. To perform classification on a sample, we perform a majority vote and select the class with the most predictions. # list each class classes_=set(np.unique(y)) classes_ # determine the number of classifiers: K=len(classes_) K*(K-1)/2 # We then train a two-class classifier on each pair of classes. We plot the different training points for each of the two classes pairs=[] left_overs=classes_.copy() #list used for classifiers my_models=[] #iterate through each class for class_ in classes_: #remove class we have seen before left_overs.remove(class_) #the second class in the pair for second_class in left_overs: pairs.append(str(class_)+' and '+str(second_class)) print(\"class {} vs class {} \".format(class_,second_class) ) temp_y=np.zeros(y.shape) #find classes in pair select=np.logical_or(y==class_ , y==second_class) #train model model=SVC(kernel='linear', gamma=.5, probability=True) model.fit(X[select,:],y[select]) my_models.append(model) #Plot decision boundary for each pair and corresponding Training samples. decision_boundary (X[select,:],y[select],model,iris,two=True) # pairs pairs # ['0 and 1', '0 and 2', '1 and 2'] # As we can see, our data is left-skewed, containing greater number of '5' star reviews. # (This means the blue regions have more data points) # Here, we are ploting the disribution of ext length pairs majority_vote_array=np.zeros((X.shape[0],3)) majority_vote_dict={} for j,(model,pair) in enumerate(zip(my_models,pairs)): majority_vote_dict[pair]=model.predict(X) majority_vote_array[:,j]=model.predict(X) pd.DataFrame(majority_vote_dict).head(10) pd.DataFrame(majority_vote_dict).iloc[60:70] # To perform classification on a sample, we perform a majority vote i.e. select the class with the most predictions. We repeat the process for each sample. one_vs_one=np.array([np.bincount(sample.astype(int)).argmax() for sample in majority_vote_array]) one_vs_one # We calculate the accuracy: accuracy_score(y,one_vs_one) # we compare it to sklearn , it's the same! accuracy_score(yhat,one_vs_one)","title":"One vs One"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/","text":"Regression Trees Source: Machine_Learning_With_Python_IBM What Is A Regression Tree As we have already seen, decision trees can be used for classification, but we can also use them for regression, commonly called regression trees. The basic idea behind regression trees is to split our data into groups based on features, like in classification, and return a prediction that is the average across the data we have already seen. Consider the housing data below, where we are using the \u2018Age\u2019 to predict the \u2018Price\u2019 of a house. import pandas as pd import matplotlib.pyplot as plt age = [1,2,5,7,7,15,39,45,32,21,55,64,75,62,79] price = [515550.73, 491775.83, 457544.34, 506443.94, 524594.98, 368796.62, 362740.81, 361467.51, 411260.00, 390396.54, 94761.54, 115555.98, 73275.04, 116153.90, 135408.55] house_pricing_data_1 = pd.DataFrame({'Age':age, 'Price':price}) house_pricing_data_1.plot.scatter(\"Age\", \"Price\") plt.show() Here, we can see the difference age has on the house prices. Ages between 0 and 10 have an average price of approximately $500,000, ages between 10 and 50 have an average price of approximately $380,000 and houses older than 50 years have an average price of approximately $100,000. Using these general ranges, we can predict the price of a house. Using the data above, we can create the regression tree, as shown below. The prices were determined by calculating the average price of the houses in the age range. Criterion The way the trees are built are similar to classification, but instead of using the entropy criterion. In Classification Trees, we choose features that increase the information gain. In Regression Trees, we choose features that minimize the error. A popular one is the Mean Absolute Error, which we have also seen previously. \\(MAE =\\frac{1}{n}\\sum_{j=1}^{n}|y_j-\\hat{y}_j|\\) How Regression Trees are Built? Take the dataset sample shown below, the first step is to decide what the first decision is. We will do this by using the criterion and checking every single feature in the dataset to see which one produces the minimal error. but first... Categorical Features Categorical features are simple, here we have Near Water so all we need to do is calculate the error if we used this as the first feature. Near Water feature has two categories: \u2018Yes\u2019 and \u2018No\u2019, therefore, we must calculate the average \u2018Price\u2019 of houses in the \u2018Yes\u2019 and \u2018No\u2019 categories. Then we use those values to calculate the average error. index_1 = [0, 1, 2, 3, 4] near_water_1 = ['No']*5 age_1 = [0,45,60,20,90] price_1 = [260831.34, 222939.35, 101882.1, 226868.52, 94868.94] average_house_price_1 = sum(price_1)/len(price_1) absolute_error_no = [abs(i-average_house_price_1) for i in price_1] index_2 = [5,6,7,8,9] near_water_2 = ['Yes']*5 age_2 = [100,5,10,55,25] price_2 = [197703.55, 347982.98, 343150.38, 206713.16, 329768.77] average_house_price_2 = sum(price_2)/len(price_2) absolute_error_yes = [abs(i-average_house_price_2) for i in price_2] Index = index_1 + index_2 Near_water = near_water_1 + near_water_2 Age = age_1 + age_2 Price = price_1 + price_2 Average_house_prices = (average_house_price_1 , average_house_price_2) Absolue_error = absolute_error_no + absolute_error_yes MAE = sum(Absolue_error)/(len(Index)) MAE # 66383.1772 Numerical Features Numerical features, like \u2018Age\u2019, are trickier to handle because we need to find a number, instead of using a category, to split the data by. We do this by creating a boundary between each point, then we calculate the error. For example, first we create the boundary between the first two data points, which are (0, 260831.34) and (5, 347982.98), so we create a boundary of x = 2.5 (The midpoint between the x component of the first two data points). house_pricing_data_2 = pd.DataFrame({\"near_water\": Near_water, \"Age\":Age, \"Price\":Price}) house_pricing_data_2.plot.scatter(\"Age\",\"Price\") plt.plot([2.5]*40, [i*10000 for i in range(1,41)]); plt.title('Housing Data') plt.show() We now find the average price of the houses on the left and right sides of this boundary and use it to calculate the MAE. house_pricing_data_2.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2.reset_index(inplace=True) # Don't forget to run! left = house_pricing_data_2['Price'][0] right = house_pricing_data_2['Price'][1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = house_pricing_data_2['Price'][0] - left_avg right_error = abs(house_pricing_data_2['Price'][1:] - right_avg) MAE = sum(left_error + right_error)/len(house_pricing_data_2) MAE # 66055.242 boundaries = [(house_pricing_data_2.iloc[i]['Age'] + house_pricing_data_2.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2.shape[0]-1)] house_pricing_data_2.plot.scatter(\"Age\",\"Price\") for b in boundaries: plt.plot([b]*40, [i*10000 for i in range(1,41)], 'r'); plt.title('Housing Data') plt.show() This process is then repeated for each boundary between each pair of consecutive points. house_pricing_data_2.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2['Price'][:i+1] right = house_pricing_data_2['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() We can see that the boundary 35 results in the lowest MAE ( 49726.547200 ) in this feature. Choosing the Decision Now, we compare the categorical MAE and the lowest numerical MAE, in this case, the categorical is 66383.1772 , and the numerical is 49726.55 . So, for the first decision, we will use the numerical \u2018Age\u2019 feature. We end up with a regression tree that looks like this: Age<35 | ---------- | | Yes No | | When do we Stop? With the regression tree above, we have two options, we can either stop here and use the average value of the \u2018Yes\u2019 (left) and \u2018No\u2019 (right) to predict the house prices, or we can continue to add more decisions to either branch. There are a few conditions that are commonly used to stop growing regression trees: \u2022 Tree depth \u2022 Number of remaining samples on a branch \u2022 Number of samples on each branch if another decision is made The depth of the tree above, is 1, because there is a single decision and the number of samples on each side is 5. Let\u2019s add more decisions until the depth of the tree is 2. First, we start with the \u2018Yes\u2019 (left) side and we calculate the MAE for the features using the houses that have \u2018Age\u2019 < 35. Adding Decisions Like before, we use the Near Water feature and calculate the MAE on houses with index 0, 3, 6, 7, and 9. Near_water = ['No','Yes','Yes','No','Yes','No','Yes','No','No','Yes'] Age = [0,5,10,20,25,45,55,60,90,100] Price = [260831.34,347982.98,343150.38,226868.52,329768.77,222939.35,206713.16,101882.10, 94868.94,197703.55] house_pricing_data_2 = pd.DataFrame({\"near_water\": Near_water, \"Age\":Age, \"Price\":Price}) # First consider the age < 35 branch house_pricing_data_2_l35 = house_pricing_data_2[house_pricing_data_2['Age'] < 35] nw_age_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'Yes']['Age'] nw_price_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'Yes']['Price'] nw_avg_house_price = sum(nw_price_l35)/len(nw_price_l35) nnw_age_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'No']['Age'] nnw_price_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'No']['Price'] nnw_avg_house_price = sum(nnw_price_l35)/len(nnw_price_l35) nw_avg_house_price nnw_avg_house_price AEs = [abs(y - nw_avg_house_price) for y in nw_price_l35] + [abs(y - nnw_avg_house_price) for y in nnw_price_l35] MAE = sum(AEs)/len(AEs) MAE # 11005.339999999991 # Now, we find the MAE for the boundaries in the \u2018Age\u2019 feature. boundaries = [(house_pricing_data_2_l35.iloc[i]['Age'] + house_pricing_data_2_l35.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2_l35.shape[0]-1)] house_pricing_data_2_l35.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2_l35.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2_l35['Price'][:i+1] right = house_pricing_data_2_l35['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2_l35['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2_l35['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2_l35) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() min(MAEs['MAE']) # lowest is 34029.657000000014 When compared to 11005.339999999991 from the near_water feature, 34029.65700000001 has more error. This means off of the 'Yes' branch or Age < 35 decision, the next decision should be near_water or not. Age<35 | ----- ----- | | Yes No | | ---------- | | near_w not_near_w # Second consider the age > 35 branch house_pricing_data_2_g35 = house_pricing_data_2[house_pricing_data_2['Age'] > 35] nw_age_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'Yes']['Age'] nw_price_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'Yes']['Price'] nw_avg_house_price = sum(nw_price_g35)/len(nw_price_g35) nnw_age_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'No']['Age'] nnw_price_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'No']['Price'] nnw_avg_house_price = sum(nnw_price_g35)/len(nnw_price_g35) nw_avg_house_price nnw_avg_house_price AEs = [abs(y - nw_avg_house_price) for y in nw_price_g35] + [abs(y - nnw_avg_house_price) for y in nnw_price_g35] MAE = sum(AEs)/len(AEs) MAE # 35018.943333333336 # Now, we find the MAE for the boundaries in the \u2018Age\u2019 feature. boundaries = [(house_pricing_data_2_g35.iloc[i]['Age'] + house_pricing_data_2_g35.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2_g35.shape[0]-1)] house_pricing_dathouse_pricing_data_2_g35a_2_g35.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2_g35.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2_g35['Price'][:i+1] right = house_pricing_data_2_g35['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2_g35['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2_g35['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2_g35) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() min(MAEs['MAE']) # lowest is 29732.71266666666 When compared to 35018.943333333336 from the near_water feature, 29732.71266666666 has less error. This means off of the 'No' branch or Age > 35 decision, the next decision should be Age again with the boundary set to 57.5. Age<35 | --------------- --------------- | | Yes No | | --Near_water-- ---Age<57.5--- | | | | Yes No Yes No Price: $A $B $C $D Price: $341,911.57 $243,849.93 $214,826.26 $131,484.86 >>> house_pricing_data_2_g35 index near_water Age Price 0 5 No 45 222939.35 1 6 Yes 55 206713.16 2 7 No 60 101882.10 3 8 No 90 94868.94 4 9 Yes 100 197703.55 >>> house_pricing_data_2_l35 index near_water Age Price 0 0 No 0 260831.34 1 1 Yes 5 347982.98 2 2 Yes 10 343150.38 3 3 No 20 226868.52 4 4 Yes 25 329768.77 >>> A = (347982.98+347982.98+329768.77)/3 # 341,911.57666666666 >>> B = (260831.34+226868.52)/2 # 243,849.93 >>> C = (222939.35+206713.16)/2 # 214,826.255 >>> D = (101882.10+94868.94+ 197703.55)/3 # 131,484.8633333333 Regression Trees Lab If the above was more the lecture, then this is hands on scikit-learn # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/real_estate_data.csv Data Imagine you are a data scientist working for a real estate company that is planning to invest in Boston real estate. You have collected information about various areas of Boston and are tasked with created a model that can predict the median price of houses for that area so it can be used to make offers. The dataset had information on areas/towns not individual houses, the features are CRIM: Crime per capita ZN: Proportion of residential land zoned for lots over 25,000 sq.ft. INDUS: Proportion of non-retail business acres per town CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX: Nitric oxides concentration (parts per 10 million) RM: Average number of rooms per dwelling AGE: Proportion of owner-occupied units built prior to 1940 DIS: Weighted distances to \ufb01ve Boston employment centers RAD: Index of accessibility to radial highways TAX: Full-value property-tax rate per $10,000 PTRAIO: Pupil-teacher ratio by town LSTAT: Percent lower status of the population MEDV: Median value of owner-occupied homes in $1000s # Pandas will allow us to create a dataframe of the data so it can be used and manipulated import pandas as pd # Regression Tree Algorithm from sklearn.tree import DecisionTreeRegressor # Split our data into a training and testing data from sklearn.model_selection import train_test_split data = pd.read_csv(\"real_estate_data.csv\") data.head() data.shape # Counts per col of missing data data.isna().sum() # Let's drop for now data.dropna(inplace=True) data.isna().sum() # Lets split the dataset into our features and what we are predicting (target) X = data.drop(columns=[\"MEDV\"]) # indep vars Y = data[\"MEDV\"] # dep var # Use our buddy train_test_split X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=1) Create Regression Tree Regression Trees are implemented using DecisionTreeRegressor from sklearn.tree . The important parameters of DecisionTreeRegressor are: criterion : {'squared_error', 'absolute_error', 'poisson', 'friedman_mse'} - The function used to measure error max_depth - The max depth the tree can be: min_samples_split - The minimum number of samples required to split a node min_samples_leaf - The minimum number of samples that a leaf can contain max_features : {\"auto\", \"sqrt\", \"log2\"} - The number of feature we examine looking for the best one, used to speed up training First lets start by creating a DecisionTreeRegressor object, setting the criterion parameter to squared_error for Squared Error: Train regression_tree = DecisionTreeRegressor(criterion = \"squared_error\") regression_tree.fit(X_train, Y_train) Evaluation To evaluate our dataset we will use the score method of the DecisionTreeRegressor object providing our testing data, this number is the \\(R^2\\) value which indicates the coefficient of determination # This seems like a predict and score all in one action regression_tree.score(X_test, Y_test) # 0.8434693703647671 # We can also find the average error in our testing set which is the average error in median home value prediction prediction = regression_tree.predict(X_test) print(\"$\",(prediction - Y_test).abs().mean()*1000) # $ 2729.1139240506322","title":"4. Regression Trees"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#regression-trees","text":"Source: Machine_Learning_With_Python_IBM","title":"Regression Trees"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#what-is-a-regression-tree","text":"As we have already seen, decision trees can be used for classification, but we can also use them for regression, commonly called regression trees. The basic idea behind regression trees is to split our data into groups based on features, like in classification, and return a prediction that is the average across the data we have already seen. Consider the housing data below, where we are using the \u2018Age\u2019 to predict the \u2018Price\u2019 of a house. import pandas as pd import matplotlib.pyplot as plt age = [1,2,5,7,7,15,39,45,32,21,55,64,75,62,79] price = [515550.73, 491775.83, 457544.34, 506443.94, 524594.98, 368796.62, 362740.81, 361467.51, 411260.00, 390396.54, 94761.54, 115555.98, 73275.04, 116153.90, 135408.55] house_pricing_data_1 = pd.DataFrame({'Age':age, 'Price':price}) house_pricing_data_1.plot.scatter(\"Age\", \"Price\") plt.show() Here, we can see the difference age has on the house prices. Ages between 0 and 10 have an average price of approximately $500,000, ages between 10 and 50 have an average price of approximately $380,000 and houses older than 50 years have an average price of approximately $100,000. Using these general ranges, we can predict the price of a house. Using the data above, we can create the regression tree, as shown below. The prices were determined by calculating the average price of the houses in the age range.","title":"What Is A Regression Tree"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#criterion","text":"The way the trees are built are similar to classification, but instead of using the entropy criterion. In Classification Trees, we choose features that increase the information gain. In Regression Trees, we choose features that minimize the error. A popular one is the Mean Absolute Error, which we have also seen previously. \\(MAE =\\frac{1}{n}\\sum_{j=1}^{n}|y_j-\\hat{y}_j|\\)","title":"Criterion"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#how-regression-trees-are-built","text":"Take the dataset sample shown below, the first step is to decide what the first decision is. We will do this by using the criterion and checking every single feature in the dataset to see which one produces the minimal error. but first...","title":"How Regression Trees are Built?"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#categorical-features","text":"Categorical features are simple, here we have Near Water so all we need to do is calculate the error if we used this as the first feature. Near Water feature has two categories: \u2018Yes\u2019 and \u2018No\u2019, therefore, we must calculate the average \u2018Price\u2019 of houses in the \u2018Yes\u2019 and \u2018No\u2019 categories. Then we use those values to calculate the average error. index_1 = [0, 1, 2, 3, 4] near_water_1 = ['No']*5 age_1 = [0,45,60,20,90] price_1 = [260831.34, 222939.35, 101882.1, 226868.52, 94868.94] average_house_price_1 = sum(price_1)/len(price_1) absolute_error_no = [abs(i-average_house_price_1) for i in price_1] index_2 = [5,6,7,8,9] near_water_2 = ['Yes']*5 age_2 = [100,5,10,55,25] price_2 = [197703.55, 347982.98, 343150.38, 206713.16, 329768.77] average_house_price_2 = sum(price_2)/len(price_2) absolute_error_yes = [abs(i-average_house_price_2) for i in price_2] Index = index_1 + index_2 Near_water = near_water_1 + near_water_2 Age = age_1 + age_2 Price = price_1 + price_2 Average_house_prices = (average_house_price_1 , average_house_price_2) Absolue_error = absolute_error_no + absolute_error_yes MAE = sum(Absolue_error)/(len(Index)) MAE # 66383.1772","title":"Categorical Features"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#numerical-features","text":"Numerical features, like \u2018Age\u2019, are trickier to handle because we need to find a number, instead of using a category, to split the data by. We do this by creating a boundary between each point, then we calculate the error. For example, first we create the boundary between the first two data points, which are (0, 260831.34) and (5, 347982.98), so we create a boundary of x = 2.5 (The midpoint between the x component of the first two data points). house_pricing_data_2 = pd.DataFrame({\"near_water\": Near_water, \"Age\":Age, \"Price\":Price}) house_pricing_data_2.plot.scatter(\"Age\",\"Price\") plt.plot([2.5]*40, [i*10000 for i in range(1,41)]); plt.title('Housing Data') plt.show() We now find the average price of the houses on the left and right sides of this boundary and use it to calculate the MAE. house_pricing_data_2.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2.reset_index(inplace=True) # Don't forget to run! left = house_pricing_data_2['Price'][0] right = house_pricing_data_2['Price'][1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = house_pricing_data_2['Price'][0] - left_avg right_error = abs(house_pricing_data_2['Price'][1:] - right_avg) MAE = sum(left_error + right_error)/len(house_pricing_data_2) MAE # 66055.242 boundaries = [(house_pricing_data_2.iloc[i]['Age'] + house_pricing_data_2.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2.shape[0]-1)] house_pricing_data_2.plot.scatter(\"Age\",\"Price\") for b in boundaries: plt.plot([b]*40, [i*10000 for i in range(1,41)], 'r'); plt.title('Housing Data') plt.show() This process is then repeated for each boundary between each pair of consecutive points. house_pricing_data_2.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2['Price'][:i+1] right = house_pricing_data_2['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() We can see that the boundary 35 results in the lowest MAE ( 49726.547200 ) in this feature.","title":"Numerical Features"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#choosing-the-decision","text":"Now, we compare the categorical MAE and the lowest numerical MAE, in this case, the categorical is 66383.1772 , and the numerical is 49726.55 . So, for the first decision, we will use the numerical \u2018Age\u2019 feature. We end up with a regression tree that looks like this: Age<35 | ---------- | | Yes No | |","title":"Choosing the Decision"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#when-do-we-stop","text":"With the regression tree above, we have two options, we can either stop here and use the average value of the \u2018Yes\u2019 (left) and \u2018No\u2019 (right) to predict the house prices, or we can continue to add more decisions to either branch. There are a few conditions that are commonly used to stop growing regression trees: \u2022 Tree depth \u2022 Number of remaining samples on a branch \u2022 Number of samples on each branch if another decision is made The depth of the tree above, is 1, because there is a single decision and the number of samples on each side is 5. Let\u2019s add more decisions until the depth of the tree is 2. First, we start with the \u2018Yes\u2019 (left) side and we calculate the MAE for the features using the houses that have \u2018Age\u2019 < 35.","title":"When do we Stop?"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#adding-decisions","text":"Like before, we use the Near Water feature and calculate the MAE on houses with index 0, 3, 6, 7, and 9. Near_water = ['No','Yes','Yes','No','Yes','No','Yes','No','No','Yes'] Age = [0,5,10,20,25,45,55,60,90,100] Price = [260831.34,347982.98,343150.38,226868.52,329768.77,222939.35,206713.16,101882.10, 94868.94,197703.55] house_pricing_data_2 = pd.DataFrame({\"near_water\": Near_water, \"Age\":Age, \"Price\":Price}) # First consider the age < 35 branch house_pricing_data_2_l35 = house_pricing_data_2[house_pricing_data_2['Age'] < 35] nw_age_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'Yes']['Age'] nw_price_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'Yes']['Price'] nw_avg_house_price = sum(nw_price_l35)/len(nw_price_l35) nnw_age_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'No']['Age'] nnw_price_l35 = house_pricing_data_2_l35[house_pricing_data_2_l35['near_water'] == 'No']['Price'] nnw_avg_house_price = sum(nnw_price_l35)/len(nnw_price_l35) nw_avg_house_price nnw_avg_house_price AEs = [abs(y - nw_avg_house_price) for y in nw_price_l35] + [abs(y - nnw_avg_house_price) for y in nnw_price_l35] MAE = sum(AEs)/len(AEs) MAE # 11005.339999999991 # Now, we find the MAE for the boundaries in the \u2018Age\u2019 feature. boundaries = [(house_pricing_data_2_l35.iloc[i]['Age'] + house_pricing_data_2_l35.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2_l35.shape[0]-1)] house_pricing_data_2_l35.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2_l35.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2_l35['Price'][:i+1] right = house_pricing_data_2_l35['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2_l35['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2_l35['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2_l35) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() min(MAEs['MAE']) # lowest is 34029.657000000014 When compared to 11005.339999999991 from the near_water feature, 34029.65700000001 has more error. This means off of the 'Yes' branch or Age < 35 decision, the next decision should be near_water or not. Age<35 | ----- ----- | | Yes No | | ---------- | | near_w not_near_w # Second consider the age > 35 branch house_pricing_data_2_g35 = house_pricing_data_2[house_pricing_data_2['Age'] > 35] nw_age_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'Yes']['Age'] nw_price_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'Yes']['Price'] nw_avg_house_price = sum(nw_price_g35)/len(nw_price_g35) nnw_age_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'No']['Age'] nnw_price_g35 = house_pricing_data_2_g35[house_pricing_data_2_g35['near_water'] == 'No']['Price'] nnw_avg_house_price = sum(nnw_price_g35)/len(nnw_price_g35) nw_avg_house_price nnw_avg_house_price AEs = [abs(y - nw_avg_house_price) for y in nw_price_g35] + [abs(y - nnw_avg_house_price) for y in nnw_price_g35] MAE = sum(AEs)/len(AEs) MAE # 35018.943333333336 # Now, we find the MAE for the boundaries in the \u2018Age\u2019 feature. boundaries = [(house_pricing_data_2_g35.iloc[i]['Age'] + house_pricing_data_2_g35.iloc[i+1]['Age'])/2 for i in range(house_pricing_data_2_g35.shape[0]-1)] house_pricing_dathouse_pricing_data_2_g35a_2_g35.sort_values(\"Age\", inplace=True) # Don't forget to run! house_pricing_data_2_g35.reset_index(inplace=True) # Don't forget to run! MAEs = pd.DataFrame({\"boundary\":[],\"MAE\":[]}) for i,boundary in enumerate(boundaries): left = house_pricing_data_2_g35['Price'][:i+1] right = house_pricing_data_2_g35['Price'][i+1:] left_avg = left if isinstance(left, float) else sum(left)/len(left) right_avg = right if isinstance(right, float) else sum(right)/len(right) left_error = [abs(y - left_avg) for y in house_pricing_data_2_g35['Price'][:i+1]] right_error = [abs(y - right_avg) for y in house_pricing_data_2_g35['Price'][i+1:]] # import pdb; pdb.set_trace() left; right; left_avg; right_avg; left_error; right_error; MAE = sum(left_error + right_error)/len(house_pricing_data_2_g35) print(f\"MAE: {MAE}\") MAEs = pd.concat([MAEs, pd.DataFrame({\"boundary\":[boundary],\"MAE\":MAE})]) MAEs.plot.scatter(\"boundary\",\"MAE\"); plt.show() min(MAEs['MAE']) # lowest is 29732.71266666666 When compared to 35018.943333333336 from the near_water feature, 29732.71266666666 has less error. This means off of the 'No' branch or Age > 35 decision, the next decision should be Age again with the boundary set to 57.5. Age<35 | --------------- --------------- | | Yes No | | --Near_water-- ---Age<57.5--- | | | | Yes No Yes No Price: $A $B $C $D Price: $341,911.57 $243,849.93 $214,826.26 $131,484.86 >>> house_pricing_data_2_g35 index near_water Age Price 0 5 No 45 222939.35 1 6 Yes 55 206713.16 2 7 No 60 101882.10 3 8 No 90 94868.94 4 9 Yes 100 197703.55 >>> house_pricing_data_2_l35 index near_water Age Price 0 0 No 0 260831.34 1 1 Yes 5 347982.98 2 2 Yes 10 343150.38 3 3 No 20 226868.52 4 4 Yes 25 329768.77 >>> A = (347982.98+347982.98+329768.77)/3 # 341,911.57666666666 >>> B = (260831.34+226868.52)/2 # 243,849.93 >>> C = (222939.35+206713.16)/2 # 214,826.255 >>> D = (101882.10+94868.94+ 197703.55)/3 # 131,484.8633333333","title":"Adding Decisions"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#regression-trees-lab","text":"If the above was more the lecture, then this is hands on scikit-learn # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/real_estate_data.csv","title":"Regression Trees Lab"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#data","text":"Imagine you are a data scientist working for a real estate company that is planning to invest in Boston real estate. You have collected information about various areas of Boston and are tasked with created a model that can predict the median price of houses for that area so it can be used to make offers. The dataset had information on areas/towns not individual houses, the features are CRIM: Crime per capita ZN: Proportion of residential land zoned for lots over 25,000 sq.ft. INDUS: Proportion of non-retail business acres per town CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX: Nitric oxides concentration (parts per 10 million) RM: Average number of rooms per dwelling AGE: Proportion of owner-occupied units built prior to 1940 DIS: Weighted distances to \ufb01ve Boston employment centers RAD: Index of accessibility to radial highways TAX: Full-value property-tax rate per $10,000 PTRAIO: Pupil-teacher ratio by town LSTAT: Percent lower status of the population MEDV: Median value of owner-occupied homes in $1000s # Pandas will allow us to create a dataframe of the data so it can be used and manipulated import pandas as pd # Regression Tree Algorithm from sklearn.tree import DecisionTreeRegressor # Split our data into a training and testing data from sklearn.model_selection import train_test_split data = pd.read_csv(\"real_estate_data.csv\") data.head() data.shape # Counts per col of missing data data.isna().sum() # Let's drop for now data.dropna(inplace=True) data.isna().sum() # Lets split the dataset into our features and what we are predicting (target) X = data.drop(columns=[\"MEDV\"]) # indep vars Y = data[\"MEDV\"] # dep var # Use our buddy train_test_split X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=1)","title":"Data"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#create-regression-tree","text":"Regression Trees are implemented using DecisionTreeRegressor from sklearn.tree . The important parameters of DecisionTreeRegressor are: criterion : {'squared_error', 'absolute_error', 'poisson', 'friedman_mse'} - The function used to measure error max_depth - The max depth the tree can be: min_samples_split - The minimum number of samples required to split a node min_samples_leaf - The minimum number of samples that a leaf can contain max_features : {\"auto\", \"sqrt\", \"log2\"} - The number of feature we examine looking for the best one, used to speed up training First lets start by creating a DecisionTreeRegressor object, setting the criterion parameter to squared_error for Squared Error:","title":"Create Regression Tree"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#train","text":"regression_tree = DecisionTreeRegressor(criterion = \"squared_error\") regression_tree.fit(X_train, Y_train)","title":"Train"},{"location":"notes/data_science/Coursera/machine_learning/Regression_Trees_IBM/#evaluation","text":"To evaluate our dataset we will use the score method of the DecisionTreeRegressor object providing our testing data, this number is the \\(R^2\\) value which indicates the coefficient of determination # This seems like a predict and score all in one action regression_tree.score(X_test, Y_test) # 0.8434693703647671 # We can also find the average error in our testing set which is the average error in median home value prediction prediction = regression_tree.predict(X_test) print(\"$\",(prediction - Y_test).abs().mean()*1000) # $ 2729.1139240506322","title":"Evaluation"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/","text":"Support Vecor Machine Classification with SVM Imagine that you've obtained a dataset containing characteristics of thousands of human cell samples extracted from patients who were believed to be at risk of developing cancer. Analysis of the original data showed that many of the characteristics differed significantly between benign and malignant samples. You can use the values of these cell characteristics in samples from other patients, to give an early indication of whether a new sample might be benign or malignant. You can use Support Vector Machine, or SVM, as a classifier to train your model to understand patterns within the data that might show, benign or malignant cells. SVM is a supervised algorithm that classifies cases by finding a separator: 1. Mapping data to a high-dimensional feature space 2. Finding a separator The data should be transformed in such a way that a separator could be drawn as a hyperplane. For example: Consider the following figure, which shows the distribution of a small set of cells only based on their unit size and clump thickness. As you can see, the data points fall into two different categories. It represents a linearly non separable data set. The two categories can be separated with a curve but not a line. We can transfer this data to a higher-dimensional space, for example, mapping it to a three-dimensional space. After the transformation, the boundary between the two categories can be defined by a hyperplane. As we are now in three-dimensional space, the separator is shown as a plane. This plane can be used to classify new or unknown cases. Therefore, the SVM algorithm outputs an optimal hyperplane that categorizes new examples. Now, there are two challenging questions to consider. First, how do we transfer data in such a way that a separator could be drawn as a hyperplane? And two, how can we find the best or optimized hyperplane separator after transformation? For the sake of simplicity, imagine that our dataset is one-dimensional data. This means we have only one feature x. As you can see, it is not linearly separable. So what can we do here? Well, we can transfer it into a two-dimensional space. For example, you can increase the dimension of data by mapping x into a new space using a function with outputs x and x squared. Now the data is linearly separable, right? Notice that as we are in a two-dimensional space, the hyperplane is a line dividing a plane into two parts where each class lays on either side. Now we can use this line to classify new cases. Basically, mapping data into a higher-dimensional space is called, kernelling. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as linear, polynomial, Radial Basis Function,or RBF, and sigmoid. Each of these functions has its own characteristics, its pros and cons, and its equation. But the good news is that you don't need to know them as most of them are already implemented in libraries of data science programming languages. Now we get to another question. Specifically, how do we find the right or optimized separator after transformation? One reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two classes. So the goal is to choose a hyperplane with as big a margin as possible. Examples closest to the hyperplane are support vectors. It is intuitive that only support vectors matter for achieving our goal. We tried to find the hyperplane in such a way that it has the maximum distance to support vectors. Please note that the hyperplane and boundary decision lines have their own equations. So finding the optimized hyperplane can be formalized using an equation which involves quite a bit more math, so I'm not going to go through it here in detail. One reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two classes. So the goal is to choose a hyperplane with as big a margin as possible. Examples closest to the hyperplane are support vectors. It is intuitive that only support vectors matter for achieving our goal. And thus, other trending examples can be ignored. We tried to find the hyperplane in such a way that it has the maximum distance to support vectors. Please note that the hyperplane and boundary decision lines have their own equations. So finding the optimized hyperplane can be formalized using an equation which involves quite a bit more math, so I'm not going to go through it here in detail. That said, the hyperplane is learned from training data using an optimization procedure that maximizes the margin. And like many other problems, this optimization problem can also be solved by gradient descent, which is out of scope. Therefore, the output of the algorithm is the values w and b for the line. You can make classifications using this estimated line. It is enough to plug in input values into the line equation. Then, you can calculate whether an unknown point is above or below the line . Pros and Cons of SVM: * Advantages: - Accurate in high-dimensional spaces - Memory efficient (sub-set of training points) * Disadvantages: - Prone to over fitting, if features >>> # of samples - No probability estimation, which are normally desirable (binary) - Small datasets as large ones take too long (inefficient) SVM Applications Image recognition Test category assignment Detecting spam Sentiment analysis Gene expression data classification Regression Outlier detection Clustering Lab # Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4; Load cancer data The example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)[http://mlearn.ics.uci.edu/MLRepository.html]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are: |Field name |Description | |-----------|---------------------------| |ID |Clump thickness | |Clump |Clump thickness | |UnifSize |Uniformity of cell size | |UnifShape |Uniformity of cell shape | |MargAdh |Marginal adhesion | |SingEpiSize|Single epithelial cell size| |BareNuc |Bare nuclei | |BlandChrom |Bland chromatin | |NormNucl |Normal nucleoli | |Mit |Mitoses | |Class |Benign or malignant | wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/cell_samples.csv\" import pandas as pd import pylab as pl import numpy as np import scipy.optimize as opt from sklearn import preprocessing from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn import svm from sklearn.metrics import classification_report, confusion_matrix import itertools from sklearn.metrics import f1_score from sklearn.metrics import jaccard_score cell_df = pd.read_csv(\"cell_samples.csv\") cell_df.head() # Clump to Mit are features and Class is prediction or target. ax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant'); cell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax); plt.show() # It looks like the BareNuc column includes some values that are not numerical. We can drop those rows: cell_df.dtypes cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()] cell_df['BareNuc'] = cell_df['BareNuc'].astype('int') cell_df.dtypes # Further prep data for algorithms feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']] X = np.asarray(feature_df) X[0:5] # We want the model to predict the value of Class (that is, benign (=2) or malignant (=4)) cell_df['Class'] = cell_df['Class'].astype('int') y = np.asarray(cell_df['Class']) y [0:5] # Train\\Test dataset X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape) Modeling The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as: 1.Linear 2.Polynomial 3.Radial basis function (RBF) 4.Sigmoid Each of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset. We usually choose different functions in turn and compare the results. Let's just use the default, RBF (Radial Basis Function) for this lab. clf = svm.SVC(kernel='rbf') clf.fit(X_train, y_train) # After being fitted, the model can then be used to predict new values yhat = clf.predict(X_test) yhat [0:5] # Evaluation def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') # Confusion Matrix # Compute confusion matrix cnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4]) np.set_printoptions(precision=2) print (classification_report(y_test, yhat)) # Plot non-normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False, title='Confusion matrix') plt.show() # You can also use the sklearn f1 score f1_score(y_test, yhat, average='weighted') # Jaccard index jaccard_score(y_test, yhat,pos_label=2)","title":"6. Support Vector Machine"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#support-vecor-machine","text":"","title":"Support Vecor Machine"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#classification-with-svm","text":"Imagine that you've obtained a dataset containing characteristics of thousands of human cell samples extracted from patients who were believed to be at risk of developing cancer. Analysis of the original data showed that many of the characteristics differed significantly between benign and malignant samples. You can use the values of these cell characteristics in samples from other patients, to give an early indication of whether a new sample might be benign or malignant. You can use Support Vector Machine, or SVM, as a classifier to train your model to understand patterns within the data that might show, benign or malignant cells. SVM is a supervised algorithm that classifies cases by finding a separator: 1. Mapping data to a high-dimensional feature space 2. Finding a separator The data should be transformed in such a way that a separator could be drawn as a hyperplane. For example: Consider the following figure, which shows the distribution of a small set of cells only based on their unit size and clump thickness. As you can see, the data points fall into two different categories. It represents a linearly non separable data set. The two categories can be separated with a curve but not a line. We can transfer this data to a higher-dimensional space, for example, mapping it to a three-dimensional space. After the transformation, the boundary between the two categories can be defined by a hyperplane. As we are now in three-dimensional space, the separator is shown as a plane. This plane can be used to classify new or unknown cases. Therefore, the SVM algorithm outputs an optimal hyperplane that categorizes new examples. Now, there are two challenging questions to consider. First, how do we transfer data in such a way that a separator could be drawn as a hyperplane? And two, how can we find the best or optimized hyperplane separator after transformation? For the sake of simplicity, imagine that our dataset is one-dimensional data. This means we have only one feature x. As you can see, it is not linearly separable. So what can we do here? Well, we can transfer it into a two-dimensional space. For example, you can increase the dimension of data by mapping x into a new space using a function with outputs x and x squared. Now the data is linearly separable, right? Notice that as we are in a two-dimensional space, the hyperplane is a line dividing a plane into two parts where each class lays on either side. Now we can use this line to classify new cases. Basically, mapping data into a higher-dimensional space is called, kernelling. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as linear, polynomial, Radial Basis Function,or RBF, and sigmoid. Each of these functions has its own characteristics, its pros and cons, and its equation. But the good news is that you don't need to know them as most of them are already implemented in libraries of data science programming languages. Now we get to another question. Specifically, how do we find the right or optimized separator after transformation? One reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two classes. So the goal is to choose a hyperplane with as big a margin as possible. Examples closest to the hyperplane are support vectors. It is intuitive that only support vectors matter for achieving our goal. We tried to find the hyperplane in such a way that it has the maximum distance to support vectors. Please note that the hyperplane and boundary decision lines have their own equations. So finding the optimized hyperplane can be formalized using an equation which involves quite a bit more math, so I'm not going to go through it here in detail. One reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two classes. So the goal is to choose a hyperplane with as big a margin as possible. Examples closest to the hyperplane are support vectors. It is intuitive that only support vectors matter for achieving our goal. And thus, other trending examples can be ignored. We tried to find the hyperplane in such a way that it has the maximum distance to support vectors. Please note that the hyperplane and boundary decision lines have their own equations. So finding the optimized hyperplane can be formalized using an equation which involves quite a bit more math, so I'm not going to go through it here in detail. That said, the hyperplane is learned from training data using an optimization procedure that maximizes the margin. And like many other problems, this optimization problem can also be solved by gradient descent, which is out of scope. Therefore, the output of the algorithm is the values w and b for the line. You can make classifications using this estimated line. It is enough to plug in input values into the line equation. Then, you can calculate whether an unknown point is above or below the line . Pros and Cons of SVM: * Advantages: - Accurate in high-dimensional spaces - Memory efficient (sub-set of training points) * Disadvantages: - Prone to over fitting, if features >>> # of samples - No probability estimation, which are normally desirable (binary) - Small datasets as large ones take too long (inefficient)","title":"Classification with SVM"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#svm-applications","text":"Image recognition Test category assignment Detecting spam Sentiment analysis Gene expression data classification Regression Outlier detection Clustering","title":"SVM Applications"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#lab","text":"# Setup Environment cd ~/Desktop; rm -r temp; # To remove cd ~/Desktop; mkdir temp; cd temp; pyenv activate venv3.10.4;","title":"Lab"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#load-cancer-data","text":"The example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)[http://mlearn.ics.uci.edu/MLRepository.html]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are: |Field name |Description | |-----------|---------------------------| |ID |Clump thickness | |Clump |Clump thickness | |UnifSize |Uniformity of cell size | |UnifShape |Uniformity of cell shape | |MargAdh |Marginal adhesion | |SingEpiSize|Single epithelial cell size| |BareNuc |Bare nuclei | |BlandChrom |Bland chromatin | |NormNucl |Normal nucleoli | |Mit |Mitoses | |Class |Benign or malignant | wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/cell_samples.csv\" import pandas as pd import pylab as pl import numpy as np import scipy.optimize as opt from sklearn import preprocessing from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn import svm from sklearn.metrics import classification_report, confusion_matrix import itertools from sklearn.metrics import f1_score from sklearn.metrics import jaccard_score cell_df = pd.read_csv(\"cell_samples.csv\") cell_df.head() # Clump to Mit are features and Class is prediction or target. ax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant'); cell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax); plt.show() # It looks like the BareNuc column includes some values that are not numerical. We can drop those rows: cell_df.dtypes cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()] cell_df['BareNuc'] = cell_df['BareNuc'].astype('int') cell_df.dtypes # Further prep data for algorithms feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']] X = np.asarray(feature_df) X[0:5] # We want the model to predict the value of Class (that is, benign (=2) or malignant (=4)) cell_df['Class'] = cell_df['Class'].astype('int') y = np.asarray(cell_df['Class']) y [0:5] # Train\\Test dataset X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) print ('Train set:', X_train.shape, y_train.shape) print ('Test set:', X_test.shape, y_test.shape)","title":"Load cancer data"},{"location":"notes/data_science/Coursera/machine_learning/Support_Vector_Machine/#modeling","text":"The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as: 1.Linear 2.Polynomial 3.Radial basis function (RBF) 4.Sigmoid Each of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset. We usually choose different functions in turn and compare the results. Let's just use the default, RBF (Radial Basis Function) for this lab. clf = svm.SVC(kernel='rbf') clf.fit(X_train, y_train) # After being fitted, the model can then be used to predict new values yhat = clf.predict(X_test) yhat [0:5] # Evaluation def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') # Confusion Matrix # Compute confusion matrix cnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4]) np.set_printoptions(precision=2) print (classification_report(y_test, yhat)) # Plot non-normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False, title='Confusion matrix') plt.show() # You can also use the sklearn f1 score f1_score(y_test, yhat, average='weighted') # Jaccard index jaccard_score(y_test, yhat,pos_label=2)","title":"Modeling"},{"location":"notes/docker/basics/","text":"Docker Recap and cheat sheet: List Docker CLI commands docker docker container --help Display Docker version and info docker --version docker version docker info Execute Docker image docker run hello-world List Docker images docker image ls docker image ls --all docker image ls -aq List Docker containers (running, all, all in quiet mode) docker container ls docker container ls --all docker container ls \u2013aq Location of Docker logs Source: stackoverflow Use this command to locate the logs: docker inspect --format='{{.LogPath}}' containername and this to view live: tail -f `docker inspect --format='{{.LogPath}}' containername` PS: Don't know why but docker logs containername > docker.log just creates a blank file. List Docker services docker service ls docker stack services getstartedlab Tasks Note the addition of '_web' to the service name docker service ps getstartedlab_web Note: container ls will show you the tasks running as well docker container ls Run without sudo To create the docker group and add your user: Create the docker group: $ sudo groupadd docker Add your user to the docker group. $ sudo usermod -aG docker $USER Log out and log back in so that your group membership is re-evaluated. If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect. On a desktop Linux environment such as X Windows, log out of your session completely and then log back in. On Linux, you can also run the following command to activate the changes to groups: $ newgrp docker Verify that you can run docker commands without sudo. $ docker run hello-world Useful Commands Use docker container inspect 4ca8ce46f817 to inspect a container. You will get a json dump of characteristics that are super useful. Example: [ { \"Id\": \"4ca8ce46f8170fc5c5eeb93bc27e8f84c2e8b32ddafc8e748a124e24fc8ff455\", \"Created\": \"2019-09-26T19:10:42.524604801Z\", \"Path\": \"python\", \"Args\": [ \"predict.py\" ], \"State\": { \"Status\": \"exited\", \"Running\": false, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 0, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2019-09-26T19:10:43.536396229Z\", \"FinishedAt\": \"2019-09-27T01:41:30.393256193Z\" . . .there is a lot more. Useful Flags -v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]] Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker container. If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host. The OPTIONS are a comma delimited list and can be: \u00b7 [rw|ro] \u00b7 [z|Z] \u00b7 [[r]shared|[r]slave|[r]private] \u00b7 [delegated|cached|consistent] \u00b7 [nocopy] -d, --detach=true|false Detached mode: run the container in the background and print the new container ID. The default is false. -i, --interactive=true|false Keep STDIN open even if not attached. The default is false. When set to true, keep stdin open even if not attached. -t, --tty=true|false Allocate a pseudo-TTY. The default is false. When set to true Docker can allocate a pseudo-tty and attach to the standard input of any container. This can be used, for example, to run a throwaway interactive shell. The default is false. The -t option is incompatible with a redirection of the docker client standard input. --name=\" \" Assign a name to the container The operator can identify a container in three ways: \u2502Identifier type \u2502 Example value \u2502 \u2502UUID long identifier \u2502 \"f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778\" \u2502 \u2502UUID short identifier \u2502 \"f78375b1c487\"\u2502 \u2502Name \u2502 \"evil_ptolemy\"| --restart=\" \" To configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following: Flag Description no Do not automatically restart the container. (the default) on-failure Restart the container if it exits due to an error, which manifests as a non-zero exit code. always Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details ) unless-stopped Similar to always , except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts. Managing and Removing Good discussion Summary docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry Investigating and playing around DockerHub: Image specifications: Account: bbearce Repo: Nomenclature: bbearce/ : To start with a clean slate: Stop all containers: docker container rm $(docker container ls -a -q) # Remove all containers docker image rm $(docker image ls -a -q) # Remove all images from this machine If you want to run an instance of an image issue this command(this will download it if you don't have the image): docker run -it -d ashok/pyashokproj bin/bash Keep in mind this is different than getting inside a running container. For this to work the container needs to be already running: docker exec -it <container-name/ID> bash Some dockers can be started and stopped indefinitely either because they are a web server or an OS image(Ubuntu). Others based on things like pythonX.X can't be started up once stopped. A lot of customization and investigation can only really be done inside the running docker. So in order to get in we need to one of two things depending on the docker: OS or web server type docker: Use docker container \u2013ls a to find the stopped container id. Next use docker start <container id> to start the container again. Now use docker ps to see that it is running Python or language image: If you attempt to start the stopped container it will run for a brief second like it already did upon instantiation and then stop. We have to explicitly use docker run -it -d <image> bin/bash . This needs all of those flags (itd) in order to start a container in an interactive session, pipe the terminal from the docker to your local terminal and to run in detached mode. Now that it is running indefinitely we can run docker exec -it <container-name/ID> bash and actually connect to it. Start a docker container: docker start <container-name/ID> Stop a docker container: docker stop <container-name/ID> Take a container and make a new image from it (Example): docker run -itd codalab/codalab-legacy bash Note: Don't forget the -d This downloads a new image and starts an instance. It doesn't have the python package 'seaborn' installed in the python so we are going to add it. We started a container and it is running in detached mode. Now let's connect to it. Use docker ps to get the container's id: docker exec -it d7ef724c4309 bash You should be looking at the prompt: root@f907b9e5d9a6:/# Follow these instructions root@f907b9e5d9a6:/# pip install seaborn ...python is importing stuff... root@f907b9e5d9a6:/# python >>> import seaborn >>> You can see we installed seaborn. Now exit the docker root@f907b9e5d9a6:/# exit . We need to build it to an image: docker commit d7ef724c4309 bbearce/codalab:legacy We just made an image and tagged it with this identifier bbearce/codalab:legacy and we can see it if we execute: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE bbearce/codalab legacy c2d04ddb825a 3 seconds ago 2.65GB codalab/codalab-legacy latest 432ce2829707 20 months ago 2.64GB Now let's push it online (Docker Hub) as an optional last step: docker login docker push bbearce/codalab:legacy Another way to share is with: docker save docker load save/load docs Internet issues Source medium.com I had issues while connected to vpn. It was solved once I disconnected...","title":"Basics"},{"location":"notes/docker/basics/#docker","text":"Recap and cheat sheet: List Docker CLI commands docker docker container --help Display Docker version and info docker --version docker version docker info Execute Docker image docker run hello-world List Docker images docker image ls docker image ls --all docker image ls -aq List Docker containers (running, all, all in quiet mode) docker container ls docker container ls --all docker container ls \u2013aq Location of Docker logs Source: stackoverflow Use this command to locate the logs: docker inspect --format='{{.LogPath}}' containername and this to view live: tail -f `docker inspect --format='{{.LogPath}}' containername` PS: Don't know why but docker logs containername > docker.log just creates a blank file. List Docker services docker service ls docker stack services getstartedlab Tasks Note the addition of '_web' to the service name docker service ps getstartedlab_web Note: container ls will show you the tasks running as well docker container ls","title":"Docker"},{"location":"notes/docker/basics/#run-without-sudo","text":"To create the docker group and add your user: Create the docker group: $ sudo groupadd docker Add your user to the docker group. $ sudo usermod -aG docker $USER Log out and log back in so that your group membership is re-evaluated. If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect. On a desktop Linux environment such as X Windows, log out of your session completely and then log back in. On Linux, you can also run the following command to activate the changes to groups: $ newgrp docker Verify that you can run docker commands without sudo. $ docker run hello-world","title":"Run without sudo"},{"location":"notes/docker/basics/#useful-commands","text":"Use docker container inspect 4ca8ce46f817 to inspect a container. You will get a json dump of characteristics that are super useful. Example: [ { \"Id\": \"4ca8ce46f8170fc5c5eeb93bc27e8f84c2e8b32ddafc8e748a124e24fc8ff455\", \"Created\": \"2019-09-26T19:10:42.524604801Z\", \"Path\": \"python\", \"Args\": [ \"predict.py\" ], \"State\": { \"Status\": \"exited\", \"Running\": false, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 0, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2019-09-26T19:10:43.536396229Z\", \"FinishedAt\": \"2019-09-27T01:41:30.393256193Z\" . . .there is a lot more.","title":"Useful Commands"},{"location":"notes/docker/basics/#useful-flags","text":"-v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]] Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker container. If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host. The OPTIONS are a comma delimited list and can be: \u00b7 [rw|ro] \u00b7 [z|Z] \u00b7 [[r]shared|[r]slave|[r]private] \u00b7 [delegated|cached|consistent] \u00b7 [nocopy] -d, --detach=true|false Detached mode: run the container in the background and print the new container ID. The default is false. -i, --interactive=true|false Keep STDIN open even if not attached. The default is false. When set to true, keep stdin open even if not attached. -t, --tty=true|false Allocate a pseudo-TTY. The default is false. When set to true Docker can allocate a pseudo-tty and attach to the standard input of any container. This can be used, for example, to run a throwaway interactive shell. The default is false. The -t option is incompatible with a redirection of the docker client standard input. --name=\" \" Assign a name to the container The operator can identify a container in three ways: \u2502Identifier type \u2502 Example value \u2502 \u2502UUID long identifier \u2502 \"f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778\" \u2502 \u2502UUID short identifier \u2502 \"f78375b1c487\"\u2502 \u2502Name \u2502 \"evil_ptolemy\"| --restart=\" \" To configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following: Flag Description no Do not automatically restart the container. (the default) on-failure Restart the container if it exits due to an error, which manifests as a non-zero exit code. always Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details ) unless-stopped Similar to always , except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts.","title":"Useful Flags"},{"location":"notes/docker/basics/#managing-and-removing","text":"Good discussion Summary docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry","title":"Managing and Removing"},{"location":"notes/docker/basics/#investigating-and-playing-around","text":"DockerHub: Image specifications: Account: bbearce Repo: Nomenclature: bbearce/ : To start with a clean slate: Stop all containers: docker container rm $(docker container ls -a -q) # Remove all containers docker image rm $(docker image ls -a -q) # Remove all images from this machine If you want to run an instance of an image issue this command(this will download it if you don't have the image): docker run -it -d ashok/pyashokproj bin/bash Keep in mind this is different than getting inside a running container. For this to work the container needs to be already running: docker exec -it <container-name/ID> bash Some dockers can be started and stopped indefinitely either because they are a web server or an OS image(Ubuntu). Others based on things like pythonX.X can't be started up once stopped. A lot of customization and investigation can only really be done inside the running docker. So in order to get in we need to one of two things depending on the docker: OS or web server type docker: Use docker container \u2013ls a to find the stopped container id. Next use docker start <container id> to start the container again. Now use docker ps to see that it is running Python or language image: If you attempt to start the stopped container it will run for a brief second like it already did upon instantiation and then stop. We have to explicitly use docker run -it -d <image> bin/bash . This needs all of those flags (itd) in order to start a container in an interactive session, pipe the terminal from the docker to your local terminal and to run in detached mode. Now that it is running indefinitely we can run docker exec -it <container-name/ID> bash and actually connect to it. Start a docker container: docker start <container-name/ID> Stop a docker container: docker stop <container-name/ID> Take a container and make a new image from it (Example): docker run -itd codalab/codalab-legacy bash Note: Don't forget the -d This downloads a new image and starts an instance. It doesn't have the python package 'seaborn' installed in the python so we are going to add it. We started a container and it is running in detached mode. Now let's connect to it. Use docker ps to get the container's id: docker exec -it d7ef724c4309 bash You should be looking at the prompt: root@f907b9e5d9a6:/# Follow these instructions root@f907b9e5d9a6:/# pip install seaborn ...python is importing stuff... root@f907b9e5d9a6:/# python >>> import seaborn >>> You can see we installed seaborn. Now exit the docker root@f907b9e5d9a6:/# exit . We need to build it to an image: docker commit d7ef724c4309 bbearce/codalab:legacy We just made an image and tagged it with this identifier bbearce/codalab:legacy and we can see it if we execute: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE bbearce/codalab legacy c2d04ddb825a 3 seconds ago 2.65GB codalab/codalab-legacy latest 432ce2829707 20 months ago 2.64GB Now let's push it online (Docker Hub) as an optional last step: docker login docker push bbearce/codalab:legacy Another way to share is with: docker save docker load save/load docs","title":"Investigating and playing around"},{"location":"notes/docker/basics/#internet-issues","text":"Source medium.com I had issues while connected to vpn. It was solved once I disconnected...","title":"Internet issues"},{"location":"notes/docker/docker-compose/","text":"Docker Compose This page is for using docker-compose which allows you to deploy multiple containers in a config file. The details of using the docker-compose command are below as well as a lengthy example using codalab's . Define and run multi-container applications with Docker. Usage: docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] docker-compose -h|--help Options: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent --env-file PATH Specify an alternate environment file Commands: build Build or rebuild services config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information Useful Tid Bits Restarting\\Rebuilding One Container By default docker-compose up will build containers based upon the *.yml definition. Usually if you build a new version of an image you want to be one of the composed containers you'd have to run docker-compose down , which kills and deletes all containers defined in the *.yml . It can annoying and time consuming to do this for all containers when only one container needs updating. Use this code to refresh and rebuild only the container you need: $ docker-compose up -d --no-deps --build <service_name> --no-deps - Don't start linked services. --build - Build images before starting containers. Example docker-compose.yml Here is an example: version: '2' services: # -------------------------------------------------------------------------- # HTTP Server # -------------------------------------------------------------------------- nginx: image: nginx ports: - ${NGINX_PORT}:${NGINX_PORT} - ${SSL_PORT}:${SSL_PORT} command: bash -x /app/docker/run_nginx.sh volumes: - ./certs:/app/certs - ./docker:/app/docker - ./codalab:/app/codalab - ${LOGGING_DIR}/nginx:/var/log/nginx/ env_file: .env links: - django:django logging: options: max-size: \"200k\" container_name: nginx # -------------------------------------------------------------------------- # Database # -------------------------------------------------------------------------- postgres: image: postgres:9.6.3 environment: - POSTGRES_DB=${DB_NAME} - POSTGRES_USER=${DB_USER} - POSTGRES_PASSWORD=${DB_PASSWORD} volumes: - ./docker:/app/docker - ${LOGGING_DIR}/psql:/var/log/psql - ${DB_DATA_PATH}:/var/lib/postgresql/data - ./backups:/app/backups env_file: .env logging: options: max-size: \"200k\" container_name: postgres # -------------------------------------------------------------------------- # Message queue # -------------------------------------------------------------------------- rabbit: build: # Custom Dockerfile for rabbit mostly to make building faster because # of envsubst dependency context: . dockerfile: docker/rabbitmq/Dockerfile hostname: rabbit command: bash -x /app/docker/run_rabbitmq.sh environment: - RABBITMQ_LOGS=/var/log/rabbitmq/output.log - RABBITMQ_SASL_LOGS=/var/log/rabbitmq/output_sasl.log env_file: .env volumes: - ./docker:/app/docker - ./certs:/app/certs - ${LOGGING_DIR}/rabbitmq:/var/log/rabbitmq - ./var/data/rabbitmq/:/var/lib/rabbitmq/mnesia ports: - ${RABBITMQ_PORT}:${RABBITMQ_PORT} - ${RABBITMQ_MANAGEMENT_PORT}:${RABBITMQ_MANAGEMENT_PORT} logging: options: max-size: \"200k\" container_name: rabbit flower: build: context: . dockerfile: docker/flower/Dockerfile hostname: flower ports: - ${FLOWER_PORT}:${FLOWER_PORT} environment: # These aren't set in .env - AMQP_USERNAME=${RABBITMQ_DEFAULT_USER} - AMQP_PASSWORD=${RABBITMQ_DEFAULT_PASS} - AMQP_HOST=rabbit - AMQP_PORT=${RABBITMQ_PORT} - FLOWER_CERTFILE=${SSL_CERTIFICATE} - FLOWER_KEYFILE=${SSL_CERTIFICATE_KEY} volumes: - ./certs:/app/certs env_file: .env links: - rabbit logging: options: max-size: \"200k\" container_name: flower # -------------------------------------------------------------------------- # Cache # -------------------------------------------------------------------------- memcached: image: memcached hostname: memcached command: \"/usr/local/bin/memcached -u memcache\" logging: options: max-size: \"200k\" container_name: memcached # -------------------------------------------------------------------------- # Django # -------------------------------------------------------------------------- django: build: context: . dockerfile: Dockerfile hostname: django ports: - ${DJANGO_PORT}:${DJANGO_PORT} command: bash /app/docker/run_django.sh volumes: - ./certs:/app/certs - ./codalab:/app/codalab - ./docker:/app/docker - ${LOGGING_DIR}/django:/var/log/django/ - ./backups:/app/backups env_file: .env environment: - CONFIG_SERVER_NAME=${CODALAB_SITE_DOMAIN} - PYTHONUNBUFFERED=1 links: - postgres - rabbit - memcached logging: options: max-size: \"200k\" container_name: django # -------------------------------------------------------------------------- # Celery Workers # -------------------------------------------------------------------------- worker_site: build: context: . dockerfile: Dockerfile command: sh /app/docker/run_site.sh volumes: - ./codalab:/app/codalab - ./docker:/app/docker - ${LOGGING_DIR}/worker_site:/var/log/ environment: # Stop memory leaks - DEBUG=False - REQUESTS_CA_BUNDLE=/usr/local/lib/python2.7/site-packages/certifi/cacert.pem env_file: .env links: - postgres - rabbit logging: options: max-size: \"200k\" container_name: worker_site worker_compute: image: codalab/competitions-v1-compute-worker:1.1.7 privileged: true volumes: - ${LOGGING_DIR}/worker_compute:/var/log/ - ${SUBMISSION_TEMP_DIR}:${SUBMISSION_TEMP_DIR} - /var/run/docker.sock:/var/run/docker.sock env_file: .env links: - rabbit logging: options: max-size: \"200k\" mem_limit: 1g memswap_limit: 1g container_name: worker_compute","title":"Docker Compose"},{"location":"notes/docker/docker-compose/#docker-compose","text":"This page is for using docker-compose which allows you to deploy multiple containers in a config file. The details of using the docker-compose command are below as well as a lengthy example using codalab's . Define and run multi-container applications with Docker. Usage: docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] docker-compose -h|--help Options: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent --env-file PATH Specify an alternate environment file Commands: build Build or rebuild services config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information","title":"Docker Compose"},{"location":"notes/docker/docker-compose/#useful-tid-bits","text":"","title":"Useful Tid Bits"},{"location":"notes/docker/docker-compose/#restartingrebuilding-one-container","text":"By default docker-compose up will build containers based upon the *.yml definition. Usually if you build a new version of an image you want to be one of the composed containers you'd have to run docker-compose down , which kills and deletes all containers defined in the *.yml . It can annoying and time consuming to do this for all containers when only one container needs updating. Use this code to refresh and rebuild only the container you need: $ docker-compose up -d --no-deps --build <service_name> --no-deps - Don't start linked services. --build - Build images before starting containers.","title":"Restarting\\Rebuilding One Container"},{"location":"notes/docker/docker-compose/#example-docker-composeyml","text":"Here is an example: version: '2' services: # -------------------------------------------------------------------------- # HTTP Server # -------------------------------------------------------------------------- nginx: image: nginx ports: - ${NGINX_PORT}:${NGINX_PORT} - ${SSL_PORT}:${SSL_PORT} command: bash -x /app/docker/run_nginx.sh volumes: - ./certs:/app/certs - ./docker:/app/docker - ./codalab:/app/codalab - ${LOGGING_DIR}/nginx:/var/log/nginx/ env_file: .env links: - django:django logging: options: max-size: \"200k\" container_name: nginx # -------------------------------------------------------------------------- # Database # -------------------------------------------------------------------------- postgres: image: postgres:9.6.3 environment: - POSTGRES_DB=${DB_NAME} - POSTGRES_USER=${DB_USER} - POSTGRES_PASSWORD=${DB_PASSWORD} volumes: - ./docker:/app/docker - ${LOGGING_DIR}/psql:/var/log/psql - ${DB_DATA_PATH}:/var/lib/postgresql/data - ./backups:/app/backups env_file: .env logging: options: max-size: \"200k\" container_name: postgres # -------------------------------------------------------------------------- # Message queue # -------------------------------------------------------------------------- rabbit: build: # Custom Dockerfile for rabbit mostly to make building faster because # of envsubst dependency context: . dockerfile: docker/rabbitmq/Dockerfile hostname: rabbit command: bash -x /app/docker/run_rabbitmq.sh environment: - RABBITMQ_LOGS=/var/log/rabbitmq/output.log - RABBITMQ_SASL_LOGS=/var/log/rabbitmq/output_sasl.log env_file: .env volumes: - ./docker:/app/docker - ./certs:/app/certs - ${LOGGING_DIR}/rabbitmq:/var/log/rabbitmq - ./var/data/rabbitmq/:/var/lib/rabbitmq/mnesia ports: - ${RABBITMQ_PORT}:${RABBITMQ_PORT} - ${RABBITMQ_MANAGEMENT_PORT}:${RABBITMQ_MANAGEMENT_PORT} logging: options: max-size: \"200k\" container_name: rabbit flower: build: context: . dockerfile: docker/flower/Dockerfile hostname: flower ports: - ${FLOWER_PORT}:${FLOWER_PORT} environment: # These aren't set in .env - AMQP_USERNAME=${RABBITMQ_DEFAULT_USER} - AMQP_PASSWORD=${RABBITMQ_DEFAULT_PASS} - AMQP_HOST=rabbit - AMQP_PORT=${RABBITMQ_PORT} - FLOWER_CERTFILE=${SSL_CERTIFICATE} - FLOWER_KEYFILE=${SSL_CERTIFICATE_KEY} volumes: - ./certs:/app/certs env_file: .env links: - rabbit logging: options: max-size: \"200k\" container_name: flower # -------------------------------------------------------------------------- # Cache # -------------------------------------------------------------------------- memcached: image: memcached hostname: memcached command: \"/usr/local/bin/memcached -u memcache\" logging: options: max-size: \"200k\" container_name: memcached # -------------------------------------------------------------------------- # Django # -------------------------------------------------------------------------- django: build: context: . dockerfile: Dockerfile hostname: django ports: - ${DJANGO_PORT}:${DJANGO_PORT} command: bash /app/docker/run_django.sh volumes: - ./certs:/app/certs - ./codalab:/app/codalab - ./docker:/app/docker - ${LOGGING_DIR}/django:/var/log/django/ - ./backups:/app/backups env_file: .env environment: - CONFIG_SERVER_NAME=${CODALAB_SITE_DOMAIN} - PYTHONUNBUFFERED=1 links: - postgres - rabbit - memcached logging: options: max-size: \"200k\" container_name: django # -------------------------------------------------------------------------- # Celery Workers # -------------------------------------------------------------------------- worker_site: build: context: . dockerfile: Dockerfile command: sh /app/docker/run_site.sh volumes: - ./codalab:/app/codalab - ./docker:/app/docker - ${LOGGING_DIR}/worker_site:/var/log/ environment: # Stop memory leaks - DEBUG=False - REQUESTS_CA_BUNDLE=/usr/local/lib/python2.7/site-packages/certifi/cacert.pem env_file: .env links: - postgres - rabbit logging: options: max-size: \"200k\" container_name: worker_site worker_compute: image: codalab/competitions-v1-compute-worker:1.1.7 privileged: true volumes: - ${LOGGING_DIR}/worker_compute:/var/log/ - ${SUBMISSION_TEMP_DIR}:${SUBMISSION_TEMP_DIR} - /var/run/docker.sock:/var/run/docker.sock env_file: .env links: - rabbit logging: options: max-size: \"200k\" mem_limit: 1g memswap_limit: 1g container_name: worker_compute","title":"Example docker-compose.yml"},{"location":"notes/docker/impersonate/","text":"Impersonate To impersonate a user on a linux machine with docker, do this: # Become someone else WHO_DO_YOU_WANT_TO_BE=bearceb; # Raw users echo $WHO_DO_YOU_WANT_TO_BE export UID_faked=$(id -u $WHO_DO_YOU_WANT_TO_BE); export GID_faked=$(id -g $WHO_DO_YOU_WANT_TO_BE); echo $UID_faked echo $GID_faked # Get group id by name if different group group_name=\"soms-slce-oph1-users\" GID_faked=$(getent group \"$group_name\" | cut -d: -f3) echo $GID_faked ## It's best to not mount / but a specific directory and laser focus on what you're doing VOL_OF_INTEREST=/projects/Chronicle/couchdb_data; # Another tutorial mentioned using the --user to attach as this user but # That user doesn't exist inside with mounting other directories ... /etc/gropups /etc/passwd...etc # I think we need to build the image with this user in there or add them and edit uid and gid to # match local. It's too complex...or just overly so. # Ex: --user $UID_faked:$GID_faked \\ # Below is a better way: docker run -it --rm \\ --workdir=\"$VOL_OF_INTEREST\" \\ --volume=\"$VOL_OF_INTEREST:$VOL_OF_INTEREST\" \\ -e UID_faked=$UID_faked \\ -e GID_faked=$GID_faked \\ ubuntu:latest bash # Check them echo $UID_faked echo $GID_faked # Now we can just change owner to the actual ids, not names chown $UID_faked:$GID_faked /projects/Chronicle/couchdb_data","title":"Impersonate"},{"location":"notes/docker/impersonate/#impersonate","text":"To impersonate a user on a linux machine with docker, do this: # Become someone else WHO_DO_YOU_WANT_TO_BE=bearceb; # Raw users echo $WHO_DO_YOU_WANT_TO_BE export UID_faked=$(id -u $WHO_DO_YOU_WANT_TO_BE); export GID_faked=$(id -g $WHO_DO_YOU_WANT_TO_BE); echo $UID_faked echo $GID_faked # Get group id by name if different group group_name=\"soms-slce-oph1-users\" GID_faked=$(getent group \"$group_name\" | cut -d: -f3) echo $GID_faked ## It's best to not mount / but a specific directory and laser focus on what you're doing VOL_OF_INTEREST=/projects/Chronicle/couchdb_data; # Another tutorial mentioned using the --user to attach as this user but # That user doesn't exist inside with mounting other directories ... /etc/gropups /etc/passwd...etc # I think we need to build the image with this user in there or add them and edit uid and gid to # match local. It's too complex...or just overly so. # Ex: --user $UID_faked:$GID_faked \\ # Below is a better way: docker run -it --rm \\ --workdir=\"$VOL_OF_INTEREST\" \\ --volume=\"$VOL_OF_INTEREST:$VOL_OF_INTEREST\" \\ -e UID_faked=$UID_faked \\ -e GID_faked=$GID_faked \\ ubuntu:latest bash # Check them echo $UID_faked echo $GID_faked # Now we can just change owner to the actual ids, not names chown $UID_faked:$GID_faked /projects/Chronicle/couchdb_data","title":"Impersonate"},{"location":"notes/docker/install/","text":"Install Docker has good documentation here . Uninstall old versions $ sudo apt-get remove docker docker-engine docker.io containerd runc Install using the repository [1] Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update and $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common [2] Add Docker\u2019s official GPG key: $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 , by searching for the last 8 characters of the fingerprint. $ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) <docker@docker.com> sub rsa4096 2017-02-22 [S] [3] Use the following command to set up the stable repository. $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" Install Docker Engine [1] Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io [2] Verify that Docker Engine is installed correctly by running the hello-world image. $ sudo docker run hello-world [3] If you would like to use Docker as a non-root user, you should now consider adding your user to the \u201cdocker\u201d group with something like: $ sudo usermod -aG docker <your-user>","title":"Install"},{"location":"notes/docker/install/#install","text":"Docker has good documentation here .","title":"Install"},{"location":"notes/docker/install/#uninstall-old-versions","text":"$ sudo apt-get remove docker docker-engine docker.io containerd runc","title":"Uninstall old versions"},{"location":"notes/docker/install/#install-using-the-repository","text":"","title":"Install using the repository"},{"location":"notes/docker/install/#1-update-the-apt-package-index-and-install-packages-to-allow-apt-to-use-a-repository-over-https","text":"sudo apt-get update and $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common","title":"[1] Update the apt package index and install packages to allow apt to use a repository over HTTPS:"},{"location":"notes/docker/install/#2-add-dockers-official-gpg-key","text":"$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 , by searching for the last 8 characters of the fingerprint. $ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) <docker@docker.com> sub rsa4096 2017-02-22 [S]","title":"[2] Add Docker\u2019s official GPG key:"},{"location":"notes/docker/install/#3-use-the-following-command-to-set-up-the-stable-repository","text":"$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"","title":"[3] Use the following command to set up the stable repository."},{"location":"notes/docker/install/#install-docker-engine","text":"[1] Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io [2] Verify that Docker Engine is installed correctly by running the hello-world image. $ sudo docker run hello-world [3] If you would like to use Docker as a non-root user, you should now consider adding your user to the \u201cdocker\u201d group with something like: $ sudo usermod -aG docker <your-user>","title":"Install Docker Engine"},{"location":"notes/docker/remote_registry/","text":"Remote Registry Courtesy of docs.docker.com Deploy a registry server Before you can deploy a registry, you need to install Docker on the host. A registry is an instance of the registry image, and runs within Docker. Run a local registry Use a command like the following to start the registry container: $ docker run -d -p 5000:5000 --restart=always --name registry registry:2 The registry is now ready to use. Warning: These first few examples show registry configurations that are only appropriate for testing. A production-ready registry must be protected by TLS and should ideally use an access-control mechanism. Keep reading and then continue to the configuration guide to deploy a production-ready registry. Copy an image from Docker Hub to your registry You can pull an image from Docker Hub and push it to your registry. The following example pulls the ubuntu:16.04 image from Docker Hub and re-tags it as my-ubuntu, then pushes it to the local registry. Finally, the ubuntu:16.04 and my-ubuntu images are deleted locally and the my-ubuntu image is pulled from the local registry. Pull the ubuntu:16.04 image from Docker Hub. $ docker pull ubuntu:16.04 Tag the image as localhost:5000/my-ubuntu. This creates an additional tag for the existing image. When the first part of the tag is a hostname and port, Docker interprets this as the location of a registry, when pushing. $ docker tag ubuntu:16.04 localhost:5000/my-ubuntu Push the image to the local registry running at localhost:5000: $ docker push localhost:5000/my-ubuntu Remove the locally-cached ubuntu:16.04 and localhost:5000/my-ubuntu images, so that you can test pulling the image from your registry. This does not remove the localhost:5000/my-ubuntu image from your registry. $ docker image remove ubuntu:16.04 $ docker image remove localhost:5000/my-ubuntu Pull the localhost:5000/my-ubuntu image from your local registry. $ docker pull localhost:5000/my-ubuntu Stop a local registry To stop the registry, use the same docker container stop command as with any other container. $ docker container stop registry To remove the container, use docker container rm. $ docker container stop registry && docker container rm -v registry SSL and HTTPS","title":"Remote Registry"},{"location":"notes/docker/remote_registry/#remote-registry","text":"Courtesy of docs.docker.com","title":"Remote Registry"},{"location":"notes/docker/remote_registry/#deploy-a-registry-server","text":"Before you can deploy a registry, you need to install Docker on the host. A registry is an instance of the registry image, and runs within Docker.","title":"Deploy a registry server"},{"location":"notes/docker/remote_registry/#run-a-local-registry","text":"Use a command like the following to start the registry container: $ docker run -d -p 5000:5000 --restart=always --name registry registry:2 The registry is now ready to use. Warning: These first few examples show registry configurations that are only appropriate for testing. A production-ready registry must be protected by TLS and should ideally use an access-control mechanism. Keep reading and then continue to the configuration guide to deploy a production-ready registry.","title":"Run a local registry"},{"location":"notes/docker/remote_registry/#copy-an-image-from-docker-hub-to-your-registry","text":"You can pull an image from Docker Hub and push it to your registry. The following example pulls the ubuntu:16.04 image from Docker Hub and re-tags it as my-ubuntu, then pushes it to the local registry. Finally, the ubuntu:16.04 and my-ubuntu images are deleted locally and the my-ubuntu image is pulled from the local registry. Pull the ubuntu:16.04 image from Docker Hub. $ docker pull ubuntu:16.04 Tag the image as localhost:5000/my-ubuntu. This creates an additional tag for the existing image. When the first part of the tag is a hostname and port, Docker interprets this as the location of a registry, when pushing. $ docker tag ubuntu:16.04 localhost:5000/my-ubuntu Push the image to the local registry running at localhost:5000: $ docker push localhost:5000/my-ubuntu Remove the locally-cached ubuntu:16.04 and localhost:5000/my-ubuntu images, so that you can test pulling the image from your registry. This does not remove the localhost:5000/my-ubuntu image from your registry. $ docker image remove ubuntu:16.04 $ docker image remove localhost:5000/my-ubuntu Pull the localhost:5000/my-ubuntu image from your local registry. $ docker pull localhost:5000/my-ubuntu","title":"Copy an image from Docker Hub to your registry"},{"location":"notes/docker/remote_registry/#stop-a-local-registry","text":"To stop the registry, use the same docker container stop command as with any other container. $ docker container stop registry To remove the container, use docker container rm. $ docker container stop registry && docker container rm -v registry","title":"Stop a local registry"},{"location":"notes/docker/remote_registry/#ssl-and-https","text":"","title":"SSL and HTTPS"},{"location":"notes/docker/tutorial/","text":"Docker Command Sumamry We will go over these fundamental commands: docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry Lists images on machine docker images Lists running containers docker ps Lists all containers stopped or otherwise docker ps -a Start a docker docker run bbearce/docker_demo:from_docker_file Delete a container docker container rm 3e64bfa61b7e Removes an image docker image rm 7a59e09539e4 Run an interactive session docker run -it bbearce/docker_demo:from_docker_file bash -i - interactive sessoin -t - join docker terminal to yours bash - run bash inside the session instead of default program(s) Detach from container and leave running <cntrl p+q> To attach back to a docker container docker attach silly_wright or docker attach <container_id> Stop a running docker like this (you'll need to to delete it) docker stop <container_id or name> To name a container --name=<name> docker run -it --name=auto_remove bbearce/docker_demo:from_docker_file bash Tag existing image as a different name (repo/image:tag) docker tag bbearce/docker_demo:from_docker_file bbearce/docker_demo:delete_me Login to your dockerhub account docker login --username=bbearce To push image online docker push bbearce/docker_demo:from_docker_file To automatically remove container when exited docker run -it --rm --name=auto_remove bbearce/docker_demo:from_docker_file bash Use bash flag -v : to mount the host folder inside the docker. docker run -it --rm --name=auto_remove -v /home/bbearce/Desktop/Docker_Demo/mount:/app bbearce/docker_demo:from_docker_file bash from a Dockerfile docker build -t bbearce/docker_demo:from_docker_file .","title":"Tutorial"},{"location":"notes/docker/tutorial/#docker-command-sumamry","text":"","title":"Docker Command Sumamry"},{"location":"notes/docker/tutorial/#we-will-go-over-these-fundamental-commands","text":"docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry","title":"We will go over these fundamental commands:"},{"location":"notes/docker/tutorial/#lists-images-on-machine","text":"docker images","title":"Lists images on machine"},{"location":"notes/docker/tutorial/#lists-running-containers","text":"docker ps","title":"Lists running containers"},{"location":"notes/docker/tutorial/#lists-all-containers-stopped-or-otherwise","text":"docker ps -a","title":"Lists all containers stopped or otherwise"},{"location":"notes/docker/tutorial/#start-a-docker","text":"docker run bbearce/docker_demo:from_docker_file","title":"Start a docker"},{"location":"notes/docker/tutorial/#delete-a-container","text":"docker container rm 3e64bfa61b7e","title":"Delete a container"},{"location":"notes/docker/tutorial/#removes-an-image","text":"docker image rm 7a59e09539e4","title":"Removes an image"},{"location":"notes/docker/tutorial/#run-an-interactive-session","text":"docker run -it bbearce/docker_demo:from_docker_file bash -i - interactive sessoin -t - join docker terminal to yours bash - run bash inside the session instead of default program(s)","title":"Run an interactive session"},{"location":"notes/docker/tutorial/#detach-from-container-and-leave-running","text":"<cntrl p+q>","title":"Detach from container and leave running"},{"location":"notes/docker/tutorial/#to-attach-back-to-a-docker-container","text":"docker attach silly_wright or docker attach <container_id>","title":"To attach back to a docker container"},{"location":"notes/docker/tutorial/#stop-a-running-docker-like-this-youll-need-to-to-delete-it","text":"docker stop <container_id or name>","title":"Stop a running docker like this (you'll need to to delete it)"},{"location":"notes/docker/tutorial/#to-name-a-container","text":"--name=<name> docker run -it --name=auto_remove bbearce/docker_demo:from_docker_file bash","title":"To name a container"},{"location":"notes/docker/tutorial/#tag-existing-image-as-a-different-name-repoimagetag","text":"docker tag bbearce/docker_demo:from_docker_file bbearce/docker_demo:delete_me","title":"Tag existing image as a different name (repo/image:tag)"},{"location":"notes/docker/tutorial/#login-to-your-dockerhub-account","text":"docker login --username=bbearce","title":"Login to your dockerhub account"},{"location":"notes/docker/tutorial/#to-push-image-online","text":"docker push bbearce/docker_demo:from_docker_file","title":"To push image online"},{"location":"notes/docker/tutorial/#to-automatically-remove-container-when-exited","text":"docker run -it --rm --name=auto_remove bbearce/docker_demo:from_docker_file bash","title":"To automatically remove container when exited"},{"location":"notes/docker/tutorial/#use-bash-flag-v-to-mount-the-host-folder-inside-the-docker","text":"docker run -it --rm --name=auto_remove -v /home/bbearce/Desktop/Docker_Demo/mount:/app bbearce/docker_demo:from_docker_file bash","title":"Use bash flag -v : to mount the host folder inside the docker."},{"location":"notes/docker/tutorial/#from-a-dockerfile","text":"docker build -t bbearce/docker_demo:from_docker_file .","title":"from a Dockerfile"},{"location":"notes/fast.ai/introduction/","text":"Introduction Source: fast.ai Fastai is based on pyTorch and if Keras simplifies Tensorflow, Fastai simplifies pyTorch. Getting Started What is a GPU? GPUs (Graphics Processing Units) are specialized computer hardware originally created to render images at high frame rates (most commonly images in video games). Since graphics texturing and shading require more matrix and vector operations executed in parallel than a CPU (Central Processing Unit) can reasonably handle, GPUs were made to perform these calculations more efficiently. Why a GPU? It so happens that Deep Learning also requires super fast matrix computations. So researchers put two and two together and started training models in GPU\u2019s and the rest is history. Deep Learning really only cares about the number of Floating Point Operations (FLOPs) per second. GPUs are highly optimized for that. Try running this inside a Jupyter Notebook: Cell [1]: import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu 1 loop, best of 3: 1.78 s per loop Cell [2]: t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu 1000 loops, best of 3: 15.4 ms per loop If you would like to train anything meaningful in deep learning, a GPU is what you need - specifically an NVIDIA GPU. Why NVIDIA? We recommend you to use an NVIDIA GPU since they are currently the best out there for a few reasons: Currently the fastest Native Pytorch support for CUDA Highly optimized for deep learning with cuDNN","title":"Introduction"},{"location":"notes/fast.ai/introduction/#introduction","text":"Source: fast.ai Fastai is based on pyTorch and if Keras simplifies Tensorflow, Fastai simplifies pyTorch.","title":"Introduction"},{"location":"notes/fast.ai/introduction/#getting-started","text":"","title":"Getting Started"},{"location":"notes/fast.ai/introduction/#what-is-a-gpu","text":"GPUs (Graphics Processing Units) are specialized computer hardware originally created to render images at high frame rates (most commonly images in video games). Since graphics texturing and shading require more matrix and vector operations executed in parallel than a CPU (Central Processing Unit) can reasonably handle, GPUs were made to perform these calculations more efficiently.","title":"What is a GPU?"},{"location":"notes/fast.ai/introduction/#why-a-gpu","text":"It so happens that Deep Learning also requires super fast matrix computations. So researchers put two and two together and started training models in GPU\u2019s and the rest is history. Deep Learning really only cares about the number of Floating Point Operations (FLOPs) per second. GPUs are highly optimized for that. Try running this inside a Jupyter Notebook: Cell [1]: import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu 1 loop, best of 3: 1.78 s per loop Cell [2]: t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu 1000 loops, best of 3: 15.4 ms per loop If you would like to train anything meaningful in deep learning, a GPU is what you need - specifically an NVIDIA GPU. Why NVIDIA? We recommend you to use an NVIDIA GPU since they are currently the best out there for a few reasons: Currently the fastest Native Pytorch support for CUDA Highly optimized for deep learning with cuDNN","title":"Why a GPU?"},{"location":"notes/fast.ai/lessons_part_1/","text":"Lessons (Part 1) Source Notebook bbearce_colab Lesson 1 - Pets Keyboard shortcuts: Ctrl+m b makes a new code block Shift+Enter to run a cell and proceed to next Cntl+Enter runs a cell and does not proceed to next Key function to remember: untar_data is an unzip utility Transfer Learning Take a pre-trained model and then further learn for your specific task. In the course we downloaded \"resnet34-333f7ec4.pth\", which is a re-trained model and then further train it on cats and dogs. The point of this is that this model was previously trained on \"image net\" which is a giant repository of general images like planes, houses, animals and other objects so that the model is already pretty good at general classification. We will then use the fact that it already knows about \"animals\" to further train it to detect species of pet or other more specifc things. Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Train it After they setup the resnet model we atually kick offf the learn step: learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # prints the architecture learn.fit_one_cycle(4) When looking at the cycles 4 is a good start. Cycles and Epochs are the same thing. They are passes throught the data. Each pass the model gets more and more accurate. Here is an example: epoch train_loss valid_loss error_rate time 0 1.372660 0.296747 0.096076 01:33 1 0.605791 0.287795 0.092016 01:32 2 0.393741 0.237194 0.078484 01:32 3 0.277990 0.229392 0.066306 01:32 Save it Now we want to save the model, and this means save the weights or in a classical sense, the coefficients. learn.save('stage-1') Interpret it We need to look at the loss functions, which tell you how well did you predict what you tried to. Incorrect predictions have high losses and good predictions have low losses","title":"Lessons (Part 1)"},{"location":"notes/fast.ai/lessons_part_1/#lessons-part-1","text":"Source Notebook bbearce_colab","title":"Lessons (Part 1)"},{"location":"notes/fast.ai/lessons_part_1/#lesson-1-pets","text":"Keyboard shortcuts: Ctrl+m b makes a new code block Shift+Enter to run a cell and proceed to next Cntl+Enter runs a cell and does not proceed to next Key function to remember: untar_data is an unzip utility","title":"Lesson 1 - Pets"},{"location":"notes/fast.ai/lessons_part_1/#transfer-learning","text":"Take a pre-trained model and then further learn for your specific task. In the course we downloaded \"resnet34-333f7ec4.pth\", which is a re-trained model and then further train it on cats and dogs. The point of this is that this model was previously trained on \"image net\" which is a giant repository of general images like planes, houses, animals and other objects so that the model is already pretty good at general classification. We will then use the fact that it already knows about \"animals\" to further train it to detect species of pet or other more specifc things. Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg...","title":"Transfer Learning"},{"location":"notes/fast.ai/lessons_part_1/#train-it","text":"After they setup the resnet model we atually kick offf the learn step: learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # prints the architecture learn.fit_one_cycle(4) When looking at the cycles 4 is a good start. Cycles and Epochs are the same thing. They are passes throught the data. Each pass the model gets more and more accurate. Here is an example: epoch train_loss valid_loss error_rate time 0 1.372660 0.296747 0.096076 01:33 1 0.605791 0.287795 0.092016 01:32 2 0.393741 0.237194 0.078484 01:32 3 0.277990 0.229392 0.066306 01:32","title":"Train it"},{"location":"notes/fast.ai/lessons_part_1/#save-it","text":"Now we want to save the model, and this means save the weights or in a classical sense, the coefficients. learn.save('stage-1')","title":"Save it"},{"location":"notes/fast.ai/lessons_part_1/#interpret-it","text":"We need to look at the loss functions, which tell you how well did you predict what you tried to. Incorrect predictions have high losses and good predictions have low losses","title":"Interpret it"},{"location":"notes/git/git_basics/","text":"Git Basics Some basic git commands and workflows Courtesy of rogerdudler.github.io Create a Repo Create a new repo with git init : bbearce@bbearce-XPS-15-9560:~/Desktop/git_practice$ git init Initialized empty Git repository in /home/bbearce/Desktop/git_practice/.git/ Checkout a Repo Checkout a repo with git clone : $ git clone /path/to/repository When using a remote server, your command will be: git clone username@host:/path/to/repository Workflow Your local repository consists of three \"trees\" maintained by git. the first one is your Working Directory which holds the actual files. The second one is the Index which acts as a staging area and finally the HEAD which points to the last commit you've made. Add and Commit Basics You can propose changes (add it to the Index) using: git add <filename> or git add * This is the first step in the basic git workflow. To actually commit these changes use: git commit -m \"Commit message\" Now the file is committed to the HEAD , but not in your remote repository yet. git remote -v will tell you which remote you are connected to. SYNOPSIS git remote [-v | --verbose] ... OPTIONS -v, --verbose Be a little more verbose and show remote url after name. NOTE: This must be placed between remote and subcommand. Add hunks Source You can use: git add --patch <filename> or for short: git add -p <filename> Git will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question: Stage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]? Here is a description of each option: y stage this hunk for the next commit n do not stage this hunk for the next commit q quit; do not stage this hunk or any of the remaining hunks a stage this hunk and all later hunks in the file d do not stage this hunk or any of the later hunks in the file g select a hunk to go to / search for a hunk matching the given regex j leave this hunk undecided, see next undecided hunk J leave this hunk undecided, see next hunk k leave this hunk undecided, see previous undecided hunk K leave this hunk undecided, see previous hunk s split the current hunk into smaller hunks e manually edit the current hunk You can then edit the hunk manually by replacing +/- by # (thanks veksen) ? print hunk help Undo a Commit Note: really good notes on the diff between reset, revert and rebase here . Reset There are two main ways to undo commits: * soft: commit undone and changes left un-staged git reset --soft HEAD~1 hard: commit undone and changes completely removed (lost forever) git reset --hard HEAD~1 Here the ~1 indicates go back one commit Revert In Git, the revert command is used to perform a revert operation, i.e., to revert some changes. It is similar to the reset command, but the only difference here is that you perform a new commit to go back to a particular commit. In short, it is fair to say that the git revert command is a commit . The Git revert command does not delete any data while performing the revert operation. Rebase Rebase can sometimes be a little confusing to understand because it is very similar to a merge. The goal of merging and rebasing both is to take the commits from my feature branch and put them on to a master branch or any other branch. Get yourself into the right state which is an up to date develop\\main branch and then checkout your feature branch which is a little behind develop\\main but has a new feature. Ex: bbearce@pop-os:~/Documents/codabench$ git checkout download_buttons Switched to branch 'download_buttons' Your branch is up to date with 'origin/download_buttons'. bbearce@pop-os:~/Documents/codabench$ git branch develop * download_buttons Adding a New Remote Courtesy of articles.assembla.com To add a new remote, use the git remote add command on the terminal, in the directory your repository is stored at. The git remote add command takes two arguments: A remote name, for example, \u201corigin\u201d A remote URL, which you can find on the Source sub-tab of your Git repo #set a new remote git remote add origin git@git.assembla.com:portfolio/space.space_name.git #Verify new remote git remote -v origin git@git.assembla.com:portfolio/space.space_name.git (fetch) origin git@git.assembla.com:portfolio/space.space_name.git (push) Pushing Changes Your changes are now in the HEAD of your local working copy. To send those changes to your remote repository, execute: git push origin master Change master to whatever branch you want to push your changes to. If you have not cloned an existing repository and want to connect your repository to a remote server, you need to add it with: git remote add origin <server> Now you are able to push your changes to the selected remote server. Push Without User:Pass Courtesy of medium.com A way to skip typing my username/password when using https://github, is by changing the HTTPs origin remote which pointing to an HTTP url into an SSH url. For example: https url: https://github.com/<Username>/<Project>.git ssh url: git@github.com:<Username>/<Project>.git You can do: git remote set-url origin git@github.com:<Username>/<Project>.git to change the url. You need to have an ssh key pair generated and added to github Steps: [1]. Generate Key Pair Open Terminal. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Generating public/private rsa key pair. This creates a new ssh key, using the provided email as a label. When you're prompted to \"Enter a file in which to save the key,\" press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, type a secure passphrase. For more information, see Working with SSH key passphrases . Enter passphrase (empty for no passphrase): [Type a passphrase]> Enter same passphrase again: [Type passphrase again] [2]. Add public key to git account or Add public key to git repo Possible Issues: If you get this error: ERROR: Permission to bbearce/code-journal.git denied to deploy key fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. You need to add your ssh key to the ssh-agent. Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key . If you have checked for existing SSH keys and find one you want to use, follow the below instructions. Start the ssh-agent in the background. $ eval \"$(ssh-agent -s)\" > Agent pid 59566 Add your SSH private key to the ssh-agent. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file. $ ssh-add ~/.ssh/<your_ssh_key> not the .pub file but the file with no extension. Add the SSH key to your GitHub account. Access Token Souce This link above helps setup a way to push with access tokens. Branching Branches are used to develop features isolated from each other. The master branch is the \"default\" branch when you create a repository. Use other branches for development and merge them back to the master branch upon completion. Create a new branch named \"feature_x\" and switch to it using: git checkout -b feature_x or git branch feature_x When switching to a new branch commit changes to master before switching. Also same for the new branch when switching to back to master. Switch back to master: git checkout master Delete the branch again: git branch -d feature_x A branch is not available to others unless you push the branch to your remote repository: git push origin <branch> Show remote branches and their local associations git remote show origin See what remote branch your local branch is tracking git branch -vv Ex: debian@codabench-test-acl-220627:~/codabench$ git branch -vv develop 46c1ff7 [origin/develop] Merge pull request #696 from codalab/remove-vnc * leaderboard_v2_comp_fixes 7c2811e edit compute worker so that yaml gets correct amount of inputs local develop tracks remote develop branch at remote \"origin\". leaderboard_v2_comp_fixes is currently not tracking a remote branch Add local branch to remote git push -u origin <branch> # short for git push --set-upstream origin <branch> Check out a PR from your repo's branch or another's: Check out a pr to a local branch git fetch origin pull/763/head:anyname git checkout anyname Update and Merge To update your local repository to the newest commit, execute: git pull In your working directory to fetch and merge remote changes. To merge another branch into your active branch (e.g. master), use: git merge <branch> In both cases git tries to auto-merge changes. Unfortunately, this is not always possible and results in conflicts. You are responsible to merge those conflicts manually by editing the files shown by git. After changing, you need to mark them as merged with: git add <filename> Before merging changes, you can also preview them by using: git diff <source_branch> <target_branch> Undo Merge Source freecodecamp Use git reflog to find commit before merge. Why? Because what if you merged in like 30 commits (I did). Then you have to go one by one to find the correct commit to go back to. Checkout reflog at the bottom of this page under Useful Hints. Using merge (safer) Git provides a safer flag which is --merge . Use the HEAD keyword with the --merge flag by running git reset --merge HEAD~1 . Run git reset --merge <previous-commit> for a specific commit. If you don\u2019t get a response from this command when you use the --merge flag, don\u2019t worry, it works. Using reset (dangerous) If you are not sure of the hash of the last commit, you can run git reset --hard HEAD~1 to go back to the commit before the merge When you get the hash of the commit you want to get back to, run git reset --hard <commit-before-the-merge> Tagging It's recommended to create tags for software releases. This is a known concept, which also exists in SVN. You can create a new tag named 1.0.0 by executing: git tag 1.0.0 1b2e1d63ff The 1b2e1d63ff stands for the first 10 characters of the commit id you want to reference with your tag. You can get the commit id by looking at the...log. Log In its simplest form, you can study repository history using: git log You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help Replace Local Changes In case you did something wrong, which for sure never happens ;), you can replace local changes using the command: git checkout -- <filename> This replaces the changes in your working tree with the last content in HEAD . Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this: git fetch origin git reset --hard origin/master Fetching Remote Branch Source Github Update: A new command, git switch has been added that simplifies the effort. If daves_branch exists on the remote repository, but not on your local branch, you can simply type: git switch daves_branch Since you do not have the branch locally, this will automatically make switch look on the remote repo. It will then also automatically set up remote branch tracking. Original Way: Checking out a local branch from a remote-tracking branch automatically creates what is called a \u201ctracking branch\u201d (and the branch it tracks is called an \u201cupstream branch\u201d). Tracking branches are local branches that have a direct relationship to a remote branch. If you\u2019re on a tracking branch and type git pull , Git automatically knows which server to fetch from and which branch to merge in. When you clone a repository, it generally automatically creates a master branch that tracks origin/master . However, you can set up other tracking branches if you wish\u2009\u2014\u2009ones that track branches on other remotes, or don\u2019t track the master branch. The simple case is the example you just saw, running git checkout -b <branch> <remote>/<branch> . This is a common enough operation that Git provides the --track shorthand: $ git checkout --track origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' In fact, this is so common that there\u2019s even a shortcut for that shortcut. If the branch name you\u2019re trying to checkout (a) doesn\u2019t exist and (b) exactly matches a name on only one remote, Git will create a tracking branch for you: $ git checkout serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' To set up a local branch with a different name than the remote branch, you can easily use the first version with a different local branch name: $ git checkout -b sf origin/serverfix Branch sf set up to track remote branch serverfix from origin. Switched to a new branch 'sf' Now, your local branch sf will automatically pull from origin/serverfix . If you already have a local branch and want to set it to a remote branch you just pulled down, or want to change the upstream branch you\u2019re tracking, you can use the -u or --set-upstream-to option to git branch to explicitly set it at any time. $ git branch -u origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Useful Hints log General git log - shows commits. Commit diff between branches 2 dots: Suppose you are on myBranch, and you want to see only changes SINCE branching from master. git log --oneline master..myBranch 3 dots: The three-dot version gives all changes from the tip of master to the tip of myBranch git log --oneline master..myBranch Reflog Source Reference logs, sometimes known as \"reflogs\" keep track of when the tips of branches and other references in the local repository are modified. Reflogs can be used in various Git operations to specify the previous value of a reference. git reflog <subcommand> <options> The git reflog can be found in the \u201c.git\u201d directory under the following paths: .git/logs/refs/heads/ .git/logs/HEAD .git/logs/refs/stash #if stash has been utilised Default ( git reflog ): git reflog show Ouptut: 82cdf5f (HEAD -> download_buttons) HEAD@{0}: merge develop: Merge made by the 'ort' strategy. 81fade0 HEAD@{1}: checkout: moving from develop to download_buttons 8b575df (origin/develop, origin/HEAD, develop) HEAD@{2}: pull: Fast-forward f55e5cc HEAD@{3}: pull: Fast-forward 2b8de96 HEAD@{4}: checkout: moving from manual_comp_create_queue to develop eca5272 (origin/manual_comp_create_queue) HEAD@{5}: commit: remove debug statement c035cbe HEAD@{6}: commit: undefined calendar var in initial phase creation Gitk If you want a built-in git GUI, use: gitk Use colorful git output: git config color.ui true To show log on just one line per commit, use: git config format.pretty oneline Use interactive adding: git add -i Clone Single Branch Courtesy of stackoverflow git clone -b <mybranch> --single-branch <git-url>","title":"Basics"},{"location":"notes/git/git_basics/#git-basics","text":"Some basic git commands and workflows Courtesy of rogerdudler.github.io","title":"Git Basics"},{"location":"notes/git/git_basics/#create-a-repo","text":"Create a new repo with git init : bbearce@bbearce-XPS-15-9560:~/Desktop/git_practice$ git init Initialized empty Git repository in /home/bbearce/Desktop/git_practice/.git/","title":"Create a Repo"},{"location":"notes/git/git_basics/#checkout-a-repo","text":"Checkout a repo with git clone : $ git clone /path/to/repository When using a remote server, your command will be: git clone username@host:/path/to/repository","title":"Checkout a Repo"},{"location":"notes/git/git_basics/#workflow","text":"Your local repository consists of three \"trees\" maintained by git. the first one is your Working Directory which holds the actual files. The second one is the Index which acts as a staging area and finally the HEAD which points to the last commit you've made.","title":"Workflow"},{"location":"notes/git/git_basics/#add-and-commit","text":"","title":"Add and Commit"},{"location":"notes/git/git_basics/#basics","text":"You can propose changes (add it to the Index) using: git add <filename> or git add * This is the first step in the basic git workflow. To actually commit these changes use: git commit -m \"Commit message\" Now the file is committed to the HEAD , but not in your remote repository yet. git remote -v will tell you which remote you are connected to. SYNOPSIS git remote [-v | --verbose] ... OPTIONS -v, --verbose Be a little more verbose and show remote url after name. NOTE: This must be placed between remote and subcommand.","title":"Basics"},{"location":"notes/git/git_basics/#add-hunks","text":"Source You can use: git add --patch <filename> or for short: git add -p <filename> Git will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question: Stage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]? Here is a description of each option: y stage this hunk for the next commit n do not stage this hunk for the next commit q quit; do not stage this hunk or any of the remaining hunks a stage this hunk and all later hunks in the file d do not stage this hunk or any of the later hunks in the file g select a hunk to go to / search for a hunk matching the given regex j leave this hunk undecided, see next undecided hunk J leave this hunk undecided, see next hunk k leave this hunk undecided, see previous undecided hunk K leave this hunk undecided, see previous hunk s split the current hunk into smaller hunks e manually edit the current hunk You can then edit the hunk manually by replacing +/- by # (thanks veksen) ? print hunk help","title":"Add hunks"},{"location":"notes/git/git_basics/#undo-a-commit","text":"Note: really good notes on the diff between reset, revert and rebase here .","title":"Undo a Commit"},{"location":"notes/git/git_basics/#reset","text":"There are two main ways to undo commits: * soft: commit undone and changes left un-staged git reset --soft HEAD~1 hard: commit undone and changes completely removed (lost forever) git reset --hard HEAD~1 Here the ~1 indicates go back one commit","title":"Reset"},{"location":"notes/git/git_basics/#revert","text":"In Git, the revert command is used to perform a revert operation, i.e., to revert some changes. It is similar to the reset command, but the only difference here is that you perform a new commit to go back to a particular commit. In short, it is fair to say that the git revert command is a commit . The Git revert command does not delete any data while performing the revert operation.","title":"Revert"},{"location":"notes/git/git_basics/#rebase","text":"Rebase can sometimes be a little confusing to understand because it is very similar to a merge. The goal of merging and rebasing both is to take the commits from my feature branch and put them on to a master branch or any other branch. Get yourself into the right state which is an up to date develop\\main branch and then checkout your feature branch which is a little behind develop\\main but has a new feature. Ex: bbearce@pop-os:~/Documents/codabench$ git checkout download_buttons Switched to branch 'download_buttons' Your branch is up to date with 'origin/download_buttons'. bbearce@pop-os:~/Documents/codabench$ git branch develop * download_buttons","title":"Rebase"},{"location":"notes/git/git_basics/#adding-a-new-remote","text":"Courtesy of articles.assembla.com To add a new remote, use the git remote add command on the terminal, in the directory your repository is stored at. The git remote add command takes two arguments: A remote name, for example, \u201corigin\u201d A remote URL, which you can find on the Source sub-tab of your Git repo #set a new remote git remote add origin git@git.assembla.com:portfolio/space.space_name.git #Verify new remote git remote -v origin git@git.assembla.com:portfolio/space.space_name.git (fetch) origin git@git.assembla.com:portfolio/space.space_name.git (push)","title":"Adding a New Remote"},{"location":"notes/git/git_basics/#pushing-changes","text":"Your changes are now in the HEAD of your local working copy. To send those changes to your remote repository, execute: git push origin master Change master to whatever branch you want to push your changes to. If you have not cloned an existing repository and want to connect your repository to a remote server, you need to add it with: git remote add origin <server> Now you are able to push your changes to the selected remote server.","title":"Pushing Changes"},{"location":"notes/git/git_basics/#push-without-userpass","text":"Courtesy of medium.com A way to skip typing my username/password when using https://github, is by changing the HTTPs origin remote which pointing to an HTTP url into an SSH url. For example: https url: https://github.com/<Username>/<Project>.git ssh url: git@github.com:<Username>/<Project>.git You can do: git remote set-url origin git@github.com:<Username>/<Project>.git to change the url. You need to have an ssh key pair generated and added to github Steps: [1]. Generate Key Pair Open Terminal. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Generating public/private rsa key pair. This creates a new ssh key, using the provided email as a label. When you're prompted to \"Enter a file in which to save the key,\" press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, type a secure passphrase. For more information, see Working with SSH key passphrases . Enter passphrase (empty for no passphrase): [Type a passphrase]> Enter same passphrase again: [Type passphrase again] [2]. Add public key to git account or Add public key to git repo","title":"Push Without User:Pass"},{"location":"notes/git/git_basics/#possible-issues","text":"If you get this error: ERROR: Permission to bbearce/code-journal.git denied to deploy key fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. You need to add your ssh key to the ssh-agent. Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key . If you have checked for existing SSH keys and find one you want to use, follow the below instructions. Start the ssh-agent in the background. $ eval \"$(ssh-agent -s)\" > Agent pid 59566 Add your SSH private key to the ssh-agent. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file. $ ssh-add ~/.ssh/<your_ssh_key> not the .pub file but the file with no extension. Add the SSH key to your GitHub account.","title":"Possible Issues:"},{"location":"notes/git/git_basics/#access-token","text":"Souce This link above helps setup a way to push with access tokens.","title":"Access Token"},{"location":"notes/git/git_basics/#branching","text":"Branches are used to develop features isolated from each other. The master branch is the \"default\" branch when you create a repository. Use other branches for development and merge them back to the master branch upon completion. Create a new branch named \"feature_x\" and switch to it using: git checkout -b feature_x or git branch feature_x When switching to a new branch commit changes to master before switching. Also same for the new branch when switching to back to master. Switch back to master: git checkout master Delete the branch again: git branch -d feature_x A branch is not available to others unless you push the branch to your remote repository: git push origin <branch>","title":"Branching"},{"location":"notes/git/git_basics/#show-remote-branches-and-their-local-associations","text":"git remote show origin","title":"Show remote branches and their local associations"},{"location":"notes/git/git_basics/#see-what-remote-branch-your-local-branch-is-tracking","text":"git branch -vv Ex: debian@codabench-test-acl-220627:~/codabench$ git branch -vv develop 46c1ff7 [origin/develop] Merge pull request #696 from codalab/remove-vnc * leaderboard_v2_comp_fixes 7c2811e edit compute worker so that yaml gets correct amount of inputs local develop tracks remote develop branch at remote \"origin\". leaderboard_v2_comp_fixes is currently not tracking a remote branch","title":"See what remote branch your local branch is tracking"},{"location":"notes/git/git_basics/#add-local-branch-to-remote","text":"git push -u origin <branch> # short for git push --set-upstream origin <branch>","title":"Add local branch to remote"},{"location":"notes/git/git_basics/#check-out-a-pr-from-your-repos-branch-or-anothers","text":"Check out a pr to a local branch git fetch origin pull/763/head:anyname git checkout anyname","title":"Check out a PR from your repo's branch or another's:"},{"location":"notes/git/git_basics/#update-and-merge","text":"To update your local repository to the newest commit, execute: git pull In your working directory to fetch and merge remote changes. To merge another branch into your active branch (e.g. master), use: git merge <branch> In both cases git tries to auto-merge changes. Unfortunately, this is not always possible and results in conflicts. You are responsible to merge those conflicts manually by editing the files shown by git. After changing, you need to mark them as merged with: git add <filename> Before merging changes, you can also preview them by using: git diff <source_branch> <target_branch>","title":"Update and Merge"},{"location":"notes/git/git_basics/#undo-merge","text":"Source freecodecamp Use git reflog to find commit before merge. Why? Because what if you merged in like 30 commits (I did). Then you have to go one by one to find the correct commit to go back to. Checkout reflog at the bottom of this page under Useful Hints.","title":"Undo Merge"},{"location":"notes/git/git_basics/#using-merge-safer","text":"Git provides a safer flag which is --merge . Use the HEAD keyword with the --merge flag by running git reset --merge HEAD~1 . Run git reset --merge <previous-commit> for a specific commit. If you don\u2019t get a response from this command when you use the --merge flag, don\u2019t worry, it works.","title":"Using merge (safer)"},{"location":"notes/git/git_basics/#using-reset-dangerous","text":"If you are not sure of the hash of the last commit, you can run git reset --hard HEAD~1 to go back to the commit before the merge When you get the hash of the commit you want to get back to, run git reset --hard <commit-before-the-merge>","title":"Using reset (dangerous)"},{"location":"notes/git/git_basics/#tagging","text":"It's recommended to create tags for software releases. This is a known concept, which also exists in SVN. You can create a new tag named 1.0.0 by executing: git tag 1.0.0 1b2e1d63ff The 1b2e1d63ff stands for the first 10 characters of the commit id you want to reference with your tag. You can get the commit id by looking at the...log.","title":"Tagging"},{"location":"notes/git/git_basics/#log","text":"In its simplest form, you can study repository history using: git log You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help","title":"Log"},{"location":"notes/git/git_basics/#replace-local-changes","text":"In case you did something wrong, which for sure never happens ;), you can replace local changes using the command: git checkout -- <filename> This replaces the changes in your working tree with the last content in HEAD . Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this: git fetch origin git reset --hard origin/master","title":"Replace Local Changes"},{"location":"notes/git/git_basics/#fetching-remote-branch","text":"Source Github","title":"Fetching Remote Branch"},{"location":"notes/git/git_basics/#update","text":"A new command, git switch has been added that simplifies the effort. If daves_branch exists on the remote repository, but not on your local branch, you can simply type: git switch daves_branch Since you do not have the branch locally, this will automatically make switch look on the remote repo. It will then also automatically set up remote branch tracking.","title":"Update:"},{"location":"notes/git/git_basics/#original-way","text":"Checking out a local branch from a remote-tracking branch automatically creates what is called a \u201ctracking branch\u201d (and the branch it tracks is called an \u201cupstream branch\u201d). Tracking branches are local branches that have a direct relationship to a remote branch. If you\u2019re on a tracking branch and type git pull , Git automatically knows which server to fetch from and which branch to merge in. When you clone a repository, it generally automatically creates a master branch that tracks origin/master . However, you can set up other tracking branches if you wish\u2009\u2014\u2009ones that track branches on other remotes, or don\u2019t track the master branch. The simple case is the example you just saw, running git checkout -b <branch> <remote>/<branch> . This is a common enough operation that Git provides the --track shorthand: $ git checkout --track origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' In fact, this is so common that there\u2019s even a shortcut for that shortcut. If the branch name you\u2019re trying to checkout (a) doesn\u2019t exist and (b) exactly matches a name on only one remote, Git will create a tracking branch for you: $ git checkout serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' To set up a local branch with a different name than the remote branch, you can easily use the first version with a different local branch name: $ git checkout -b sf origin/serverfix Branch sf set up to track remote branch serverfix from origin. Switched to a new branch 'sf' Now, your local branch sf will automatically pull from origin/serverfix . If you already have a local branch and want to set it to a remote branch you just pulled down, or want to change the upstream branch you\u2019re tracking, you can use the -u or --set-upstream-to option to git branch to explicitly set it at any time. $ git branch -u origin/serverfix Branch serverfix set up to track remote branch serverfix from origin.","title":"Original Way:"},{"location":"notes/git/git_basics/#useful-hints","text":"","title":"Useful Hints"},{"location":"notes/git/git_basics/#log_1","text":"","title":"log"},{"location":"notes/git/git_basics/#general","text":"git log - shows commits.","title":"General"},{"location":"notes/git/git_basics/#commit-diff-between-branches","text":"2 dots: Suppose you are on myBranch, and you want to see only changes SINCE branching from master. git log --oneline master..myBranch 3 dots: The three-dot version gives all changes from the tip of master to the tip of myBranch git log --oneline master..myBranch","title":"Commit diff between branches"},{"location":"notes/git/git_basics/#reflog","text":"Source Reference logs, sometimes known as \"reflogs\" keep track of when the tips of branches and other references in the local repository are modified. Reflogs can be used in various Git operations to specify the previous value of a reference. git reflog <subcommand> <options> The git reflog can be found in the \u201c.git\u201d directory under the following paths: .git/logs/refs/heads/ .git/logs/HEAD .git/logs/refs/stash #if stash has been utilised Default ( git reflog ): git reflog show Ouptut: 82cdf5f (HEAD -> download_buttons) HEAD@{0}: merge develop: Merge made by the 'ort' strategy. 81fade0 HEAD@{1}: checkout: moving from develop to download_buttons 8b575df (origin/develop, origin/HEAD, develop) HEAD@{2}: pull: Fast-forward f55e5cc HEAD@{3}: pull: Fast-forward 2b8de96 HEAD@{4}: checkout: moving from manual_comp_create_queue to develop eca5272 (origin/manual_comp_create_queue) HEAD@{5}: commit: remove debug statement c035cbe HEAD@{6}: commit: undefined calendar var in initial phase creation","title":"Reflog"},{"location":"notes/git/git_basics/#gitk","text":"If you want a built-in git GUI, use: gitk Use colorful git output: git config color.ui true To show log on just one line per commit, use: git config format.pretty oneline Use interactive adding: git add -i","title":"Gitk"},{"location":"notes/git/git_basics/#clone-single-branch","text":"Courtesy of stackoverflow git clone -b <mybranch> --single-branch <git-url>","title":"Clone Single Branch"},{"location":"notes/git/git_sub_trees/","text":"Subtrees Courtesty of github Typically, a subtree merge is used to contain a repository within a repository. The \"subrepository\" is stored in a folder of the main repository. The best way to explain subtree merges is to show by example. We will: Make an empty repository called test that represents our project Merge another repository into it as a subtree called Spoon-Knife . The test project will use that subproject as if it were part of the same repository. Fetch updates from Spoon-Knife into our test project. Setting up the empty repository for a subtree merge Open Terminal. Create a new directory and navigate to it. $ mkdir test $ cd test Initialize a new Git repository. $ git init > Initialized empty Git repository in /Users/octocat/tmp/test/.git/ Create and commit a new file. $ touch .gitignore $ git add .gitignore $ git commit -m \"initial commit\" > [master (root-commit) 3146c2a] initial commit > 0 files changed, 0 insertions(+), 0 deletions(-) > create mode 100644 .gitignore Adding a new repository as a subtree Add a new remote URL pointing to the separate project that we're interested in. $ git remote add -f spoon-knife git@github.com:octocat/Spoon-Knife.git > Updating spoon-knife > warning: no common commits > remote: Counting objects: 1732, done. > remote: Compressing objects: 100% (750/750), done. > remote: Total 1732 (delta 1086), reused 1558 (delta 967) > Receiving objects: 100% (1732/1732), 528.19 KiB | 621 KiB/s, done. > Resolving deltas: 100% (1086/1086), done. > From git://github.com/octocat/Spoon-Knife > * [new branch] master -> Spoon-Knife/master Merge the Spoon-Knife project into the local Git project. This doesn't change any of your files locally, but it does prepare Git for the next step. If you're using Git 2.9 or above: $ git merge -s ours --no-commit --allow-unrelated-histories spoon-knife/master > Automatic merge went well; stopped before committing as requested If you're using Git 2.8 or below: $ git merge -s ours --no-commit spoon-knife/master > Automatic merge went well; stopped before committing as requested Create a new directory called spoon-knife , and copy the Git history of the Spoon-Knife project into it. $ git read-tree --prefix=spoon-knife/ -u spoon-knife/master Commit the changes to keep them safe. $ git commit -m \"Subtree merged in spoon-knife\" > [master fe0ca25] Subtree merged in spoon-knife Although we've only added one subproject, any number of subprojects can be incorporated into a Git repository. Tip: If you create a fresh clone of the repository in the future, the remotes you've added will not be created for you. You will have to add them again using the git remote add command. Synchronizing with updates and changes When a subproject is added, it is not automatically kept in sync with the upstream changes. You will need to update the subproject with the following command: $ git pull -s subtree remotename branchname For the example above, this would be: $ git pull -s subtree spoon-knife master","title":"Subtrees"},{"location":"notes/git/git_sub_trees/#subtrees","text":"Courtesty of github Typically, a subtree merge is used to contain a repository within a repository. The \"subrepository\" is stored in a folder of the main repository. The best way to explain subtree merges is to show by example. We will: Make an empty repository called test that represents our project Merge another repository into it as a subtree called Spoon-Knife . The test project will use that subproject as if it were part of the same repository. Fetch updates from Spoon-Knife into our test project.","title":"Subtrees"},{"location":"notes/git/git_sub_trees/#setting-up-the-empty-repository-for-a-subtree-merge","text":"Open Terminal. Create a new directory and navigate to it. $ mkdir test $ cd test Initialize a new Git repository. $ git init > Initialized empty Git repository in /Users/octocat/tmp/test/.git/ Create and commit a new file. $ touch .gitignore $ git add .gitignore $ git commit -m \"initial commit\" > [master (root-commit) 3146c2a] initial commit > 0 files changed, 0 insertions(+), 0 deletions(-) > create mode 100644 .gitignore","title":"Setting up the empty repository for a subtree merge"},{"location":"notes/git/git_sub_trees/#adding-a-new-repository-as-a-subtree","text":"Add a new remote URL pointing to the separate project that we're interested in. $ git remote add -f spoon-knife git@github.com:octocat/Spoon-Knife.git > Updating spoon-knife > warning: no common commits > remote: Counting objects: 1732, done. > remote: Compressing objects: 100% (750/750), done. > remote: Total 1732 (delta 1086), reused 1558 (delta 967) > Receiving objects: 100% (1732/1732), 528.19 KiB | 621 KiB/s, done. > Resolving deltas: 100% (1086/1086), done. > From git://github.com/octocat/Spoon-Knife > * [new branch] master -> Spoon-Knife/master Merge the Spoon-Knife project into the local Git project. This doesn't change any of your files locally, but it does prepare Git for the next step. If you're using Git 2.9 or above: $ git merge -s ours --no-commit --allow-unrelated-histories spoon-knife/master > Automatic merge went well; stopped before committing as requested If you're using Git 2.8 or below: $ git merge -s ours --no-commit spoon-knife/master > Automatic merge went well; stopped before committing as requested Create a new directory called spoon-knife , and copy the Git history of the Spoon-Knife project into it. $ git read-tree --prefix=spoon-knife/ -u spoon-knife/master Commit the changes to keep them safe. $ git commit -m \"Subtree merged in spoon-knife\" > [master fe0ca25] Subtree merged in spoon-knife Although we've only added one subproject, any number of subprojects can be incorporated into a Git repository. Tip: If you create a fresh clone of the repository in the future, the remotes you've added will not be created for you. You will have to add them again using the git remote add command.","title":"Adding a new repository as a subtree"},{"location":"notes/git/git_sub_trees/#synchronizing-with-updates-and-changes","text":"When a subproject is added, it is not automatically kept in sync with the upstream changes. You will need to update the subproject with the following command: $ git pull -s subtree remotename branchname For the example above, this would be: $ git pull -s subtree spoon-knife master","title":"Synchronizing with updates and changes"},{"location":"notes/git/git_submodules/","text":"Submodules Courtesy of github Awesome demo Git Submodules basic explanation Why submodules? In Git you can add a submodule to a repository. This is basically a repository embedded in your main repository. This can be very useful. A couple of advantages of using submodules: You can separate the code into different repositories. Useful if you have a codebase with big components, you could make a component a submodule. This way you'll have a cleaner Git log (commits are specific to a certain component). You can add the submodule to multiple repositories. Useful if you have multiple repositories that share the same components. With this approach you can easily update those components in all the repositories that added them as a submodule. This is a lot more convienient than copy-pasting the code into the repositories. Basics When you add a submodule in Git, you don't add the code of the submodule to the main repository, you only add information about the submodule that is added to the main repository. This information describes which commit the submodule is pointing at. This way, the submodule's code won't automatically be updated if the submodule's repository is updated. This is good, because your code might not work with the latest commit of the submodule, it prevents unexpected behaviour. Adding a submodule You can add a submodule to a repository like this: git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule With default configuration, this will check out the code of the awesome_submodule.git repository to the path_to_awesome_submodule directory, and will add information to the main repository about this submodule, which contains the commit the submodule points to , which will be the current commit of the default branch (usually the master branch) at the time this command is executed. After this operation, if you do a git status you'll see two files in the Changes to be committed list: the .gitmodules file and the path to the submodule. When you commit and push these files you commit/push the submodule to the origin. Getting the submodule's code If a new submodule is created by one person, the other people in the team need to initiate this submodule. First you have to get the information about the submodule, this is retrieved by a normal git pull . If there are new submodules you'll see it in the output of git pull . Then you'll have to initiate them with: git submodule init This will pull all the code from the submodule and place it in the directory that it's configured to. If you've cloned a repository that makes use of submodules, you should also run this command to get the submodule's code. This is not automatically done by git clone . Pushing updates in the submodule The submodule is just a separate resository. If you want to make changes to it, you should make the changes in this repository and push them like in a regular Git repository (just execute the git commands in the submodule's directory). However, you should also let the main repository know that you've updated the submodule's repository, and make it use the latest commit of the repository of the submodule. Because if you make new commits inside a submodule, the main repository will still point to the old commit . So, if you want to have these changes in your main repository too, you should tell the main repository to use the latest commit of the submodule. Now how do you do this? So you've made changes in the submodule's repository and committed them in its repository. If you now do a git status in the main repository, you'll see that the submodule is in the list Changes not staged for commit and it has the text (modified content) behind it. This means that the code of the submodule is checked out on a different commit than the main repository is pointing to . To make the main repository point to this new commit, you just add this change with git add and then commit and push it. Keeping your submodules up-to-date If someone updated a submodule, the other team-members should update the code of their submodules. This is not automatically done by git pull , because with git pull it only retrieves the information that the submodule is pointing to another commit , but doesn't update the submodule's code . To update the code of your submodules, you should run: git submodule update What happens if you don't run this command? If you don't run this command, the code of your submodule is checked out to an old commit. When you do git status you will see the submodule in the Changes not staged for commit list with the text (modified content) behind it. This is not because you changed the submodule's code, but because its code is checked out to a different commit. So Git sees this as a change, but actually you just didn't update the submodule's code. So if you're working with submodules, don't forget to keep your submodules up-to-date. Making it easier for everyone It is sometimes annoying if you forget to initiate and update your submodules. Fortunately, there are some tricks to make it easier: git submodule update --init This will update the submodules, and if they're not initiated yet, will initiate them. You can also have submodules inside of submodules. In this case you'll want to update/initiate the submodules recursively: git submodule update --init --recursive This is a lot to type, so you can make an alias: git config --global alias.update '!git pull && git submodule update --init --recursive' Now whenever you execute git update , it will execute a git pull and a git submodule update --init --recursive , thus updating all the code in your project. Courtesy of github To remove a submodule you need to: Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule \" Delete the now untracked submodule files rm -rf path_to_submodule","title":"Submodules"},{"location":"notes/git/git_submodules/#submodules","text":"Courtesy of github Awesome demo Git Submodules basic explanation","title":"Submodules"},{"location":"notes/git/git_submodules/#why-submodules","text":"In Git you can add a submodule to a repository. This is basically a repository embedded in your main repository. This can be very useful. A couple of advantages of using submodules: You can separate the code into different repositories. Useful if you have a codebase with big components, you could make a component a submodule. This way you'll have a cleaner Git log (commits are specific to a certain component). You can add the submodule to multiple repositories. Useful if you have multiple repositories that share the same components. With this approach you can easily update those components in all the repositories that added them as a submodule. This is a lot more convienient than copy-pasting the code into the repositories.","title":"Why submodules?"},{"location":"notes/git/git_submodules/#basics","text":"When you add a submodule in Git, you don't add the code of the submodule to the main repository, you only add information about the submodule that is added to the main repository. This information describes which commit the submodule is pointing at. This way, the submodule's code won't automatically be updated if the submodule's repository is updated. This is good, because your code might not work with the latest commit of the submodule, it prevents unexpected behaviour.","title":"Basics"},{"location":"notes/git/git_submodules/#adding-a-submodule","text":"You can add a submodule to a repository like this: git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule With default configuration, this will check out the code of the awesome_submodule.git repository to the path_to_awesome_submodule directory, and will add information to the main repository about this submodule, which contains the commit the submodule points to , which will be the current commit of the default branch (usually the master branch) at the time this command is executed. After this operation, if you do a git status you'll see two files in the Changes to be committed list: the .gitmodules file and the path to the submodule. When you commit and push these files you commit/push the submodule to the origin.","title":"Adding a submodule"},{"location":"notes/git/git_submodules/#getting-the-submodules-code","text":"If a new submodule is created by one person, the other people in the team need to initiate this submodule. First you have to get the information about the submodule, this is retrieved by a normal git pull . If there are new submodules you'll see it in the output of git pull . Then you'll have to initiate them with: git submodule init This will pull all the code from the submodule and place it in the directory that it's configured to. If you've cloned a repository that makes use of submodules, you should also run this command to get the submodule's code. This is not automatically done by git clone .","title":"Getting the submodule's code"},{"location":"notes/git/git_submodules/#pushing-updates-in-the-submodule","text":"The submodule is just a separate resository. If you want to make changes to it, you should make the changes in this repository and push them like in a regular Git repository (just execute the git commands in the submodule's directory). However, you should also let the main repository know that you've updated the submodule's repository, and make it use the latest commit of the repository of the submodule. Because if you make new commits inside a submodule, the main repository will still point to the old commit . So, if you want to have these changes in your main repository too, you should tell the main repository to use the latest commit of the submodule. Now how do you do this? So you've made changes in the submodule's repository and committed them in its repository. If you now do a git status in the main repository, you'll see that the submodule is in the list Changes not staged for commit and it has the text (modified content) behind it. This means that the code of the submodule is checked out on a different commit than the main repository is pointing to . To make the main repository point to this new commit, you just add this change with git add and then commit and push it.","title":"Pushing updates in the submodule"},{"location":"notes/git/git_submodules/#keeping-your-submodules-up-to-date","text":"If someone updated a submodule, the other team-members should update the code of their submodules. This is not automatically done by git pull , because with git pull it only retrieves the information that the submodule is pointing to another commit , but doesn't update the submodule's code . To update the code of your submodules, you should run: git submodule update","title":"Keeping your submodules up-to-date"},{"location":"notes/git/git_submodules/#what-happens-if-you-dont-run-this-command","text":"If you don't run this command, the code of your submodule is checked out to an old commit. When you do git status you will see the submodule in the Changes not staged for commit list with the text (modified content) behind it. This is not because you changed the submodule's code, but because its code is checked out to a different commit. So Git sees this as a change, but actually you just didn't update the submodule's code. So if you're working with submodules, don't forget to keep your submodules up-to-date.","title":"What happens if you don't run this command?"},{"location":"notes/git/git_submodules/#making-it-easier-for-everyone","text":"It is sometimes annoying if you forget to initiate and update your submodules. Fortunately, there are some tricks to make it easier: git submodule update --init This will update the submodules, and if they're not initiated yet, will initiate them. You can also have submodules inside of submodules. In this case you'll want to update/initiate the submodules recursively: git submodule update --init --recursive This is a lot to type, so you can make an alias: git config --global alias.update '!git pull && git submodule update --init --recursive' Now whenever you execute git update , it will execute a git pull and a git submodule update --init --recursive , thus updating all the code in your project. Courtesy of github To remove a submodule you need to: Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule \" Delete the now untracked submodule files rm -rf path_to_submodule","title":"Making it easier for everyone"},{"location":"notes/git/git_tricks_with_rob/","text":"Git Tricks with Rob # Show config git config -l # Show origin tracking git remote show origin https://stackoverflow.com/questions/6591213/how-do-i-rename-a-local-git-branch # To rename a branch while pointed to any branch: git branch -m <oldname> <newname> To rename the current branch: # Rename local branch git branch -m <newname> # ssh key login ssh -T git@github.com # remote git branch -r git remote update # get's all new remote branches; fetch for all versus 1 remote git hist upstream # git checkout upstream/cervix_fgs -- <path to file name - relative> # Unstage (un-add) back to git branch creation if this is a new branch git reset HEAD <path to file name - relative>","title":"Git Tricks w Rob"},{"location":"notes/git/git_tricks_with_rob/#git-tricks-with-rob","text":"# Show config git config -l # Show origin tracking git remote show origin https://stackoverflow.com/questions/6591213/how-do-i-rename-a-local-git-branch # To rename a branch while pointed to any branch: git branch -m <oldname> <newname> To rename the current branch: # Rename local branch git branch -m <newname> # ssh key login ssh -T git@github.com # remote git branch -r git remote update # get's all new remote branches; fetch for all versus 1 remote git hist upstream # git checkout upstream/cervix_fgs -- <path to file name - relative> # Unstage (un-add) back to git branch creation if this is a new branch git reset HEAD <path to file name - relative>","title":"Git Tricks with Rob"},{"location":"notes/gpu_notes/a100_mig/","text":"A100 Multi-Instance GPU Source Intro Terminology: SM: A streaming multiprocessor (SM) executes compute instructions on the GPU. There are 7. GPU Engine: A GPU engine is what executes work on the GPU. The most commonly used engine is the Compute/Graphics engine that executes the compute instructions. GPU Slice: A GPU slice is the smallest fraction of the GA100 GPU that combines a single GPU memory slice and a single GPU SM slice. GPU Instance (GI): A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc.). Compute Instance: A GPU instance can be subdivided into multiple compute instances. A Compute Instance (CI) contains a subset of the parent GPU instance\u2019s SM slices and other GPU engines (DMAs, NVDECs, etc.). The CIs share memory and engines. Configurations available: All configuration combinations: Example Configuration: Things to consider: MIG functionality is provided as part of the NVIDIA GPU driver starting with the CUDA 11.0 / R450 release. No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device. Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes. Each physical GPU is represented by its own device node - e.g. nvidia0, nvidia1 etc. This is shown below for a 2-GPU system. /dev \u251c\u2500\u2500 nvidiactl \u251c\u2500\u2500 nvidia-modeset \u251c\u2500\u2500 nvidia-uvm \u251c\u2500\u2500 nvidia-uvm-tools \u251c\u2500\u2500 nvidia-nvswitchctl \u251c\u2500\u2500 nvidia0 \u2514\u2500\u2500 nvidia1 Getting Started Turning on MIG and inspecting the GPUs MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its command-line-interface, nvidia-smi. Note that for brevity, some of the nvidia-smi output in the following examples may be cropped to showcase the relevant sections of interest. Summary for each GPU: By default, MIG mode is not enabled on the NVIDIA A100. For example, running nvidia-smi shows that MIG mode is disabled: GPU 1: nvidia-smi -i 0 GPU 2: nvidia-smi -i 1 MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1 Need sudo . MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1. The GPUs can be selected using comma separated GPU indexes, PCI Bus Ids or UUIDs. If no GPU ID is specified, then MIG mode is applied to all the GPUs on the system. Note that MIG mode (Disabled or Enabled states) is persistent across system reboots. nvidia-smi -i 0 -mig 1 List GPU Instances\\Profiles sudo nvidia-smi mig -lgip Ex: [bb927@glacier ~]$ sudo nvidia-smi mig -lgip Your password [ bb927 ] : +--------------------------------------------------------------------------+ | GPU instance profiles: | | GPU Name ID Instances Memory P2P SM DEC ENC | | Free/Total GiB CE JPEG OFA | |==========================================================================| | 0 MIG 1g.5gb 19 7/7 4.75 No 14 0 0 | | 1 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 2g.10gb 14 3/3 9.75 No 28 1 0 | | 2 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 3g.20gb 9 2/2 19.62 No 42 2 0 | | 3 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 4g.20gb 5 1/1 19.62 No 56 2 0 | | 4 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 7g.40gb 0 1/1 39.50 No 98 5 0 | | 7 1 1 | +--------------------------------------------------------------------------+ List the possible placements available using the following command: sudo nvidia-smi mig -lgipp Ex: [bb927@glacier ~]$ sudo nvidia-smi mig -lgipp GPU 0 Profile ID 19 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 14 Placements: {0,2,4}:2 GPU 0 Profile ID 9 Placements: {0,4}:4 GPU 0 Profile ID 5 Placement : {0}:4 GPU 0 Profile ID 0 Placement : {0}:8 The command shows that the user can create two instances of type 3g.20gb (profile ID 9) or seven instances of 1g.5gb (profile ID 19). Create GPU Instances Before starting to use MIG, the user needs to create GPU instances using the -cgi option. One of three options can be used to specify the instance profiles to be created: Profile ID (e.g. 9, 14, 5) Short name of the profile (e.g. 3g.20gb Full profile name of the instance (e.g. MIG 3g.20gb) Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI). By using the -C option, nvidia-smi creates these instances. When only a single CI is created (that consumes the entire compute capacity of the GI), then the CI sizing is implied in the device name. -C is the same as --default-compute-instance and does not require a value. Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads. The table below highlights what the name of a MIG device would look like in this case. The example shown is for subdividing a 3g.20gb device into a set of sub-devices with different Compute Instance slice counts. Also note that, the created MIG devices are not persistent across system reboots. Thus, the user or system administrator needs to recreate the desired MIG configurations if the GPU or system is reset. Example: sudo nvidia-smi mig -cgi 9,3g.20gb -C -i 0 Successfully created GPU instance ID 2 on GPU 0 using profile MIG 3g.20gb (ID 9) Successfully created compute instance ID 0 on GPU 0 GPU instance ID 2 using profile MIG 3g.20gb (ID 2) Successfully created GPU instance ID 1 on GPU 0 using profile MIG 3g.20gb (ID 9) Successfully created compute instance ID 0 on GPU 0 GPU instance ID 1 using profile MIG 3g.20gb (ID 2) List the GPU Instances: sudo nvidia-smi mig -lgi List the GPU Compute Instances: sudo nvidia-smi mig -lci Destroy GPU Instances You can specify GPU instances to delete -dci sudo nvidia-smi mig -dgi -gi 1,2 -i 0 Successfully destroyed GPU instance ID 1 from GPU 0 Successfully destroyed GPU instance ID 2 from GPU 0 Or Compute Instances -dgi sudo nvidia-smi mig -dci -ci 0 -gi 1,2 -i 0","title":"MIG"},{"location":"notes/gpu_notes/a100_mig/#a100-multi-instance-gpu","text":"Source","title":"A100 Multi-Instance GPU"},{"location":"notes/gpu_notes/a100_mig/#intro","text":"Terminology: SM: A streaming multiprocessor (SM) executes compute instructions on the GPU. There are 7. GPU Engine: A GPU engine is what executes work on the GPU. The most commonly used engine is the Compute/Graphics engine that executes the compute instructions. GPU Slice: A GPU slice is the smallest fraction of the GA100 GPU that combines a single GPU memory slice and a single GPU SM slice. GPU Instance (GI): A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc.). Compute Instance: A GPU instance can be subdivided into multiple compute instances. A Compute Instance (CI) contains a subset of the parent GPU instance\u2019s SM slices and other GPU engines (DMAs, NVDECs, etc.). The CIs share memory and engines. Configurations available: All configuration combinations: Example Configuration: Things to consider: MIG functionality is provided as part of the NVIDIA GPU driver starting with the CUDA 11.0 / R450 release. No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device. Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes. Each physical GPU is represented by its own device node - e.g. nvidia0, nvidia1 etc. This is shown below for a 2-GPU system. /dev \u251c\u2500\u2500 nvidiactl \u251c\u2500\u2500 nvidia-modeset \u251c\u2500\u2500 nvidia-uvm \u251c\u2500\u2500 nvidia-uvm-tools \u251c\u2500\u2500 nvidia-nvswitchctl \u251c\u2500\u2500 nvidia0 \u2514\u2500\u2500 nvidia1","title":"Intro"},{"location":"notes/gpu_notes/a100_mig/#getting-started","text":"","title":"Getting Started"},{"location":"notes/gpu_notes/a100_mig/#turning-on-mig-and-inspecting-the-gpus","text":"MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its command-line-interface, nvidia-smi. Note that for brevity, some of the nvidia-smi output in the following examples may be cropped to showcase the relevant sections of interest. Summary for each GPU: By default, MIG mode is not enabled on the NVIDIA A100. For example, running nvidia-smi shows that MIG mode is disabled: GPU 1: nvidia-smi -i 0 GPU 2: nvidia-smi -i 1 MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1 Need sudo . MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1. The GPUs can be selected using comma separated GPU indexes, PCI Bus Ids or UUIDs. If no GPU ID is specified, then MIG mode is applied to all the GPUs on the system. Note that MIG mode (Disabled or Enabled states) is persistent across system reboots. nvidia-smi -i 0 -mig 1","title":"Turning on MIG and inspecting the GPUs"},{"location":"notes/gpu_notes/a100_mig/#list-gpu-instancesprofiles","text":"sudo nvidia-smi mig -lgip Ex: [bb927@glacier ~]$ sudo nvidia-smi mig -lgip Your password [ bb927 ] : +--------------------------------------------------------------------------+ | GPU instance profiles: | | GPU Name ID Instances Memory P2P SM DEC ENC | | Free/Total GiB CE JPEG OFA | |==========================================================================| | 0 MIG 1g.5gb 19 7/7 4.75 No 14 0 0 | | 1 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 2g.10gb 14 3/3 9.75 No 28 1 0 | | 2 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 3g.20gb 9 2/2 19.62 No 42 2 0 | | 3 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 4g.20gb 5 1/1 19.62 No 56 2 0 | | 4 0 0 | +--------------------------------------------------------------------------+ | 0 MIG 7g.40gb 0 1/1 39.50 No 98 5 0 | | 7 1 1 | +--------------------------------------------------------------------------+ List the possible placements available using the following command: sudo nvidia-smi mig -lgipp Ex: [bb927@glacier ~]$ sudo nvidia-smi mig -lgipp GPU 0 Profile ID 19 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 14 Placements: {0,2,4}:2 GPU 0 Profile ID 9 Placements: {0,4}:4 GPU 0 Profile ID 5 Placement : {0}:4 GPU 0 Profile ID 0 Placement : {0}:8 The command shows that the user can create two instances of type 3g.20gb (profile ID 9) or seven instances of 1g.5gb (profile ID 19).","title":"List GPU Instances\\Profiles"},{"location":"notes/gpu_notes/a100_mig/#create-gpu-instances","text":"Before starting to use MIG, the user needs to create GPU instances using the -cgi option. One of three options can be used to specify the instance profiles to be created: Profile ID (e.g. 9, 14, 5) Short name of the profile (e.g. 3g.20gb Full profile name of the instance (e.g. MIG 3g.20gb) Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI). By using the -C option, nvidia-smi creates these instances. When only a single CI is created (that consumes the entire compute capacity of the GI), then the CI sizing is implied in the device name. -C is the same as --default-compute-instance and does not require a value. Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads. The table below highlights what the name of a MIG device would look like in this case. The example shown is for subdividing a 3g.20gb device into a set of sub-devices with different Compute Instance slice counts. Also note that, the created MIG devices are not persistent across system reboots. Thus, the user or system administrator needs to recreate the desired MIG configurations if the GPU or system is reset. Example: sudo nvidia-smi mig -cgi 9,3g.20gb -C -i 0 Successfully created GPU instance ID 2 on GPU 0 using profile MIG 3g.20gb (ID 9) Successfully created compute instance ID 0 on GPU 0 GPU instance ID 2 using profile MIG 3g.20gb (ID 2) Successfully created GPU instance ID 1 on GPU 0 using profile MIG 3g.20gb (ID 9) Successfully created compute instance ID 0 on GPU 0 GPU instance ID 1 using profile MIG 3g.20gb (ID 2) List the GPU Instances: sudo nvidia-smi mig -lgi List the GPU Compute Instances: sudo nvidia-smi mig -lci","title":"Create GPU Instances"},{"location":"notes/gpu_notes/a100_mig/#destroy-gpu-instances","text":"You can specify GPU instances to delete -dci sudo nvidia-smi mig -dgi -gi 1,2 -i 0 Successfully destroyed GPU instance ID 1 from GPU 0 Successfully destroyed GPU instance ID 2 from GPU 0 Or Compute Instances -dgi sudo nvidia-smi mig -dci -ci 0 -gi 1,2 -i 0","title":"Destroy GPU Instances"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/","text":"NVIDIA and CUDA Setup Intro This almost never works however I did just get it to work so I will document what worked. First (extremely abbreviated): Deployed Azure VM Type: Standard NC6s v3 (6 vcpus, 112 GiB memory) Expanded the disk (not required but I ran out of room some how) azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Extraction failed. Ensure there is enough space in /tmp and that the installation package is not corrupt Signal caught, cleaning up so...you need more than 8GB of disk space... Use these docs : docs.nvidia.com Following along (summary) 2.1. Verify You Have a CUDA-Capable GPU $ lspci | grep -i nvidia 0001:00:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] (rev a1) Sweet we are good to go. 2.2. Verify You Have a Supported Version of Linux x86_64 DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION=\"Ubuntu 20.04.3 LTS\" NAME=\"Ubuntu\" VERSION=\"20.04.3 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.3 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Ubuntu 20+ is fairly new as of 06_02_2023 so I'd say yes... 2.3. Verify the System Has gcc Installed $ gcc --version gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. I think I had to install but that was trivial: $ sudo apt install gcc 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed Under the ubuntu section I used: sudo apt-get install linux-headers-$(uname -r) 2.5. Install MLNX_OFED Totally skipped this section...don't know what it is... 2.6. Choose an Installation Method Secret sauce right here! ...from docs \"The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages) . The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution\u2019s native package management system. The distribution-specific packages interface with the distribution\u2019s native package management system. It is recommended to use the distribution-specific packages, where possible.\" I'm sure the distribution-specific is better but it wasn't working and I don't have time for that...so independent it is! A little confusing but the link you want is in the next section...so on we go. 2.7. Download the NVIDIA CUDA Toolkit The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Code of course: wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run sudo sh cuda_12.1.1_530.30.02_linux.run YaY! it worked right? Of course not. Why would such a good UI with filters let you down...2023 baby...life ain't perfect yet.........so here we go wtf... First Error: Disk space azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Extraction failed. Ensure there is enough space in /tmp and that the installation package is not corrupt Signal caught, cleaning up Add more space... Second Error: Nouveau??? We have to disable the Nauveau kernel driver. Enter the stack . This discussion led to this link NVIDIA developer zone . The thing is this is the same guide we've been using! However WHAT YOU NEED is much further down. azureuser@gpu-worker:~$ cat /var/log/cuda-installer.log [INFO]: Driver not installed. [INFO]: Checking compiler version... [INFO]: gcc location: /usr/bin/gcc [INFO]: gcc version: gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1) [INFO]: Initializing menu [INFO]: nvidia-fs.setKOVersion(2.15.3) [INFO]: Setup complete [INFO]: Installing: Driver [INFO]: Installing: 530.30.02 [INFO]: Executing NVIDIA-Linux-x86_64-530.30.02.run --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd 2>&1 [INFO]: Finished with code: 256 [ERROR]: Install of driver component failed. Consult the driver log at /var/log/nvidia-installer.log for more details. [ERROR]: Install of 530.30.02 failed, quitting .... we need to look in /var/log/nvidia-installer.log .... azureuser@gpu-worker:~$ cat /var/log/nvidia-installer.log nvidia-installer log file '/var/log/nvidia-installer.log' creation time: Fri Jun 2 20:09:52 2023 installer version: 530.30.02 PATH: /tmp/makeself.qlAaARJl:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin nvidia-installer command line: ./nvidia-installer --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd Using built-in stream user interface -> Detected 6 CPUs online; setting concurrency level to 6. -> Installing NVIDIA driver version 530.30.02. ERROR: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver, and must be disabled before proceeding. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. WARNING: One or more modprobe configuration files to disable Nouveau are already present at: /usr/lib/modprobe.d/nvidia-installer-disable-nouveau.conf, /etc/modprobe.d/nvidia-installer-disable-nouveau.conf. Please be sure you have rebooted your system since these files were written. If you have rebooted, then Nouveau may be enabled for other reasons, such as being included in the system initial ramdisk or in your X configuration file. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. -> For some distributions, Nouveau can be disabled by adding a file in the modprobe configuration directory. Would you like nvidia-installer to attempt to create this modprobe file for you? (Answer: Yes) -> One or more modprobe configuration files to disable Nouveau have been written. For some distributions, this may be sufficient to disable Nouveau; other distributions may require modification of the initial ramdisk. Please reboot your system and attempt NVIDIA driver installation again. Note if you later wish to re-enable Nouveau, you will need to delete these files: /usr/lib/modprobe.d/nvidia-installer-disable-nouveau.conf, /etc/modprobe.d/nvidia-installer-disable-nouveau.conf ERROR: Installation has failed. Please see the file '/var/log/nvidia-installer.log' for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com. Notice lots of text about Nouveau...Let's fix that. Section: 8.3.6. Ubuntu\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u I restarted the VM after that. Not sure if that was necessary but the stack over flow discussion recommended it. azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Installation failed. See log at /var/log/cuda-installer.log for details. Mother....! Third Error: install make azureuser@gpu-worker:~$ cat /var/log/cuda-installer.log ... [ERROR]: Install of driver component failed. Consult the driver log at /var/log/nvidia-installer.log for more details. [ERROR]: Install of 530.30.02 failed, quitting azureuser@gpu-worker:~$ cat /var/log/nvidia-installer.log ... ERROR: Unable to find the development tool `make` in your path; please make sure that you have the package 'make' installed. If make is installed on your system, then please check that `make` is in your PATH. ERROR: Installation has failed. Please see the file '/var/log/nvidia-installer.log' for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com. Ok this one is easy: azureuser@gpu-worker:~$ sudo apt install make Success! azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run =========== = Summary = =========== Driver: Installed Toolkit: Installed in /usr/local/cuda-12.1/ Please make sure that - PATH includes /usr/local/cuda-12.1/bin - LD_LIBRARY_PATH includes /usr/local/cuda-12.1/lib64, or, add /usr/local/cuda-12.1/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-12.1/bin To uninstall the NVIDIA Driver, run nvidia-uninstall Logfile is /var/log/cuda-installer.log As they suggest add paths to PATH and LD_LIBRARY_PATH. Add this to ~/.bashrc # cuda export PATH=/usr/local/cuda-12.1/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH azureuser@gpu-worker:~$ nvidia-smi Fri Jun 2 20:24:58 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-PCIE-16GB Off| 00000001:00:00.0 Off | Off | | N/A 22C P0 34W / 250W| 0MiB / 16384MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+","title":2023},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#nvidia-and-cuda-setup","text":"","title":"NVIDIA and CUDA Setup"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#intro","text":"This almost never works however I did just get it to work so I will document what worked. First (extremely abbreviated): Deployed Azure VM Type: Standard NC6s v3 (6 vcpus, 112 GiB memory) Expanded the disk (not required but I ran out of room some how) azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Extraction failed. Ensure there is enough space in /tmp and that the installation package is not corrupt Signal caught, cleaning up so...you need more than 8GB of disk space... Use these docs : docs.nvidia.com","title":"Intro"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#following-along-summary","text":"","title":"Following along (summary)"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#21-verify-you-have-a-cuda-capable-gpu","text":"$ lspci | grep -i nvidia 0001:00:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] (rev a1) Sweet we are good to go.","title":"2.1. Verify You Have a CUDA-Capable GPU"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#22-verify-you-have-a-supported-version-of-linux","text":"x86_64 DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION=\"Ubuntu 20.04.3 LTS\" NAME=\"Ubuntu\" VERSION=\"20.04.3 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.3 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Ubuntu 20+ is fairly new as of 06_02_2023 so I'd say yes...","title":"2.2. Verify You Have a Supported Version of Linux"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#23-verify-the-system-has-gcc-installed","text":"$ gcc --version gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. I think I had to install but that was trivial: $ sudo apt install gcc","title":"2.3. Verify the System Has gcc Installed"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#24-verify-the-system-has-the-correct-kernel-headers-and-development-packages-installed","text":"Under the ubuntu section I used: sudo apt-get install linux-headers-$(uname -r)","title":"2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#25-install-mlnx_ofed","text":"Totally skipped this section...don't know what it is...","title":"2.5. Install MLNX_OFED"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#26-choose-an-installation-method","text":"Secret sauce right here! ...from docs \"The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages) . The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution\u2019s native package management system. The distribution-specific packages interface with the distribution\u2019s native package management system. It is recommended to use the distribution-specific packages, where possible.\" I'm sure the distribution-specific is better but it wasn't working and I don't have time for that...so independent it is! A little confusing but the link you want is in the next section...so on we go.","title":"2.6. Choose an Installation Method"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#27-download-the-nvidia-cuda-toolkit","text":"The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Code of course: wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run sudo sh cuda_12.1.1_530.30.02_linux.run","title":"2.7. Download the NVIDIA CUDA Toolkit"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#yay-it-worked-right","text":"Of course not. Why would such a good UI with filters let you down...2023 baby...life ain't perfect yet.........so here we go wtf...","title":"YaY! it worked right?"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#first-error-disk-space","text":"azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Extraction failed. Ensure there is enough space in /tmp and that the installation package is not corrupt Signal caught, cleaning up Add more space...","title":"First Error: Disk space"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#second-error-nouveau","text":"We have to disable the Nauveau kernel driver. Enter the stack . This discussion led to this link NVIDIA developer zone . The thing is this is the same guide we've been using! However WHAT YOU NEED is much further down. azureuser@gpu-worker:~$ cat /var/log/cuda-installer.log [INFO]: Driver not installed. [INFO]: Checking compiler version... [INFO]: gcc location: /usr/bin/gcc [INFO]: gcc version: gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1) [INFO]: Initializing menu [INFO]: nvidia-fs.setKOVersion(2.15.3) [INFO]: Setup complete [INFO]: Installing: Driver [INFO]: Installing: 530.30.02 [INFO]: Executing NVIDIA-Linux-x86_64-530.30.02.run --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd 2>&1 [INFO]: Finished with code: 256 [ERROR]: Install of driver component failed. Consult the driver log at /var/log/nvidia-installer.log for more details. [ERROR]: Install of 530.30.02 failed, quitting .... we need to look in /var/log/nvidia-installer.log .... azureuser@gpu-worker:~$ cat /var/log/nvidia-installer.log nvidia-installer log file '/var/log/nvidia-installer.log' creation time: Fri Jun 2 20:09:52 2023 installer version: 530.30.02 PATH: /tmp/makeself.qlAaARJl:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin nvidia-installer command line: ./nvidia-installer --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd Using built-in stream user interface -> Detected 6 CPUs online; setting concurrency level to 6. -> Installing NVIDIA driver version 530.30.02. ERROR: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver, and must be disabled before proceeding. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. WARNING: One or more modprobe configuration files to disable Nouveau are already present at: /usr/lib/modprobe.d/nvidia-installer-disable-nouveau.conf, /etc/modprobe.d/nvidia-installer-disable-nouveau.conf. Please be sure you have rebooted your system since these files were written. If you have rebooted, then Nouveau may be enabled for other reasons, such as being included in the system initial ramdisk or in your X configuration file. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. -> For some distributions, Nouveau can be disabled by adding a file in the modprobe configuration directory. Would you like nvidia-installer to attempt to create this modprobe file for you? (Answer: Yes) -> One or more modprobe configuration files to disable Nouveau have been written. For some distributions, this may be sufficient to disable Nouveau; other distributions may require modification of the initial ramdisk. Please reboot your system and attempt NVIDIA driver installation again. Note if you later wish to re-enable Nouveau, you will need to delete these files: /usr/lib/modprobe.d/nvidia-installer-disable-nouveau.conf, /etc/modprobe.d/nvidia-installer-disable-nouveau.conf ERROR: Installation has failed. Please see the file '/var/log/nvidia-installer.log' for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com. Notice lots of text about Nouveau...Let's fix that.","title":"Second Error: Nouveau???"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#section-836-ubuntu","text":"Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u I restarted the VM after that. Not sure if that was necessary but the stack over flow discussion recommended it. azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run Installation failed. See log at /var/log/cuda-installer.log for details. Mother....!","title":"Section: 8.3.6. Ubuntu\uf0c1"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#third-error-install-make","text":"azureuser@gpu-worker:~$ cat /var/log/cuda-installer.log ... [ERROR]: Install of driver component failed. Consult the driver log at /var/log/nvidia-installer.log for more details. [ERROR]: Install of 530.30.02 failed, quitting azureuser@gpu-worker:~$ cat /var/log/nvidia-installer.log ... ERROR: Unable to find the development tool `make` in your path; please make sure that you have the package 'make' installed. If make is installed on your system, then please check that `make` is in your PATH. ERROR: Installation has failed. Please see the file '/var/log/nvidia-installer.log' for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com. Ok this one is easy: azureuser@gpu-worker:~$ sudo apt install make","title":"Third Error: install make"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2023/#success","text":"azureuser@gpu-worker:~$ sudo sh cuda_12.1.1_530.30.02_linux.run =========== = Summary = =========== Driver: Installed Toolkit: Installed in /usr/local/cuda-12.1/ Please make sure that - PATH includes /usr/local/cuda-12.1/bin - LD_LIBRARY_PATH includes /usr/local/cuda-12.1/lib64, or, add /usr/local/cuda-12.1/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-12.1/bin To uninstall the NVIDIA Driver, run nvidia-uninstall Logfile is /var/log/cuda-installer.log As they suggest add paths to PATH and LD_LIBRARY_PATH. Add this to ~/.bashrc # cuda export PATH=/usr/local/cuda-12.1/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH azureuser@gpu-worker:~$ nvidia-smi Fri Jun 2 20:24:58 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-PCIE-16GB Off| 00000001:00:00.0 Off | Off | | N/A 22C P0 34W / 250W| 0MiB / 16384MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+","title":"Success!"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2024/","text":"Install cuda drivers (cuda-toolkit) cuda-toolkit wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run sudo apt-get update sudo sh cuda_12.4.1_550.54.15_linux.run install nvidia-docker-toolkit nvidia-docker-toolkit sudo apt-get install -y nvidia-container-toolkit curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list nvidia sample image nvidia docker images IMAGE=python:3.9 IMAGE=nvcr.io/nvidia/cuda:12.6.0-cudnn-devel-ubuntu24.04 # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi # doesn't work docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi output: azureuser@gpu-worker:~$ IMAGE=python:3.9 azureuser@gpu-worker:~$ # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi Unable to find image 'python:3.9' locally 3.9: Pulling from library/python 903681d87777: Already exists 3cbbe86a28c2: Already exists 6ed93aa58a52: Already exists 787c78da4383: Already exists 74e6988984f8: Already exists 8f13d3184846: Already exists a28b5664f47e: Already exists 777e0be36342: Already exists Digest: sha256:8e0fd8e1bd3caa6ea74d1c408ad5a854c6b8f0e32c2ef9886d27d4d1bb9caeac Status: Downloaded newer image for python:3.9 Fri Aug 23 15:22:19 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.54.15 Driver Version: 550.54.15 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla V100-PCIE-16GB Off | 00000001:00:00.0 Off | 0 | | N/A 34C P0 38W / 250W | 0MiB / 16384MiB | 1% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ azureuser@gpu-worker:~$ IMAGE=nvcr.io/nvidia/cuda:12.6.0-cudnn-devel-ubuntu24.04 azureuser@gpu-worker:~$ # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi ========== == CUDA == ========== CUDA Version 12.6.0 Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. Fri Aug 23 15:23:46 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.54.15 Driver Version: 550.54.15 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla V100-PCIE-16GB Off | 00000001:00:00.0 Off | 0 | | N/A 35C P0 38W / 250W | 0MiB / 16384MiB | 1% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ azureuser@gpu-worker:~$ azureuser@gpu-worker:~$ # doesn't work docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi ========== == CUDA == ========== CUDA Version 12.6.0 Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. Failed to initialize NVML: Unknown Error azureuser@gpu-worker:~$","title":2024},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2024/#install-cuda-drivers-cuda-toolkit","text":"cuda-toolkit wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run sudo apt-get update sudo sh cuda_12.4.1_550.54.15_linux.run","title":"Install cuda drivers (cuda-toolkit)"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2024/#install-nvidia-docker-toolkit","text":"nvidia-docker-toolkit sudo apt-get install -y nvidia-container-toolkit curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list","title":"install nvidia-docker-toolkit"},{"location":"notes/gpu_notes/NVIDIA_CUDA/nvidia_cuda_install_notes_2024/#nvidia-sample-image","text":"nvidia docker images IMAGE=python:3.9 IMAGE=nvcr.io/nvidia/cuda:12.6.0-cudnn-devel-ubuntu24.04 # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi # doesn't work docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi output: azureuser@gpu-worker:~$ IMAGE=python:3.9 azureuser@gpu-worker:~$ # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi Unable to find image 'python:3.9' locally 3.9: Pulling from library/python 903681d87777: Already exists 3cbbe86a28c2: Already exists 6ed93aa58a52: Already exists 787c78da4383: Already exists 74e6988984f8: Already exists 8f13d3184846: Already exists a28b5664f47e: Already exists 777e0be36342: Already exists Digest: sha256:8e0fd8e1bd3caa6ea74d1c408ad5a854c6b8f0e32c2ef9886d27d4d1bb9caeac Status: Downloaded newer image for python:3.9 Fri Aug 23 15:22:19 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.54.15 Driver Version: 550.54.15 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla V100-PCIE-16GB Off | 00000001:00:00.0 Off | 0 | | N/A 34C P0 38W / 250W | 0MiB / 16384MiB | 1% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ azureuser@gpu-worker:~$ IMAGE=nvcr.io/nvidia/cuda:12.6.0-cudnn-devel-ubuntu24.04 azureuser@gpu-worker:~$ # works docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi ========== == CUDA == ========== CUDA Version 12.6.0 Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. Fri Aug 23 15:23:46 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.54.15 Driver Version: 550.54.15 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla V100-PCIE-16GB Off | 00000001:00:00.0 Off | 0 | | N/A 35C P0 38W / 250W | 0MiB / 16384MiB | 1% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ azureuser@gpu-worker:~$ azureuser@gpu-worker:~$ # doesn't work docker run \\ -it \\ --rm \\ --gpus all \\ --runtime=nvidia \\ -v /var/run/docker.sock:/var/run/docker.sock \\ $IMAGE nvidia-smi ========== == CUDA == ========== CUDA Version 12.6.0 Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. Failed to initialize NVML: Unknown Error azureuser@gpu-worker:~$","title":"nvidia sample image"},{"location":"notes/html/html_template/","text":"HTML The basic html tree is as follows: <!DOCTYPE html> <html> <head> <title></title> </head> <body> </body> </html>","title":"HTML"},{"location":"notes/html/html_template/#html","text":"The basic html tree is as follows: <!DOCTYPE html> <html> <head> <title></title> </head> <body> </body> </html>","title":"HTML"},{"location":"notes/internet_of_things/General%20Notes/","text":"Notes About the Internet IP Address Types IPv4 IPv4 (Internet Protocol Version 4) is the fourth revision of the Internet Protocol (IP) used to to identify devices on a network through an addressing system. The Internet Protocol is designed for use in interconnected systems of packet-switched computer communication networks IPv4 uses a 32-bit address scheme allowing for a total of 2^32 addresses (just over 4 billion addresses). IPv6 A new Internet addressing system Internet Protocol version 6 (IPv6) is being deployed to fulfill the need for more Internet addresses. IPv6 (Internet Protocol Version 6) is also called IPng (Internet Protocol next generation) and it is the newest version of the Internet Protocol (IP) reviewed in the IETF standards committees to replace the current version of IPv4 (Internet Protocol Version 4). IPv6 addresses are 128-bit IP address written in hexadecimal and separated by colons. An example IPv6 address could be written like this: 3ffe:1900:4545:3:200:f8ff:fe21:67cf.","title":"General Notes"},{"location":"notes/internet_of_things/General%20Notes/#notes-about-the-internet","text":"","title":"Notes About the Internet"},{"location":"notes/internet_of_things/General%20Notes/#ip-address-types","text":"","title":"IP Address Types"},{"location":"notes/internet_of_things/General%20Notes/#ipv4","text":"IPv4 (Internet Protocol Version 4) is the fourth revision of the Internet Protocol (IP) used to to identify devices on a network through an addressing system. The Internet Protocol is designed for use in interconnected systems of packet-switched computer communication networks IPv4 uses a 32-bit address scheme allowing for a total of 2^32 addresses (just over 4 billion addresses).","title":"IPv4"},{"location":"notes/internet_of_things/General%20Notes/#ipv6","text":"A new Internet addressing system Internet Protocol version 6 (IPv6) is being deployed to fulfill the need for more Internet addresses. IPv6 (Internet Protocol Version 6) is also called IPng (Internet Protocol next generation) and it is the newest version of the Internet Protocol (IP) reviewed in the IETF standards committees to replace the current version of IPv4 (Internet Protocol Version 4). IPv6 addresses are 128-bit IP address written in hexadecimal and separated by colons. An example IPv6 address could be written like this: 3ffe:1900:4545:3:200:f8ff:fe21:67cf.","title":"IPv6"},{"location":"notes/internet_of_things/AWS/storage/","text":"Storage In my quest to get started with AWS, I noticed lots of different storage types. I'll make some quick notes about them here. EC2 \u2013 Elastic Compute Cloud \u2013 The VMs While not storage, EC2 instances are the basis for anything in AWS EBS \u2013 Elastic Block Storage - $$ - Can be scaled but upgrading costs money S3 \u2013 Simple Storage Service \u2013 files and objects - $$ EFS \u2013 Elastic File System - - $$$ Glacier \u2013 Low cost option for long term storage - $ AWS Snowball - Snowball is designed to make such transfers easy without incurring astronomical network usage fees. FSx for Lustre - High performance computing for fast processing of workloads. Integrates with S3 and you pay as you go. AWS Storage Gateway - Seamlessly links your on-premises environment to Amazon cloud storage Cloud Data Migration Services - A portfolio of services to help simplify and accelerate moving data of all types and sizes into and out of the AWS cloud AWS Backup - Backups","title":"AWS"},{"location":"notes/internet_of_things/AWS/storage/#storage","text":"In my quest to get started with AWS, I noticed lots of different storage types. I'll make some quick notes about them here. EC2 \u2013 Elastic Compute Cloud \u2013 The VMs While not storage, EC2 instances are the basis for anything in AWS EBS \u2013 Elastic Block Storage - $$ - Can be scaled but upgrading costs money S3 \u2013 Simple Storage Service \u2013 files and objects - $$ EFS \u2013 Elastic File System - - $$$ Glacier \u2013 Low cost option for long term storage - $ AWS Snowball - Snowball is designed to make such transfers easy without incurring astronomical network usage fees. FSx for Lustre - High performance computing for fast processing of workloads. Integrates with S3 and you pay as you go. AWS Storage Gateway - Seamlessly links your on-premises environment to Amazon cloud storage Cloud Data Migration Services - A portfolio of services to help simplify and accelerate moving data of all types and sizes into and out of the AWS cloud AWS Backup - Backups","title":"Storage"},{"location":"notes/internet_of_things/Azure/overview/","text":"Azure Cloud Notes Azure Active Directory Azure Active Directory (Azure AD) is Microsoft\u2019s cloud-based identity and access management service, which helps your employees sign in and access resources in: External resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications. Internal resources, such as apps on your corporate network and intranet, along with any cloud apps developed by your own organization. For more information about creating a tenant for your organization, see Quickstart: Create a new tenant in Azure Active Directory. Azure Tenant A dedicated and trusted instance of Azure AD that's automatically created when your organization signs up for a Microsoft cloud service subscription, such as Microsoft Azure, Microsoft Intune, or Microsoft 365. An Azure tenant represents a single organization. Azure Subscription Used to pay for Azure cloud services. You can have many subscriptions and they're linked to a credit card. More terms here Service Principal Object To access resources that are secured by an Azure AD tenant, the entity that requires access must be represented by a security principal. This requirement is true for both users (user principal) and applications (service principal). The security principal defines the access policy and permissions for the user/application in the Azure AD tenant. This enables core features such as authentication of the user/application during sign-in, and authorization during resource access. There are three types of service principal: application , managed identity , and legacy . Application: The first type of service principal is the local representation, or application instance, of a global application object in a single tenant or directory. In this case, a service principal is a concrete instance created from the application object and inherits certain properties from that application object. A service principal is created in each tenant where the application is used and references the globally unique app object. The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access. When an application is given permission to access resources in a tenant (upon registration or consent), a service principal object is created. Managed Identity: The second type of service principal is used to represent a managed identity. Managed identities eliminate the need for developers to manage credentials. Managed identities provide an identity for applications to use when connecting to resources that support Azure AD authentication. When a managed identity is enabled, a service principal representing that managed identity is created in your tenant. Service principals representing managed identities can be granted access and permissions, but cannot be updated or modified directly. Legacy: The third type of service principal represents a legacy app (an app created before app registrations were introduced or created through legacy experiences). A legacy service principal can have credentials, service principal names, reply URLs, and other properties which are editable by an authorized user, but does not have an associated app registration. The service principal can only be used in the tenant where it was created.","title":"Overview"},{"location":"notes/internet_of_things/Azure/overview/#azure-cloud-notes","text":"","title":"Azure Cloud Notes"},{"location":"notes/internet_of_things/Azure/overview/#azure-active-directory","text":"Azure Active Directory (Azure AD) is Microsoft\u2019s cloud-based identity and access management service, which helps your employees sign in and access resources in: External resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications. Internal resources, such as apps on your corporate network and intranet, along with any cloud apps developed by your own organization. For more information about creating a tenant for your organization, see Quickstart: Create a new tenant in Azure Active Directory.","title":"Azure Active Directory"},{"location":"notes/internet_of_things/Azure/overview/#azure-tenant","text":"A dedicated and trusted instance of Azure AD that's automatically created when your organization signs up for a Microsoft cloud service subscription, such as Microsoft Azure, Microsoft Intune, or Microsoft 365. An Azure tenant represents a single organization.","title":"Azure Tenant"},{"location":"notes/internet_of_things/Azure/overview/#azure-subscription","text":"Used to pay for Azure cloud services. You can have many subscriptions and they're linked to a credit card. More terms here","title":"Azure Subscription"},{"location":"notes/internet_of_things/Azure/overview/#service-principal-object","text":"To access resources that are secured by an Azure AD tenant, the entity that requires access must be represented by a security principal. This requirement is true for both users (user principal) and applications (service principal). The security principal defines the access policy and permissions for the user/application in the Azure AD tenant. This enables core features such as authentication of the user/application during sign-in, and authorization during resource access. There are three types of service principal: application , managed identity , and legacy . Application: The first type of service principal is the local representation, or application instance, of a global application object in a single tenant or directory. In this case, a service principal is a concrete instance created from the application object and inherits certain properties from that application object. A service principal is created in each tenant where the application is used and references the globally unique app object. The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access. When an application is given permission to access resources in a tenant (upon registration or consent), a service principal object is created. Managed Identity: The second type of service principal is used to represent a managed identity. Managed identities eliminate the need for developers to manage credentials. Managed identities provide an identity for applications to use when connecting to resources that support Azure AD authentication. When a managed identity is enabled, a service principal representing that managed identity is created in your tenant. Service principals representing managed identities can be granted access and permissions, but cannot be updated or modified directly. Legacy: The third type of service principal represents a legacy app (an app created before app registrations were introduced or created through legacy experiences). A legacy service principal can have credentials, service principal names, reply URLs, and other properties which are editable by an authorized user, but does not have an associated app registration. The service principal can only be used in the tenant where it was created.","title":"Service Principal Object"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_disk_backup/","text":"az cli install curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash az login Copy snapshot from one region to another: SUBSCRIPTION_ID=a7ab1d40-7176-42a9-9156-0eb7fbe1ae3e az disk create \\ --resource-group CHALLENGES \\ --name qtim-challenges-os-disk-backup-disk-new \\ --source qtim-challenges-os-disk-backup-eastus \\ --zone 1 \\ --location eastus az disk create \\ --resource-group CHALLENGES \\ --name qtim-challenges-disk-eastus \\ --source /subscriptions/$SUBSCRIPTION_ID/resourceGroups/CHALLENGES/providers/Microsoft.Compute/disks/qtim-challenges_OsDisk_1_2739395aad8147d1954d583529de7dab \\ --location eastus az snapshot show \\ --resource-group CHALLENGES \\ --name qtim-challenges-os-disk-backup-eastus az snapshot create \\ --resource-group CHALLENGES \\ --source /subscriptions/$SUBSCRIPTION_ID/resourceGroups/CHALLENGES/providers/Microsoft.Compute/disks/qtim-challenges_OsDisk_1_2739395aad8147d1954d583529de7dab \\ --name qtim-challenges-os-disk-backup-eastus \\ --location eastus az vm disk attach \\ --resource-group CHALLENGES \\ --vm-name qtim-challenges-new \\ --name qtim-challenges-os-disk-backup-disk az snapshot create \\ --resource-group CHALLENGES \\ --source qtim-challenges-os-disk-backup-disk \\ --name qtim-challenges-os-disk-backup-disk-snapshot","title":"Azure Disk Backup"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/","text":"Overview of Azure Storage Blob Container Code # https://learn.microsoft.com/en-us/azure/storage/blobs/blobfuse2-how-to-deploy?tabs=Ubuntu # How to install BlobFuse2 ## Option 1: Install BlobFuse2 from the Microsoft software repositories for Linux ### To check your version of Linux, run the following command: cat /etc/*-release ### Install libfuse-dev sudo wget https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install libfuse-dev # ubuntu 18.04 # sudo apt-get install libfuse3-dev fuse3 # didn't work; this is for ubuntu > 18.04 ### Install BlobFuse2 sudo apt-get install blobfuse2 # How to configure BlobFuse2 ## Create Blobfuse Config File(s) CONFIG=/home/azureuser/blobfuse-config.yaml touch $CONFIG vim $CONFIG ## Create an empty directory to mount the blob container CONTAINER_DIR=/mnt/dlsparseviewctchallenge/bundles sudo mkdir -p $CONTAINER_DIR # echo $CONTAINER_DIR sudo blobfuse2 mount $CONTAINER_DIR --config-file=$CONFIG ls $CONTAINER_DIR Config Sample # Refer ./setup/baseConfig.yaml for full set of config parameters allow-other: true logging: type: syslog level: log_debug components: - libfuse - stream - attr_cache - azstorage libfuse: attribute-expiration-sec: 120 entry-expiration-sec: 120 negative-entry-expiration-sec: 240 stream: block-size-mb: 8 blocks-per-file: 3 cache-size-mb: 1024 attr_cache: timeout-sec: 7200 azstorage: type: block account-name: mystorageaccount account-key: mystoragekey endpoint: https://mystorageaccount-CHANGEME.blob.core.windows.net mode: key container: mycontainer File Share sudo apt update -y; sudo apt remove azure-cli -y && sudo apt autoremove -y curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash # Microsoft Account az login --use-device-code RESOURCE_GROUP_NAME=\"\" STORAGE_ACCOUNT_NAME=\"\" # This command assumes you have logged in with az login HTTP_ENDPOINT=$(az storage account show \\ --resource-group $RESOURCE_GROUP_NAME \\ --name $STORAGE_ACCOUNT_NAME \\ --query \"primaryEndpoints.file\" --output tsv | tr -d '\"') SMBPATH=$(echo $HTTP_ENDPOINT | cut -c7-${#HTTP_ENDPOINT}) FILE_HOST=$(echo $SMBPATH | tr -d \"/\") # Test nc -zvw3 $FILE_HOST 445 FILE_SHARE_NAME=\"\" MNT_ROOT=\"/mnt\" MNT_PATH=\"$MNT_ROOT/$STORAGE_ACCOUNT_NAME/$FILE_SHARE_NAME\" sudo mkdir -p $MNT_PATH # This command assumes you have logged in with az login HTTP_ENDPOINT=$(az storage account show \\ --resource-group $RESOURCE_GROUP_NAME \\ --name $STORAGE_ACCOUNT_NAME \\ --query \"primaryEndpoints.file\" --output tsv | tr -d '\"') SMB_PATH=$(echo $HTTP_ENDPOINT | cut -c7-${#HTTP_ENDPOINT})$FILE_SHARE_NAME STORAGE_ACCOUNT_KEY=$(az storage account keys list \\ --resource-group $RESOURCE_GROUP_NAME \\ --account-name $STORAGE_ACCOUNT_NAME \\ --query \"[0].value\" --output tsv | tr -d '\"') sudo mount -t cifs $SMB_PATH $MNT_PATH -o username=$STORAGE_ACCOUNT_NAME,password=$STORAGE_ACCOUNT_KEY,uid=$(id -u),gid=$(id -g),serverino,nosharesock,actimeo=30,mfsymlinks ls $MNT_PATH","title":"Azure Blob"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/#overview-of-azure-storage","text":"","title":"Overview of Azure Storage"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/#blob-container","text":"","title":"Blob Container"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/#code","text":"# https://learn.microsoft.com/en-us/azure/storage/blobs/blobfuse2-how-to-deploy?tabs=Ubuntu # How to install BlobFuse2 ## Option 1: Install BlobFuse2 from the Microsoft software repositories for Linux ### To check your version of Linux, run the following command: cat /etc/*-release ### Install libfuse-dev sudo wget https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install libfuse-dev # ubuntu 18.04 # sudo apt-get install libfuse3-dev fuse3 # didn't work; this is for ubuntu > 18.04 ### Install BlobFuse2 sudo apt-get install blobfuse2 # How to configure BlobFuse2 ## Create Blobfuse Config File(s) CONFIG=/home/azureuser/blobfuse-config.yaml touch $CONFIG vim $CONFIG ## Create an empty directory to mount the blob container CONTAINER_DIR=/mnt/dlsparseviewctchallenge/bundles sudo mkdir -p $CONTAINER_DIR # echo $CONTAINER_DIR sudo blobfuse2 mount $CONTAINER_DIR --config-file=$CONFIG ls $CONTAINER_DIR","title":"Code"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/#config","text":"Sample # Refer ./setup/baseConfig.yaml for full set of config parameters allow-other: true logging: type: syslog level: log_debug components: - libfuse - stream - attr_cache - azstorage libfuse: attribute-expiration-sec: 120 entry-expiration-sec: 120 negative-entry-expiration-sec: 240 stream: block-size-mb: 8 blocks-per-file: 3 cache-size-mb: 1024 attr_cache: timeout-sec: 7200 azstorage: type: block account-name: mystorageaccount account-key: mystoragekey endpoint: https://mystorageaccount-CHANGEME.blob.core.windows.net mode: key container: mycontainer","title":"Config"},{"location":"notes/internet_of_things/Azure/Azure_Storage/azure_storage/#file-share","text":"sudo apt update -y; sudo apt remove azure-cli -y && sudo apt autoremove -y curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash # Microsoft Account az login --use-device-code RESOURCE_GROUP_NAME=\"\" STORAGE_ACCOUNT_NAME=\"\" # This command assumes you have logged in with az login HTTP_ENDPOINT=$(az storage account show \\ --resource-group $RESOURCE_GROUP_NAME \\ --name $STORAGE_ACCOUNT_NAME \\ --query \"primaryEndpoints.file\" --output tsv | tr -d '\"') SMBPATH=$(echo $HTTP_ENDPOINT | cut -c7-${#HTTP_ENDPOINT}) FILE_HOST=$(echo $SMBPATH | tr -d \"/\") # Test nc -zvw3 $FILE_HOST 445 FILE_SHARE_NAME=\"\" MNT_ROOT=\"/mnt\" MNT_PATH=\"$MNT_ROOT/$STORAGE_ACCOUNT_NAME/$FILE_SHARE_NAME\" sudo mkdir -p $MNT_PATH # This command assumes you have logged in with az login HTTP_ENDPOINT=$(az storage account show \\ --resource-group $RESOURCE_GROUP_NAME \\ --name $STORAGE_ACCOUNT_NAME \\ --query \"primaryEndpoints.file\" --output tsv | tr -d '\"') SMB_PATH=$(echo $HTTP_ENDPOINT | cut -c7-${#HTTP_ENDPOINT})$FILE_SHARE_NAME STORAGE_ACCOUNT_KEY=$(az storage account keys list \\ --resource-group $RESOURCE_GROUP_NAME \\ --account-name $STORAGE_ACCOUNT_NAME \\ --query \"[0].value\" --output tsv | tr -d '\"') sudo mount -t cifs $SMB_PATH $MNT_PATH -o username=$STORAGE_ACCOUNT_NAME,password=$STORAGE_ACCOUNT_KEY,uid=$(id -u),gid=$(id -g),serverino,nosharesock,actimeo=30,mfsymlinks ls $MNT_PATH","title":"File Share"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/","text":"TLS Security Courtesy of cloudfare Summary What is Transport Layer Security (TLS)? Transport Layer Security, or TLS, is a widely adopted security protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and voice over IP (VOIP). In this article we will focus on the role of TLS in web application security. TLS was proposed by the Internet Engineering Task Force (IETF), an international standards organization, and the first version of the protocol was published in 1999. The most recent version is TLS 1.3, which was published in 2018. What\u2019s the difference between TLS and SSL? TLS evolved from a previous encryption protocol called Secure Socket Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably. What\u2019s the difference between TLS and HTTPS? HTTPS is an implementation of TLS encryption on top of the HTTP protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption. Why should you use TLS? TLS encryption can help protect web applications from attacks such as data breaches, and DDoS attacks. Additionally, TLS-protected HTTPS is quickly becoming a standard practice for websites. For example, the Google Chrome browser is cracking down on non-HTTPS sites, and everyday Internet users are starting to become more wary of websites that don\u2019t feature the HTTPS padlock icon. How does TLS work? TLS can be used on top of a transport-layer security protocol like TCP. There are three main components to TLS: Encryption, Authentication, and Integrity. Encryption: hides the data being transferred from third parties. Authentication: ensures that the parties exchanging information are who they claim to be. Integrity: verifies that the data has not been forged or tampered with. A TLS connection is initiated using a sequence known as the TLS handshake. The TLS handshake establishes a cypher suite for each communication session. The cypher suite is a set of algorithms that specifies details such as which shared encryption keys, or session keys, will be used for that particular session. TLS is able to set the matching session keys over an unencrypted channel thanks to a technology known as public key cryptography. The handshake also handles authentication, which usually consists of the server proving its identity to the client. This is done using public keys. Public keys are encryption keys that use one-way encryption, meaning that anyone can unscramble data encrypted with the private key to ensure its authenticity, but only the original sender can encrypt data with the private key. Once data is encrypted and authenticated, it is then signed with a message authentication code (MAC). The recipient can then verify the MAC to ensure the integrity of the data. This is kind of like the tamper-proof foil found on a bottle of aspirin; the consumer knows no one has tampered with their medicine because the foil is intact when they purchase it. How does TLS affect web application performance? Because of the complex process involved in setting up a TLS connection, some load time and computational power must be expended. The client and server must communicate back and forth several times before any data is transmitted, and that eats up precious milliseconds of load times for web applications, as well as some memory for both the client and the server. Thankfully there are technologies in place that help to mitigate the lag created by the TLS handshake. One is TLS False Start, which lets the server and client start transmitting data before the TLS handshake is complete. Another technology to speed up TLS is TLS Session Resumption, which allows clients and servers that have previously communicated to use an abbreviated handshake. These improvements have helped to make TLS a very fast protocol that shouldn\u2019t noticeably affect load times. As for the computational costs associated with TLS, they are mostly negligible by today\u2019s standards. For example, when Google moved their entire Gmail platform to HTTPS in 2010, there was no need for them to enable any additional hardware. The extra load on their servers as a result of TLS encryption was less than 1%. How to start implementing TLS on a website All Cloudflare users automatically have HTTPS protection from Cloudflare. Via Universal SSL, Cloudflare offers free TLS/SSL certificates to all users. Anyone who doesn't use Cloudflare will have to acquire an SSL certificate from a certificate authority, often for a fee, and install the certificate on their origin servers. For more on how TLS/SSL certificates work, see What is an SSL certificate? Get a CA Certificate Notes to self sign for testing, but in practice I still see the \"Not Secure\" when useing this method, though https will work: Source Useful Stackoverflow article discussing file extensions and protocols\\file types: Helpful! - from serverfault In summary, there are four different ways to present certificates and their components: PEM - Governed by RFCs, its used preferentially by open-source software. It can have a variety of extensions (.pem, .key, .cer, .cert, more) PKCS7 - An open standard used by Java and supported by Windows. Does not contain private key material. PKCS12 - A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl. DER - The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows. Also Helpful! Above link is from certbot, a CA, and has useful notes regarding locations of certificates once generated and definitions of each file type. The follows notes are for generating certificates from them. Generate a Self-Signed Certificate Use this method if you want to use HTTPS (HTTP over TLS) to secure your Apache HTTP or Nginx web server, and you do not require that your certificate is signed by a CA. This command creates a 2048-bit private key ( domain.key ) and a self-signed certificate ( domain.crt ) from scratch: openssl req \\ -newkey rsa:2048 -nodes -keyout domain.key \\ -x509 -days 365 -out domain.crt The -x509 option tells req to create a self-signed cerificate. The -days 365 option specifies that the certificate will be valid for 365 days. A temporary CSR is generated to gather information to associate with the certificate. Talks about the CSR and getting a real CA verified key pair. source a2hosting CA Authority: certbot Source SSH into the server SSH into the server running your HTTP website as a user with sudo privileges. Add Certbot PPA You'll need to add the Certbot PPA to your list of repositories. To do so, run the following commands on the command line on the machine: sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update Install Certbot Run this command on the command line on the machine to install Certbot. sudo apt-get install certbot Choose how you'd like to run Certbot Are you ok with temporarily stopping your website? Yes, my web server is not currently running on this machine. Stop your webserver, then run this command to get a certificate. Certbot will temporarily spin up a webserver on your machine. sudo certbot certonly --standalone This gives this output: bbearce@miccai2019:~$ sudo certbot certonly --standalone Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator standalone, Installer None Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Please enter in your domain name(s) (comma and/or space separated) (Enter 'c' to cancel): miccai2020.eastus.cloudapp.azure.com Cert not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/miccai2020.eastus.cloudapp.azure.com.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Keep the existing certificate for now 2: Renew & replace the cert (limit ~5 per 7 days) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/privkey.pem Your cert will expire on 2020-08-02. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \"certbot renew\" - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le Notice where it put the cert: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ bbearce@miccai2019:~$ sudo ls /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ cert.pem chain.pem fullchain.pem privkey.pem README The README has this info: bbearce@miccai2019:~$ sudo cat /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/README This directory contains your keys and certificates. `privkey.pem` : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem` : used for OCSP stapling in Nginx >=1.3.7. `cert.pem` : will break many server configurations, and should not be used without reading further documentation (see link below). WARNING: DO NOT MOVE OR RENAME THESE FILES! Certbot expects these files to remain in this location in order to function properly! We recommend not moving these files. For more information, see the Certbot User Guide at https://certbot.eff.org/docs/using.html#where-are-my-certificates. Let's look inside just to get a feel for what is going on: fullchain cert and chain Note how the fullchain is really just the cert and chain together in one file. privkey If you need to keep my web server running. If you have a webserver that's already using port 80 and don't want to stop it while Certbot runs, run this command and follow the instructions in the terminal. sudo certbot certonly --webroot Install your certificate You'll need to install your new certificate in the configuration file for your webserver. Renew $ sudo certbot renew Ex: azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo certbot renew Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/spie-aapm-nci-dair.westus2.cloudapp.azure.com.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Renewing an existing certificate for spie-aapm-nci-dair.westus2.cloudapp.azure.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all renewals succeeded: /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/fullchain.pem (success) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain.pem privkey.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ mv certs/fullchain.pem certs/fullchain_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ mv certs/privkey.pem certs/privkey_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain_old.pem privkey_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo cp /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/privkey.pem certs/ azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo cp /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/fullchain.pem certs/ azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain.pem fullchain_old.pem privkey.pem privkey_old.pem","title":"TLS Security"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#tls-security","text":"Courtesy of cloudfare","title":"TLS Security"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#summary","text":"","title":"Summary"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#what-is-transport-layer-security-tls","text":"Transport Layer Security, or TLS, is a widely adopted security protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and voice over IP (VOIP). In this article we will focus on the role of TLS in web application security. TLS was proposed by the Internet Engineering Task Force (IETF), an international standards organization, and the first version of the protocol was published in 1999. The most recent version is TLS 1.3, which was published in 2018.","title":"What is Transport Layer Security (TLS)?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#whats-the-difference-between-tls-and-ssl","text":"TLS evolved from a previous encryption protocol called Secure Socket Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably.","title":"What\u2019s the difference between TLS and SSL?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#whats-the-difference-between-tls-and-https","text":"HTTPS is an implementation of TLS encryption on top of the HTTP protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption.","title":"What\u2019s the difference between TLS and HTTPS?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#why-should-you-use-tls","text":"TLS encryption can help protect web applications from attacks such as data breaches, and DDoS attacks. Additionally, TLS-protected HTTPS is quickly becoming a standard practice for websites. For example, the Google Chrome browser is cracking down on non-HTTPS sites, and everyday Internet users are starting to become more wary of websites that don\u2019t feature the HTTPS padlock icon.","title":"Why should you use TLS?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-does-tls-work","text":"TLS can be used on top of a transport-layer security protocol like TCP. There are three main components to TLS: Encryption, Authentication, and Integrity. Encryption: hides the data being transferred from third parties. Authentication: ensures that the parties exchanging information are who they claim to be. Integrity: verifies that the data has not been forged or tampered with. A TLS connection is initiated using a sequence known as the TLS handshake. The TLS handshake establishes a cypher suite for each communication session. The cypher suite is a set of algorithms that specifies details such as which shared encryption keys, or session keys, will be used for that particular session. TLS is able to set the matching session keys over an unencrypted channel thanks to a technology known as public key cryptography. The handshake also handles authentication, which usually consists of the server proving its identity to the client. This is done using public keys. Public keys are encryption keys that use one-way encryption, meaning that anyone can unscramble data encrypted with the private key to ensure its authenticity, but only the original sender can encrypt data with the private key. Once data is encrypted and authenticated, it is then signed with a message authentication code (MAC). The recipient can then verify the MAC to ensure the integrity of the data. This is kind of like the tamper-proof foil found on a bottle of aspirin; the consumer knows no one has tampered with their medicine because the foil is intact when they purchase it.","title":"How does TLS work?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-does-tls-affect-web-application-performance","text":"Because of the complex process involved in setting up a TLS connection, some load time and computational power must be expended. The client and server must communicate back and forth several times before any data is transmitted, and that eats up precious milliseconds of load times for web applications, as well as some memory for both the client and the server. Thankfully there are technologies in place that help to mitigate the lag created by the TLS handshake. One is TLS False Start, which lets the server and client start transmitting data before the TLS handshake is complete. Another technology to speed up TLS is TLS Session Resumption, which allows clients and servers that have previously communicated to use an abbreviated handshake. These improvements have helped to make TLS a very fast protocol that shouldn\u2019t noticeably affect load times. As for the computational costs associated with TLS, they are mostly negligible by today\u2019s standards. For example, when Google moved their entire Gmail platform to HTTPS in 2010, there was no need for them to enable any additional hardware. The extra load on their servers as a result of TLS encryption was less than 1%.","title":"How does TLS affect web application performance?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-to-start-implementing-tls-on-a-website","text":"All Cloudflare users automatically have HTTPS protection from Cloudflare. Via Universal SSL, Cloudflare offers free TLS/SSL certificates to all users. Anyone who doesn't use Cloudflare will have to acquire an SSL certificate from a certificate authority, often for a fee, and install the certificate on their origin servers. For more on how TLS/SSL certificates work, see What is an SSL certificate?","title":"How to start implementing TLS on a website"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#get-a-ca-certificate","text":"Notes to self sign for testing, but in practice I still see the \"Not Secure\" when useing this method, though https will work: Source Useful Stackoverflow article discussing file extensions and protocols\\file types: Helpful! - from serverfault In summary, there are four different ways to present certificates and their components: PEM - Governed by RFCs, its used preferentially by open-source software. It can have a variety of extensions (.pem, .key, .cer, .cert, more) PKCS7 - An open standard used by Java and supported by Windows. Does not contain private key material. PKCS12 - A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl. DER - The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows. Also Helpful! Above link is from certbot, a CA, and has useful notes regarding locations of certificates once generated and definitions of each file type. The follows notes are for generating certificates from them.","title":"Get a CA Certificate"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#generate-a-self-signed-certificate","text":"Use this method if you want to use HTTPS (HTTP over TLS) to secure your Apache HTTP or Nginx web server, and you do not require that your certificate is signed by a CA. This command creates a 2048-bit private key ( domain.key ) and a self-signed certificate ( domain.crt ) from scratch: openssl req \\ -newkey rsa:2048 -nodes -keyout domain.key \\ -x509 -days 365 -out domain.crt The -x509 option tells req to create a self-signed cerificate. The -days 365 option specifies that the certificate will be valid for 365 days. A temporary CSR is generated to gather information to associate with the certificate.","title":"Generate a Self-Signed Certificate"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#talks-about-the-csr-and-getting-a-real-ca-verified-key-pair","text":"source a2hosting","title":"Talks about the CSR and getting a real CA verified key pair."},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#ca-authority-certbot","text":"Source","title":"CA Authority: certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#ssh-into-the-server","text":"SSH into the server running your HTTP website as a user with sudo privileges.","title":"SSH into the server"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#add-certbot-ppa","text":"You'll need to add the Certbot PPA to your list of repositories. To do so, run the following commands on the command line on the machine: sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update","title":"Add Certbot PPA"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#install-certbot","text":"Run this command on the command line on the machine to install Certbot. sudo apt-get install certbot","title":"Install Certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#choose-how-youd-like-to-run-certbot","text":"Are you ok with temporarily stopping your website? Yes, my web server is not currently running on this machine. Stop your webserver, then run this command to get a certificate. Certbot will temporarily spin up a webserver on your machine. sudo certbot certonly --standalone This gives this output: bbearce@miccai2019:~$ sudo certbot certonly --standalone Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator standalone, Installer None Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Please enter in your domain name(s) (comma and/or space separated) (Enter 'c' to cancel): miccai2020.eastus.cloudapp.azure.com Cert not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/miccai2020.eastus.cloudapp.azure.com.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Keep the existing certificate for now 2: Renew & replace the cert (limit ~5 per 7 days) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/privkey.pem Your cert will expire on 2020-08-02. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \"certbot renew\" - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le Notice where it put the cert: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ bbearce@miccai2019:~$ sudo ls /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ cert.pem chain.pem fullchain.pem privkey.pem README The README has this info: bbearce@miccai2019:~$ sudo cat /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/README This directory contains your keys and certificates. `privkey.pem` : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem` : used for OCSP stapling in Nginx >=1.3.7. `cert.pem` : will break many server configurations, and should not be used without reading further documentation (see link below). WARNING: DO NOT MOVE OR RENAME THESE FILES! Certbot expects these files to remain in this location in order to function properly! We recommend not moving these files. For more information, see the Certbot User Guide at https://certbot.eff.org/docs/using.html#where-are-my-certificates. Let's look inside just to get a feel for what is going on:","title":"Choose how you'd like to run Certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#fullchain","text":"","title":"fullchain"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#cert-and-chain","text":"Note how the fullchain is really just the cert and chain together in one file.","title":"cert and chain"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#privkey","text":"If you need to keep my web server running. If you have a webserver that's already using port 80 and don't want to stop it while Certbot runs, run this command and follow the instructions in the terminal. sudo certbot certonly --webroot","title":"privkey"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#install-your-certificate","text":"You'll need to install your new certificate in the configuration file for your webserver.","title":"Install your certificate"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#renew","text":"$ sudo certbot renew Ex: azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo certbot renew Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/spie-aapm-nci-dair.westus2.cloudapp.azure.com.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Renewing an existing certificate for spie-aapm-nci-dair.westus2.cloudapp.azure.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all renewals succeeded: /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/fullchain.pem (success) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain.pem privkey.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ mv certs/fullchain.pem certs/fullchain_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ mv certs/privkey.pem certs/privkey_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain_old.pem privkey_old.pem azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo cp /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/privkey.pem certs/ azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ sudo cp /etc/letsencrypt/live/spie-aapm-nci-dair.westus2.cloudapp.azure.com/fullchain.pem certs/ azureuser@spie-aapm-nci-dair:~/src/MedICI/codalab$ ls certs fullchain.pem fullchain_old.pem privkey.pem privkey_old.pem","title":"Renew"},{"location":"notes/javascript/basics/","text":"Basics All JavaScript examples use ES2015 syntax, which include: const and let instead of var Arrow functions ( d => d instead of function(d) { return d; } ) where appropriate Spread operators [... iterable] , chained expressions, maps, sets and promises Template strings literals, defined using backticks Iterable collections, such as maps and sets Arrays const colors = [\"red\", \"blue\", \"green\"]; const geocoords = [27.2345, 34.9937]; const numbers = [1,2,3,4,5,6]; const empty = []; Each array has a length property that returns the number of elements. It's very useful to iterate using the array index: for(let i = 0; i < colors.length; i++) { console.log(colors[i]); } You can also loop over the elements of an array using the of operator (introduced in ES2015), when you don't need the index: for(let color of colors) { console.log(color); } And you can use the forEach() method, which runs a function for each element and also allows access to the index , item and array inside the function: colors.forEach(function(i, color, colors) { console.log((i+1) + \": \" + color); }) You need to pass something into the funtion and most by default use index, item, array, but as you can see other more descriptive placeholders can be used. Multi-dimensional arrays are created in JavaScript as arrays of arrays: const points = [[200,300], [150,100], [100,300]]; You can retrieve individual items like this: const firstPoint = points[0]; const middleX = points[1][0]; JavaScript provides many ways to extract and insert data into an array. It's usually recommended to use methods whenever possible. The following table lists useful methods you can use on arrays. Some modify the array; others return new arrays and other types. The examples provided use the colors and numbers arrays as declared above. Try them out using your browser's JavaScript console: Method Description Example push(item) Modifies the array adding an item to the end. colors.push(\"yellow\"); // [\"red\", \"blue\", \"green\", \"yellow\"]; pop() Modifies the array, removing and returning the last item. const green = colors.pop(); // [\"red\", \"blue\"]; unshift(item) Modifies the array inserting an item at the beginning. colors.unshift(\"yellow\"); // [\"yellow\", \"red\", \"blue\", \"green\"]; shift() Modifies the array, removing and returning the first item. const red = colors.shift(); // [\"blue\", \"green\"]; splice(p, n, i) Modifies the array, starting at position p. Can be used to delete items, insert or replace. const s = numbers.splice(2,3); // s = [3,4,5] // numbers = [1,2,6] reverse() Modifies the array, reversing its order. numbers.reverse(); // [6,5,4,3,2,1] sort() Modifies the array sorting by string order (if no args) or by a comparator function. numbers.sort((a,b) => b \u2013 a); // numbers = [6,5,4,3,2,1] slice(b,e) Returns a shallow copy of the array between b and e. const mid = numbers.slice(2,4) // mid = [3,4] filter(function) Returns new array where the elements pass the test implemented by the function. const even = numbers.filter(n => n%2==0); // [2,4,6] find(function) Returns the first element that satisfies the test function const two = numbers.find(n => n%2==0); // 2 indexOf(item) Returns the index of the first occurrence of item in the array. const n = numbers.indexOf(3); // 4 includes(item) Returns true if an array contains item among its entries. const n = numbers.includes(3); // true lastIndexOf(item) Returns the index of the last occurrence of the item in the array. const n = colors.lastIndexOf(\"blue\"); // 1 concat(other) Returns a new array that merges the current array with another. const eight = numbers.concat([7,8]); // [1,2,3,4,5,6,7,8] join()join(delim) Returns a comma-separated string of the elements in the array (an optional delimiter may be used) const csv = numbers.join(); // \"1,2,3,4,5,6\"Copyconst conc = numbers.join(\"\"); // \"123456\" map(function) Returns new array with each element modified by function. const squares = numbers.map(n => n*n); // [1,4,9,16,25,36] reduce(function) Returns the result of an accumulation operation using the values in the array. const sum = numbers.reduce((a, n) => a + n); forEach(function) Executes the provided function once for each element in the array. const squares = []; numbers.forEach(n => squares.push(n*n) // squares = [1,4,9,16,26,36] Strings Strings are primitive types in JavaScript that can be created with single quotes or double quotes. There is no difference. It's only a matter of style. ES2015 introduced two new string features: template literals and multiline strings . Multiline strings can be created adding a backslash at the end of each line: const line = \"Multiline strings can be \\ reated adding a backslash \\ at the end of each line\"; Template literals are strings created with backticks . They allow the inclusion of JavaScript expressions inside ${} placeholders. The result is concatenated as a single string. const template = `The square root of 2 is ${Math.sqrt(2)}`; If you need to use a special character in a string, such as a double quote in a double quoted string, or a backslash, you need to precede it with a backslash: const s = \"This is a backslash \\\\ and this is a double quote \\\"\"; There are several methods for string manipulation. They all return new strings or other types. No methods modify the original strings. Method Description Example startsWith(s) Returns true if string starts with the string passed as a parameter const s = \"This is a test string\" const r = s.startsWith(\"This\"); // true endsWith(s) Returns true if string ends with the string passed as a parameter const s = \"This is a test string\" const r = s.endsWith(\"This\"); // false substring(s,e) Returns a substring between start (incl.) and end indexes (not incl.) const k = \"Aardvark\" const ardva = k.substring(1,6); split(regx)split(delim) Splits a string by a delimiter character or regular expression and returns an array const result = s.split(\" \"); // result = // [\"This\",\"is\",\"a\",\"test\",\"string\"] indexOf(s) Returns the index of the first occurrence of a substring const k = \"Aardvark\" const i = k.indexOf(\"ar\"); // i = 1 lastIndexOf(s) Returns the index of the last occurrence of a substring const k = \"Aardvark\" const i = k.lastIndexOf(\"ar\"); // i = 5 charAt(i) Returns char at index i. Also supported as \u2018string\u2019[i] const k = \"Aardvark\" const v = k.charAt(4); trim() Removes whitespace from both ends of a string. const text = \" data \" const r = data.trim(); // r = \"data\" match(regx) Returns an array as the result of matchin a regular expression against the string. const k = \"Aardvark\" const v = k.match(/[a-f]/g); // v = [\"a\", \"d\", \"a\"] replace(regx,r)replace(s,t) Returns new string replacing match of regexp applied to the string with replacement, or all occurrences of source string with a target string. const k = \"Aardvark\" const a = k.replace(/a/gi, 'u') // a = \"uurdvurk\" const b = k.replace('ardv', 'ntib') // b = \"Antibark\" Functions Functions are typically created in JavaScript using the function keyword, using one of the forms below: function f() { console.log('function1', this); } const g = function(name) { console.log('function ' + name, this); } f(); // calls f g('test'); // calls g() with a parameter The this keyword refers to the object that owns the function. If this code runs in a browser, and this is a top-level function created in the <script> block, the owner is the global window object. Any properties accessed via this refer to that object. const obj = {a: 5, b: 6} obj.method = function() { console.log('method', this) } obj.method() output: method {a: 5, b: 6, method: \u0192} Arrow functions were introduced in ES2015. They are much more compact and can lead to cleaner code, but the scope of this is no longer retained by the object. In the code below, it refers to the global window object. Code that uses this.a and this.b will not find any data in the object and will return undefined . obj.arrow = () => console.log('arrow', this) obj.arrow() output: arrow Window {parent: Window, opener: null, top: Window, length: 0, frames: Window, \u2026} Objects An object is an unordered collection of data. Values in an object are stored as key-value pairs. You can create an object by declaring a comma-separated list of key:value pairs within curly braces, or simply a pair of opening-closing curly braces if you want to start with an empty object: const color = {name: \"red\", code: \"ff0000\"}; const empty = {}; Objects can contain other objects and arrays, which can also contain objects. They can also contain functions (which have access to local properties and behave as methods): const city = {name: \"Sao Paulo\", location: {latitude: 23.555, longitude: 46.63}, airports: [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; const circle = { x: 200, y: 100, r: 50, area: function() { return this.r * this.r * 3.14; } } A typical dataset used by a simple chart usually consists of an array of objects. var array2 = [ {continent: \"Asia\", areakm2: 43820000}, {continent: \"Europe\", areakm2: 10180000}, {continent: \"Africa\", areakm2: 30370000}, {continent: \"South America\", areakm2: 17840000}, {continent: \"Oceania\", areakm2: 9008500}, {continent: \"North America\", areakm2:24490000} ]; You can access the properties of an object using the dot operator or brackets containing the key as a string. You can run its methods using the dot operator: const latitude = city.location.latitude; const oceania = array2[4].continent; const code = color[\"code\"]; circle.r = 100; const area = circle.area(); You can also loop over the properties of an object: for(let key in color) { console.log(key + \": \" + color[key]); } Properties and functions can be added to objects. It's common to write code that declares an empty object in a global context so that operations in other contexts add data to it: coords = {lat:34.342352435, lng: 108.98375984} const map = {}; function getCoords(coords) { map.latitude = coords.lat; map.longitude = coords.lng; } console.log(map) output: {lat:34.342352435, lng: 108.98375984} Objects can also be created with a constructor. You can create an object that contains the current date/time using: const now = new Date(); JSON is a data format based on JavaScript objects. It has the same structure as JavaScript object but the property keys have to be placed within double quotes: {\"name\": \"Sao Paulo\", \"location\": {\"latitude\": 23.555, \"longitude\": 46.63}, \"airports\": [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; To use a JSON string in JavaScript you have to parse it. Maps and Sets Besides arrays, ES2015 also introduced two new data structures: Map , an associative array with key-value pairs easier to use than simple objects, and Set , which doesn't allow repeated values. Both can be transformed to and from arrays. You can create a new Set object using: const set = new Set(); And add elements to it using: set.add(5); set.add(7); If you try to add elements that already exist, they won't be included in the set: set.add(1); set.add(5); // not added set.add(7); // not added console.log(set.size); // prints 3 You can check if a Set contains an element: console.log(set.has(3)); // false And convert a Set object into an array: const array1 = [... set]; // spread operator A Map can be more efficient than using an object to store key-value pairs in an associative array: const map = new Map(); map.set(\"a\", 123) map.set(\"b\", 456); console.log(map) output: Map(2) {\"a\" => 123, \"b\" => 456} You can then retrieve the value for each key using get(key): console.log(map.get(\"b\")) Or using iterative operations and the keys(), values() and entries() methods: for (let key of map.keys()) { console.log(key, map.get(key)) } for (let [key, value] of map.entries()) { console.log(key, value) } map.forEach ((k, v) => console.log(k, v)) Maps can be converted into arrays with the spread operator or Arrays.from(): const array2 = [... map.values()]; // an array of values const array3 = Array.from(map.values()); // an array of keys Promises Source Web Dev Simplified Define a promise. 1+1 is what we want the promise to do. The object itself accepts a function that has inputs resolve and reject . let p = new Promise((resolve, reject) => { let a = 1 + 1 // if you change of one these to 2 to make it fail you have to have the p.then().catch() code implemented of you will get a \"Uncaught (in promise) Failed\" message if (a == 2) { resolve('Success') } else { reject('Failed') } }) Interact with promises. the .then method runs if the promise resolves . If the promise rejects it runs the .catch method. p.then((message) => { console.log('This is in the then ' + message) }).catch ((message) => { console.log('This is in the catch '+ message) }) Promises versus Async Functions var start = function(){ console.log(\"start\") return new Promise(function(resolve, reject){ setTimeout(() => resolve(\"start sending this off\"), 1000); }) }; var func1 = function(from_before){ console.log(from_before); return new Promise(function(resolve, reject){ setTimeout(() => {console.log(\"func1\"); resolve(\"func1 sending this off\")}, 1000); }) }; var func2 = function(from_before){ console.log(from_before); return new Promise(function(resolve, reject){ setTimeout(() => {console.log(\"func2\"); resolve(\"func2 sending this off\")}, 1000); }) }; start() .then(func1) .then(func2) .then(function(res){ console.log(res) }); // async await var func1 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func1 sending this off\") }, 1000) }) }; var func2 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func2 sending this off\") }, 1000) }) }; var func3 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func3 sending this off\") }, 1000) }) }; var start = async function(){ console.log(\"start\") fun1_out = await func1(\"start sending this off\") fun2_out = await func2(fun1_out) fun3_out = await func3(fun2_out) }; start();","title":"Basics"},{"location":"notes/javascript/basics/#basics","text":"All JavaScript examples use ES2015 syntax, which include: const and let instead of var Arrow functions ( d => d instead of function(d) { return d; } ) where appropriate Spread operators [... iterable] , chained expressions, maps, sets and promises Template strings literals, defined using backticks Iterable collections, such as maps and sets","title":"Basics"},{"location":"notes/javascript/basics/#arrays","text":"const colors = [\"red\", \"blue\", \"green\"]; const geocoords = [27.2345, 34.9937]; const numbers = [1,2,3,4,5,6]; const empty = []; Each array has a length property that returns the number of elements. It's very useful to iterate using the array index: for(let i = 0; i < colors.length; i++) { console.log(colors[i]); } You can also loop over the elements of an array using the of operator (introduced in ES2015), when you don't need the index: for(let color of colors) { console.log(color); } And you can use the forEach() method, which runs a function for each element and also allows access to the index , item and array inside the function: colors.forEach(function(i, color, colors) { console.log((i+1) + \": \" + color); }) You need to pass something into the funtion and most by default use index, item, array, but as you can see other more descriptive placeholders can be used. Multi-dimensional arrays are created in JavaScript as arrays of arrays: const points = [[200,300], [150,100], [100,300]]; You can retrieve individual items like this: const firstPoint = points[0]; const middleX = points[1][0]; JavaScript provides many ways to extract and insert data into an array. It's usually recommended to use methods whenever possible. The following table lists useful methods you can use on arrays. Some modify the array; others return new arrays and other types. The examples provided use the colors and numbers arrays as declared above. Try them out using your browser's JavaScript console: Method Description Example push(item) Modifies the array adding an item to the end. colors.push(\"yellow\"); // [\"red\", \"blue\", \"green\", \"yellow\"]; pop() Modifies the array, removing and returning the last item. const green = colors.pop(); // [\"red\", \"blue\"]; unshift(item) Modifies the array inserting an item at the beginning. colors.unshift(\"yellow\"); // [\"yellow\", \"red\", \"blue\", \"green\"]; shift() Modifies the array, removing and returning the first item. const red = colors.shift(); // [\"blue\", \"green\"]; splice(p, n, i) Modifies the array, starting at position p. Can be used to delete items, insert or replace. const s = numbers.splice(2,3); // s = [3,4,5] // numbers = [1,2,6] reverse() Modifies the array, reversing its order. numbers.reverse(); // [6,5,4,3,2,1] sort() Modifies the array sorting by string order (if no args) or by a comparator function. numbers.sort((a,b) => b \u2013 a); // numbers = [6,5,4,3,2,1] slice(b,e) Returns a shallow copy of the array between b and e. const mid = numbers.slice(2,4) // mid = [3,4] filter(function) Returns new array where the elements pass the test implemented by the function. const even = numbers.filter(n => n%2==0); // [2,4,6] find(function) Returns the first element that satisfies the test function const two = numbers.find(n => n%2==0); // 2 indexOf(item) Returns the index of the first occurrence of item in the array. const n = numbers.indexOf(3); // 4 includes(item) Returns true if an array contains item among its entries. const n = numbers.includes(3); // true lastIndexOf(item) Returns the index of the last occurrence of the item in the array. const n = colors.lastIndexOf(\"blue\"); // 1 concat(other) Returns a new array that merges the current array with another. const eight = numbers.concat([7,8]); // [1,2,3,4,5,6,7,8] join()join(delim) Returns a comma-separated string of the elements in the array (an optional delimiter may be used) const csv = numbers.join(); // \"1,2,3,4,5,6\"Copyconst conc = numbers.join(\"\"); // \"123456\" map(function) Returns new array with each element modified by function. const squares = numbers.map(n => n*n); // [1,4,9,16,25,36] reduce(function) Returns the result of an accumulation operation using the values in the array. const sum = numbers.reduce((a, n) => a + n); forEach(function) Executes the provided function once for each element in the array. const squares = []; numbers.forEach(n => squares.push(n*n) // squares = [1,4,9,16,26,36]","title":"Arrays"},{"location":"notes/javascript/basics/#strings","text":"Strings are primitive types in JavaScript that can be created with single quotes or double quotes. There is no difference. It's only a matter of style. ES2015 introduced two new string features: template literals and multiline strings . Multiline strings can be created adding a backslash at the end of each line: const line = \"Multiline strings can be \\ reated adding a backslash \\ at the end of each line\"; Template literals are strings created with backticks . They allow the inclusion of JavaScript expressions inside ${} placeholders. The result is concatenated as a single string. const template = `The square root of 2 is ${Math.sqrt(2)}`; If you need to use a special character in a string, such as a double quote in a double quoted string, or a backslash, you need to precede it with a backslash: const s = \"This is a backslash \\\\ and this is a double quote \\\"\"; There are several methods for string manipulation. They all return new strings or other types. No methods modify the original strings. Method Description Example startsWith(s) Returns true if string starts with the string passed as a parameter const s = \"This is a test string\" const r = s.startsWith(\"This\"); // true endsWith(s) Returns true if string ends with the string passed as a parameter const s = \"This is a test string\" const r = s.endsWith(\"This\"); // false substring(s,e) Returns a substring between start (incl.) and end indexes (not incl.) const k = \"Aardvark\" const ardva = k.substring(1,6); split(regx)split(delim) Splits a string by a delimiter character or regular expression and returns an array const result = s.split(\" \"); // result = // [\"This\",\"is\",\"a\",\"test\",\"string\"] indexOf(s) Returns the index of the first occurrence of a substring const k = \"Aardvark\" const i = k.indexOf(\"ar\"); // i = 1 lastIndexOf(s) Returns the index of the last occurrence of a substring const k = \"Aardvark\" const i = k.lastIndexOf(\"ar\"); // i = 5 charAt(i) Returns char at index i. Also supported as \u2018string\u2019[i] const k = \"Aardvark\" const v = k.charAt(4); trim() Removes whitespace from both ends of a string. const text = \" data \" const r = data.trim(); // r = \"data\" match(regx) Returns an array as the result of matchin a regular expression against the string. const k = \"Aardvark\" const v = k.match(/[a-f]/g); // v = [\"a\", \"d\", \"a\"] replace(regx,r)replace(s,t) Returns new string replacing match of regexp applied to the string with replacement, or all occurrences of source string with a target string. const k = \"Aardvark\" const a = k.replace(/a/gi, 'u') // a = \"uurdvurk\" const b = k.replace('ardv', 'ntib') // b = \"Antibark\"","title":"Strings"},{"location":"notes/javascript/basics/#functions","text":"Functions are typically created in JavaScript using the function keyword, using one of the forms below: function f() { console.log('function1', this); } const g = function(name) { console.log('function ' + name, this); } f(); // calls f g('test'); // calls g() with a parameter The this keyword refers to the object that owns the function. If this code runs in a browser, and this is a top-level function created in the <script> block, the owner is the global window object. Any properties accessed via this refer to that object. const obj = {a: 5, b: 6} obj.method = function() { console.log('method', this) } obj.method() output: method {a: 5, b: 6, method: \u0192} Arrow functions were introduced in ES2015. They are much more compact and can lead to cleaner code, but the scope of this is no longer retained by the object. In the code below, it refers to the global window object. Code that uses this.a and this.b will not find any data in the object and will return undefined . obj.arrow = () => console.log('arrow', this) obj.arrow() output: arrow Window {parent: Window, opener: null, top: Window, length: 0, frames: Window, \u2026}","title":"Functions"},{"location":"notes/javascript/basics/#objects","text":"An object is an unordered collection of data. Values in an object are stored as key-value pairs. You can create an object by declaring a comma-separated list of key:value pairs within curly braces, or simply a pair of opening-closing curly braces if you want to start with an empty object: const color = {name: \"red\", code: \"ff0000\"}; const empty = {}; Objects can contain other objects and arrays, which can also contain objects. They can also contain functions (which have access to local properties and behave as methods): const city = {name: \"Sao Paulo\", location: {latitude: 23.555, longitude: 46.63}, airports: [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; const circle = { x: 200, y: 100, r: 50, area: function() { return this.r * this.r * 3.14; } } A typical dataset used by a simple chart usually consists of an array of objects. var array2 = [ {continent: \"Asia\", areakm2: 43820000}, {continent: \"Europe\", areakm2: 10180000}, {continent: \"Africa\", areakm2: 30370000}, {continent: \"South America\", areakm2: 17840000}, {continent: \"Oceania\", areakm2: 9008500}, {continent: \"North America\", areakm2:24490000} ]; You can access the properties of an object using the dot operator or brackets containing the key as a string. You can run its methods using the dot operator: const latitude = city.location.latitude; const oceania = array2[4].continent; const code = color[\"code\"]; circle.r = 100; const area = circle.area(); You can also loop over the properties of an object: for(let key in color) { console.log(key + \": \" + color[key]); } Properties and functions can be added to objects. It's common to write code that declares an empty object in a global context so that operations in other contexts add data to it: coords = {lat:34.342352435, lng: 108.98375984} const map = {}; function getCoords(coords) { map.latitude = coords.lat; map.longitude = coords.lng; } console.log(map) output: {lat:34.342352435, lng: 108.98375984} Objects can also be created with a constructor. You can create an object that contains the current date/time using: const now = new Date(); JSON is a data format based on JavaScript objects. It has the same structure as JavaScript object but the property keys have to be placed within double quotes: {\"name\": \"Sao Paulo\", \"location\": {\"latitude\": 23.555, \"longitude\": 46.63}, \"airports\": [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; To use a JSON string in JavaScript you have to parse it.","title":"Objects"},{"location":"notes/javascript/basics/#maps-and-sets","text":"Besides arrays, ES2015 also introduced two new data structures: Map , an associative array with key-value pairs easier to use than simple objects, and Set , which doesn't allow repeated values. Both can be transformed to and from arrays. You can create a new Set object using: const set = new Set(); And add elements to it using: set.add(5); set.add(7); If you try to add elements that already exist, they won't be included in the set: set.add(1); set.add(5); // not added set.add(7); // not added console.log(set.size); // prints 3 You can check if a Set contains an element: console.log(set.has(3)); // false And convert a Set object into an array: const array1 = [... set]; // spread operator A Map can be more efficient than using an object to store key-value pairs in an associative array: const map = new Map(); map.set(\"a\", 123) map.set(\"b\", 456); console.log(map) output: Map(2) {\"a\" => 123, \"b\" => 456} You can then retrieve the value for each key using get(key): console.log(map.get(\"b\")) Or using iterative operations and the keys(), values() and entries() methods: for (let key of map.keys()) { console.log(key, map.get(key)) } for (let [key, value] of map.entries()) { console.log(key, value) } map.forEach ((k, v) => console.log(k, v)) Maps can be converted into arrays with the spread operator or Arrays.from(): const array2 = [... map.values()]; // an array of values const array3 = Array.from(map.values()); // an array of keys","title":"Maps and Sets"},{"location":"notes/javascript/basics/#promises","text":"Source Web Dev Simplified Define a promise. 1+1 is what we want the promise to do. The object itself accepts a function that has inputs resolve and reject . let p = new Promise((resolve, reject) => { let a = 1 + 1 // if you change of one these to 2 to make it fail you have to have the p.then().catch() code implemented of you will get a \"Uncaught (in promise) Failed\" message if (a == 2) { resolve('Success') } else { reject('Failed') } }) Interact with promises. the .then method runs if the promise resolves . If the promise rejects it runs the .catch method. p.then((message) => { console.log('This is in the then ' + message) }).catch ((message) => { console.log('This is in the catch '+ message) })","title":"Promises"},{"location":"notes/javascript/basics/#promises-versus-async-functions","text":"var start = function(){ console.log(\"start\") return new Promise(function(resolve, reject){ setTimeout(() => resolve(\"start sending this off\"), 1000); }) }; var func1 = function(from_before){ console.log(from_before); return new Promise(function(resolve, reject){ setTimeout(() => {console.log(\"func1\"); resolve(\"func1 sending this off\")}, 1000); }) }; var func2 = function(from_before){ console.log(from_before); return new Promise(function(resolve, reject){ setTimeout(() => {console.log(\"func2\"); resolve(\"func2 sending this off\")}, 1000); }) }; start() .then(func1) .then(func2) .then(function(res){ console.log(res) }); // async await var func1 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func1 sending this off\") }, 1000) }) }; var func2 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func2 sending this off\") }, 1000) }) }; var func3 = async function(from_before){ console.log(from_before); return new Promise(resolve => { setTimeout(function(){ resolve(\"func3 sending this off\") }, 1000) }) }; var start = async function(){ console.log(\"start\") fun1_out = await func1(\"start sending this off\") fun2_out = await func2(fun1_out) fun3_out = await func3(fun2_out) }; start();","title":"Promises versus Async Functions"},{"location":"notes/javascript/D3js/links_to_checkout/","text":"Links For Tutorials That I Liked D3 Mapping tutrial","title":"Links To Checkout"},{"location":"notes/javascript/D3js/links_to_checkout/#links-for-tutorials-that-i-liked","text":"D3 Mapping tutrial","title":"Links For Tutorials That I Liked"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/","text":"References: D3.js documentation: d3js.org D3.js API reference: github.com/d3/d3/blob/master/API.md D3.js module repository: github.com/d3 D3.js gallery: github.com/d3/d3/wiki/Gallery D3.js wiki: github.com/d3/d3/wiki Bl.ocks portfolios: bl.ocks.org Observable notebooks: observablehq.com Source github What is D3? D3 Stands for Data Driven Documents. It is a javascript library but not a charting library. It focuses on the data. It will replace the DOM API and libraries such as jQuery. It uses data to drive everything. Data is first. \"This API is also used to bind and dispatch events, and to generate animated transitions. It can also parse different data formats, such as JSON and CSV, perform general data manipulation on objects and arrays. A typical D3.js script uses CSS selectors to select HTML or SVG elements and binds them to individual data items, removing, updating, or appending graphical elements automatically, when necessary. From the intro it seems you use CSS to create a selection object on a group of HTML tags and then bind data to them. You can also make a selection bound to data when the HTML doen't exist yet and have the HTML created automatically. Use https://d3js.org/ and click on any of the hexagon example to be taken to a tutorial. Two other platforms are https://www.bl.ocks.org and https://www.observablehq.com which contain interactive tutorials. Environment Setup npm $ npm install d3 From there in my home directory is: $ ls node_modules/d3/dist/ d3.js d3.min.js d3.node.js package.js Just source the min after copying it to your development directory as follows: <script src=\"js/d3/d3.v5.min.js\"></script> CDN link You can also just source it from the internet if you always have internet: <script src=\"https://d3js.org/d3.v5.min.js\"></script> Simple Web Server with node For simple examples using the file system to open the file is enough, but for larger ones that load external files, it is better to use a web server. $ npm install http-server -g Once installed move to the working directory and execute: $ http-server I had an issue running http-server out of the box and this thread fixed it. Ex: Create a sim link called node that points to nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ sudo ln -s /usr/bin/nodejs /usr/bin/node bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ http-server Starting up http-server, serving ./ Available on: http://127.0.0.1:8080 http://192.168.0.46:8080 Hit CTRL-C to stop the server Notice: bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/node lrwxrwxrwx 1 root root 15 Feb 27 22:53 /usr/bin/node -> /usr/bin/nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/nodejs -rwxr-xr-x 1 root root 11187096 Aug 9 2018 /usr/bin/nodejs Don't forget about codepen and jsfiddle as possible development environments Hello World Put this is the body tag and then the js in a separate doc and source or put it in a script tag. HTML <svg id=\"chart\" width=\"600\" height=\"200\"></svg> Constant const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") // select #chart .selectAll(\"circle\") // declares the elements we would like to create; Really this selects all circle elements in the <svg> tag but there are none so the selection is empty .data([100]) // sets data to drive element creation .enter() // creates a selection to add elements per data item .append(\"circle\") // appends an element of this type to each data item .attr(\"r\", \"10\") // sets \"r\" attribute .attr(\"cy\", 100) // sets \"cy\" attribute .attr(\"cx\", d => d) // sets \"cx\" attribute (same as function(d){ return d }); This is being set by the data; Binding (we changed [100] in .data() to array) const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") .selectAll(\"circle\") .data(array) // here we change [100] to array for multiple circles .enter() .append(\"circle\") .attr(\"r\", \"10\") .attr(\"cy\", 100) .attr(\"cx\", d => d) Update Now skip a line and add this code after the selection. After 2 seconds there is a 1/4 second transition. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) One very nice feature of D3 is how simple it is to animate transitions. You just need to chain a transition() command before the attributes and styles that changed. The default transition takes a quarter of a second. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .transition().duration(1000) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) You'll notice the transition is much smoother now, taking time to switch to red dots. Modules (microlibraries) You can load select libraries like so: <script src=\"https://d3js.org/d3-selection.v1.min.js\"></script> <script src=\"https://d3js.org/d3-transition.v1.min.js\"></script> But if you need axes, maps, and other features, you will require more modules and dependencies. In this case, either use the default bundle, or set up a development environment where you can install each module using npm, since it automatically includes any dependencies. npm install module-name Data Manipulation The following modules listed are used to generate, manipulate, transform, parse, and format data, in the form of numbers, text, arrays, objects, and files. They are all included in the default d3.js bundle: Module Bundled (d3v5) Description d3-array Yes Several array utilities that extend the basic ES6 functions, optimized for use with datasets. Dependencies: none. d3-collection Yes Maps and sets optimized for use with datasets; functions for object collections and nesting data. Dependencies: none. d3-random Yes Random number generators. Dependencies: none. d3-dsv Yes Parser functions for delimiter-separated data. Dependencies: none. d3-interpolate Yes Several functions for interpolating numbers, colors, strings, and so on. Dependencies: d3-color. d3-scale Yes Generator functions to map data dimensions to graphical dimensions. Dependencies: d3-array, d3-collection, d3-format, d3-interpolate, d3-time-format, d3-time. d3-time Yes API for operations with time (intervals, ranges, and so on). Dependencies: none. d3-format Yes Locale-sensitive methods for number formatting. Dependencies: none. d3-time-format Yes Locale-sensitive methods for date/time formatting. Dependencies: d3-time. Documents Manipulation These are core modules in D3 used to select and manipulate HTML or SVG elements by providing a concise API to the DOM. With these modules, you can select and filter elements (using CSS selectors), create elements, append, insert, or remove from the DOM tree, add attributes and contents, change styles or classes, connect event handlers, and join data. Practically any D3 application uses at least d3-selection. Module Bundled (d3v5) Description d3-selection Yes Contains the essential DOM API for selection and manipulation of DOM elements. Dependencies: none. d3-selection-multi No Adds optional support for setting multiple attributes, styles, or properties in selections and transitions using an object syntax. Dependencies: d3-selection, d3-transition. There are more located here There are: colors ajax 2D geometry graphing API Spherical geometry and geometrical maps Layouts: node-link hierarchies, graphs, trees, networks, flow diagrams, tiles, and Voronoi diagrams","title":"Chapter 1"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#references","text":"D3.js documentation: d3js.org D3.js API reference: github.com/d3/d3/blob/master/API.md D3.js module repository: github.com/d3 D3.js gallery: github.com/d3/d3/wiki/Gallery D3.js wiki: github.com/d3/d3/wiki Bl.ocks portfolios: bl.ocks.org Observable notebooks: observablehq.com Source github","title":"References:"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#what-is-d3","text":"D3 Stands for Data Driven Documents. It is a javascript library but not a charting library. It focuses on the data. It will replace the DOM API and libraries such as jQuery. It uses data to drive everything. Data is first. \"This API is also used to bind and dispatch events, and to generate animated transitions. It can also parse different data formats, such as JSON and CSV, perform general data manipulation on objects and arrays. A typical D3.js script uses CSS selectors to select HTML or SVG elements and binds them to individual data items, removing, updating, or appending graphical elements automatically, when necessary. From the intro it seems you use CSS to create a selection object on a group of HTML tags and then bind data to them. You can also make a selection bound to data when the HTML doen't exist yet and have the HTML created automatically. Use https://d3js.org/ and click on any of the hexagon example to be taken to a tutorial. Two other platforms are https://www.bl.ocks.org and https://www.observablehq.com which contain interactive tutorials.","title":"What is D3?"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#environment-setup","text":"","title":"Environment Setup"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#npm","text":"$ npm install d3 From there in my home directory is: $ ls node_modules/d3/dist/ d3.js d3.min.js d3.node.js package.js Just source the min after copying it to your development directory as follows: <script src=\"js/d3/d3.v5.min.js\"></script>","title":"npm"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#cdn-link","text":"You can also just source it from the internet if you always have internet: <script src=\"https://d3js.org/d3.v5.min.js\"></script>","title":"CDN link"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#simple-web-server-with-node","text":"For simple examples using the file system to open the file is enough, but for larger ones that load external files, it is better to use a web server. $ npm install http-server -g Once installed move to the working directory and execute: $ http-server I had an issue running http-server out of the box and this thread fixed it. Ex: Create a sim link called node that points to nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ sudo ln -s /usr/bin/nodejs /usr/bin/node bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ http-server Starting up http-server, serving ./ Available on: http://127.0.0.1:8080 http://192.168.0.46:8080 Hit CTRL-C to stop the server Notice: bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/node lrwxrwxrwx 1 root root 15 Feb 27 22:53 /usr/bin/node -> /usr/bin/nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/nodejs -rwxr-xr-x 1 root root 11187096 Aug 9 2018 /usr/bin/nodejs Don't forget about codepen and jsfiddle as possible development environments","title":"Simple Web Server with node"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#hello-world","text":"Put this is the body tag and then the js in a separate doc and source or put it in a script tag. HTML <svg id=\"chart\" width=\"600\" height=\"200\"></svg>","title":"Hello World"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#constant","text":"const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") // select #chart .selectAll(\"circle\") // declares the elements we would like to create; Really this selects all circle elements in the <svg> tag but there are none so the selection is empty .data([100]) // sets data to drive element creation .enter() // creates a selection to add elements per data item .append(\"circle\") // appends an element of this type to each data item .attr(\"r\", \"10\") // sets \"r\" attribute .attr(\"cy\", 100) // sets \"cy\" attribute .attr(\"cx\", d => d) // sets \"cx\" attribute (same as function(d){ return d }); This is being set by the data;","title":"Constant"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#binding","text":"(we changed [100] in .data() to array) const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") .selectAll(\"circle\") .data(array) // here we change [100] to array for multiple circles .enter() .append(\"circle\") .attr(\"r\", \"10\") .attr(\"cy\", 100) .attr(\"cx\", d => d)","title":"Binding"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#update","text":"Now skip a line and add this code after the selection. After 2 seconds there is a 1/4 second transition. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) One very nice feature of D3 is how simple it is to animate transitions. You just need to chain a transition() command before the attributes and styles that changed. The default transition takes a quarter of a second. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .transition().duration(1000) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) You'll notice the transition is much smoother now, taking time to switch to red dots.","title":"Update"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#modules-microlibraries","text":"You can load select libraries like so: <script src=\"https://d3js.org/d3-selection.v1.min.js\"></script> <script src=\"https://d3js.org/d3-transition.v1.min.js\"></script> But if you need axes, maps, and other features, you will require more modules and dependencies. In this case, either use the default bundle, or set up a development environment where you can install each module using npm, since it automatically includes any dependencies. npm install module-name","title":"Modules (microlibraries)"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#data-manipulation","text":"The following modules listed are used to generate, manipulate, transform, parse, and format data, in the form of numbers, text, arrays, objects, and files. They are all included in the default d3.js bundle: Module Bundled (d3v5) Description d3-array Yes Several array utilities that extend the basic ES6 functions, optimized for use with datasets. Dependencies: none. d3-collection Yes Maps and sets optimized for use with datasets; functions for object collections and nesting data. Dependencies: none. d3-random Yes Random number generators. Dependencies: none. d3-dsv Yes Parser functions for delimiter-separated data. Dependencies: none. d3-interpolate Yes Several functions for interpolating numbers, colors, strings, and so on. Dependencies: d3-color. d3-scale Yes Generator functions to map data dimensions to graphical dimensions. Dependencies: d3-array, d3-collection, d3-format, d3-interpolate, d3-time-format, d3-time. d3-time Yes API for operations with time (intervals, ranges, and so on). Dependencies: none. d3-format Yes Locale-sensitive methods for number formatting. Dependencies: none. d3-time-format Yes Locale-sensitive methods for date/time formatting. Dependencies: d3-time.","title":"Data Manipulation"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#documents-manipulation","text":"These are core modules in D3 used to select and manipulate HTML or SVG elements by providing a concise API to the DOM. With these modules, you can select and filter elements (using CSS selectors), create elements, append, insert, or remove from the DOM tree, add attributes and contents, change styles or classes, connect event handlers, and join data. Practically any D3 application uses at least d3-selection. Module Bundled (d3v5) Description d3-selection Yes Contains the essential DOM API for selection and manipulation of DOM elements. Dependencies: none. d3-selection-multi No Adds optional support for setting multiple attributes, styles, or properties in selections and transitions using an object syntax. Dependencies: d3-selection, d3-transition. There are more located here There are: colors ajax 2D geometry graphing API Spherical geometry and geometrical maps Layouts: node-link hierarchies, graphs, trees, networks, flow diagrams, tiles, and Voronoi diagrams","title":"Documents Manipulation"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/","text":"Source github Technical Fundamentals This chapter covers fundamental standard web technologies used by D3: SVG, JavaScript (ES 2015), HTML Canvas and standard data formats such as JSON and CSV. It is intended as a general reference to these topics. Most data visualizations created with D3.js generate SVG graphics. Good knowledge of SVG is important to make the most of D3, but you only really need to know the basics. 1) SVG SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. Scalable Vector Graphics (SVG) SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. To control SVG elements with D3 you should understand basic SVG syntax and rules, how a document is structured, how each element is rendered, the effects caused by attributes and styles, as well as nesting and transformation rules. All the code used in this section is available in the SVG/ folder, from the GitHub repository for this chapter. You can see the results simply loading the pages in your browser. Viewport SVG graphics context (viewport) When SVG is embedded in HTML it creates a viewport. Ex (note the light grey style): <style> svg { border: solid 1px lightgray; background-color: hsla(240,100%,50%,0.2) } </style> <body> <h2>SVG viewport</h2> <svg width=\"600\" height=\"300\"></svg> </body> You can also create an SVG element using the DOM API, or D3, which is much simpler. The result is identical: <body> <script> d3.select(\"body\").append(\"svg\").attr(\"width\", 400).attr(\"height\", 300); </script> </body> Shapes Shapes are positioned in the viewport using x and y coordinates. They described by XML tags like <rect> , <circle> , <ellipse> , <path> , <polygon> and others. You create SVG graphics by placing these tags inside the <svg> element. Each supports attributes that configure their position in the viewport, and specific properties for each shape, such as radii, vertices or dimensions. A circle can be drawn in SVG using the <circle> element and at least the r attribute (radius). If you don't provide any other attributes, you will only see the lower-right quarter of the circle, since the default coordinates for its center will be (0,0). To illustrate an important js and raw html equivalency we will show code to draw 4 circles in the exact same way: HTML: <svg width=\"400\" height=\"300\"> <circle r=\"25\"></circle> <circle cx=\"250\" cy=\"200\" r=\"50\"></circle> <circle cx=\"50\" cy=\"50\" r=\"20\"></circle> <circle cx=\"400\" cy=\"300\" r=\"50\"></circle> </svg> JS: const svg = d3.select(\"body\") .append(\"svg\") .attr(\"width\", 400) .attr(\"height\", 300); svg.append(\"circle\").attr(\"r\", 25); svg.append(\"circle\").attr(\"cx\", 250).attr(\"cy\", 200).attr(\"r\", 50); svg.append(\"circle\").attr(\"cx\", 50).attr(\"cy\", 50).attr(\"r\", 20); svg.append(\"circle\").attr(\"cx\", 400).attr(\"cy\", 300).attr(\"r\", 50); Fills and Strokes Shapes have default black fill colors and transparent stroke borders, unless you assign different color strings to the SVG attributes (or CSS properties) fill and stroke. In the following SVG, three straight lines were drawn using the mandatory x1/y1 and x2/y2 attributes for elements. They would be invisible it the stroke attribute wasn't present. A stroke-width has a default value of 1. <svg width=\"400\" height=\"300\"> <line x2=\"400\" stroke=\"red\" stroke-width=\"5\"/> <line y2=\"150\" stroke=\"blue\" stroke-width=\"5\"/> <line x2=\"200\" y2=\"150\" stroke=\"black\" stroke-width=\"1\"/> </svg> You can also style with css: line { stroke-width: 20px; /* Overrides attr */ } Note lines at edge of screen might appear thinner as half the width could be off screen You can also apply CSS class and style attributes to different SVG objects. For example, consider the following CSS class declarations: .reds { fill: red; } .semitr { fill-opacity: 0.5; } Since the following rectangles each belong to one or more of these classes, they will inherit the style properties declared for each class: <svg width=\"600\" height=\"200\"> <rect x=\"50\" y=\"50\" width=\"90\" height=\"90\" class=\"semitr\"/> <rect x=\"200\" y=\"50\" width=\"175\" height=\"100\" rx=\"40\" ry=\"40\" class=\"reds semitr\"/> <rect x=\"450\" y=\"25\" width=\"100\" height=\"150\" class=\"reds\"/> </svg> One class applies the red fill, the other applies 50% transparency. The rectangle in the middle belongs to both classes, so it's both red and semi-transparent, as follows: Transparency Transparency When objects overlap in SVG, the code order determines which element will appear over the other. Preceding siblings are always overlapped by siblings that are declared after them. The CSS z-index property doesn\u2019t work in SVG. To move an object to the front, you have to modify the DOM tree. You can see through objects that overlap by changing their transparency. You can apply opacity levels to fills and strokes separately, using fill-opacity or stroke-opacity, or for the entire object using opacity. All attributes require a value between 0 (invisible) and 1 (opaque). An alternative, which achieves the same result, is to use the alpha component in rgba or hsla color strings (for example, \u2018rgba(255, 0, 0, 0.5)\u2019). The three squares below apply different transparency parameters on fills and strokes: <rect x=\"50\" y=\"50\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" stroke=\"red\" stroke-width=\"10\" fill-opacity=\"0\"/> <rect x=\"75\" y=\"75\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"gray\" stroke=\"black\" stroke-width=\"10\" fill-opacity=\".7\"/> <rect x=\"100\" y=\"100\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"yellow\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\".6\"/> In the following code, two thin vertical rectangles appear behind a wide horizontal rectangle, and two other vertical rectangles appear in front of it: <rect x=\"300\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"400\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> <rect x=\"250\" y=\"100\" height=\"50\" width=\"300\" fill=\"red\" stroke=\"orange\" stroke-width=\"10\" stroke-opacity=\".5\" /> <rect x=\"350\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"450\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> The following image shows the result: Drawing rectangles with different fills, strokes and transparency. Code: 7-rect-opacity.html. Ellipses Ellipses have a center in cx and cy like circles, but also have two radii, which are set with rx and ry. The following code creates three ellipses in different positions. <svg width=\"600\" height=\"300\"> <ellipse cx=\"150\" cy=\"100\" rx=\"150\" ry=\"100\" fill=\"yellow\" fill-opacity=\"0.5\" stroke=\"blue\" stroke-width=\"1\" stroke-dasharray=\"5 5\"/> <ellipse cx=\"400\" cy=\"150\" rx=\"75\" ry=\"125\" fill=\"red\" fill-opacity=\"0.2\" stroke=\"red\" stroke-width=\"5\" stroke-opacity=\"0.5\"/> <ellipse cx=\"400\" cy=\"250\" rx=\"150\" ry=\"40\" fill=\"black\" fill-opacity=\"0\" stroke=\"green\" stroke-width=\"20\" stroke-opacity=\"0.2\" /> </svg> The preceding code produces the following result: Ellipses created with SVG. Code: SVG/8-ellipse.html. Polygons and polylines Polygons ( <polygon> ) and polylines ( <polyline> ) render closed or open shapes using straight lines specified by a list of vertices passed in the points attribute. The only difference between them is that polygons close the shape. To close a polyline you need to repeat the initial coordinates. They also have a fill-rule attribute that controls the winding order, and determines if a hole will be drawn inside the shape when an outline crosses with itself. The following code creates two polygons. The second one has a fill-rule that will reveal a hole: <svg width=\"600\" height=\"300\"> <polygon points=\"150,150 50,150 100,20 150,50 200,200 50,200 20,154 48,82 32,20\" fill=\"blue\"/> <polygon points=\"450,150 350,150 400,20 450,50 500,200 350,200 320,154 348,82 332,20\" fill=\"red\" fill-rule=\"evenodd\"/> </svg> The result is shown as follows: Polygons created with SVG. The second polygon uses fill-rule=\u201deven-odd\u201d and reveals a hole. Code: SVG/10-polygon. Paths With paths you can draw open and closed shapes mixing lines, curves and arcs using a compact drawing language in the d attribute of the <path> element. It can be used to create arbitrary paths mixing straight lines and curves: <path d=\"M100,200 C200,50 300,100 300,200 L400,250 500,100\" fill=\"yellow\" stroke=\"red\" stroke-width=\"4\"/> Don't worry about all those numbers and letters in the d attribute. It's the most important part of the path, but it can always be generated for you. Most of the shape generators you will use in D3 to create lines, pie slices and other arbitrary shapes generate path strings that you can use in the d attribute. The simple <path> above renders the image below (the dots are added separately and show the control points): A curve described by a element. Code: SVG/11-path-line.html. Text Unlike HTML, you can't simply insert text inside any element. You have to create text objects using the <text> element with text contents. You can position text using x and y attributes, but you must remember that y is actually the baseline (default). If y is zero or not present, only the parts of the text that extend below the baseline will be visible inside the graphics context. This example places both text and a rectangle in the same position: <rect x=\"0\" y=\"0\" height=\"36\" width=\"200\"/> <text font-size=\"36\" x=\"0\" y=\"0\" fill=\"lightgray\">ghijklmnop</text> The following illustration shows the result, at left. Note that only the parts of the text that extend below the baseline actually appear over the rectangle. The other two examples show text with a different baseline alignment: alignment-baseline=\"middle\" and alignment-baseline=\"hanging\". Drawing text in SVG and the baseline. Code: SVG/14-text.html. You can also align text horizontally using the text-anchor attribute. The following illustration describes properties and values used to align text horizontally or vertically. If you intend to rotate text relative to its position, these parameters will affect the result. Properties for aligning text. Code: SVG/15-text-align.html. It's best to configure baselines and alignments in CSS instead of using XML attributes. If your text spans multiple lines, you can use the <tspan> element inside <text> to move words or letters to positions relative to the parent <text> element (see SVG/16-tspan.html). Group containers You can group several shapes in SVG with the <g> element. It's analogous to a <div> in HTML. This is an invisible element and it's positioned at the center of coordinates. CSS properties applied to a group affect all the objects it contains. You can also apply matrix transforms to groups to move, rotate and scale all its contents. A group container can also contain other group containers. In the following SVG, circles and ellipses are in a group, and rectangles are in another. The color of the elements in each group is declared in CSS, and they are translated, scaled and rotated together: <style> svg { border: solid 1px lightgray; } #bars { fill: red;} #round { opacity: .7; fill: blue; } </style> <body> <svg width=\"600\" height=\"300\"> <g id=\"bars\" transform=\"translate(0,100) rotate(-90, 100, 150)\"> <rect x=\"100\" y=\"150\" height=\"20\" width=\"150\"></rect> <rect x=\"100\" y=\"180\" height=\"20\" width=\"100\"></rect> <rect x=\"100\" y=\"210\" height=\"20\" width=\"200\"></rect> </g> <g id=\"round\" transform=\"translate(200,100) scale(.3) \"> <circle cx=\"280\" cy=\"220\" r=\"50\"></circle> <ellipse cx=\"150\" cy=\"90\" rx=\"80\" ry=\"50\"></ellipse> </g> </svg> </body> The following screenshot shows the groups before applying any transforms or styles to their groups (at left), and after applying the transforms and styles from the code above (right): Left pic is without the <g> tags or with no arguments to them if they are there. Applying styles and transforms to grouped shapes. Code: SVG/18-groups-transforms.html. Styles applied directly to individual elements in the groups have precedence and will override any styles declared at the group level. Positions of elements inside the group are always relative to the coordinate system introduced by any transforms applied to the group. Reusing objects: use and defs An SVG can have a <defs> header containing shapes, groups and other elements that will not be displayed. Filters, clipping masks, gradients and reusable shapes are usually declared in the <defs> header. You will need to assign an id to each element you wish to reuse later. The element can be displayed later declaring the <use> element outside the <defs> block. This element references an existing element by id using standard xlink notation. In the following example, two rectangles are created at 0.0. Since they are in the <defs> header, they will not be displayed. Each is previously configured with colors, dimensions and position. Outside the <defs> header, each element is displayed twice when referenced by each <use> element, translating each one to a different position: <svg width=\"600\" height=\"200\"> <defs> <rect id=\"black\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(64,32,32)\" /> <rect id=\"white\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(255,225,200)\" /> </defs> <g transform=\"translate(10,10) scale(3)\"> <use xlink:href=\"#black\" /> <use xlink:href=\"#white\" transform=\"translate(20)\"/> <use xlink:href=\"#white\" transform=\"translate(0,20)\"/> <use xlink:href=\"#black\" transform=\"translate(20,20)\"/> </g> </svg> The result is shown in the following image. You can use this to create a checkerboard. Reusing objects with <defs> and <use> . Code: SVG/22-defs-use.html. You will rarely use <defs> and <use> in D3, but placing reusable code such as clipping masks, filters and gradients in a <defs> header is good practice. Matrix transforms Matrix transforms are used to scale, translate, skew or rotate any shape or group, or the entire SVG viewport. Transform commands are functions used in the transform XML attribute, in text notation and separated by spaces, for example: <g transform=\"translate(100,100) scale(3) rotate(-90)\"> \u2026 </g> The order is significant. If you call scale(.5) and then rotate(90), the result will be different if you call them in the reverse order. In translate(), scale() and skew(), the first parameter is an x coordinate value, and the second, if present, the y coordinate. In rotate(), the first parameter is an angle in degrees, and the next two parameters, if present, are the coordinates of the center of rotation (if not present, the object will rotate around 0,0 and may disappear from the viewport if the angle is big enough). Flipping an object can be achieved by scaling with negative values for x and/or y. The translate() transform can be used to move groups to different positions. In this case, the x and y coordinates of each object should be considered relative to the group. When creating objects that will be treated as a group, you might also choose to position all objects at the origin (not declaring any x or y coordinates, or use only values relative to the group) to later control the position using translate() You can also apply transforms as CSS styles, but you will need to use explicit units for degrees and distances, for example: <g style=\"transform: translate(100px,100px) scale(3) rotate(-90deg)\"> \u2026 </g> Consider the following SVG drawing of a pair of SVG coordinate axes: <g id=\"coords\"> <line x1=\"10\" y1=\"10\" x2=\"200\" y2=\"10\" /> <line x1=\"10\" y1=\"10\" x2=\"10\" y2=\"150\" /> <text x=\"200\" y=\"20\">x</text> <text x=\"20\" y=\"150\">y</text> </g> The following images show the results of applying translate, rotate/scale and skew to an image of the SVG coordinate axes, compared to the original object(in black): Applying transforms to shapes and groups. Code: SVG/19-translate.html, 20-scale-rotate.html, 21-skew.html. Configuring the SVG viewport You can configure the SVG viewport by changing the values in the viewBox attribute (which can also be used in some reusable SVG elements). The viewBox attribute contains four numbers separated by spaces. The first two are the center of coordinates, which default to 0 0, and the last two are the width and height, which default to the declared or default height and width of the SVG. If present, this attribute can move the viewport to a different position and its scale. For example, if you have an SVG with dimensions 400x300, the default viewport will be 0 0 400 300. If you declare a viewBox of 0 0 800 600, all the objects inside the SVG will be displayed at half the size, since a declared value of 100 is no longer 1/4 of the viewport's width, but 1/8. You can also change the origin of coordinates. For example, you can move it to the center of the SVG and position elements with negative coordinates if you have a viewBox of -200 -150 400 300 (remember that the coordinates start at the top-left corner). Consider the following SVG: <svg width=\"400\" height=\"300\" viewBox=\"...\"> <line x1=\"0\" y1=\"-300\" x2=\"0\" y2=\"300\"/> <line y1=\"0\" x1=\"-400\" y2=\"0\" x2=\"400\"/> <rect x=\"-130\" y=\"-130\" height=\"20\" width=\"200\" fill=\"red\"/> <rect x=\"130\" y=\"-130\" height=\"200\" width=\"20\" fill=\"blue\"/> <rect x=\"0\" y=\"100\" height=\"20\" width=\"200\" fill=\"green\"/> <rect x=\"-130\" y=\"-65\" height=\"200\" width=\"20\" fill=\"orange\"/> </svg> The images below show what would appear on the screen, depending on the values you include for the viewBox attribute. The first one is the default. Scaling and translating the viewport with the viewBox attribute. Code: SVG/26-viewBox-default.html, 28-viewBox-center.html, 28-viewBox-scaled.html You may rarely use viewBox with D3 since you can achieve the same results using matrix transforms, which are simpler. Gradients Linear gradients are created perpendicular to a line, so it requires the same attributes as the <line> element. Radial gradients use circle attributes. Gradients should declare at least one child <stop> with a non-zero offset and stop-color different than black, since the default color is black and the default offset is zero. Typically, gradients declare two or more stop colors. Gradients are used used as a fill or stroke value. They are usually defined in <defs> with an id that can be referenced later using url(#id). The following SVG code declares two gradients and applies one of them to a square, and the other to a circle: <svg width=\"600\" height=\"300\"> <defs> <linearGradient x2=\"1\" id=\"rainbow\"> <stop offset=\"0\" stop-color=\"rgb(255,0,0)\" /> <stop offset=\"0.25\" stop-color=\"rgb(255,255,64)\" /> <stop offset=\"0.5\" stop-color=\"rgb(64,255,64)\" /> <stop offset=\"0.75\" stop-color=\"rgb(64,64,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </linearGradient> <radialGradient cx=\"0.35\" cy=\"0.35\" r=\"1\" id=\"glow\"> <stop offset=\"0\" stop-color=\"rgb(255,255,255)\" /> <stop offset=\"0.5\" stop-color=\"rgb(0,128,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </radialGradient> </defs> <rect x=\"0\" y=\"0\" width=\"200\" height=\"200\" fill=\"url(#rainbow)\" transform=\"translate(50,50)\"/> <circle cx=\"100\" cy=\"100\" r=\"100\" fill=\"url(#glow)\" transform=\"translate(300,50)\"/> </svg> The result is shown in the following screenshot: Linear and radial gradients. Code: SVG/23-gradient.html. Clipping A clipping mask is created with any shape declared inside the <clipPath> element. This is usually done in <defs> , setting an id for the clipping mask that can be referenced in the object that should be clipped, using url(#id) inside a clip-path attribute, as shown below: <svg width=\"425\" height=\"425\"> <defs> <clipPath id=\"poly\"> <circle r=\"139\" cx=\"200\" cy=\"199\"/> </clipPath> </defs> <!-- Clipped image --> <image x=\"25\" y=\"25\" height=\"350\" width=\"350\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#poly)\"/> </svg> The following SVG screenshots show the circular clipping mask above applied to an image. The SVG at left shows the original image and the clipped image is shown at right. Clipping an image in SVG. Code: SVG/24-clipping.html. Filters SVG filters can be applied to any shapes, text or images. They are usually created in the <defs> header with an id that can be referenced via url(#id) using the filter attribute. The <filter> element can contain several different filter types and you can also create composite filters. The following example creates two different configurations for the <feGaussianBlur> filter, applied to a text element and a circle: <svg width=\"600\" height=\"300\"> <defs> <filter id=\"filter1\"> <feGaussianBlur stdDeviation=\"7\" /> </filter> <filter id=\"filter2\" x=\"-100\" y=\"-100\" height=\"200\" width=\"200\"> <feGaussianBlur stdDeviation=\"0,5\" in=\"SourceGraphic\" /> </filter> </defs> <text id=\"text\" font-size=\"40\" fill=\"black\" x=\"50\" y=\"60\" filter=\"url(#filter2)\">Do you need glasses?</text> <g id=\"stardot\" transform=\"translate(100,25) scale(0.5)\"> <polygon id=\"star\" points=\"250,0 400,500 0,200 500,200 100,500\" fill=\"red\" fill-rule=\"evenodd\"/> <circle id=\"circ\" cx=\"250\" cy=\"283\" r=\"75\" fill=\"blue\" filter=\"url(#filter1)\" /> </g> </svg> The result is shown as follows: A Gaussian blur filter applied to different shapes. Code: SVG/25-filter.html. An SVG example The following code uses several SVG elements described in this section to draw some shapes, shadows, gradients and text: <svg width=\"300\" height=\"300\"> <defs> <filter id=\"shadow\"> <feDropShadow style=\"flood-color: green\" dx=\"5\" dy=\"5\" stdDeviation=\"3\"/> </filter> <linearGradient id=\"grad\" x1=\"0\" y1=\"0\" x2=\"100%\" y2=\"0\"> <stop offset=\"0%\" stop-color=\"magenta\"/> <stop offset=\"100%\" stop-color=\"yellow\"/> </linearGradient> <clipPath id=\"circle\"> <circle r=\"40\" cx=\"175\" cy=\"75\"/> </clipPath> </defs> <!-- rectangle --> <rect x=\"50\" y=\"50\" height=\"50\" width=\"50\" fill=\"red\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\"0.5\"/> <!-- dashed shape --> <path id=\"path1\" d=\"M150,200 L150,150 L100,150 C100,200 150,250 200,250 L200,200 Z\" stroke-dasharray=\"5 2 1 2\" stroke-width=\"2\" stroke=\"blue\" fill=\"none\" style=\"filter:url(#shadow)\"/> <!-- gray quarter-circle --> <path d=\"M0,0 L0,-100 A100,100 0 0,0 -100,0 L0,0 Z\" transform=\"translate(100,250) scale(0.5) \" stroke=\"red\" stroke-opacity=\".5\" stroke-width=\"4\" fill-opacity=\".2\"/> <text fill=\"url(#grad)\" font-size=\"20\" x=\"200\" y=\"100\"> Scalable <tspan dy=\"20\" x=\"200\">Vector</tspan> <tspan dy=\"20\" x=\"200\">Graphics</tspan> </text> <image x=\"125\" y=\"25\" height=\"100\" width=\"100\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#circle)\" opacity=\"0.75\"/> <!-- raindow half-circle --> <path d=\"M100,200 C100,100 250,100 250,200\" transform=\"scale(0.6) rotate(180,295,225) \" fill=\"url(#grad)\"/> </svg> Compare this code and the following image it generates it with an identical image created using D3 (SVG-with-D3/29-example.html) and HTML Canvas (Canvas/1-canvas-svg-compare.html): An image created using SVG. Code: SVG/29-example.html. 2) Essential JavaScript data structures I'm actually going to organize this somewhere else to help with my code journal. Link to the Basics 3) HTML Canvas HTML5 Canvas Most of your D3 applications will render graphics using SVG, but several shape generators in SVG can also generate Canvas, and you may choose to use Canvas in all or part of your application to improve performance if you have memory problems due to excessive objects created in the DOM. To draw using Canvas you need to create a <canvas> element in your page. You can do that using plain HTML: <body> <canvas id=\"canvas\" width=\"400\" height=\"300\"></canvas> </body> Or using D3: d3.select(\"body\").append(\"canvas\").attr(\"width\", 400).attr(\"height\", 300); If you declare the Canvas element in HTML, you can reference it by its ID using the DOM or D3: const canvas = d3.select(\"#canvas\").node(); // node() returns the DOM object Once you have a canvas object, you obtain a 2D graphics context and start drawing: const ctx = canvas.getContext(\"2d\"); Practically all the Canvas API consists of methods and properties called from the graphics context. Before drawing, you set properties such as font , fillStyle , strokeStyle : ctx.fillStyle = \"red\"; ctx.strokeStyle = \"rgba(255,127,0,0.7)\"; ctx.lineWidth = 10; And then fill or stroke rectangles and arbitrary paths containing lines and curves. The following commands will draw a red 50x50 pixel square with a 10 pixel wide yellow semi-transparent border at position 50,50: ctx.fillRect(50,50,50,50); ctx.strokeRect(50,50,50,50); You can also draw other shapes, text and images on the same canvas. The context properties will not change unless they are redefined or a previously saved context is restored. It's a good practice to save the context to the stack before applying properties or transforms, and restore it when you are done drawing an object. This allows you to always start with a clean context: Note: So far the below code does nothing that I can see with my eyes. ctx.save(); ctx.transform(50,60); ctx.scale(2); // \u2026 ctx.restore(); // starting with a new context Property or method Description fillStyle Sets the color to be used in fill() commands. strokeStyle Sets the color to be used in stroke() commands. lineWidth Sets the line width to be used in stroke() commands. lineCap Sets the style of the line caps: can be butt (default), round or square. textAlign Sets the alignment for text: can be start (default), center, end, left or right. textBaseline Sets the baseline for text: can be middle, hanging, top, bottom, ideographic or alphabetic (default). font Sets the font to be used in text commands, using the compact CSS font syntax. globalAlpha Sets the global opacity (0 = transparent, 1 = opaque) for the context. shadowBlur, shadowColor, shadowOffsetX, shadowOffsetY Sets shadow properties. Default shadow color is transparent black. Default numeric values are zero. setLineDash(dasharray) Sets the dash array (alternating line and space lengths) for strokes. translate(x,y) Sets the current translate transform for the context. scale(x,y) Sets the current scale transform for the context. rotate(angle) Sets the current rotate transform for the context. save() Saves the state of the current context (pushes into a stack). restore() Restores the state of the last context that was saved (pops it from the stack). The fillRect() command is typically used to clear the entire canvas before redrawing, but you can also use it to draw arbitrary rectangles. The following table lists methods you can use to draw rectangles, draw text and images: Method Description fillRect(x,y,w,h); Fills a rectangle. Typically used to clear the Canvas on redrawing. strokeRect(x,y,w,h) Draws a border around a rectangle. fillText(text,x,y); Fills text at position x, y (depends on current textAlign and textBaseline). strokeText(text, x, y); Draws a border around text. drawImage(image, x, y, w, h) Draws an image at x,y with width w and height h. Canvas context methods used to draw rectangles, text and images A path is a series of commands to move to points, draw lines, curves or arcs. To draw a path you need to first call ctx.beginPath() , then call a sequence of commands that move to points, draw lines and curves, and when you are done you can close the path (if it's a closed path) and call fill() and/or stroke() to draw it using the current styles. The following table lists several commands you can use in a path: Method Description beginPath() Starts a path. closePath() Closes a path. moveTo(x,y) Moves the cursor to a position in the path. lineTo(x,y) Moves the cursor to a position in the path, drawing a line along the way. bezierCurveTo(c1x,c1y,c2x,c2y,x,y) quadraticCurveTo(cx,cy,x,y) Draws curves with one (quadratic) or two (Bezier) control points in a path. arc(x,y,r,sa,ea) Draws an arc by specifying the center, radius, start and end angles in a path. arcTo(sx,sy,r,ex,ey) Draws an arc by specifying the coordinates of the starting point, the radius and the coordinates of the ending point. rect(x,y,w,h) Draws a rectangle in a path with coordinates of top-left corner, width and height. clip() Creates a clipping region with the shapes drawn by the path that will affect objects that are drawn afterwards. fill() Fills a path with the current color. Call this to fill the path when done. stroke() Strokes the path with the current color. Call this to stroke the path when done. A Canvas Example The following code uses several of the methods above to draw different shapes on the same Canvas context. It draws some shapes, text, images and paths, and applies transforms, shadows, clipping and gradients. Compare it to the example shown before in SVG that draws the image (see Canvas/1-canvas-svg-compare.html ): const canvas = document.getElementById(\"canvas\"); const ctx = canvas.getContext(\"2d\"); // rectangle ctx.save(); // save default context ctx.fillStyle = \"#ff0000\"; ctx.strokeStyle = \"blue\"; ctx.lineWidth = 10; ctx.fillRect(50,50,50,50); ctx.globalAlpha = 0.5; ctx.strokeRect(50,50,50,50); // dashed shape ctx.restore(); ctx.save(); ctx.strokeStyle = \"blue\"; ctx.lineWidth = 2; ctx.shadowBlur = 6; ctx.shadowColor = \"green\"; ctx.shadowOffsetX = ctx.shadowOffsetY = 5; ctx.setLineDash([5,2,1,2]); ctx.beginPath(); ctx.moveTo(150,200); ctx.lineTo(150,150); ctx.lineTo(100,150); ctx.bezierCurveTo(100,200,150,250,200,250); ctx.lineTo(200,200); ctx.closePath(); ctx.stroke(); ctx.restore(); ctx.save(); // quarter-circle ctx.translate(100,250); ctx.scale(0.5, 0.5); ctx.strokeStyle = \"red\"; ctx.lineWidth = 4; ctx.globalAlpha = 0.5; ctx.beginPath(); ctx.moveTo(0,0); ctx.lineTo(0,-100); ctx.arcTo(-100,-100,-100,0,100); ctx.lineTo(0,0); ctx.stroke(); ctx.globalAlpha = 0.2; ctx.beginPath(); ctx.arc(0,0,100,3.14,-1.57,false); ctx.lineTo(0,0); ctx.closePath(); ctx.fill(); ctx.restore(); ctx.save(); // text and half-circle const text = \"Canvas\" ctx.translate(250,150); ctx.font = \"24px monospace\"; const textWidth = ctx.measureText(text).width; const gradient = ctx.createLinearGradient(-50,-50,-50 + textWidth,-50); gradient.addColorStop(0,\"magenta\"); gradient.addColorStop(1, \"yellow\"); ctx.fillStyle = gradient; ctx.shadowColor = \"transparent\"; ctx.fillText(text, -45, -5); ctx.scale(1.1, 1.1) ctx.rotate(3.14); ctx.beginPath(); ctx.arc(0,0,40,3.14,0,false); ctx.fill(); ctx.restore(); ctx.save(); // image and clip ctx.beginPath(); ctx.arc(175,75,40,0,6.28,false); ctx.clip(); const image = new Image(100,100); image.onload = function() { ctx.globalAlpha = 0.75; ctx.drawImage(this, 125, 25, this.width, this.height); } image.src = \"reuse.png\"; ctx.save(); I don't have the pluto pic so the code won't show the planet. Data Formats Data used in visualizations are usually distributed in a standard format that can be shared. Even when the data is served from a database, the data is usually delivered in some standard format. Popular proprietary formats such as Excel spreadsheets are common, but most statistical data is stored or delivered in CSV, XML or JSON formats. CSV You can load and parse CSV in D3 using the d3.csv() function. const csv = d3.csv(\"Data/continents.csv\") csv output: Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: \"579024000\", areakm2: \"24490000\"} 1: {continent: \"Asia\", population: \"4436224000\", areakm2: \"43820000\"} 2: {continent: \"Europe\", population: \"738849000\", areakm2: \"10180000\"} 3: {continent: \"Africa\", population: \"1216130000\", areakm2: \"30370000\"} 4: {continent: \"South America\", population: \"422535000\", areakm2: \"17840000\"} 5: {continent: \"Oceania\", population: \"39901000\", areakm2: \"9008500\"} 6: {continent: \"Antarctica\", population: \"1106\", areakm2: \"13720000\"} columns: (3) [\"continent\", \"population\", \"areakm2\"] length: 7 __proto__: Array(0) XML XML \u2013 eXtensible Markup Language is a very popular data format. Ajax responses from web services are usually returned as text or XML. It has standard native support in JavaScript via the DOM (document object model) APIs and doesn't require additional parsing. Although it is still common to find data in XML format, CSV and JSON alternatives, if available, are usually smaller and easier to work with. <continents> <continent> <name>North America</name> <population>579024000</population> <area unit=\"km\">24490000</area> </continent> <continent> <name>Asia</name> <population>4436224000</population> <area unit=\"km\">43820000</area> </continent> <continent> <name>Antarctica</name> <population>1106</population> <area>13720000</area> </continent> </continents> const xml = d3.xml(\"Data/continents.xml\") Not sure how to parse but who uses XML anymore gosh! JSON JSON stands for JavaScript Object Notation. It looks a lot like a JavaScript Object, but it has stricter formation rules. It's probably the easiest format to work with. It's compact and easy to parse, and it's gradually replacing XML as a preferred data format in Web Services. The data file containing continent data is shown below in JSON format ( Data/continents.json ). [ { \"continent\": \"North America\", \"population\": 579024000, \"areakm2\": 24490000 },{ \"continent\": \"Asia\", \"population\": 4436224000, \"areakm2\": 43820000 },{ \"continent\": \"Europe\", \"population\": 738849000, \"areakm2\": 10180000 },{ \"continent\": \"Africa\", \"population\": 1216130000, \"areakm2\": 30370000 },{ \"continent\": \"South America\", \"population\": 422535000, \"areakm2\": 17840000 },{ \"continent\": \"Oceania\", \"population\": 39901000, \"areakm2\": 9008500 },{ \"continent\": \"Antarctica\", \"population\": 1106, \"areakm2\": 13720000 } ] JSON is the preferred format for data manipulation in JavaScript. There are many online tools you can use to transform CSV and XML files into JSON. You can load and parse JSON in D3 using the d3.json() function. Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: 579024000, areakm2: 24490000} 1: {continent: \"Asia\", population: 4436224000, areakm2: 43820000} 2: {continent: \"Europe\", population: 738849000, areakm2: 10180000} 3: {continent: \"Africa\", population: 1216130000, areakm2: 30370000} 4: {continent: \"South America\", population: 422535000, areakm2: 17840000} 5: {continent: \"Oceania\", population: 39901000, areakm2: 9008500} 6: {continent: \"Antarctica\", population: 1106, areakm2: 13720000} length: 7 __proto__: Array(0) Note that both csv and json loaded an easy to work with object. Just try to xml code....It'll make you sick.","title":"Chapter 2"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#technical-fundamentals","text":"This chapter covers fundamental standard web technologies used by D3: SVG, JavaScript (ES 2015), HTML Canvas and standard data formats such as JSON and CSV. It is intended as a general reference to these topics. Most data visualizations created with D3.js generate SVG graphics. Good knowledge of SVG is important to make the most of D3, but you only really need to know the basics.","title":"Technical Fundamentals"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#1-svg","text":"SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. Scalable Vector Graphics (SVG) SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. To control SVG elements with D3 you should understand basic SVG syntax and rules, how a document is structured, how each element is rendered, the effects caused by attributes and styles, as well as nesting and transformation rules. All the code used in this section is available in the SVG/ folder, from the GitHub repository for this chapter. You can see the results simply loading the pages in your browser.","title":"1) SVG"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#viewport","text":"SVG graphics context (viewport) When SVG is embedded in HTML it creates a viewport. Ex (note the light grey style): <style> svg { border: solid 1px lightgray; background-color: hsla(240,100%,50%,0.2) } </style> <body> <h2>SVG viewport</h2> <svg width=\"600\" height=\"300\"></svg> </body> You can also create an SVG element using the DOM API, or D3, which is much simpler. The result is identical: <body> <script> d3.select(\"body\").append(\"svg\").attr(\"width\", 400).attr(\"height\", 300); </script> </body>","title":"Viewport"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#shapes","text":"Shapes are positioned in the viewport using x and y coordinates. They described by XML tags like <rect> , <circle> , <ellipse> , <path> , <polygon> and others. You create SVG graphics by placing these tags inside the <svg> element. Each supports attributes that configure their position in the viewport, and specific properties for each shape, such as radii, vertices or dimensions. A circle can be drawn in SVG using the <circle> element and at least the r attribute (radius). If you don't provide any other attributes, you will only see the lower-right quarter of the circle, since the default coordinates for its center will be (0,0). To illustrate an important js and raw html equivalency we will show code to draw 4 circles in the exact same way: HTML: <svg width=\"400\" height=\"300\"> <circle r=\"25\"></circle> <circle cx=\"250\" cy=\"200\" r=\"50\"></circle> <circle cx=\"50\" cy=\"50\" r=\"20\"></circle> <circle cx=\"400\" cy=\"300\" r=\"50\"></circle> </svg> JS: const svg = d3.select(\"body\") .append(\"svg\") .attr(\"width\", 400) .attr(\"height\", 300); svg.append(\"circle\").attr(\"r\", 25); svg.append(\"circle\").attr(\"cx\", 250).attr(\"cy\", 200).attr(\"r\", 50); svg.append(\"circle\").attr(\"cx\", 50).attr(\"cy\", 50).attr(\"r\", 20); svg.append(\"circle\").attr(\"cx\", 400).attr(\"cy\", 300).attr(\"r\", 50);","title":"Shapes"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#fills-and-strokes","text":"Shapes have default black fill colors and transparent stroke borders, unless you assign different color strings to the SVG attributes (or CSS properties) fill and stroke. In the following SVG, three straight lines were drawn using the mandatory x1/y1 and x2/y2 attributes for elements. They would be invisible it the stroke attribute wasn't present. A stroke-width has a default value of 1. <svg width=\"400\" height=\"300\"> <line x2=\"400\" stroke=\"red\" stroke-width=\"5\"/> <line y2=\"150\" stroke=\"blue\" stroke-width=\"5\"/> <line x2=\"200\" y2=\"150\" stroke=\"black\" stroke-width=\"1\"/> </svg> You can also style with css: line { stroke-width: 20px; /* Overrides attr */ } Note lines at edge of screen might appear thinner as half the width could be off screen You can also apply CSS class and style attributes to different SVG objects. For example, consider the following CSS class declarations: .reds { fill: red; } .semitr { fill-opacity: 0.5; } Since the following rectangles each belong to one or more of these classes, they will inherit the style properties declared for each class: <svg width=\"600\" height=\"200\"> <rect x=\"50\" y=\"50\" width=\"90\" height=\"90\" class=\"semitr\"/> <rect x=\"200\" y=\"50\" width=\"175\" height=\"100\" rx=\"40\" ry=\"40\" class=\"reds semitr\"/> <rect x=\"450\" y=\"25\" width=\"100\" height=\"150\" class=\"reds\"/> </svg> One class applies the red fill, the other applies 50% transparency. The rectangle in the middle belongs to both classes, so it's both red and semi-transparent, as follows:","title":"Fills and Strokes"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#transparency","text":"Transparency When objects overlap in SVG, the code order determines which element will appear over the other. Preceding siblings are always overlapped by siblings that are declared after them. The CSS z-index property doesn\u2019t work in SVG. To move an object to the front, you have to modify the DOM tree. You can see through objects that overlap by changing their transparency. You can apply opacity levels to fills and strokes separately, using fill-opacity or stroke-opacity, or for the entire object using opacity. All attributes require a value between 0 (invisible) and 1 (opaque). An alternative, which achieves the same result, is to use the alpha component in rgba or hsla color strings (for example, \u2018rgba(255, 0, 0, 0.5)\u2019). The three squares below apply different transparency parameters on fills and strokes: <rect x=\"50\" y=\"50\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" stroke=\"red\" stroke-width=\"10\" fill-opacity=\"0\"/> <rect x=\"75\" y=\"75\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"gray\" stroke=\"black\" stroke-width=\"10\" fill-opacity=\".7\"/> <rect x=\"100\" y=\"100\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"yellow\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\".6\"/> In the following code, two thin vertical rectangles appear behind a wide horizontal rectangle, and two other vertical rectangles appear in front of it: <rect x=\"300\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"400\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> <rect x=\"250\" y=\"100\" height=\"50\" width=\"300\" fill=\"red\" stroke=\"orange\" stroke-width=\"10\" stroke-opacity=\".5\" /> <rect x=\"350\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"450\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> The following image shows the result: Drawing rectangles with different fills, strokes and transparency. Code: 7-rect-opacity.html.","title":"Transparency"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#ellipses","text":"Ellipses have a center in cx and cy like circles, but also have two radii, which are set with rx and ry. The following code creates three ellipses in different positions. <svg width=\"600\" height=\"300\"> <ellipse cx=\"150\" cy=\"100\" rx=\"150\" ry=\"100\" fill=\"yellow\" fill-opacity=\"0.5\" stroke=\"blue\" stroke-width=\"1\" stroke-dasharray=\"5 5\"/> <ellipse cx=\"400\" cy=\"150\" rx=\"75\" ry=\"125\" fill=\"red\" fill-opacity=\"0.2\" stroke=\"red\" stroke-width=\"5\" stroke-opacity=\"0.5\"/> <ellipse cx=\"400\" cy=\"250\" rx=\"150\" ry=\"40\" fill=\"black\" fill-opacity=\"0\" stroke=\"green\" stroke-width=\"20\" stroke-opacity=\"0.2\" /> </svg> The preceding code produces the following result: Ellipses created with SVG. Code: SVG/8-ellipse.html.","title":"Ellipses"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#polygons-and-polylines","text":"Polygons ( <polygon> ) and polylines ( <polyline> ) render closed or open shapes using straight lines specified by a list of vertices passed in the points attribute. The only difference between them is that polygons close the shape. To close a polyline you need to repeat the initial coordinates. They also have a fill-rule attribute that controls the winding order, and determines if a hole will be drawn inside the shape when an outline crosses with itself. The following code creates two polygons. The second one has a fill-rule that will reveal a hole: <svg width=\"600\" height=\"300\"> <polygon points=\"150,150 50,150 100,20 150,50 200,200 50,200 20,154 48,82 32,20\" fill=\"blue\"/> <polygon points=\"450,150 350,150 400,20 450,50 500,200 350,200 320,154 348,82 332,20\" fill=\"red\" fill-rule=\"evenodd\"/> </svg> The result is shown as follows: Polygons created with SVG. The second polygon uses fill-rule=\u201deven-odd\u201d and reveals a hole. Code: SVG/10-polygon.","title":"Polygons and polylines"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#paths","text":"With paths you can draw open and closed shapes mixing lines, curves and arcs using a compact drawing language in the d attribute of the <path> element. It can be used to create arbitrary paths mixing straight lines and curves: <path d=\"M100,200 C200,50 300,100 300,200 L400,250 500,100\" fill=\"yellow\" stroke=\"red\" stroke-width=\"4\"/> Don't worry about all those numbers and letters in the d attribute. It's the most important part of the path, but it can always be generated for you. Most of the shape generators you will use in D3 to create lines, pie slices and other arbitrary shapes generate path strings that you can use in the d attribute. The simple <path> above renders the image below (the dots are added separately and show the control points): A curve described by a element. Code: SVG/11-path-line.html.","title":"Paths"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#text","text":"Unlike HTML, you can't simply insert text inside any element. You have to create text objects using the <text> element with text contents. You can position text using x and y attributes, but you must remember that y is actually the baseline (default). If y is zero or not present, only the parts of the text that extend below the baseline will be visible inside the graphics context. This example places both text and a rectangle in the same position: <rect x=\"0\" y=\"0\" height=\"36\" width=\"200\"/> <text font-size=\"36\" x=\"0\" y=\"0\" fill=\"lightgray\">ghijklmnop</text> The following illustration shows the result, at left. Note that only the parts of the text that extend below the baseline actually appear over the rectangle. The other two examples show text with a different baseline alignment: alignment-baseline=\"middle\" and alignment-baseline=\"hanging\". Drawing text in SVG and the baseline. Code: SVG/14-text.html. You can also align text horizontally using the text-anchor attribute. The following illustration describes properties and values used to align text horizontally or vertically. If you intend to rotate text relative to its position, these parameters will affect the result. Properties for aligning text. Code: SVG/15-text-align.html. It's best to configure baselines and alignments in CSS instead of using XML attributes. If your text spans multiple lines, you can use the <tspan> element inside <text> to move words or letters to positions relative to the parent <text> element (see SVG/16-tspan.html).","title":"Text"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#group-containers","text":"You can group several shapes in SVG with the <g> element. It's analogous to a <div> in HTML. This is an invisible element and it's positioned at the center of coordinates. CSS properties applied to a group affect all the objects it contains. You can also apply matrix transforms to groups to move, rotate and scale all its contents. A group container can also contain other group containers. In the following SVG, circles and ellipses are in a group, and rectangles are in another. The color of the elements in each group is declared in CSS, and they are translated, scaled and rotated together: <style> svg { border: solid 1px lightgray; } #bars { fill: red;} #round { opacity: .7; fill: blue; } </style> <body> <svg width=\"600\" height=\"300\"> <g id=\"bars\" transform=\"translate(0,100) rotate(-90, 100, 150)\"> <rect x=\"100\" y=\"150\" height=\"20\" width=\"150\"></rect> <rect x=\"100\" y=\"180\" height=\"20\" width=\"100\"></rect> <rect x=\"100\" y=\"210\" height=\"20\" width=\"200\"></rect> </g> <g id=\"round\" transform=\"translate(200,100) scale(.3) \"> <circle cx=\"280\" cy=\"220\" r=\"50\"></circle> <ellipse cx=\"150\" cy=\"90\" rx=\"80\" ry=\"50\"></ellipse> </g> </svg> </body> The following screenshot shows the groups before applying any transforms or styles to their groups (at left), and after applying the transforms and styles from the code above (right): Left pic is without the <g> tags or with no arguments to them if they are there. Applying styles and transforms to grouped shapes. Code: SVG/18-groups-transforms.html. Styles applied directly to individual elements in the groups have precedence and will override any styles declared at the group level. Positions of elements inside the group are always relative to the coordinate system introduced by any transforms applied to the group.","title":"Group containers"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#reusing-objects-use-and-defs","text":"An SVG can have a <defs> header containing shapes, groups and other elements that will not be displayed. Filters, clipping masks, gradients and reusable shapes are usually declared in the <defs> header. You will need to assign an id to each element you wish to reuse later. The element can be displayed later declaring the <use> element outside the <defs> block. This element references an existing element by id using standard xlink notation. In the following example, two rectangles are created at 0.0. Since they are in the <defs> header, they will not be displayed. Each is previously configured with colors, dimensions and position. Outside the <defs> header, each element is displayed twice when referenced by each <use> element, translating each one to a different position: <svg width=\"600\" height=\"200\"> <defs> <rect id=\"black\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(64,32,32)\" /> <rect id=\"white\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(255,225,200)\" /> </defs> <g transform=\"translate(10,10) scale(3)\"> <use xlink:href=\"#black\" /> <use xlink:href=\"#white\" transform=\"translate(20)\"/> <use xlink:href=\"#white\" transform=\"translate(0,20)\"/> <use xlink:href=\"#black\" transform=\"translate(20,20)\"/> </g> </svg> The result is shown in the following image. You can use this to create a checkerboard. Reusing objects with <defs> and <use> . Code: SVG/22-defs-use.html. You will rarely use <defs> and <use> in D3, but placing reusable code such as clipping masks, filters and gradients in a <defs> header is good practice. Matrix transforms Matrix transforms are used to scale, translate, skew or rotate any shape or group, or the entire SVG viewport. Transform commands are functions used in the transform XML attribute, in text notation and separated by spaces, for example: <g transform=\"translate(100,100) scale(3) rotate(-90)\"> \u2026 </g> The order is significant. If you call scale(.5) and then rotate(90), the result will be different if you call them in the reverse order. In translate(), scale() and skew(), the first parameter is an x coordinate value, and the second, if present, the y coordinate. In rotate(), the first parameter is an angle in degrees, and the next two parameters, if present, are the coordinates of the center of rotation (if not present, the object will rotate around 0,0 and may disappear from the viewport if the angle is big enough). Flipping an object can be achieved by scaling with negative values for x and/or y. The translate() transform can be used to move groups to different positions. In this case, the x and y coordinates of each object should be considered relative to the group. When creating objects that will be treated as a group, you might also choose to position all objects at the origin (not declaring any x or y coordinates, or use only values relative to the group) to later control the position using translate() You can also apply transforms as CSS styles, but you will need to use explicit units for degrees and distances, for example: <g style=\"transform: translate(100px,100px) scale(3) rotate(-90deg)\"> \u2026 </g> Consider the following SVG drawing of a pair of SVG coordinate axes: <g id=\"coords\"> <line x1=\"10\" y1=\"10\" x2=\"200\" y2=\"10\" /> <line x1=\"10\" y1=\"10\" x2=\"10\" y2=\"150\" /> <text x=\"200\" y=\"20\">x</text> <text x=\"20\" y=\"150\">y</text> </g> The following images show the results of applying translate, rotate/scale and skew to an image of the SVG coordinate axes, compared to the original object(in black): Applying transforms to shapes and groups. Code: SVG/19-translate.html, 20-scale-rotate.html, 21-skew.html.","title":"Reusing objects: use and defs"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#configuring-the-svg-viewport","text":"You can configure the SVG viewport by changing the values in the viewBox attribute (which can also be used in some reusable SVG elements). The viewBox attribute contains four numbers separated by spaces. The first two are the center of coordinates, which default to 0 0, and the last two are the width and height, which default to the declared or default height and width of the SVG. If present, this attribute can move the viewport to a different position and its scale. For example, if you have an SVG with dimensions 400x300, the default viewport will be 0 0 400 300. If you declare a viewBox of 0 0 800 600, all the objects inside the SVG will be displayed at half the size, since a declared value of 100 is no longer 1/4 of the viewport's width, but 1/8. You can also change the origin of coordinates. For example, you can move it to the center of the SVG and position elements with negative coordinates if you have a viewBox of -200 -150 400 300 (remember that the coordinates start at the top-left corner). Consider the following SVG: <svg width=\"400\" height=\"300\" viewBox=\"...\"> <line x1=\"0\" y1=\"-300\" x2=\"0\" y2=\"300\"/> <line y1=\"0\" x1=\"-400\" y2=\"0\" x2=\"400\"/> <rect x=\"-130\" y=\"-130\" height=\"20\" width=\"200\" fill=\"red\"/> <rect x=\"130\" y=\"-130\" height=\"200\" width=\"20\" fill=\"blue\"/> <rect x=\"0\" y=\"100\" height=\"20\" width=\"200\" fill=\"green\"/> <rect x=\"-130\" y=\"-65\" height=\"200\" width=\"20\" fill=\"orange\"/> </svg> The images below show what would appear on the screen, depending on the values you include for the viewBox attribute. The first one is the default. Scaling and translating the viewport with the viewBox attribute. Code: SVG/26-viewBox-default.html, 28-viewBox-center.html, 28-viewBox-scaled.html You may rarely use viewBox with D3 since you can achieve the same results using matrix transforms, which are simpler.","title":"Configuring the SVG viewport"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#gradients","text":"Linear gradients are created perpendicular to a line, so it requires the same attributes as the <line> element. Radial gradients use circle attributes. Gradients should declare at least one child <stop> with a non-zero offset and stop-color different than black, since the default color is black and the default offset is zero. Typically, gradients declare two or more stop colors. Gradients are used used as a fill or stroke value. They are usually defined in <defs> with an id that can be referenced later using url(#id). The following SVG code declares two gradients and applies one of them to a square, and the other to a circle: <svg width=\"600\" height=\"300\"> <defs> <linearGradient x2=\"1\" id=\"rainbow\"> <stop offset=\"0\" stop-color=\"rgb(255,0,0)\" /> <stop offset=\"0.25\" stop-color=\"rgb(255,255,64)\" /> <stop offset=\"0.5\" stop-color=\"rgb(64,255,64)\" /> <stop offset=\"0.75\" stop-color=\"rgb(64,64,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </linearGradient> <radialGradient cx=\"0.35\" cy=\"0.35\" r=\"1\" id=\"glow\"> <stop offset=\"0\" stop-color=\"rgb(255,255,255)\" /> <stop offset=\"0.5\" stop-color=\"rgb(0,128,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </radialGradient> </defs> <rect x=\"0\" y=\"0\" width=\"200\" height=\"200\" fill=\"url(#rainbow)\" transform=\"translate(50,50)\"/> <circle cx=\"100\" cy=\"100\" r=\"100\" fill=\"url(#glow)\" transform=\"translate(300,50)\"/> </svg> The result is shown in the following screenshot: Linear and radial gradients. Code: SVG/23-gradient.html.","title":"Gradients"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#clipping","text":"A clipping mask is created with any shape declared inside the <clipPath> element. This is usually done in <defs> , setting an id for the clipping mask that can be referenced in the object that should be clipped, using url(#id) inside a clip-path attribute, as shown below: <svg width=\"425\" height=\"425\"> <defs> <clipPath id=\"poly\"> <circle r=\"139\" cx=\"200\" cy=\"199\"/> </clipPath> </defs> <!-- Clipped image --> <image x=\"25\" y=\"25\" height=\"350\" width=\"350\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#poly)\"/> </svg> The following SVG screenshots show the circular clipping mask above applied to an image. The SVG at left shows the original image and the clipped image is shown at right. Clipping an image in SVG. Code: SVG/24-clipping.html.","title":"Clipping"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#filters","text":"SVG filters can be applied to any shapes, text or images. They are usually created in the <defs> header with an id that can be referenced via url(#id) using the filter attribute. The <filter> element can contain several different filter types and you can also create composite filters. The following example creates two different configurations for the <feGaussianBlur> filter, applied to a text element and a circle: <svg width=\"600\" height=\"300\"> <defs> <filter id=\"filter1\"> <feGaussianBlur stdDeviation=\"7\" /> </filter> <filter id=\"filter2\" x=\"-100\" y=\"-100\" height=\"200\" width=\"200\"> <feGaussianBlur stdDeviation=\"0,5\" in=\"SourceGraphic\" /> </filter> </defs> <text id=\"text\" font-size=\"40\" fill=\"black\" x=\"50\" y=\"60\" filter=\"url(#filter2)\">Do you need glasses?</text> <g id=\"stardot\" transform=\"translate(100,25) scale(0.5)\"> <polygon id=\"star\" points=\"250,0 400,500 0,200 500,200 100,500\" fill=\"red\" fill-rule=\"evenodd\"/> <circle id=\"circ\" cx=\"250\" cy=\"283\" r=\"75\" fill=\"blue\" filter=\"url(#filter1)\" /> </g> </svg> The result is shown as follows: A Gaussian blur filter applied to different shapes. Code: SVG/25-filter.html. An SVG example The following code uses several SVG elements described in this section to draw some shapes, shadows, gradients and text: <svg width=\"300\" height=\"300\"> <defs> <filter id=\"shadow\"> <feDropShadow style=\"flood-color: green\" dx=\"5\" dy=\"5\" stdDeviation=\"3\"/> </filter> <linearGradient id=\"grad\" x1=\"0\" y1=\"0\" x2=\"100%\" y2=\"0\"> <stop offset=\"0%\" stop-color=\"magenta\"/> <stop offset=\"100%\" stop-color=\"yellow\"/> </linearGradient> <clipPath id=\"circle\"> <circle r=\"40\" cx=\"175\" cy=\"75\"/> </clipPath> </defs> <!-- rectangle --> <rect x=\"50\" y=\"50\" height=\"50\" width=\"50\" fill=\"red\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\"0.5\"/> <!-- dashed shape --> <path id=\"path1\" d=\"M150,200 L150,150 L100,150 C100,200 150,250 200,250 L200,200 Z\" stroke-dasharray=\"5 2 1 2\" stroke-width=\"2\" stroke=\"blue\" fill=\"none\" style=\"filter:url(#shadow)\"/> <!-- gray quarter-circle --> <path d=\"M0,0 L0,-100 A100,100 0 0,0 -100,0 L0,0 Z\" transform=\"translate(100,250) scale(0.5) \" stroke=\"red\" stroke-opacity=\".5\" stroke-width=\"4\" fill-opacity=\".2\"/> <text fill=\"url(#grad)\" font-size=\"20\" x=\"200\" y=\"100\"> Scalable <tspan dy=\"20\" x=\"200\">Vector</tspan> <tspan dy=\"20\" x=\"200\">Graphics</tspan> </text> <image x=\"125\" y=\"25\" height=\"100\" width=\"100\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#circle)\" opacity=\"0.75\"/> <!-- raindow half-circle --> <path d=\"M100,200 C100,100 250,100 250,200\" transform=\"scale(0.6) rotate(180,295,225) \" fill=\"url(#grad)\"/> </svg> Compare this code and the following image it generates it with an identical image created using D3 (SVG-with-D3/29-example.html) and HTML Canvas (Canvas/1-canvas-svg-compare.html): An image created using SVG. Code: SVG/29-example.html.","title":"Filters"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#2-essential-javascript-data-structures","text":"I'm actually going to organize this somewhere else to help with my code journal. Link to the Basics","title":"2) Essential JavaScript data structures"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#3-html-canvas","text":"HTML5 Canvas Most of your D3 applications will render graphics using SVG, but several shape generators in SVG can also generate Canvas, and you may choose to use Canvas in all or part of your application to improve performance if you have memory problems due to excessive objects created in the DOM. To draw using Canvas you need to create a <canvas> element in your page. You can do that using plain HTML: <body> <canvas id=\"canvas\" width=\"400\" height=\"300\"></canvas> </body> Or using D3: d3.select(\"body\").append(\"canvas\").attr(\"width\", 400).attr(\"height\", 300); If you declare the Canvas element in HTML, you can reference it by its ID using the DOM or D3: const canvas = d3.select(\"#canvas\").node(); // node() returns the DOM object Once you have a canvas object, you obtain a 2D graphics context and start drawing: const ctx = canvas.getContext(\"2d\"); Practically all the Canvas API consists of methods and properties called from the graphics context. Before drawing, you set properties such as font , fillStyle , strokeStyle : ctx.fillStyle = \"red\"; ctx.strokeStyle = \"rgba(255,127,0,0.7)\"; ctx.lineWidth = 10; And then fill or stroke rectangles and arbitrary paths containing lines and curves. The following commands will draw a red 50x50 pixel square with a 10 pixel wide yellow semi-transparent border at position 50,50: ctx.fillRect(50,50,50,50); ctx.strokeRect(50,50,50,50); You can also draw other shapes, text and images on the same canvas. The context properties will not change unless they are redefined or a previously saved context is restored. It's a good practice to save the context to the stack before applying properties or transforms, and restore it when you are done drawing an object. This allows you to always start with a clean context: Note: So far the below code does nothing that I can see with my eyes. ctx.save(); ctx.transform(50,60); ctx.scale(2); // \u2026 ctx.restore(); // starting with a new context Property or method Description fillStyle Sets the color to be used in fill() commands. strokeStyle Sets the color to be used in stroke() commands. lineWidth Sets the line width to be used in stroke() commands. lineCap Sets the style of the line caps: can be butt (default), round or square. textAlign Sets the alignment for text: can be start (default), center, end, left or right. textBaseline Sets the baseline for text: can be middle, hanging, top, bottom, ideographic or alphabetic (default). font Sets the font to be used in text commands, using the compact CSS font syntax. globalAlpha Sets the global opacity (0 = transparent, 1 = opaque) for the context. shadowBlur, shadowColor, shadowOffsetX, shadowOffsetY Sets shadow properties. Default shadow color is transparent black. Default numeric values are zero. setLineDash(dasharray) Sets the dash array (alternating line and space lengths) for strokes. translate(x,y) Sets the current translate transform for the context. scale(x,y) Sets the current scale transform for the context. rotate(angle) Sets the current rotate transform for the context. save() Saves the state of the current context (pushes into a stack). restore() Restores the state of the last context that was saved (pops it from the stack). The fillRect() command is typically used to clear the entire canvas before redrawing, but you can also use it to draw arbitrary rectangles. The following table lists methods you can use to draw rectangles, draw text and images: Method Description fillRect(x,y,w,h); Fills a rectangle. Typically used to clear the Canvas on redrawing. strokeRect(x,y,w,h) Draws a border around a rectangle. fillText(text,x,y); Fills text at position x, y (depends on current textAlign and textBaseline). strokeText(text, x, y); Draws a border around text. drawImage(image, x, y, w, h) Draws an image at x,y with width w and height h. Canvas context methods used to draw rectangles, text and images A path is a series of commands to move to points, draw lines, curves or arcs. To draw a path you need to first call ctx.beginPath() , then call a sequence of commands that move to points, draw lines and curves, and when you are done you can close the path (if it's a closed path) and call fill() and/or stroke() to draw it using the current styles. The following table lists several commands you can use in a path: Method Description beginPath() Starts a path. closePath() Closes a path. moveTo(x,y) Moves the cursor to a position in the path. lineTo(x,y) Moves the cursor to a position in the path, drawing a line along the way. bezierCurveTo(c1x,c1y,c2x,c2y,x,y) quadraticCurveTo(cx,cy,x,y) Draws curves with one (quadratic) or two (Bezier) control points in a path. arc(x,y,r,sa,ea) Draws an arc by specifying the center, radius, start and end angles in a path. arcTo(sx,sy,r,ex,ey) Draws an arc by specifying the coordinates of the starting point, the radius and the coordinates of the ending point. rect(x,y,w,h) Draws a rectangle in a path with coordinates of top-left corner, width and height. clip() Creates a clipping region with the shapes drawn by the path that will affect objects that are drawn afterwards. fill() Fills a path with the current color. Call this to fill the path when done. stroke() Strokes the path with the current color. Call this to stroke the path when done.","title":"3) HTML Canvas"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#a-canvas-example","text":"The following code uses several of the methods above to draw different shapes on the same Canvas context. It draws some shapes, text, images and paths, and applies transforms, shadows, clipping and gradients. Compare it to the example shown before in SVG that draws the image (see Canvas/1-canvas-svg-compare.html ): const canvas = document.getElementById(\"canvas\"); const ctx = canvas.getContext(\"2d\"); // rectangle ctx.save(); // save default context ctx.fillStyle = \"#ff0000\"; ctx.strokeStyle = \"blue\"; ctx.lineWidth = 10; ctx.fillRect(50,50,50,50); ctx.globalAlpha = 0.5; ctx.strokeRect(50,50,50,50); // dashed shape ctx.restore(); ctx.save(); ctx.strokeStyle = \"blue\"; ctx.lineWidth = 2; ctx.shadowBlur = 6; ctx.shadowColor = \"green\"; ctx.shadowOffsetX = ctx.shadowOffsetY = 5; ctx.setLineDash([5,2,1,2]); ctx.beginPath(); ctx.moveTo(150,200); ctx.lineTo(150,150); ctx.lineTo(100,150); ctx.bezierCurveTo(100,200,150,250,200,250); ctx.lineTo(200,200); ctx.closePath(); ctx.stroke(); ctx.restore(); ctx.save(); // quarter-circle ctx.translate(100,250); ctx.scale(0.5, 0.5); ctx.strokeStyle = \"red\"; ctx.lineWidth = 4; ctx.globalAlpha = 0.5; ctx.beginPath(); ctx.moveTo(0,0); ctx.lineTo(0,-100); ctx.arcTo(-100,-100,-100,0,100); ctx.lineTo(0,0); ctx.stroke(); ctx.globalAlpha = 0.2; ctx.beginPath(); ctx.arc(0,0,100,3.14,-1.57,false); ctx.lineTo(0,0); ctx.closePath(); ctx.fill(); ctx.restore(); ctx.save(); // text and half-circle const text = \"Canvas\" ctx.translate(250,150); ctx.font = \"24px monospace\"; const textWidth = ctx.measureText(text).width; const gradient = ctx.createLinearGradient(-50,-50,-50 + textWidth,-50); gradient.addColorStop(0,\"magenta\"); gradient.addColorStop(1, \"yellow\"); ctx.fillStyle = gradient; ctx.shadowColor = \"transparent\"; ctx.fillText(text, -45, -5); ctx.scale(1.1, 1.1) ctx.rotate(3.14); ctx.beginPath(); ctx.arc(0,0,40,3.14,0,false); ctx.fill(); ctx.restore(); ctx.save(); // image and clip ctx.beginPath(); ctx.arc(175,75,40,0,6.28,false); ctx.clip(); const image = new Image(100,100); image.onload = function() { ctx.globalAlpha = 0.75; ctx.drawImage(this, 125, 25, this.width, this.height); } image.src = \"reuse.png\"; ctx.save(); I don't have the pluto pic so the code won't show the planet.","title":"A Canvas Example"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#data-formats","text":"Data used in visualizations are usually distributed in a standard format that can be shared. Even when the data is served from a database, the data is usually delivered in some standard format. Popular proprietary formats such as Excel spreadsheets are common, but most statistical data is stored or delivered in CSV, XML or JSON formats.","title":"Data Formats"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#csv","text":"You can load and parse CSV in D3 using the d3.csv() function. const csv = d3.csv(\"Data/continents.csv\") csv output: Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: \"579024000\", areakm2: \"24490000\"} 1: {continent: \"Asia\", population: \"4436224000\", areakm2: \"43820000\"} 2: {continent: \"Europe\", population: \"738849000\", areakm2: \"10180000\"} 3: {continent: \"Africa\", population: \"1216130000\", areakm2: \"30370000\"} 4: {continent: \"South America\", population: \"422535000\", areakm2: \"17840000\"} 5: {continent: \"Oceania\", population: \"39901000\", areakm2: \"9008500\"} 6: {continent: \"Antarctica\", population: \"1106\", areakm2: \"13720000\"} columns: (3) [\"continent\", \"population\", \"areakm2\"] length: 7 __proto__: Array(0)","title":"CSV"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#xml","text":"XML \u2013 eXtensible Markup Language is a very popular data format. Ajax responses from web services are usually returned as text or XML. It has standard native support in JavaScript via the DOM (document object model) APIs and doesn't require additional parsing. Although it is still common to find data in XML format, CSV and JSON alternatives, if available, are usually smaller and easier to work with. <continents> <continent> <name>North America</name> <population>579024000</population> <area unit=\"km\">24490000</area> </continent> <continent> <name>Asia</name> <population>4436224000</population> <area unit=\"km\">43820000</area> </continent> <continent> <name>Antarctica</name> <population>1106</population> <area>13720000</area> </continent> </continents> const xml = d3.xml(\"Data/continents.xml\") Not sure how to parse but who uses XML anymore gosh!","title":"XML"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#json","text":"JSON stands for JavaScript Object Notation. It looks a lot like a JavaScript Object, but it has stricter formation rules. It's probably the easiest format to work with. It's compact and easy to parse, and it's gradually replacing XML as a preferred data format in Web Services. The data file containing continent data is shown below in JSON format ( Data/continents.json ). [ { \"continent\": \"North America\", \"population\": 579024000, \"areakm2\": 24490000 },{ \"continent\": \"Asia\", \"population\": 4436224000, \"areakm2\": 43820000 },{ \"continent\": \"Europe\", \"population\": 738849000, \"areakm2\": 10180000 },{ \"continent\": \"Africa\", \"population\": 1216130000, \"areakm2\": 30370000 },{ \"continent\": \"South America\", \"population\": 422535000, \"areakm2\": 17840000 },{ \"continent\": \"Oceania\", \"population\": 39901000, \"areakm2\": 9008500 },{ \"continent\": \"Antarctica\", \"population\": 1106, \"areakm2\": 13720000 } ] JSON is the preferred format for data manipulation in JavaScript. There are many online tools you can use to transform CSV and XML files into JSON. You can load and parse JSON in D3 using the d3.json() function. Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: 579024000, areakm2: 24490000} 1: {continent: \"Asia\", population: 4436224000, areakm2: 43820000} 2: {continent: \"Europe\", population: 738849000, areakm2: 10180000} 3: {continent: \"Africa\", population: 1216130000, areakm2: 30370000} 4: {continent: \"South America\", population: 422535000, areakm2: 17840000} 5: {continent: \"Oceania\", population: 39901000, areakm2: 9008500} 6: {continent: \"Antarctica\", population: 1106, areakm2: 13720000} length: 7 __proto__: Array(0) Note that both csv and json loaded an easy to work with object. Just try to xml code....It'll make you sick.","title":"JSON"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/","text":"Source github Selecting and Binding Data Selecting and appending The selection methods select() and selectAll() receive a CSS selector expression and return a D3 handle for a node or a set of nodes. You can convert the D3 handle into a DOM object calling node() or nodes() . Add the following HTML to the <body> of your HTML file (or use Selecting/1-select.html): <p>See results in console log.</p> <div id=\"section\"> <p class=first>Paragraph 1</p> <p>Paragraph 2</p> </div> <p>Paragraph 3</p> Now you can use the JavaScript console (or a <script> block in your page) to type in the following code, which shows how select() and selectAll() can be used: const div = d3.select(\"#section\"); // selects element with the // \"section\" ID const domDiv = div.node(); // converts the object into a // DOM node const firstP = d3.select(\"p\"); // selects the first <p> // in the page const allParagraphs = d3.selectAll(\"p\"); // selects all four <p> nodes // in the page const allPDom = allParagraphs.nodes(); // converts selection into // DOM nodelist const sectionParagraphs = d3.select(\"div\") .selectAll(\"p\"); // selects the two <p> nodes // inside <div> A selection object serves as a handle to elements in your page. You can use it to change their styles, properties, attributes, classes, and contents, using methods that can receive static or dynamic values (using functions) as parameters. The following examples demonstrate some of these methods (see Selecting/2-attributes-styles.html ): div.style('border', 'solid blue 2px'); // draws blue border around <div> firstP.classed('big', true); // adds class \u2018.big\u2019 to first paragraph firstP.text('This is paragraph zero'); // replaces contents of first paragraph allParagraphs.style('font-weight', 'bold'); // applies style to set of paragraphs // this looks promising, however there is nothing tagged with class \"first\" so nothing happes d3.select('.first') // selects first element of class \u2018.first\u2019 .attr('title', 'Tooltip') // adds an attribute .style('font-variant', 'small-caps') // applies a style .html('This is a <b>bold</b> paragraph.'); // replaces contents with HTML The select() method will always return only one object (if the selector matches more items, only the first will be returned). The selectAll() method returns a collection, which can be iterated with the each() method, which receives a callback function, which receives up to three parameters. Inside the function, this refers to the current DOM element, and d3.select(this) wraps it in a D3 selection object, which can be manipulated with selection methods, as follows: sectionParagraphs.each(function(d, i) { // i is the index d3.select(this).classed('red', true); console.log(`Paragraph ${i}: `, this); // this refers to the current element }); ``` This code applies the class ```.red``` to each element of a collection. If you open the JavaScript console, it will print each element and its index (starting in zero). You can also use selection methods to modify the DOM tree. Calling ```remove()``` on a D3 selection removes elements in the selection. You can call ```append()``` on a selection to add an element as its last child, or use ```insert()``` to add the new element in a position determined by a selector passed as the second argument. These methods are demonstrated as follows (see ```Selecting/3-append-remove.html)```: ```js d3.select(\"#section\") // current context is element with id #section .insert(\"p\", '.first') // add a <p> before the child being of class \u2018.first\u2019 .append('a') // context is now <p>; inserts an <a> in <p> .attr('href', 'https://d3js.org') // context is now <a> .text(\"D3 website\"); // contents of the <a> element d3.select('div') // context is first <div> in page .select('p:last-of-type') // context is now last <p> in <div> .remove(); // context is detached <p> (parentNode is null) const div = d3.select('div') div.append(\"p\") // adds a new <p> as last child of selected <div> .text(\"New paragraph\"); // sets the text contents of <p> d3.select(\"body\") // selects the <body> element of the page .insert(\"h1\", \"*\") // adds a new <h1> as the first child (before *) .text(\"New title\"); // sets the text contents of <h1> d3.select(\"body\") .append(\"h2\") // adds a new <h2> as the last child .text(\"Footer\"); The append() and insert() methods can also be chained, since they return the nodes they added. You just have to pay attention to the current so you don't modify or add data to the wrong elements. The remove() method returns the removed selection (a node or a set of nodes) after it's detached from the DOM ( null parent). D3 selections are not limited to HTML. In fact, they are most commonly applied to SVG elements, but can also be used with any embedded XML. This was a brief introduction. In Chapter 4, Data Binding , we will explore these methods in detail. Binding Data D3 makes it almost trivial to bind data to DOM elements. Data binding requires calling at least two of the following four methods in the context of a selection: data(array) or datum(object) : Receives an array or an object/value that contains data that should be mapped to DOM elements join() , enter() or exit() : Binds the data to DOM elements by populating an update array returned as a selection of elements that should be added or removed from the document The exit() method returns a selection of unbound elements. After an exit() , remove() is usually called, which removes the entire selection. The enter() method returns a selection of placeholders for new elements that need to be created. After an enter() , either append() or insert() is called, which connects the elements to the DOM tree. The join() method is a magic method that replaces enter() and exit() , automatically updating, removing, or adding elements, as necessary. Once data is connected to an element, the attributes, style, and contents of each element can be modified using the attr() , style() , text() , and html() methods that take a callback function as the second argument. This binding process is illustrated as follows: Let's try an example. Create an HTML page with this code in the <body> and open it in a browser (or use Binding/1-binding.html ): In a <script> block, create this array of numbers (or type it in your JavaScript console): const numbers = [6, 2, 5, 7, 9]; The following code will bind each number in the array to a paragraph. The data will be copied to an internal array and can be used to change the attributes, styles, or contents of the existing paragraphs: const selection = d3.select(\"#section\") .selectAll(\"p\") .data(numbers); The data() method returns the current selection. The value of each item in the array is received as the first parameter of a callback function in attr(),style(),text(), and so on. The text() method is used in the following code that replaces the contents of each paragraph with the first two values of the array: selection.text(d => d); // binds to existing paragraphs (update) But we need more paragraphs, since we have five elements of data. By calling enter() , a new selection is created with three more placeholder nodes, containing the remaining data, but not yet mapped to any element: const newSelection = selection.enter(); // binds data to a selection of placeholders We can't just call text() on the selection yet. First, we need to append or insert those elements into the DOM tree. You call append() only once, and it will be executed three times. Now, you can use text() to print the paragraph with the data: newSelection.append(\"p\") .text(d => d); // binds to new paragraphs (enter) Normally, the entire selecting-binding-appending process is written as a chain of commands. The following code achieves the same result, using the same array and paragraphs (see Binding/3-chain-text.html ): d3.select(\"body\") // select the body element .select(\"div\") // select the first div element inside <body> .selectAll(\"p\") // select all <p> elements (there are two) .data(numbers) // load the data from the numbers array .text(d => d) // set contents of existing paragraphs .enter() // bind remaining data to array with 3 elements .append(\"p\") // add 3 new <p> elements to end of <div> .text(d => d); // set the contents of the new paragraphs It's simpler (and more common, when using D3) to use JavaScript (and not HTML) to create all of the elements you need. After all, data is dynamic and you won't always know how many items you have to display. Most of the time, HTML is just used to provide the basic structure, such as the <body> tag, a container <div> , or <svg> . The following code achieves the same result starting with an empty <body> tag (and the same data array). The <div> and all the <p> are created using D3 commands (see Binding/4-empty-binding.html ): d3.select(\"body\") // select the body element .append(\"div\") // append a div element inside <body> .selectAll(\"p\") // select all <p> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"p\") // create 5 new <p> elements and add to end of <div> .text(d => d); // set the contents of the new paragraphs Note that even though there are no <p> elements in the page, the selectAll(\"p\") command is still necessary, since it provides the selection context for the data binding. The selectAll() command can use any CSS selectors to locate its elements, but it should return a selection containing the same type of elements added by the append() command. As you have seen, once you have data bound to DOM elements, you can use the data to change attributes, styles, classes, and properties, using callbacks. The callback function has a second parameter that contains the index of the data array, which is used in the following example to change the contents and style of list items: d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // create an enter array with 5 objects .append(\"li\") // create 5 new <li> elements and append to <ul> .text(function(d, i) { return \"Item \" + (i+1) + \": \" + d; }) .style(\"font-size\", function(d, i) { return ((i+2) * 5) + \"pt\"; }); The result is shown in the following screenshot: D3 used to create and style new elements from data. Code: Binding/5-callbacks.html . Callback functions also have a third parameter that contains the array of elements in the selection. You might need it if you want to obtain the current element when using arrow functions, since the this reference in these functions doesn't refer to the current element: .text( (d, i, nodes) => console.log(\"Current element: \" + nodes[i]) ); You can easily reuse the preceding code with different types of data. If you have an array of objects containing values of different types, you don't need to convert it to an array of numbers. You can directly pass the entire object array to the data() method, and later select which properties of each object you wish to use. For example, you can use practically the same code that was shown previously to display the contents of the following array: const distances = [ {name: \"Mercury\", distance: 0.387}, {name: \"Venus\", distance: 0.723}, {name: \"Earth\", distance: 1}, {name: \"Mars\", distance: 1.52}, {name: \"Jupiter\", distance: 5.2}, {name: \"Saturn\", distance: 9.54}, {name: \"Uranus\", distance: 19.2}, {name: \"Neptune\", distance: 30.1}, {name: \"Ceres\", distance: 2.765}, {name: \"Pluto\", distance: 39.481}, {name: \"Eris\", distance: 67.67}, {name: \"Haumea\", distance: 43}, {name: \"Makemake\", distance: 45.346} ]; The object array is received by the data() command. Each array item is available as the first parameter of the callback, which can be used to extract the property that contains the data (see Binding/6-object-array.html ): d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(distances) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"li\") // create <li> from enter selection and append to <ul> .text(function(d, i) { return d.name + \": \" + d.distance; }); Since D3 Version 5.8, you can replace enter().append(element) in simple binding operations, as shown with the join(element) method (see Binding/7-join.html ): d3.select(\"body\") .append(\"ul\") .selectAll(\"li\") .data(distances) .join(\"li\") // obtains enter selection and appends 5 new <li> elements .text(function(d, i) { return d.name + \": \" + d.distance; }); The preceding code will generate an HTML list with the data. Creating a bar chart with D3 and HTML Now that you have had a quick introduction to D3's basic data-binding mechanisms, let's create some full data visualization examples that will demonstrate the power of D3. You should know HTML very well, so we will start with a simple bar chart using HTML and CSS. Later, we will repeat the exact same procedure using SVG, the main standard used by D3 to create visualizations. All the code for these examples is available in the GitHub repository for this chapter. Binding data to HTML In this example, we will draw a horizontal bar chart using HTML <div> elements. The colors and length of each bar are controlled by CSS, which we can configure using the selection.style() command. We will use the object array containing planetary distances from the last example. First, let's create a style sheet with classes for the bar-chart and each individual bar : <style> .bar-chart { border: solid 1px gray; /* a gray border around the container */ position: relative; } .bar { height: 20px; background-color: orange; position: absolute; } </style> The bars are positioned absolutely. Since each bar has an equal height (20px), we need to position each bar slightly below the previous one. This can be achieved by setting the value for the CSS top property (in pixels) so that it contains the sum of the heights of all the previous bars, plus an extra pixel to keep them slightly apart. The top property can be dynamically calculated as proportional to each entry's array index, using a callback in selection.style() (see HTML_Bar/1-bar-chart.html ): d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // container div for the chart .style(\"height\", () => distances.length * 21 + \"px\") // set chart height .selectAll(\"div\").data(distances) // binds data .enter().append(\"div\") // appends a div for each data element .attr(\"class\", \"bar\") // these divs are the bars of the chart .style(\"top\", (d,i) => i * 21 + \"px\") // stacks bars .style(\"width\", \"100px\") // fixed width output: This prints all the bars, one on top of the other, but they all have the same static width! We need to use a callback to change the width of each individual bar. In a bar chart, the actual width should be proportional to the distance. A solution would be to multiply each value by 10. This will make the smallest bar three pixels wide, and the largest over 670 pixels: .style(\"width\", d => (d.distance * 10) + \"px\") Now, the lengths of the bars are proportional to the data values. This code is in HTML_Bar/2-bar-width.html . The result is shown as follows: HTML bar chart with D3: Drawing DIVs and using the data to change the CSS width. Code: HTML_Bar/2-bar-width.html Scales What if a new planet is discovered that is 10 times the largest distance? The bars wouldn't fit and their widths would need to be recalculated! D3 also provides a solution for this: scales . Scales are mappings between different dimensions. In our case, we have to deal with two dimensions: the dimension of the data, called the domain , and the dimension of the graphics context where the data will be displayed, called the range . Before using a scale, you have to configure its domain and range. There are several different kinds of scales in D3, including linear and logarithmic scales, with many configuration options. A scale is created with a special generator function available in the d3-scale module. To use this module, you need to load several dependencies, so let's replace the <script> tag with the default bundle: <script src=\"https://d3js.org/d3.v5.min.js\"></script> To create a scale function, you call a special generator function. A linear scale function can be created using the following: const barScale = d3.scaleLinear(); To use the scale function you created, you pass a value as the argument and receive the converted result: const result = barScale(45); // returns 45 This will return 45 , because the scale hasn't been configured yet (the default scale is 1:1). To configure the scale, you need to call two methods to set up the domain (data dimensions) and range (graphical dimensions). Each method receives an array. For example, we can set up the domain as follows: barScale.domain([0, 100]); // input domain from 0 to 100 This fits all the distances in our data. The range is set up in a similar fashion: barScale.range([0, 600]); // output domain from 0 to 600 This means that zero is mapped to zero, and 100 astronomical units (AU) is mapped to 600 (pixels). Intermediate values will be interpolated. If you now call barScale(45) , you will get 270 . We can use the scale to convert the distances in AU to pixels, replacing the previous expression with a scale conversion (see HTML_Bar/3-scales.html ): .style(\"width\", d => barScale(d.distance) + \"px\"); Array utilities We chose 100 as the upper limit in our scale domain because it's larger than any of the distances in the data, but the choice was rather arbitrary. We could have chosen the largest value in the array. If you have hundreds of lines of data, you can use JavaScript array functions (see Chapter 1, Introduction ) to find out the maximum value. D3 also includes a collection of array manipulation functions that extend JavaScript's native functions. Some look similar but may be more efficient for data manipulation (for example, by ignoring null , NaN , and undefined values). In order to use them, you need the d3-array module (which is also part of the default bundle). Here, we changed the configuration of our barScale() function so that it uses the largest distance from the distances object array as the upper value for the domain. This is achieved with by calling the d3.max() function, which receives an array and an accessor function for each array element, as follows: const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); The d3.max() function will scan the distances array and compare the distance property of each object, returning the largest one. There are many more useful functions in d3-array , which we will cover in the next chapter. Two of them, d3.descending(a,b) and d3.ascending(a,b) , are used to provide a sorting rule for JavaScript's native sort() method. We can use it to sort the array by the distance: distances.sort((a,b) => d3.ascending(a.distance, b.distance)); See and run the code after these transformations in HTML_Bar/4-max-sort.html . Adding labels Since the <div> elements in HTML can contain text, you can call the text() method for each <div> and set its contents based on the data: d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // ...all previous code to build chart .text(d => d.distance); // display distance in the <div> With CSS, we can right-align the text and adjust the fonts. Since these styles are static and don't change with the data, instead of calling style() for each property, you should use a style sheet: <style> /* ... */ .bar { height: 20px; left: 100px; background-color: orange; position: absolute; text-align: right; padding: 0 5px; font-family: sans-serif; font-size: 9pt; } </style> As a result, the labels are placed inside the bars, as follows: Adding labels to the bars. Code: HTML_Bar/5-labels.html . More labels, formatting, and colors Each object in our data array also contains a name property that can be used to label each bar. To position text outside the bar, we will need to refactor the code so that each data entry contains a container <div> , which will be bound to the data values. This entry <div> will then contain three other <div> elements: a category label (the name of the planet), the bar, and a value label (the distance). Classes will be used to identify each <div> . The following style sheet contains the static properties for these elements: <style> .bar-chart { /* The container <div> for the entire chart */ border: solid 1px gray; position: relative; width: 800px; } .entry { /* a container <div> for each data entry */ position: absolute; width: 100%; } .bar { /* the colored rectangle */ height: 20px; top: 1px; left: 100px; background-color: orange; position: absolute; } .label { /* a text label */ padding: 4px 5px; font-family: sans-serif; font-size: 9pt; position: absolute; height: 20px; } .category { /* the category text label at left (name) */ text-align: right; width: 80px; } .value { /* the value text label at right (distance) */ text-align: left; } </style> Since each entry <div> has three children, we need to keep a reference to its selection so that the child elements can be appended. The following code saves references for each one of the container <div> elements. The chart constant contains a selection of the root <div> element, and the entries constant contains the selection of all entry <div> elements: // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); You can now use the entries constant to append the child elements to each entry <div> (see HTML_Bar/6-entries.html ): entries.append(\"div\").attr(\"class\", \"label category\") .text(d => d.name); entries.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", d => barScale(d.distance) + \"px\"); entries.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", d => (barScale(d.distance) + 100) + \"px\") .text(d => d.distance + \" AU\"); Each append() shown is called once for each entry. The attributes, style, and text are set using the data that was bound to the parent container. You can add child elements without having to break the selection chain with the selection.each() method, which calls a function for each entry. Inside it, you can obtain a selection to the current element using d3.select(this) . The following code produces the same result (see HTML_Bar/7-entries-each.html ): entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\"); entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d.distance + \" AU\"); }); We can improve the rendering of the labels by formatting the numbers to display only two decimal places, using the d3.format() generator function (from the d3-format module). The following code creates a function fmt() that can be used to format numbers: const fmt = d3.format(\".2f\"); Now, we can use it to format the distances: .text(d => fmt(d.distance) + \" AU\"); The final result is shown as follows: Adding category names and placing the formatted label values outside the bars. Code: HTML_Bar/7-entries-each.html Changing colors The d3-colors module contains functions to generate, convert, and transform colors. Passing any CSS-compatible color representation to d3.color() generates an object with methods that can be used to modify the color. The darker() and brighter() methods receive a value between 0 (no change) and 1 (maximum change) to adjust the lightness component of a color and return a hexadecimal color string. Let's use this feature to darken the bars when the distance from the sun increases. We will need a scale that maps the distance domain to the [0-1] range, which contains the values accepted by the darker() function: colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) Now, this colorScale can be used to generate different tones for the bars: entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) See the code in HTML_Bar/8-colors.html . The result is shown as follows: Changing bar colors with the distance. Code: HTML_Bar/8-colors.html Loading external files Usually, your data will come from external files that need to be loaded with an Ajax request. You can, of course, use jQuery or ECMAScript fetch() commands (and promises, if you need to load multiple files). After loading, you will also need to parse the data and convert it into JavaScript arrays and objects. D3 again provides a more efficient solution in the d3-fetch module: a set of convenient methods for loading and parsing files in popular formats, such as XML, JSON, or CSV. The d3-fetch module is also included in the default bundle. The planetary data we used in the previous examples is actually part of a larger JSON file containing several properties for planets, asteroids, and satellites. The basic structure of this file ( Data/sol_2016.json ) is shown as follows: { \"star\":{\u2026}, \"planets\":[ { \"id\":\"p1\", \"name\":\"Mercury\", \"diameterKm\":4879, \"semiMajorAxisAU\":0.387, \u2026 },{ \"id\":\"p2\", \"name\":\"Venus\", \"diameterKm\":12104, \"semiMajorAxisAU\":0.723, \u2026 }, \u2026 } ``` You can load the data using the ```d3.json()``` function. After it loads and parses the file, it will become available in a callback provided as a parameter to the ```then()``` method (which is a JavaScript promise). We don't need all the loaded data. The distance used in our examples is stored in the property called ```semiMajorAxisAU```. After loading the file, we can filter the dataset to only save the ```semiMajorAxisAU``` and ```name``` properties. The following code demonstrates this. It loads the file, and it then uses the data obtained in the ```then()``` callback to loop through the planets array, adding only the chosen properties to a new object and pushing it into an array. The array is used to create an HTML list with these properties (see``` Loading/1-loading-json.html```): ```js //d3.json(\"../Data/sol_2016.json\") // if you down load it d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({ name: obj.name, distance: obj.semiMajorAxisAU }); }); draw(planets); }); function draw(distances) { d3.select(\"body\").append(\"ol\") .selectAll(\"li\") .data(distances) .join(\"li\") .text(d => d.name + \" (\" + d.distance + \" AU)\"); } To apply this to our chart, you just need to replace the contents of the draw() method shown with the entire code used in the previous examples (except for the distances array). See an example in HTML_Bar/9-load-json.html . As an exercise, try feeding the bar chart with a list of Jupiter's moons, instead of the planets (use data.planets[4].satellites to obtain the array, and the semiMajorAxisKm property as the distance, and d3.format(\",.0f\") to format the distance values). Full Example: d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({name: obj.name, distance: obj.semiMajorAxisAU}); }); draw(planets); }); function draw(distances) { const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances.sort((a,b) => d3.ascending(a.distance, b.distance))) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); entries.each(function(d) { const entry = d3.select(this); // the current entry const fmt = d3.format(\".2f\"); // format for each entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d => fmt(d.distance) + \" AU\"); }); } Creating a bar chart with D3 and SVG It's easy to create a horizontal bar chart in HTML. It's a bit trickier to make a vertical one, and quite challenging to create other charts, such as line charts, pie charts, and network diagrams, since HTML wasn't intended for vector graphics. While you can create such charts using HTML and CSS, it's not at all simple, nor is it recommended . You can, of course, use Canvas, but there are no graphical DOM elements to bind in Canvas. Canvas is used in D3 charts, but mostly for performance optimization. D3 works best with SVG . The same bar chart that we created with HTML can be created with SVG. Do you think you can do it? Why not try it as an exercise before proceeding? You already learned how to use D3 and HTML, and you know the basics of SVG rectangles, attributes, and styles. That's all you need! Most of the code is the same. You can reuse the scales, formatting function, sorting function, and JSON parsing code. The CSS will be simpler, since positioning is done in SVG. You only need to change the selection code, replacing the bar <div> element with <rect> , text label <div> elements with <text> , and container <div> elements with <g> . Remember to use fill (and not background-color) to fill the bars. The same step-by-step files that were used in the HTML example are available for the SVG example in the SVG_Bar/ folder . The following, final code produces exactly the same chart as the HTML version. This is the CSS style sheet that's used. Since all positioning is done in SVG, it's much smaller: <style> * { font-family: sans-serif; } .bar-chart { border: solid 1px gray; width: 800px; } .bar { height: 20px; fill: orange; } .label { font-size: 9pt; } .category { text-anchor: end; } </style> The main container is the <svg> element, which is appended to the body, as shown in the following code snippet. We renamed the chart object svg : // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); The containers used for each entry are <g> elements, which group several child elements and can have their own coordinate system configured using a transform . Although the transform property is supported in both CSS and SVG, you should use the SVG version (selected with the attr() method), because it considers pixel values and degrees as the default. You don't have to append deg or px to any values: // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); The entries are appended to the container <g> in the each() method, as follows. Compare this code to the HTML version: // sort distances distances = distances.sort((a,b) => d3.ascending(a.distance, b.distance)); // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); // barScales const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); // colorScale const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // decimal format const fmt = d3.format(\".2f\"); // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"width\", barScale(d.distance) + \"px\") .style(\"fill\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d.distance) + 105) .text(fmt(d.distance) + \" AU\"); }); Try the full code from SVG_Bar/9-load-json.html . If you open it in your browser, you will notice that the result is identical to the one you get with HTML_Bar/9-load-json.html . To explore more D3 features, in the next sections, we will use the SVG version of this bar chart. Updating Data Visualizations Once the data is bound to SVG elements, you can change the original data values and reflect the changes in the chart. Values can change immediately or transition smoothly. This section will provide an introduction on how to trigger data updates and configure smooth transitions using D3. First, we need more data. Let's change the code where we create our data object and include two more values that use the same dimensions: the aphelium (the longest distance between a planet and the sun) in the max property, and the perihelium (the shortest distance) in the min property. The distance is now stored in the avg property (see Updating/1-three-charts.html ): const planets = []; // this array will store all the data when loaded d3.json(\"../Data/sol_2016.json\") .then(function(data) { data.planets.forEach(function(obj) { planets.push({ name: obj.name, avg: obj.semiMajorAxisAU, max: obj.apheliumAU, min: obj.periheliumAU}); }); init(); }); You can create bar charts with any of these three values, or all of them. Let's create an application where the user can choose which chart to display using HTML buttons. The following is the HTML code for the page; it includes an <svg> element and some buttons: <body> <h1><span id=\"chart\">Average</span> distance from the Sun</h1> <svg class=\"bar-chart\"></svg> <form> <button type=\"button\" id=\"avg\">Average</button> <button type=\"button\" id=\"max\">Maximum</button> <button type=\"button\" id=\"min\">Minimum</button> </form> <script>...</script> </body> These buttons will be attached to event handlers. They will select the data that should be displayed in the bar charts. The following array relates a key to a title and a color. The key contains the name of a property from each element in the planets array. It's also used for the button IDs: const charts = [ {key: \"avg\", title: \"Average\", color: \"orange\"}, {key: \"max\", title: \"Maximum\", color: \"blue\"}, {key: \"min\", title: \"Minimum\", color: \"red\"}, ]; The chart object stores the dimensions of the current chart (which may have a variable height) and the chart that is currently displayed: const chart = { width: 800, height: 0, // the height is set after data is loaded current: charts[0] // chart to display first } These other global constants initialize scales, a formatting function, and a selection of the svg object: const barScale = d3.scaleLinear().range([0, 600]); const colorScale = d3.scaleLinear().range([0, 1]); const format = d3.format(\".2f\"); const svg = d3.select(\"svg.bar-chart\"); // the container SVG The init() function that is called right after the data is loaded and the planets array is populated performs basic initialization that requires the loaded data. In this case, it sets the height of the chart: function init() { // runs once chart.height = planets.length * 21; svg.attr(\"width\", chart.width) .attr(\"height\", chart.height); setupView(); // sets up scales; Not defined yet...see below // ... } The setupView() function called inside init() configures the current view. It disables the button that refers to the currently displayed view, replaces the <span> element in the title with the title of the current chart, sorts the planets array, and initializes the domains of the scales, based on the current data: function setupView() { // disable all buttons d3.selectAll(\"button\").property(\"disabled\", false); // enable only buttons that are not current chart d3.select(\"#\" + chart.current.key).property(\"disabled\", true); // update page title d3.select(\"#chart\").text(chart.current.title); // sort the planets using current data planets.sort((a,b) => d3.ascending(a[chart.current.key], b[chart.current.key])); // update scale domain with current data const maxValue = d3.max(planets, d => d[chart.current.key]); barScale.domain([0, maxValue]); colorScale.domain([0, maxValue]); } After setting up the view, the init() function uses the current data to render the chart. This code is identical to the code we used in the previous examples. The only difference is that it uses a key reference to access the data property: instead of d.avg , it uses d[chart.current.key] . This will allow the chart to reference other properties when the current key changes: function init() { // ... setupView(); svg.selectAll(\"g\") .data(planets) .enter().append(\"g\").attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0,${i * 21})`) .each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"height\", 20) .attr(\"width\", barScale(d[chart.current.key]) ) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key])) ) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } The result is shown as follows. It's the same chart we created before, with a title and some buttons that don't work yet: The previous example adapted to show three different charts. Code: Updating/1-three-charts.html. Handling events The on() method is used to handle events, and it can be called from any selection. The first parameter is a standard JavaScript event name string (such as click or mouseover ), and the second parameter is the handler function that will execute when the event happens. The following code obtains a selection containing all button objects and attaches an event handler to all of them. It obtains the id of the button that was clicked and uses it to change the current chart by assigning a corresponding object from the charts array. After changing the current chart, it calls the draw() function, which will update the chart. This code should be placed in a global context, since it only needs to run once: d3.selectAll(\"button\") .on(\"click\", function() { chart.current = charts.filter(c => c.key == this.id)[0]; draw(); }); The draw function in this example only prints the current array. You can use it to test whether the buttons are selecting the correct chart, as expected. The call to setupView() will disable/enable the buttons and update the chart's title according to the current view (see Updating/2-events.html ): function draw() { console.log(chart.current.key); setupView(); } Data updates To update the data on a selection, you just need to update the styles and attributes. If the data has changed, you should call the selection.data() method with the new data, and then update everything that depends on it, such as attributes and styles, and any functions called by them, such as scales. In our case, the data bound to the container g.entry object is the entire planets array, which may have been sorted in a different order (in the setupView() function). It can be updated simply by reassigning it to the selection: svg.selectAll(\"g.entry\").data(planets) Then, you need to update the attributes and styles, which depend on properties from this array that have changed, but before doing that, you need to update the scales' domains, since they are called from the style() and attr() methods, and their maximum value depends on the new data. The scales were updated in the setupView() function. The rest of the draw() function contains the data updates: function draw() { setupView(); // sorts data and updates scales svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\") .text(d.name); // the order may have changed d3.select(this).select(\".bar\") .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } Now, when you click any button, a new bar chart will be displayed. The chart reuses the same graphical elements, changing its colors, dimensions, and text contents. Bars are always sorted in ascending order (see Updating/3-updates.html ): Each button replaces the current chart with a new chart, reusing the same chart elements. Code: Updating/3-updates.html . Smooth transitions Instead of immediately replacing one chart with another, you can add transitions, so that they occur in smooth animations. Adding a transition is very easy. You just need to call the transition() method on a selection before setting the new properties and styles. Instead of changing the old data with the new data, it will interpolate intermediate values during a quarter of a second. Add it to your update selections, and you will notice that when you change the chart, the bars and labels will animate to their new sizes, colors, and positions. In our examples, transitions were added before changing the color and width of each bar, and before changing the position of each value label (see Updating/4-transitions.html ): svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\").text(d.name); d3.select(this).select(\".bar\") .transition() // 1) transition fill and width .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .transition() // 2) transition x position .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); If you want a slower transition, just add duration(value) after the transition() command, with a value in milliseconds. The following code will make the animation last one second (see Updating/5-durations.html ): d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); You can also configure a delay before the transition starts with the delay() method. It can receive a fixed value in milliseconds, or a callback function that will apply a different delay for each object. In the following example, each bar will wait an amount of milliseconds proportional to its array index (the second parameter in the each() function), making each bar animate in a sequence: .each(function (d,i) { // include the i (index) parameter // ... d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .delay(50 * i) // longer animations for each bar .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); // ... }); The following diagram shows a snapshot of the animation captured in the middle of a transition. Try it out and see the effects for yourself (see the code in Updating/6-delays.html ): Transition with delay. Code: Updating/6-delays.html . So far, you've used the most common features of D3, rendering charts in HTML and SVG. We will finish this chapter with a D3 visualization of a map of the world, using the same methods you have used in the previous examples.","title":"Chapter 3"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#selecting-and-binding-data","text":"","title":"Selecting and Binding Data"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#selecting-and-appending","text":"The selection methods select() and selectAll() receive a CSS selector expression and return a D3 handle for a node or a set of nodes. You can convert the D3 handle into a DOM object calling node() or nodes() . Add the following HTML to the <body> of your HTML file (or use Selecting/1-select.html): <p>See results in console log.</p> <div id=\"section\"> <p class=first>Paragraph 1</p> <p>Paragraph 2</p> </div> <p>Paragraph 3</p> Now you can use the JavaScript console (or a <script> block in your page) to type in the following code, which shows how select() and selectAll() can be used: const div = d3.select(\"#section\"); // selects element with the // \"section\" ID const domDiv = div.node(); // converts the object into a // DOM node const firstP = d3.select(\"p\"); // selects the first <p> // in the page const allParagraphs = d3.selectAll(\"p\"); // selects all four <p> nodes // in the page const allPDom = allParagraphs.nodes(); // converts selection into // DOM nodelist const sectionParagraphs = d3.select(\"div\") .selectAll(\"p\"); // selects the two <p> nodes // inside <div> A selection object serves as a handle to elements in your page. You can use it to change their styles, properties, attributes, classes, and contents, using methods that can receive static or dynamic values (using functions) as parameters. The following examples demonstrate some of these methods (see Selecting/2-attributes-styles.html ): div.style('border', 'solid blue 2px'); // draws blue border around <div> firstP.classed('big', true); // adds class \u2018.big\u2019 to first paragraph firstP.text('This is paragraph zero'); // replaces contents of first paragraph allParagraphs.style('font-weight', 'bold'); // applies style to set of paragraphs // this looks promising, however there is nothing tagged with class \"first\" so nothing happes d3.select('.first') // selects first element of class \u2018.first\u2019 .attr('title', 'Tooltip') // adds an attribute .style('font-variant', 'small-caps') // applies a style .html('This is a <b>bold</b> paragraph.'); // replaces contents with HTML The select() method will always return only one object (if the selector matches more items, only the first will be returned). The selectAll() method returns a collection, which can be iterated with the each() method, which receives a callback function, which receives up to three parameters. Inside the function, this refers to the current DOM element, and d3.select(this) wraps it in a D3 selection object, which can be manipulated with selection methods, as follows: sectionParagraphs.each(function(d, i) { // i is the index d3.select(this).classed('red', true); console.log(`Paragraph ${i}: `, this); // this refers to the current element }); ``` This code applies the class ```.red``` to each element of a collection. If you open the JavaScript console, it will print each element and its index (starting in zero). You can also use selection methods to modify the DOM tree. Calling ```remove()``` on a D3 selection removes elements in the selection. You can call ```append()``` on a selection to add an element as its last child, or use ```insert()``` to add the new element in a position determined by a selector passed as the second argument. These methods are demonstrated as follows (see ```Selecting/3-append-remove.html)```: ```js d3.select(\"#section\") // current context is element with id #section .insert(\"p\", '.first') // add a <p> before the child being of class \u2018.first\u2019 .append('a') // context is now <p>; inserts an <a> in <p> .attr('href', 'https://d3js.org') // context is now <a> .text(\"D3 website\"); // contents of the <a> element d3.select('div') // context is first <div> in page .select('p:last-of-type') // context is now last <p> in <div> .remove(); // context is detached <p> (parentNode is null) const div = d3.select('div') div.append(\"p\") // adds a new <p> as last child of selected <div> .text(\"New paragraph\"); // sets the text contents of <p> d3.select(\"body\") // selects the <body> element of the page .insert(\"h1\", \"*\") // adds a new <h1> as the first child (before *) .text(\"New title\"); // sets the text contents of <h1> d3.select(\"body\") .append(\"h2\") // adds a new <h2> as the last child .text(\"Footer\"); The append() and insert() methods can also be chained, since they return the nodes they added. You just have to pay attention to the current so you don't modify or add data to the wrong elements. The remove() method returns the removed selection (a node or a set of nodes) after it's detached from the DOM ( null parent). D3 selections are not limited to HTML. In fact, they are most commonly applied to SVG elements, but can also be used with any embedded XML. This was a brief introduction. In Chapter 4, Data Binding , we will explore these methods in detail.","title":"Selecting and appending"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#binding-data","text":"D3 makes it almost trivial to bind data to DOM elements. Data binding requires calling at least two of the following four methods in the context of a selection: data(array) or datum(object) : Receives an array or an object/value that contains data that should be mapped to DOM elements join() , enter() or exit() : Binds the data to DOM elements by populating an update array returned as a selection of elements that should be added or removed from the document The exit() method returns a selection of unbound elements. After an exit() , remove() is usually called, which removes the entire selection. The enter() method returns a selection of placeholders for new elements that need to be created. After an enter() , either append() or insert() is called, which connects the elements to the DOM tree. The join() method is a magic method that replaces enter() and exit() , automatically updating, removing, or adding elements, as necessary. Once data is connected to an element, the attributes, style, and contents of each element can be modified using the attr() , style() , text() , and html() methods that take a callback function as the second argument. This binding process is illustrated as follows: Let's try an example. Create an HTML page with this code in the <body> and open it in a browser (or use Binding/1-binding.html ): In a <script> block, create this array of numbers (or type it in your JavaScript console): const numbers = [6, 2, 5, 7, 9]; The following code will bind each number in the array to a paragraph. The data will be copied to an internal array and can be used to change the attributes, styles, or contents of the existing paragraphs: const selection = d3.select(\"#section\") .selectAll(\"p\") .data(numbers); The data() method returns the current selection. The value of each item in the array is received as the first parameter of a callback function in attr(),style(),text(), and so on. The text() method is used in the following code that replaces the contents of each paragraph with the first two values of the array: selection.text(d => d); // binds to existing paragraphs (update) But we need more paragraphs, since we have five elements of data. By calling enter() , a new selection is created with three more placeholder nodes, containing the remaining data, but not yet mapped to any element: const newSelection = selection.enter(); // binds data to a selection of placeholders We can't just call text() on the selection yet. First, we need to append or insert those elements into the DOM tree. You call append() only once, and it will be executed three times. Now, you can use text() to print the paragraph with the data: newSelection.append(\"p\") .text(d => d); // binds to new paragraphs (enter) Normally, the entire selecting-binding-appending process is written as a chain of commands. The following code achieves the same result, using the same array and paragraphs (see Binding/3-chain-text.html ): d3.select(\"body\") // select the body element .select(\"div\") // select the first div element inside <body> .selectAll(\"p\") // select all <p> elements (there are two) .data(numbers) // load the data from the numbers array .text(d => d) // set contents of existing paragraphs .enter() // bind remaining data to array with 3 elements .append(\"p\") // add 3 new <p> elements to end of <div> .text(d => d); // set the contents of the new paragraphs It's simpler (and more common, when using D3) to use JavaScript (and not HTML) to create all of the elements you need. After all, data is dynamic and you won't always know how many items you have to display. Most of the time, HTML is just used to provide the basic structure, such as the <body> tag, a container <div> , or <svg> . The following code achieves the same result starting with an empty <body> tag (and the same data array). The <div> and all the <p> are created using D3 commands (see Binding/4-empty-binding.html ): d3.select(\"body\") // select the body element .append(\"div\") // append a div element inside <body> .selectAll(\"p\") // select all <p> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"p\") // create 5 new <p> elements and add to end of <div> .text(d => d); // set the contents of the new paragraphs Note that even though there are no <p> elements in the page, the selectAll(\"p\") command is still necessary, since it provides the selection context for the data binding. The selectAll() command can use any CSS selectors to locate its elements, but it should return a selection containing the same type of elements added by the append() command. As you have seen, once you have data bound to DOM elements, you can use the data to change attributes, styles, classes, and properties, using callbacks. The callback function has a second parameter that contains the index of the data array, which is used in the following example to change the contents and style of list items: d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // create an enter array with 5 objects .append(\"li\") // create 5 new <li> elements and append to <ul> .text(function(d, i) { return \"Item \" + (i+1) + \": \" + d; }) .style(\"font-size\", function(d, i) { return ((i+2) * 5) + \"pt\"; }); The result is shown in the following screenshot: D3 used to create and style new elements from data. Code: Binding/5-callbacks.html . Callback functions also have a third parameter that contains the array of elements in the selection. You might need it if you want to obtain the current element when using arrow functions, since the this reference in these functions doesn't refer to the current element: .text( (d, i, nodes) => console.log(\"Current element: \" + nodes[i]) ); You can easily reuse the preceding code with different types of data. If you have an array of objects containing values of different types, you don't need to convert it to an array of numbers. You can directly pass the entire object array to the data() method, and later select which properties of each object you wish to use. For example, you can use practically the same code that was shown previously to display the contents of the following array: const distances = [ {name: \"Mercury\", distance: 0.387}, {name: \"Venus\", distance: 0.723}, {name: \"Earth\", distance: 1}, {name: \"Mars\", distance: 1.52}, {name: \"Jupiter\", distance: 5.2}, {name: \"Saturn\", distance: 9.54}, {name: \"Uranus\", distance: 19.2}, {name: \"Neptune\", distance: 30.1}, {name: \"Ceres\", distance: 2.765}, {name: \"Pluto\", distance: 39.481}, {name: \"Eris\", distance: 67.67}, {name: \"Haumea\", distance: 43}, {name: \"Makemake\", distance: 45.346} ]; The object array is received by the data() command. Each array item is available as the first parameter of the callback, which can be used to extract the property that contains the data (see Binding/6-object-array.html ): d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(distances) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"li\") // create <li> from enter selection and append to <ul> .text(function(d, i) { return d.name + \": \" + d.distance; }); Since D3 Version 5.8, you can replace enter().append(element) in simple binding operations, as shown with the join(element) method (see Binding/7-join.html ): d3.select(\"body\") .append(\"ul\") .selectAll(\"li\") .data(distances) .join(\"li\") // obtains enter selection and appends 5 new <li> elements .text(function(d, i) { return d.name + \": \" + d.distance; }); The preceding code will generate an HTML list with the data.","title":"Binding Data"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#creating-a-bar-chart-with-d3-and-html","text":"Now that you have had a quick introduction to D3's basic data-binding mechanisms, let's create some full data visualization examples that will demonstrate the power of D3. You should know HTML very well, so we will start with a simple bar chart using HTML and CSS. Later, we will repeat the exact same procedure using SVG, the main standard used by D3 to create visualizations. All the code for these examples is available in the GitHub repository for this chapter.","title":"Creating a bar chart with D3 and HTML"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#binding-data-to-html","text":"In this example, we will draw a horizontal bar chart using HTML <div> elements. The colors and length of each bar are controlled by CSS, which we can configure using the selection.style() command. We will use the object array containing planetary distances from the last example. First, let's create a style sheet with classes for the bar-chart and each individual bar : <style> .bar-chart { border: solid 1px gray; /* a gray border around the container */ position: relative; } .bar { height: 20px; background-color: orange; position: absolute; } </style> The bars are positioned absolutely. Since each bar has an equal height (20px), we need to position each bar slightly below the previous one. This can be achieved by setting the value for the CSS top property (in pixels) so that it contains the sum of the heights of all the previous bars, plus an extra pixel to keep them slightly apart. The top property can be dynamically calculated as proportional to each entry's array index, using a callback in selection.style() (see HTML_Bar/1-bar-chart.html ): d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // container div for the chart .style(\"height\", () => distances.length * 21 + \"px\") // set chart height .selectAll(\"div\").data(distances) // binds data .enter().append(\"div\") // appends a div for each data element .attr(\"class\", \"bar\") // these divs are the bars of the chart .style(\"top\", (d,i) => i * 21 + \"px\") // stacks bars .style(\"width\", \"100px\") // fixed width output: This prints all the bars, one on top of the other, but they all have the same static width! We need to use a callback to change the width of each individual bar. In a bar chart, the actual width should be proportional to the distance. A solution would be to multiply each value by 10. This will make the smallest bar three pixels wide, and the largest over 670 pixels: .style(\"width\", d => (d.distance * 10) + \"px\") Now, the lengths of the bars are proportional to the data values. This code is in HTML_Bar/2-bar-width.html . The result is shown as follows: HTML bar chart with D3: Drawing DIVs and using the data to change the CSS width. Code: HTML_Bar/2-bar-width.html","title":"Binding data to HTML"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#scales","text":"What if a new planet is discovered that is 10 times the largest distance? The bars wouldn't fit and their widths would need to be recalculated! D3 also provides a solution for this: scales . Scales are mappings between different dimensions. In our case, we have to deal with two dimensions: the dimension of the data, called the domain , and the dimension of the graphics context where the data will be displayed, called the range . Before using a scale, you have to configure its domain and range. There are several different kinds of scales in D3, including linear and logarithmic scales, with many configuration options. A scale is created with a special generator function available in the d3-scale module. To use this module, you need to load several dependencies, so let's replace the <script> tag with the default bundle: <script src=\"https://d3js.org/d3.v5.min.js\"></script> To create a scale function, you call a special generator function. A linear scale function can be created using the following: const barScale = d3.scaleLinear(); To use the scale function you created, you pass a value as the argument and receive the converted result: const result = barScale(45); // returns 45 This will return 45 , because the scale hasn't been configured yet (the default scale is 1:1). To configure the scale, you need to call two methods to set up the domain (data dimensions) and range (graphical dimensions). Each method receives an array. For example, we can set up the domain as follows: barScale.domain([0, 100]); // input domain from 0 to 100 This fits all the distances in our data. The range is set up in a similar fashion: barScale.range([0, 600]); // output domain from 0 to 600 This means that zero is mapped to zero, and 100 astronomical units (AU) is mapped to 600 (pixels). Intermediate values will be interpolated. If you now call barScale(45) , you will get 270 . We can use the scale to convert the distances in AU to pixels, replacing the previous expression with a scale conversion (see HTML_Bar/3-scales.html ): .style(\"width\", d => barScale(d.distance) + \"px\");","title":"Scales"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#array-utilities","text":"We chose 100 as the upper limit in our scale domain because it's larger than any of the distances in the data, but the choice was rather arbitrary. We could have chosen the largest value in the array. If you have hundreds of lines of data, you can use JavaScript array functions (see Chapter 1, Introduction ) to find out the maximum value. D3 also includes a collection of array manipulation functions that extend JavaScript's native functions. Some look similar but may be more efficient for data manipulation (for example, by ignoring null , NaN , and undefined values). In order to use them, you need the d3-array module (which is also part of the default bundle). Here, we changed the configuration of our barScale() function so that it uses the largest distance from the distances object array as the upper value for the domain. This is achieved with by calling the d3.max() function, which receives an array and an accessor function for each array element, as follows: const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); The d3.max() function will scan the distances array and compare the distance property of each object, returning the largest one. There are many more useful functions in d3-array , which we will cover in the next chapter. Two of them, d3.descending(a,b) and d3.ascending(a,b) , are used to provide a sorting rule for JavaScript's native sort() method. We can use it to sort the array by the distance: distances.sort((a,b) => d3.ascending(a.distance, b.distance)); See and run the code after these transformations in HTML_Bar/4-max-sort.html .","title":"Array utilities"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#adding-labels","text":"Since the <div> elements in HTML can contain text, you can call the text() method for each <div> and set its contents based on the data: d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // ...all previous code to build chart .text(d => d.distance); // display distance in the <div> With CSS, we can right-align the text and adjust the fonts. Since these styles are static and don't change with the data, instead of calling style() for each property, you should use a style sheet: <style> /* ... */ .bar { height: 20px; left: 100px; background-color: orange; position: absolute; text-align: right; padding: 0 5px; font-family: sans-serif; font-size: 9pt; } </style> As a result, the labels are placed inside the bars, as follows: Adding labels to the bars. Code: HTML_Bar/5-labels.html .","title":"Adding labels"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#more-labels-formatting-and-colors","text":"Each object in our data array also contains a name property that can be used to label each bar. To position text outside the bar, we will need to refactor the code so that each data entry contains a container <div> , which will be bound to the data values. This entry <div> will then contain three other <div> elements: a category label (the name of the planet), the bar, and a value label (the distance). Classes will be used to identify each <div> . The following style sheet contains the static properties for these elements: <style> .bar-chart { /* The container <div> for the entire chart */ border: solid 1px gray; position: relative; width: 800px; } .entry { /* a container <div> for each data entry */ position: absolute; width: 100%; } .bar { /* the colored rectangle */ height: 20px; top: 1px; left: 100px; background-color: orange; position: absolute; } .label { /* a text label */ padding: 4px 5px; font-family: sans-serif; font-size: 9pt; position: absolute; height: 20px; } .category { /* the category text label at left (name) */ text-align: right; width: 80px; } .value { /* the value text label at right (distance) */ text-align: left; } </style> Since each entry <div> has three children, we need to keep a reference to its selection so that the child elements can be appended. The following code saves references for each one of the container <div> elements. The chart constant contains a selection of the root <div> element, and the entries constant contains the selection of all entry <div> elements: // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); You can now use the entries constant to append the child elements to each entry <div> (see HTML_Bar/6-entries.html ): entries.append(\"div\").attr(\"class\", \"label category\") .text(d => d.name); entries.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", d => barScale(d.distance) + \"px\"); entries.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", d => (barScale(d.distance) + 100) + \"px\") .text(d => d.distance + \" AU\"); Each append() shown is called once for each entry. The attributes, style, and text are set using the data that was bound to the parent container. You can add child elements without having to break the selection chain with the selection.each() method, which calls a function for each entry. Inside it, you can obtain a selection to the current element using d3.select(this) . The following code produces the same result (see HTML_Bar/7-entries-each.html ): entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\"); entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d.distance + \" AU\"); }); We can improve the rendering of the labels by formatting the numbers to display only two decimal places, using the d3.format() generator function (from the d3-format module). The following code creates a function fmt() that can be used to format numbers: const fmt = d3.format(\".2f\"); Now, we can use it to format the distances: .text(d => fmt(d.distance) + \" AU\"); The final result is shown as follows: Adding category names and placing the formatted label values outside the bars. Code: HTML_Bar/7-entries-each.html","title":"More labels, formatting, and colors"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#changing-colors","text":"The d3-colors module contains functions to generate, convert, and transform colors. Passing any CSS-compatible color representation to d3.color() generates an object with methods that can be used to modify the color. The darker() and brighter() methods receive a value between 0 (no change) and 1 (maximum change) to adjust the lightness component of a color and return a hexadecimal color string. Let's use this feature to darken the bars when the distance from the sun increases. We will need a scale that maps the distance domain to the [0-1] range, which contains the values accepted by the darker() function: colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) Now, this colorScale can be used to generate different tones for the bars: entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) See the code in HTML_Bar/8-colors.html . The result is shown as follows: Changing bar colors with the distance. Code: HTML_Bar/8-colors.html","title":"Changing colors"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#loading-external-files","text":"Usually, your data will come from external files that need to be loaded with an Ajax request. You can, of course, use jQuery or ECMAScript fetch() commands (and promises, if you need to load multiple files). After loading, you will also need to parse the data and convert it into JavaScript arrays and objects. D3 again provides a more efficient solution in the d3-fetch module: a set of convenient methods for loading and parsing files in popular formats, such as XML, JSON, or CSV. The d3-fetch module is also included in the default bundle. The planetary data we used in the previous examples is actually part of a larger JSON file containing several properties for planets, asteroids, and satellites. The basic structure of this file ( Data/sol_2016.json ) is shown as follows: { \"star\":{\u2026}, \"planets\":[ { \"id\":\"p1\", \"name\":\"Mercury\", \"diameterKm\":4879, \"semiMajorAxisAU\":0.387, \u2026 },{ \"id\":\"p2\", \"name\":\"Venus\", \"diameterKm\":12104, \"semiMajorAxisAU\":0.723, \u2026 }, \u2026 } ``` You can load the data using the ```d3.json()``` function. After it loads and parses the file, it will become available in a callback provided as a parameter to the ```then()``` method (which is a JavaScript promise). We don't need all the loaded data. The distance used in our examples is stored in the property called ```semiMajorAxisAU```. After loading the file, we can filter the dataset to only save the ```semiMajorAxisAU``` and ```name``` properties. The following code demonstrates this. It loads the file, and it then uses the data obtained in the ```then()``` callback to loop through the planets array, adding only the chosen properties to a new object and pushing it into an array. The array is used to create an HTML list with these properties (see``` Loading/1-loading-json.html```): ```js //d3.json(\"../Data/sol_2016.json\") // if you down load it d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({ name: obj.name, distance: obj.semiMajorAxisAU }); }); draw(planets); }); function draw(distances) { d3.select(\"body\").append(\"ol\") .selectAll(\"li\") .data(distances) .join(\"li\") .text(d => d.name + \" (\" + d.distance + \" AU)\"); } To apply this to our chart, you just need to replace the contents of the draw() method shown with the entire code used in the previous examples (except for the distances array). See an example in HTML_Bar/9-load-json.html . As an exercise, try feeding the bar chart with a list of Jupiter's moons, instead of the planets (use data.planets[4].satellites to obtain the array, and the semiMajorAxisKm property as the distance, and d3.format(\",.0f\") to format the distance values). Full Example: d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({name: obj.name, distance: obj.semiMajorAxisAU}); }); draw(planets); }); function draw(distances) { const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances.sort((a,b) => d3.ascending(a.distance, b.distance))) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); entries.each(function(d) { const entry = d3.select(this); // the current entry const fmt = d3.format(\".2f\"); // format for each entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d => fmt(d.distance) + \" AU\"); }); }","title":"Loading external files"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#creating-a-bar-chart-with-d3-and-svg","text":"It's easy to create a horizontal bar chart in HTML. It's a bit trickier to make a vertical one, and quite challenging to create other charts, such as line charts, pie charts, and network diagrams, since HTML wasn't intended for vector graphics. While you can create such charts using HTML and CSS, it's not at all simple, nor is it recommended . You can, of course, use Canvas, but there are no graphical DOM elements to bind in Canvas. Canvas is used in D3 charts, but mostly for performance optimization. D3 works best with SVG . The same bar chart that we created with HTML can be created with SVG. Do you think you can do it? Why not try it as an exercise before proceeding? You already learned how to use D3 and HTML, and you know the basics of SVG rectangles, attributes, and styles. That's all you need! Most of the code is the same. You can reuse the scales, formatting function, sorting function, and JSON parsing code. The CSS will be simpler, since positioning is done in SVG. You only need to change the selection code, replacing the bar <div> element with <rect> , text label <div> elements with <text> , and container <div> elements with <g> . Remember to use fill (and not background-color) to fill the bars. The same step-by-step files that were used in the HTML example are available for the SVG example in the SVG_Bar/ folder . The following, final code produces exactly the same chart as the HTML version. This is the CSS style sheet that's used. Since all positioning is done in SVG, it's much smaller: <style> * { font-family: sans-serif; } .bar-chart { border: solid 1px gray; width: 800px; } .bar { height: 20px; fill: orange; } .label { font-size: 9pt; } .category { text-anchor: end; } </style> The main container is the <svg> element, which is appended to the body, as shown in the following code snippet. We renamed the chart object svg : // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); The containers used for each entry are <g> elements, which group several child elements and can have their own coordinate system configured using a transform . Although the transform property is supported in both CSS and SVG, you should use the SVG version (selected with the attr() method), because it considers pixel values and degrees as the default. You don't have to append deg or px to any values: // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); The entries are appended to the container <g> in the each() method, as follows. Compare this code to the HTML version: // sort distances distances = distances.sort((a,b) => d3.ascending(a.distance, b.distance)); // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); // barScales const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); // colorScale const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // decimal format const fmt = d3.format(\".2f\"); // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"width\", barScale(d.distance) + \"px\") .style(\"fill\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d.distance) + 105) .text(fmt(d.distance) + \" AU\"); }); Try the full code from SVG_Bar/9-load-json.html . If you open it in your browser, you will notice that the result is identical to the one you get with HTML_Bar/9-load-json.html . To explore more D3 features, in the next sections, we will use the SVG version of this bar chart.","title":"Creating a bar chart with D3 and SVG"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#updating-data-visualizations","text":"Once the data is bound to SVG elements, you can change the original data values and reflect the changes in the chart. Values can change immediately or transition smoothly. This section will provide an introduction on how to trigger data updates and configure smooth transitions using D3. First, we need more data. Let's change the code where we create our data object and include two more values that use the same dimensions: the aphelium (the longest distance between a planet and the sun) in the max property, and the perihelium (the shortest distance) in the min property. The distance is now stored in the avg property (see Updating/1-three-charts.html ): const planets = []; // this array will store all the data when loaded d3.json(\"../Data/sol_2016.json\") .then(function(data) { data.planets.forEach(function(obj) { planets.push({ name: obj.name, avg: obj.semiMajorAxisAU, max: obj.apheliumAU, min: obj.periheliumAU}); }); init(); }); You can create bar charts with any of these three values, or all of them. Let's create an application where the user can choose which chart to display using HTML buttons. The following is the HTML code for the page; it includes an <svg> element and some buttons: <body> <h1><span id=\"chart\">Average</span> distance from the Sun</h1> <svg class=\"bar-chart\"></svg> <form> <button type=\"button\" id=\"avg\">Average</button> <button type=\"button\" id=\"max\">Maximum</button> <button type=\"button\" id=\"min\">Minimum</button> </form> <script>...</script> </body> These buttons will be attached to event handlers. They will select the data that should be displayed in the bar charts. The following array relates a key to a title and a color. The key contains the name of a property from each element in the planets array. It's also used for the button IDs: const charts = [ {key: \"avg\", title: \"Average\", color: \"orange\"}, {key: \"max\", title: \"Maximum\", color: \"blue\"}, {key: \"min\", title: \"Minimum\", color: \"red\"}, ]; The chart object stores the dimensions of the current chart (which may have a variable height) and the chart that is currently displayed: const chart = { width: 800, height: 0, // the height is set after data is loaded current: charts[0] // chart to display first } These other global constants initialize scales, a formatting function, and a selection of the svg object: const barScale = d3.scaleLinear().range([0, 600]); const colorScale = d3.scaleLinear().range([0, 1]); const format = d3.format(\".2f\"); const svg = d3.select(\"svg.bar-chart\"); // the container SVG The init() function that is called right after the data is loaded and the planets array is populated performs basic initialization that requires the loaded data. In this case, it sets the height of the chart: function init() { // runs once chart.height = planets.length * 21; svg.attr(\"width\", chart.width) .attr(\"height\", chart.height); setupView(); // sets up scales; Not defined yet...see below // ... } The setupView() function called inside init() configures the current view. It disables the button that refers to the currently displayed view, replaces the <span> element in the title with the title of the current chart, sorts the planets array, and initializes the domains of the scales, based on the current data: function setupView() { // disable all buttons d3.selectAll(\"button\").property(\"disabled\", false); // enable only buttons that are not current chart d3.select(\"#\" + chart.current.key).property(\"disabled\", true); // update page title d3.select(\"#chart\").text(chart.current.title); // sort the planets using current data planets.sort((a,b) => d3.ascending(a[chart.current.key], b[chart.current.key])); // update scale domain with current data const maxValue = d3.max(planets, d => d[chart.current.key]); barScale.domain([0, maxValue]); colorScale.domain([0, maxValue]); } After setting up the view, the init() function uses the current data to render the chart. This code is identical to the code we used in the previous examples. The only difference is that it uses a key reference to access the data property: instead of d.avg , it uses d[chart.current.key] . This will allow the chart to reference other properties when the current key changes: function init() { // ... setupView(); svg.selectAll(\"g\") .data(planets) .enter().append(\"g\").attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0,${i * 21})`) .each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"height\", 20) .attr(\"width\", barScale(d[chart.current.key]) ) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key])) ) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } The result is shown as follows. It's the same chart we created before, with a title and some buttons that don't work yet: The previous example adapted to show three different charts. Code: Updating/1-three-charts.html.","title":"Updating Data Visualizations"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#handling-events","text":"The on() method is used to handle events, and it can be called from any selection. The first parameter is a standard JavaScript event name string (such as click or mouseover ), and the second parameter is the handler function that will execute when the event happens. The following code obtains a selection containing all button objects and attaches an event handler to all of them. It obtains the id of the button that was clicked and uses it to change the current chart by assigning a corresponding object from the charts array. After changing the current chart, it calls the draw() function, which will update the chart. This code should be placed in a global context, since it only needs to run once: d3.selectAll(\"button\") .on(\"click\", function() { chart.current = charts.filter(c => c.key == this.id)[0]; draw(); }); The draw function in this example only prints the current array. You can use it to test whether the buttons are selecting the correct chart, as expected. The call to setupView() will disable/enable the buttons and update the chart's title according to the current view (see Updating/2-events.html ): function draw() { console.log(chart.current.key); setupView(); }","title":"Handling events"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#data-updates","text":"To update the data on a selection, you just need to update the styles and attributes. If the data has changed, you should call the selection.data() method with the new data, and then update everything that depends on it, such as attributes and styles, and any functions called by them, such as scales. In our case, the data bound to the container g.entry object is the entire planets array, which may have been sorted in a different order (in the setupView() function). It can be updated simply by reassigning it to the selection: svg.selectAll(\"g.entry\").data(planets) Then, you need to update the attributes and styles, which depend on properties from this array that have changed, but before doing that, you need to update the scales' domains, since they are called from the style() and attr() methods, and their maximum value depends on the new data. The scales were updated in the setupView() function. The rest of the draw() function contains the data updates: function draw() { setupView(); // sorts data and updates scales svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\") .text(d.name); // the order may have changed d3.select(this).select(\".bar\") .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } Now, when you click any button, a new bar chart will be displayed. The chart reuses the same graphical elements, changing its colors, dimensions, and text contents. Bars are always sorted in ascending order (see Updating/3-updates.html ): Each button replaces the current chart with a new chart, reusing the same chart elements. Code: Updating/3-updates.html .","title":"Data updates"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#smooth-transitions","text":"Instead of immediately replacing one chart with another, you can add transitions, so that they occur in smooth animations. Adding a transition is very easy. You just need to call the transition() method on a selection before setting the new properties and styles. Instead of changing the old data with the new data, it will interpolate intermediate values during a quarter of a second. Add it to your update selections, and you will notice that when you change the chart, the bars and labels will animate to their new sizes, colors, and positions. In our examples, transitions were added before changing the color and width of each bar, and before changing the position of each value label (see Updating/4-transitions.html ): svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\").text(d.name); d3.select(this).select(\".bar\") .transition() // 1) transition fill and width .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .transition() // 2) transition x position .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); If you want a slower transition, just add duration(value) after the transition() command, with a value in milliseconds. The following code will make the animation last one second (see Updating/5-durations.html ): d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); You can also configure a delay before the transition starts with the delay() method. It can receive a fixed value in milliseconds, or a callback function that will apply a different delay for each object. In the following example, each bar will wait an amount of milliseconds proportional to its array index (the second parameter in the each() function), making each bar animate in a sequence: .each(function (d,i) { // include the i (index) parameter // ... d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .delay(50 * i) // longer animations for each bar .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); // ... }); The following diagram shows a snapshot of the animation captured in the middle of a transition. Try it out and see the effects for yourself (see the code in Updating/6-delays.html ): Transition with delay. Code: Updating/6-delays.html . So far, you've used the most common features of D3, rendering charts in HTML and SVG. We will finish this chapter with a D3 visualization of a map of the world, using the same methods you have used in the previous examples.","title":"Smooth transitions"},{"location":"notes/javascript/D3js/Rocha/chapter_4/chapter-4/","text":"Not started","title":"Chapter 4"},{"location":"notes/javascript/D3js/Rocha/chapter_4/chapter-4/#not-started","text":"","title":"Not started"},{"location":"notes/javascript/D3js/d3js.org/basics/","text":"Basics of D3 Source square.github.io and the version of D3 is v5 from https://d3js.org/ I'll be using this template that you can use as well. Paste html code into the <body> and the css and js will be in separate files. Example: <!DOCTYPE html> <html> <head> <title>Title</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"styles.css\"> <script src=\"https://d3js.org/d3.v5.min.js\"></script> </head> <body> <!-- Insert HTML here!!! --> <script type=\"text/javascript\" src=\"javascript.js\"></script> </body> </html> DOM API versus the D3 API A comparison of the DOM API for the regular javascript and D3: Ex 1: Basic Selection HTML <div> <p>Normal paragraph</p> <p class=\"red\">Red paragraph</p> </div> <ol> <li id=\"some-id\">Unique element</li> <li>Another list element</li> <li> <p>Paragraph inside list element</p> <p>Second paragraph</p> </li> </ol> DOM API d3.select('#some-id')['_groups'][0][0] // <li id=\"some-id\">Unique element</li> d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" D3 API d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" Ex 2: CSS HTML <h1 id=\"click-me\"> Click on me! </h1> <p class=\"hover-me\"> Hover over me! </p> <p class=\"hover-me\"> OK now hover over here! </p> <p class=\"hover-me\"> Hover here too! </p> DOM API // DOM API var clickMe = document.getElementById('click-me'); clickMe.onclick = function() { if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'red'; } } var hoverMe = document.getElementsByClassName('hover-me'); for(var i=0; i<hoverMe.length;i++){ hoverMe[i].onmouseover = function(){ if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'yellow'; } } } D3 DOM // D3 Selection API. Note: it attaches the // callbacks to each element in the selection d3.selectAll('.hover-me') .on('mouseover', function() { this.style.backgroundColor = 'yellow'; }) .on('mouseleave', function() { this.style.backgroundColor = ''; }); Note: In the D3 examples, the methods on the selection can chain (that is, they return themselves, so we can group them visually). Ex 3: SVG SVG (Scalable Vector Graphics) is an XML format used for drawing. You can think of SVG in a lot of the same terms as the DOM \u2013 there are elements with parents and children and attributes, and you can respond to the same mouse/touch events. Even CSS styles and selectors can apply to SVG elements. The CSS attribute names for SVG come from the SVG definition, so they are sometimes different from their HTML brethren. (For example, to change the background color of a div to red you would select it then set background-color: red but to get the same effect on an SVG rectangle you would instead use the attribute fill: red since an SVG rect doesn\u2019t respond to background-color for styling.) SVG defines tags for lots of basic shapes, like <rect> and <circle> and <line> . HTML <svg width=\"300\" height=\"180\"> <circle cx=\"30\" cy=\"50\" r=\"25\" /> <circle cx=\"90\" cy=\"50\" r=\"25\" class=\"red\" /> <circle cx=\"150\" cy=\"50\" r=\"25\" class=\"fancy\" /> <rect x=\"10\" y=\"80\" width=\"40\" height=\"40\" fill=\"steelBlue\" /> <rect x=\"70\" y=\"80\" width=\"40\" height=\"40\" style=\"fill: steelBlue\" /> <rect x=\"130\" y=\"80\" width=\"40\" height=\"40\" class=\"fancy\" /> </svg> CSS .red { fill: red; /* not background-color! */ } .fancy { fill: none; stroke: black; /* similar to border-color */ stroke-width: 3pt; /* similar to border-width */ stroke-dasharray: 3,5,10; } Ex4: <g> tag Where HTML has the and tags, SVG has the tag for an arbitrary group. You\u2019ll see a lot in D3 examples. They\u2019re great for applying styles to a group (including re-positioning the groups). The tag is good for simple labels. The tag is powerful but complex, it can be used for either lines or arbitrary filled-in shapes depending on the styling. <svg width=\"300\" height=\"180\"> <g transform=\"translate(5, 15)\"> <text x=\"0\" y=\"0\">Howdy!</text> </g> <g transform=\"translate(5, 55)\"> <!-- M: move to (jump) L: line to Q: curve to (quadratic) --> <path d=\"M0,50 L50,0 Q100,0 100,50\" fill=\"none\" stroke-width=\"3\" stroke=\"black\" /> </g> <g transform=\"translate(5, 105)\"> <!-- C: curve to (cubic) Z: close shape --> <path d=\"M0,100 C0,0 25,0 125,100 Z\" fill=\"black\" /> </g> </svg>","title":"d3js.org"},{"location":"notes/javascript/D3js/d3js.org/basics/#basics-of-d3","text":"Source square.github.io and the version of D3 is v5 from https://d3js.org/ I'll be using this template that you can use as well. Paste html code into the <body> and the css and js will be in separate files. Example: <!DOCTYPE html> <html> <head> <title>Title</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"styles.css\"> <script src=\"https://d3js.org/d3.v5.min.js\"></script> </head> <body> <!-- Insert HTML here!!! --> <script type=\"text/javascript\" src=\"javascript.js\"></script> </body> </html>","title":"Basics of D3"},{"location":"notes/javascript/D3js/d3js.org/basics/#dom-api-versus-the-d3-api","text":"A comparison of the DOM API for the regular javascript and D3:","title":"DOM API versus the D3 API"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-1-basic-selection","text":"HTML <div> <p>Normal paragraph</p> <p class=\"red\">Red paragraph</p> </div> <ol> <li id=\"some-id\">Unique element</li> <li>Another list element</li> <li> <p>Paragraph inside list element</p> <p>Second paragraph</p> </li> </ol> DOM API d3.select('#some-id')['_groups'][0][0] // <li id=\"some-id\">Unique element</li> d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" D3 API d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\"","title":"Ex 1: Basic Selection"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-2-css","text":"HTML <h1 id=\"click-me\"> Click on me! </h1> <p class=\"hover-me\"> Hover over me! </p> <p class=\"hover-me\"> OK now hover over here! </p> <p class=\"hover-me\"> Hover here too! </p> DOM API // DOM API var clickMe = document.getElementById('click-me'); clickMe.onclick = function() { if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'red'; } } var hoverMe = document.getElementsByClassName('hover-me'); for(var i=0; i<hoverMe.length;i++){ hoverMe[i].onmouseover = function(){ if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'yellow'; } } } D3 DOM // D3 Selection API. Note: it attaches the // callbacks to each element in the selection d3.selectAll('.hover-me') .on('mouseover', function() { this.style.backgroundColor = 'yellow'; }) .on('mouseleave', function() { this.style.backgroundColor = ''; }); Note: In the D3 examples, the methods on the selection can chain (that is, they return themselves, so we can group them visually).","title":"Ex 2: CSS"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-3-svg","text":"SVG (Scalable Vector Graphics) is an XML format used for drawing. You can think of SVG in a lot of the same terms as the DOM \u2013 there are elements with parents and children and attributes, and you can respond to the same mouse/touch events. Even CSS styles and selectors can apply to SVG elements. The CSS attribute names for SVG come from the SVG definition, so they are sometimes different from their HTML brethren. (For example, to change the background color of a div to red you would select it then set background-color: red but to get the same effect on an SVG rectangle you would instead use the attribute fill: red since an SVG rect doesn\u2019t respond to background-color for styling.) SVG defines tags for lots of basic shapes, like <rect> and <circle> and <line> . HTML <svg width=\"300\" height=\"180\"> <circle cx=\"30\" cy=\"50\" r=\"25\" /> <circle cx=\"90\" cy=\"50\" r=\"25\" class=\"red\" /> <circle cx=\"150\" cy=\"50\" r=\"25\" class=\"fancy\" /> <rect x=\"10\" y=\"80\" width=\"40\" height=\"40\" fill=\"steelBlue\" /> <rect x=\"70\" y=\"80\" width=\"40\" height=\"40\" style=\"fill: steelBlue\" /> <rect x=\"130\" y=\"80\" width=\"40\" height=\"40\" class=\"fancy\" /> </svg> CSS .red { fill: red; /* not background-color! */ } .fancy { fill: none; stroke: black; /* similar to border-color */ stroke-width: 3pt; /* similar to border-width */ stroke-dasharray: 3,5,10; }","title":"Ex 3: SVG"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex4-g-tag","text":"Where HTML has the and tags, SVG has the tag for an arbitrary group. You\u2019ll see a lot in D3 examples. They\u2019re great for applying styles to a group (including re-positioning the groups). The tag is good for simple labels. The tag is powerful but complex, it can be used for either lines or arbitrary filled-in shapes depending on the styling. <svg width=\"300\" height=\"180\"> <g transform=\"translate(5, 15)\"> <text x=\"0\" y=\"0\">Howdy!</text> </g> <g transform=\"translate(5, 55)\"> <!-- M: move to (jump) L: line to Q: curve to (quadratic) --> <path d=\"M0,50 L50,0 Q100,0 100,50\" fill=\"none\" stroke-width=\"3\" stroke=\"black\" /> </g> <g transform=\"translate(5, 105)\"> <!-- C: curve to (cubic) Z: close shape --> <path d=\"M0,100 C0,0 25,0 125,100 Z\" fill=\"black\" /> </g> </svg>","title":"Ex4: &lt;g&gt; tag"},{"location":"notes/javascript/NodeJS/issues_solved/","text":"Issues Solved Cannot upgrade Node.js using n How to Update Node.js to Latest Version","title":"Issues Solved"},{"location":"notes/javascript/NodeJS/issues_solved/#issues-solved","text":"Cannot upgrade Node.js using n How to Update Node.js to Latest Version","title":"Issues Solved"},{"location":"notes/javascript/Vue/links_to_checkout/","text":"Links To Checkout For Later Single Page app in Flask and Vue Similar tutorial related to the single_page_app(Vue tutorial with Flask) Component Architecture Code Sandbox Google Maps Demo Global Event Bus \"this\" is undefined in Vue Component How to catch Google Map Event with listener in vue.js","title":"Links To Checkout"},{"location":"notes/javascript/Vue/links_to_checkout/#links-to-checkout-for-later","text":"Single Page app in Flask and Vue Similar tutorial related to the single_page_app(Vue tutorial with Flask) Component Architecture Code Sandbox Google Maps Demo Global Event Bus \"this\" is undefined in Vue Component How to catch Google Map Event with listener in vue.js","title":"Links To Checkout For Later"},{"location":"notes/javascript/Vue/single_page_app/basics/","text":"Vue and Flask Single Page App Source testdriven ...I should fill this in.","title":"Vue - Flask Integration"},{"location":"notes/javascript/Vue/single_page_app/basics/#vue-and-flask-single-page-app","text":"Source testdriven ...I should fill this in.","title":"Vue and Flask Single Page App"},{"location":"notes/kubernetes/azure_ingress_cert_static_ip/","text":"Azure Ingress\\Cert for Static IP Create an ingress controller myResourceGroup=\"flask_demo\" myAKSCluster=\"flaskAKSCluster\" az aks show --resource-group $myResourceGroup --name $myAKSCluster --query nodeResourceGroup -o tsv Node Resource Group: MC_flask_demo_flaskAKSCluster_eastus Next, create a public IP address with the static allocation method using the az network public-ip create command . az network public-ip create --name --resource-group [--allocation-method {Dynamic, Static}] [--dns-name] [--idle-timeout] [--ip-tags] [--location] [--public-ip-prefix] [--reverse-fqdn] [--sku {Basic, Standard}] [--subscription] [--tags] [--version {IPv4, IPv6}] [--zone {1, 2, 3}] The following example creates a public IP address named myAKSPublicIP in the AKS cluster resource group obtained in the previous step: az network public-ip create --resource-group MC_flask_demo_flaskAKSCluster_eastus --name myAKSPublicIP --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv Public IP: 20.62.186.98 The above commands create an IP address that will be deleted if you delete your AKS cluster. Now deploy the nginx-ingress chart with Helm. You must pass two additional parameters to the Helm release so the ingress controller is made aware both of the static IP address of the load balancer to be allocated to the ingress controller service, and of the DNS name label being applied to the public IP address resource. For the HTTPS certificates to work correctly, a DNS name label is used to configure an FQDN for the ingress controller IP address. Add the --set controller.service.loadBalancerIP parameter. Specify your own public IP address that was created in the previous step. Add the --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\" parameter. Specify a DNS name label to be applied to the public IP address that was created in the previous step. Tip: The following example creates a Kubernetes namespace for the ingress resources named ingress-basic . Specify a namespace for your own environment as needed. If your AKS cluster is not RBAC enabled, add --set rbac.create=false to the Helm commands. # Create a namespace for your ingress resources kubectl create namespace ingress-basic # Add the ingress-nginx repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx # Use Helm to deploy an NGINX ingress controller STATIC_IP=\"20.62.186.98\" helm install nginx-ingress ingress-nginx/ingress-nginx \\ --namespace ingress-basic \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=$STATIC_IP \\ --set rbac.create=false \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"anything-will-do\" No ingress rules have been created yet, so the NGINX ingress controller's default 404 page is displayed if you browse to the public IP address. Ingress rules are configured in the following steps. az network public-ip list --resource-group mc_flask_demo_flaskakscluster_eastus --query \"[?name=='myAKSPublicIP'].[dnsSettings.fqdn]\" -o tsv anything-will-do.eastus.cloudapp.azure.com Install cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace ingress-basic cert-manager.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ cert-manager \\ --namespace \"ingress-basic\" \\ --version \"v0.16.1\" \\ --set installCRDs=true \\ --set rbac.create=false \\ --set nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ jetstack/cert-manager","title":"Azure Ingress\\Cert\\Static IP"},{"location":"notes/kubernetes/azure_ingress_cert_static_ip/#azure-ingresscert-for-static-ip","text":"","title":"Azure Ingress\\Cert for Static IP"},{"location":"notes/kubernetes/azure_ingress_cert_static_ip/#create-an-ingress-controller","text":"myResourceGroup=\"flask_demo\" myAKSCluster=\"flaskAKSCluster\" az aks show --resource-group $myResourceGroup --name $myAKSCluster --query nodeResourceGroup -o tsv Node Resource Group: MC_flask_demo_flaskAKSCluster_eastus Next, create a public IP address with the static allocation method using the az network public-ip create command . az network public-ip create --name --resource-group [--allocation-method {Dynamic, Static}] [--dns-name] [--idle-timeout] [--ip-tags] [--location] [--public-ip-prefix] [--reverse-fqdn] [--sku {Basic, Standard}] [--subscription] [--tags] [--version {IPv4, IPv6}] [--zone {1, 2, 3}] The following example creates a public IP address named myAKSPublicIP in the AKS cluster resource group obtained in the previous step: az network public-ip create --resource-group MC_flask_demo_flaskAKSCluster_eastus --name myAKSPublicIP --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv Public IP: 20.62.186.98 The above commands create an IP address that will be deleted if you delete your AKS cluster. Now deploy the nginx-ingress chart with Helm. You must pass two additional parameters to the Helm release so the ingress controller is made aware both of the static IP address of the load balancer to be allocated to the ingress controller service, and of the DNS name label being applied to the public IP address resource. For the HTTPS certificates to work correctly, a DNS name label is used to configure an FQDN for the ingress controller IP address. Add the --set controller.service.loadBalancerIP parameter. Specify your own public IP address that was created in the previous step. Add the --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\" parameter. Specify a DNS name label to be applied to the public IP address that was created in the previous step. Tip: The following example creates a Kubernetes namespace for the ingress resources named ingress-basic . Specify a namespace for your own environment as needed. If your AKS cluster is not RBAC enabled, add --set rbac.create=false to the Helm commands. # Create a namespace for your ingress resources kubectl create namespace ingress-basic # Add the ingress-nginx repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx # Use Helm to deploy an NGINX ingress controller STATIC_IP=\"20.62.186.98\" helm install nginx-ingress ingress-nginx/ingress-nginx \\ --namespace ingress-basic \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=$STATIC_IP \\ --set rbac.create=false \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"anything-will-do\" No ingress rules have been created yet, so the NGINX ingress controller's default 404 page is displayed if you browse to the public IP address. Ingress rules are configured in the following steps. az network public-ip list --resource-group mc_flask_demo_flaskakscluster_eastus --query \"[?name=='myAKSPublicIP'].[dnsSettings.fqdn]\" -o tsv anything-will-do.eastus.cloudapp.azure.com","title":"Create an ingress controller"},{"location":"notes/kubernetes/azure_ingress_cert_static_ip/#install-cert-manager","text":"# Label the cert-manager namespace to disable resource validation kubectl label namespace ingress-basic cert-manager.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ cert-manager \\ --namespace \"ingress-basic\" \\ --version \"v0.16.1\" \\ --set installCRDs=true \\ --set rbac.create=false \\ --set nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ jetstack/cert-manager","title":"Install cert-manager"},{"location":"notes/kubernetes/basics/","text":"Cheat Sheet Source kubernetes Namespace: Pod: Container: My main commands: # List all pods in all namespaces $ kubectl get pods -A $ kubectl get pods --all-namespaces # Additional Info: $ kubectl get pods -A -o wide # Get logs for a pod $ kubectl logs my-pod $ kubectl logs my-pod -c my-container $ kubectl logs -f my-pod -c my-container # Get interactive bash shell $ kubectl exec -it my-pod -c my-container -- bash $ kubectl get ingress # Get nodes $ kubectl get nodes // get pods var pods = document.getElementsByClassName(\"pod\") // get containers var containers = document.getElementsByClassName(\"pod-container\") // get inputs var namespace = document.getElementById(\"namespace\"); var pod = document.getElementById('pod'); var container = document.getElementById('container'); // add listeners namespace.addEventListener('input',updateValue) pod.addEventListener('input',updateValue) container.addEventListener('input',updateValue) // define what to update function updateValue(e) { // debugger // loop through both to switch in the new name switch (e.currentTarget.id) { case \"namespace\": console.log(\"placeholder for namespaces\") break; case \"pod\": for(let i=0;i < pods.length;i++){ pods[i].innerText = e.currentTarget.value } break; case \"container\": for(let j=0;j < containers.length;j++){ containers[j].innerText = e.currentTarget.value } break; } } Viewing, finding resources # Get commands with basic output kubectl get services # List all services in the namespace kubectl get pods --all-namespaces # List all pods in all namespaces kubectl get pods -o wide # List all pods in the current namespace, with more details kubectl get deployment my-dep # List a particular deployment kubectl get pods # List all pods in the namespace kubectl get pod my-pod -o yaml # Get a pod's YAML # Describe commands with verbose output kubectl describe nodes my-node kubectl describe pods my-pod # List Services Sorted by Name kubectl get services --sort-by=.metadata.name # List pods Sorted by Restart Count kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # List PersistentVolumes sorted by capacity kubectl get pv --sort-by=.spec.capacity.storage # Get the version label of all pods with label app=cassandra kubectl get pods --selector=app=cassandra -o \\ jsonpath='{.items[*].metadata.labels.version}' # Retrieve the value of a key with dots, e.g. 'ca.crt' kubectl get configmap myconfig \\ -o jsonpath='{.data.ca\\.crt}' # Get all worker nodes (use a selector to exclude results that have a label # named 'node-role.kubernetes.io/master') kubectl get node --selector='!node-role.kubernetes.io/master' # Get all running pods in the namespace kubectl get pods --field-selector=status.phase=Running # Get ExternalIPs of all nodes kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # List Names of Pods that belong to Particular RC # \"jq\" command useful for transformations that are too complex for jsonpath, it can be found at https://stedolan.github.io/jq/ sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?} echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # Show labels for all pods (or any other Kubernetes object that supports labelling) kubectl get pods --show-labels # Check which nodes are ready JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\" # List all Secrets currently in use by a pod kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq # List all containerIDs of initContainer of all pods # Helpful when cleaning up stopped containers, while avoiding removal of initContainers. kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3 # List Events sorted by timestamp kubectl get events --sort-by=.metadata.creationTimestamp # Compares the current state of the cluster against the state that the cluster would be in if the manifest was applied. kubectl diff -f ./my-manifest.yaml # Produce a period-delimited tree of all keys returned for nodes # Helpful when locating a key within a complex nested JSON structure kubectl get nodes -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")' # Produce a period-delimited tree of all keys returned for pods, etc kubectl get pods -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")' Deleting resources kubectl delete -f ./pod.json # Delete a pod using the type and name specified in pod.json kubectl delete pod,service baz foo # Delete pods and services with same names \"baz\" and \"foo\" kubectl delete pods,services -l name=myLabel # Delete pods and services with label name=myLabel kubectl -n my-ns delete pod,svc --all # Delete all pods and services in namespace my-ns, # Delete all pods matching the awk pattern1 or pattern2 kubectl get pods -n mynamespace --no-headers=true | awk '/pattern1|pattern2/{print $1}' | xargs kubectl delete -n mynamespace pod Interacting with running Pods kubectl logs my-pod # dump pod logs (stdout) kubectl logs -l name=myLabel # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod --previous # dump pod logs (stdout) for a previous instantiation of a container kubectl logs my-pod -c my-container # dump pod container logs (stdout, multi-container case) kubectl logs -l name=myLabel -c my-container # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod -c my-container --previous # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container kubectl logs -f my-pod # stream pod logs (stdout) kubectl logs -f my-pod -c my-container # stream pod container logs (stdout, multi-container case) kubectl logs -f -l name=myLabel --all-containers # stream all pods logs with label name=myLabel (stdout) kubectl run -i --tty busybox --image=busybox -- sh # Run pod as interactive shell kubectl run nginx --image=nginx -n mynamespace # Run pod nginx in a specific namespace kubectl run nginx --image=nginx # Run pod nginx and write its spec into a file called pod.yaml --dry-run=client -o yaml > pod.yaml kubectl attach my-pod -i # Attach to Running Container kubectl port-forward my-pod 5000:6000 # Listen on port 5000 on the local machine and forward to port 6000 on my-pod kubectl exec my-pod -- ls / # Run command in existing pod (1 container case) kubectl exec my-pod -c my-container -- ls / # Run command in existing pod (multi-container case) kubectl top pod POD_NAME --containers # Show metrics for a given pod and its containers ## Interacting with Nodes and cluster ```bash kubectl cordon my-node # Mark my-node as unschedulable kubectl drain my-node # Drain my-node in preparation for maintenance kubectl uncordon my-node # Mark my-node as schedulable kubectl top node my-node # Show metrics for a given node kubectl cluster-info # Display addresses of the master and services kubectl cluster-info dump # Dump current cluster state to stdout kubectl cluster-info dump --output-directory=/path/to/cluster-state # Dump current cluster state to /path/to/cluster-state # If a taint with that key and effect already exists, its value is replaced as specified. kubectl taint nodes foo dedicated=special-user:NoSchedule","title":"Cheat Sheet"},{"location":"notes/kubernetes/basics/#cheat-sheet","text":"Source kubernetes Namespace: Pod: Container:","title":"Cheat Sheet"},{"location":"notes/kubernetes/basics/#my-main-commands","text":"# List all pods in all namespaces $ kubectl get pods -A $ kubectl get pods --all-namespaces # Additional Info: $ kubectl get pods -A -o wide # Get logs for a pod $ kubectl logs my-pod $ kubectl logs my-pod -c my-container $ kubectl logs -f my-pod -c my-container # Get interactive bash shell $ kubectl exec -it my-pod -c my-container -- bash $ kubectl get ingress # Get nodes $ kubectl get nodes // get pods var pods = document.getElementsByClassName(\"pod\") // get containers var containers = document.getElementsByClassName(\"pod-container\") // get inputs var namespace = document.getElementById(\"namespace\"); var pod = document.getElementById('pod'); var container = document.getElementById('container'); // add listeners namespace.addEventListener('input',updateValue) pod.addEventListener('input',updateValue) container.addEventListener('input',updateValue) // define what to update function updateValue(e) { // debugger // loop through both to switch in the new name switch (e.currentTarget.id) { case \"namespace\": console.log(\"placeholder for namespaces\") break; case \"pod\": for(let i=0;i < pods.length;i++){ pods[i].innerText = e.currentTarget.value } break; case \"container\": for(let j=0;j < containers.length;j++){ containers[j].innerText = e.currentTarget.value } break; } }","title":"My main commands:"},{"location":"notes/kubernetes/basics/#viewing-finding-resources","text":"# Get commands with basic output kubectl get services # List all services in the namespace kubectl get pods --all-namespaces # List all pods in all namespaces kubectl get pods -o wide # List all pods in the current namespace, with more details kubectl get deployment my-dep # List a particular deployment kubectl get pods # List all pods in the namespace kubectl get pod my-pod -o yaml # Get a pod's YAML # Describe commands with verbose output kubectl describe nodes my-node kubectl describe pods my-pod # List Services Sorted by Name kubectl get services --sort-by=.metadata.name # List pods Sorted by Restart Count kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # List PersistentVolumes sorted by capacity kubectl get pv --sort-by=.spec.capacity.storage # Get the version label of all pods with label app=cassandra kubectl get pods --selector=app=cassandra -o \\ jsonpath='{.items[*].metadata.labels.version}' # Retrieve the value of a key with dots, e.g. 'ca.crt' kubectl get configmap myconfig \\ -o jsonpath='{.data.ca\\.crt}' # Get all worker nodes (use a selector to exclude results that have a label # named 'node-role.kubernetes.io/master') kubectl get node --selector='!node-role.kubernetes.io/master' # Get all running pods in the namespace kubectl get pods --field-selector=status.phase=Running # Get ExternalIPs of all nodes kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # List Names of Pods that belong to Particular RC # \"jq\" command useful for transformations that are too complex for jsonpath, it can be found at https://stedolan.github.io/jq/ sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?} echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # Show labels for all pods (or any other Kubernetes object that supports labelling) kubectl get pods --show-labels # Check which nodes are ready JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\" # List all Secrets currently in use by a pod kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq # List all containerIDs of initContainer of all pods # Helpful when cleaning up stopped containers, while avoiding removal of initContainers. kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3 # List Events sorted by timestamp kubectl get events --sort-by=.metadata.creationTimestamp # Compares the current state of the cluster against the state that the cluster would be in if the manifest was applied. kubectl diff -f ./my-manifest.yaml # Produce a period-delimited tree of all keys returned for nodes # Helpful when locating a key within a complex nested JSON structure kubectl get nodes -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")' # Produce a period-delimited tree of all keys returned for pods, etc kubectl get pods -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")'","title":"Viewing, finding resources"},{"location":"notes/kubernetes/basics/#deleting-resources","text":"kubectl delete -f ./pod.json # Delete a pod using the type and name specified in pod.json kubectl delete pod,service baz foo # Delete pods and services with same names \"baz\" and \"foo\" kubectl delete pods,services -l name=myLabel # Delete pods and services with label name=myLabel kubectl -n my-ns delete pod,svc --all # Delete all pods and services in namespace my-ns, # Delete all pods matching the awk pattern1 or pattern2 kubectl get pods -n mynamespace --no-headers=true | awk '/pattern1|pattern2/{print $1}' | xargs kubectl delete -n mynamespace pod","title":"Deleting resources"},{"location":"notes/kubernetes/basics/#interacting-with-running-pods","text":"kubectl logs my-pod # dump pod logs (stdout) kubectl logs -l name=myLabel # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod --previous # dump pod logs (stdout) for a previous instantiation of a container kubectl logs my-pod -c my-container # dump pod container logs (stdout, multi-container case) kubectl logs -l name=myLabel -c my-container # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod -c my-container --previous # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container kubectl logs -f my-pod # stream pod logs (stdout) kubectl logs -f my-pod -c my-container # stream pod container logs (stdout, multi-container case) kubectl logs -f -l name=myLabel --all-containers # stream all pods logs with label name=myLabel (stdout) kubectl run -i --tty busybox --image=busybox -- sh # Run pod as interactive shell kubectl run nginx --image=nginx -n mynamespace # Run pod nginx in a specific namespace kubectl run nginx --image=nginx # Run pod nginx and write its spec into a file called pod.yaml --dry-run=client -o yaml > pod.yaml kubectl attach my-pod -i # Attach to Running Container kubectl port-forward my-pod 5000:6000 # Listen on port 5000 on the local machine and forward to port 6000 on my-pod kubectl exec my-pod -- ls / # Run command in existing pod (1 container case) kubectl exec my-pod -c my-container -- ls / # Run command in existing pod (multi-container case) kubectl top pod POD_NAME --containers # Show metrics for a given pod and its containers ## Interacting with Nodes and cluster ```bash kubectl cordon my-node # Mark my-node as unschedulable kubectl drain my-node # Drain my-node in preparation for maintenance kubectl uncordon my-node # Mark my-node as schedulable kubectl top node my-node # Show metrics for a given node kubectl cluster-info # Display addresses of the master and services kubectl cluster-info dump # Dump current cluster state to stdout kubectl cluster-info dump --output-directory=/path/to/cluster-state # Dump current cluster state to /path/to/cluster-state # If a taint with that key and effect already exists, its value is replaced as specified. kubectl taint nodes foo dedicated=special-user:NoSchedule","title":"Interacting with running Pods"},{"location":"notes/kubernetes/greenfield_deployment/","text":"Greenfield Deployment If I've learned one thing it is that just because this works (if it does) it will not necessarily work in a month. Probably best to go to this link (below) and do the tutorial fresh from the docs each time... :( This is the official (take with grain of salt the size of hail) Azure tutorial including many extra things from more simple tutorials. Azure seems to have the most mature and complex kubernetes setup. Source The instructions below assume Application Gateway Ingress Controller (AGIC) will be installed in an environment with no pre-existing components. Create an identity Create AD Service Principal: az ad sp create-for-rbac --skip-assignment -o json > auth.json appId=$(jq -r \".appId\" auth.json) password=$(jq -r \".password\" auth.json) which makes this auth.json: { \"appId\": \"3c3a801c-5ad1-468b-b7e7-787363800352\", \"displayName\": \"azure-cli-2020-10-23-23-08-43\", \"name\": \"http://azure-cli-2020-10-23-23-08-43\", \"password\": \"FNhe_Ln_nCiIiSRVv_lAm7bD-miRxhNpVD\", \"tenant\": \"e72101ce-ef5d-49e3-baec-01191775dcc7\" } Get object ID: This is the new Service Principal id. objectId=$(az ad sp show --id $appId --query \"objectId\" -o tsv) $ echo $objectId dd3b3bcb-9b0c-4a64-8316-0f0150d39e0d Create parameters.json for use in the ARM (Azure Resource Manager) template. cat <<EOF > parameters.json { \"aksServicePrincipalAppId\": { \"value\": \"$appId\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"$password\" }, \"aksServicePrincipalObjectId\": { \"value\": \"$objectId\" }, \"aksEnableRBAC\": { \"value\": false } } EOF mine became: { \"aksServicePrincipalAppId\": { \"value\": \"3c3a801c-5ad1-468b-b7e7-787363800352\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"FNhe_Ln_nCiIiSRVv_lAm7bD-miRxhNpVD\" }, \"aksServicePrincipalObjectId\": { \"value\": \"dd3b3bcb-9b0c-4a64-8316-0f0150d39e0d\" }, \"aksEnableRBAC\": { \"value\": false } } To deploy an RBAC (Role Based Authentication Control) Id cluster, set the aksEnabledRBAC field to true . Deploy Components The next few steps will add the following list of components to your Azure subscription: Azure Kubernetes Service Application Gateway v2 Virtual Network with 2 subnets Public IP Address Managed Identity , which will be used by AAD Pod Identity Download ARM template into template.json: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/deploy/azuredeploy.json -O template.json Note: The template had version 1.15 for kubernetes. I changed to 1.19 after running az aks get-versions --location westus Deploy the ARM template with az and use flags and their values to set certain values: resourceGroupName=\"flask_demo\" location=\"eastus\" deploymentName=\"ingress-appgw\" az group create -n $resourceGroupName -l $location az group deployment create -g $resourceGroupName -n $deploymentName --template-file template.json --parameters parameters.json","title":"Greenfield"},{"location":"notes/kubernetes/greenfield_deployment/#greenfield-deployment","text":"If I've learned one thing it is that just because this works (if it does) it will not necessarily work in a month. Probably best to go to this link (below) and do the tutorial fresh from the docs each time... :( This is the official (take with grain of salt the size of hail) Azure tutorial including many extra things from more simple tutorials. Azure seems to have the most mature and complex kubernetes setup. Source The instructions below assume Application Gateway Ingress Controller (AGIC) will be installed in an environment with no pre-existing components.","title":"Greenfield Deployment"},{"location":"notes/kubernetes/greenfield_deployment/#create-an-identity","text":"Create AD Service Principal: az ad sp create-for-rbac --skip-assignment -o json > auth.json appId=$(jq -r \".appId\" auth.json) password=$(jq -r \".password\" auth.json) which makes this auth.json: { \"appId\": \"3c3a801c-5ad1-468b-b7e7-787363800352\", \"displayName\": \"azure-cli-2020-10-23-23-08-43\", \"name\": \"http://azure-cli-2020-10-23-23-08-43\", \"password\": \"FNhe_Ln_nCiIiSRVv_lAm7bD-miRxhNpVD\", \"tenant\": \"e72101ce-ef5d-49e3-baec-01191775dcc7\" } Get object ID: This is the new Service Principal id. objectId=$(az ad sp show --id $appId --query \"objectId\" -o tsv) $ echo $objectId dd3b3bcb-9b0c-4a64-8316-0f0150d39e0d Create parameters.json for use in the ARM (Azure Resource Manager) template. cat <<EOF > parameters.json { \"aksServicePrincipalAppId\": { \"value\": \"$appId\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"$password\" }, \"aksServicePrincipalObjectId\": { \"value\": \"$objectId\" }, \"aksEnableRBAC\": { \"value\": false } } EOF mine became: { \"aksServicePrincipalAppId\": { \"value\": \"3c3a801c-5ad1-468b-b7e7-787363800352\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"FNhe_Ln_nCiIiSRVv_lAm7bD-miRxhNpVD\" }, \"aksServicePrincipalObjectId\": { \"value\": \"dd3b3bcb-9b0c-4a64-8316-0f0150d39e0d\" }, \"aksEnableRBAC\": { \"value\": false } } To deploy an RBAC (Role Based Authentication Control) Id cluster, set the aksEnabledRBAC field to true .","title":"Create an identity"},{"location":"notes/kubernetes/greenfield_deployment/#deploy-components","text":"The next few steps will add the following list of components to your Azure subscription: Azure Kubernetes Service Application Gateway v2 Virtual Network with 2 subnets Public IP Address Managed Identity , which will be used by AAD Pod Identity Download ARM template into template.json: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/deploy/azuredeploy.json -O template.json Note: The template had version 1.15 for kubernetes. I changed to 1.19 after running az aks get-versions --location westus Deploy the ARM template with az and use flags and their values to set certain values: resourceGroupName=\"flask_demo\" location=\"eastus\" deploymentName=\"ingress-appgw\" az group create -n $resourceGroupName -l $location az group deployment create -g $resourceGroupName -n $deploymentName --template-file template.json --parameters parameters.json","title":"Deploy Components"},{"location":"notes/kubernetes/minikube/","text":"Minikube Installation curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Start Your Cluster minikube start Interact With Your Cluster kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-w8nkf 1/1 Running 0 57m kube-system etcd-minikube 1/1 Running 0 57m kube-system kube-apiserver-minikube 1/1 Running 0 57m kube-system kube-controller-manager-minikube 1/1 Running 0 57m kube-system kube-proxy-tqb79 1/1 Running 0 57m kube-system kube-scheduler-minikube 1/1 Running 0 57m kube-system storage-provisioner 1/1 Running 1 58m Deploy Applications kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080","title":"Minikube"},{"location":"notes/kubernetes/minikube/#minikube","text":"","title":"Minikube"},{"location":"notes/kubernetes/minikube/#installation","text":"curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube","title":"Installation"},{"location":"notes/kubernetes/minikube/#start-your-cluster","text":"minikube start","title":"Start Your Cluster"},{"location":"notes/kubernetes/minikube/#interact-with-your-cluster","text":"kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-w8nkf 1/1 Running 0 57m kube-system etcd-minikube 1/1 Running 0 57m kube-system kube-apiserver-minikube 1/1 Running 0 57m kube-system kube-controller-manager-minikube 1/1 Running 0 57m kube-system kube-proxy-tqb79 1/1 Running 0 57m kube-system kube-scheduler-minikube 1/1 Running 0 57m kube-system storage-provisioner 1/1 Running 1 58m","title":"Interact With Your Cluster"},{"location":"notes/kubernetes/minikube/#deploy-applications","text":"kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080","title":"Deploy Applications"},{"location":"notes/kubernetes/sample-django-deployment/","text":"Sample Django Deployment Source medium Stack-Scope As I already revealed we are going to deploy a Django web application on a managed Kubernetes cluster on Azure Kubernetes Service. The Django application is written in Python 3, and built locally with Docker. To manage some Azure resources, like Azure Container Registry (ACR), Load Balancer, static IP and the AKS cluster itself we will use the Azure CLI. In order to manage deployments and services within the cluster we use kubectl, the official Kubernetes command-line tool. For installing the NGINX Ingress controller and setting up Jetstack/Cert-Manager we utilize the power of Helm 3, a Kubernetes package manager. python3 docker Azure Container Registry (ACR) Load Balancer Azure CLI kubectl helm 3 Kubernetes Concepts Before we roll up our sleeves, let\u2019s review some of the main Kubernetes concepts. Make sure to peek at the official documentation if you have some spare time, to obtain a better understanding of the various parts and abstractions of Kubernetes. Pod \u2014 A Pod represents a single instance of an application in Kubernetes and is the basic used unit of deployment. In common use cases the \u201cone-container-per-pod\u201d model is the way to go, but you can also run multiple containers in a single Pod. You can think of the Pod as a wrapper around your \u2014 Docker \u2014 container which describes it\u2019s resources, network configuration and other options. Service \u2014 A Service in Kubernetes is an abstract networking layer for multiple Pods. It has the ability to target multiple Pods by their label and defines the policy of how Pods can access each other, also known as microservice context management. The Service can be given an unique DNS name as a single Service internal access point. ReplicaSet \u2014 A ReplicaSet describes how many replica Pods of a specific Pod should be running at any given time. With the replica Pods of your application running you can make sure that your service will be available when one of the Pods crashes. Because a Kubernetes Deployment is a higher-level conceptual controller that manages ReplicaSets, you will most likely not use this controller directly in your projects. However, abstract knowledge about this concept helps understanding the basic concepts and utilities of Kubernetes. Deployment \u2014 A Deployment describes a desired state and can be applied as declarative update or release for Pods and ReplicaSets. Wait what? Imagine that your Django web application is running in a Pod on your Kubernetes cluster. When you decide to update your application with a newer version of your code, you can declare the new desired state \u2014 which most likely points to an updated container image \u2014 in a Deployment description which you can apply in your cluster. The native Kubernetes Deployment Controller will detect the changes and will handle the rollout by changing the current state of a Pod or ReplicaSet to the desired state. Ingress \u2014 An Ingress describes the routing rules for your cluster. It is an advanced routing layer between your cluster services and the internet. Next to the Ingress you need an Ingress controller to satisfy the rules that you described in the Ingress. Together they form a very powerful couple to expose your services. An Ingress also has many advantages relative to using other external traffic routing methods i.e. the primitive NodePort or costly LoadBalancer. Node \u2014 Last but not least, a Node represents a virtual or physical machine where Kubernetes will run your Pods\u2019 container. Most of the time the nodes will be Linux machines, however Azure offers the possibility now to use Windows machines as well! Excellent! We learned to walk, now we can start to run! Step 1: Build Locally Clone: $ git clone https://github.com/wmarcuse/django-azure-aks-ingress.git Build: $ docker build -t bbearce/django-aks:v1.0.0 . Run: $ docker run --rm -it -p 8010:8010 bbearce/django-aks:v1.0.0 Visit: $ Visit http://127.0.0.1:8010 Step 2: Manage ACR Login to Azure: $ az login Opening in existing browser session. [1009/183807.113943:ERROR:nacl_helper_linux.cc(308)] NaCl helper process running without a sandbox! Most likely you need to configure your SUID sandbox correctly You have logged in. Now let us find all the subscriptions to which you have access... [ { \"cloudName\": \"AzureCloud\", \"homeTenantId\": \"####################################\", \"id\": \"####################################\", \"isDefault\": true, \"managedByTenants\": [], \"name\": \"Pay-As-You-Go\", \"state\": \"Enabled\", \"tenantId\": \"####################################\", \"user\": { \"name\": \"someone@mail.com\", \"type\": \"user\" } } ] Create a resource group: $ az group create --name djangoAKS --location westeurope djangoAKS --location westeurope { \"id\": \"/subscriptions/####################################/resourceGroups/djangoAKS\", \"location\": \"westeurope\", \"managedBy\": null, \"name\": \"djangoAKS\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } Create the ACR resource Create the ACR resource in the newly created resource group specifying the basic service level : $ az acr create --resource-group djangoAKS --name djangoAksRegistryBB --sku Basic {- Finished .. \"adminUserEnabled\": false, \"creationDate\": \"2020-10-09T22:45:49.761838+00:00\", \"dataEndpointEnabled\": false, \"dataEndpointHostNames\": [], \"encryption\": { \"keyVaultProperties\": null, \"status\": \"disabled\" }, \"id\": \"/subscriptions/####################################/resourceGroups/djangoAKS/providers/Microsoft.ContainerRegistry/registries/djangoAksRegistryBB\", \"identity\": null, \"location\": \"westeurope\", \"loginServer\": \"djangoaksregistrybb.azurecr.io\", \"name\": \"djangoAksRegistryBB\", \"networkRuleSet\": null, \"policies\": { \"quarantinePolicy\": { \"status\": \"disabled\" }, \"retentionPolicy\": { \"days\": 7, \"lastUpdatedTime\": \"2020-10-09T22:45:54.821697+00:00\", \"status\": \"disabled\" }, \"trustPolicy\": { \"status\": \"disabled\", \"type\": \"Notary\" } }, \"privateEndpointConnections\": [], \"provisioningState\": \"Succeeded\", \"publicNetworkAccess\": \"Enabled\", \"resourceGroup\": \"djangoAKS\", \"sku\": { \"name\": \"Basic\", \"tier\": \"Basic\" }, \"status\": null, \"storageAccount\": null, \"systemData\": { \"createdAt\": \"2020-10-09T22:45:49.7618387+00:00\", \"createdBy\": \"someone@mail.com\", \"createdByType\": \"User\", \"lastModifiedAt\": \"2020-10-09T22:45:49.7618387+00:00\", \"lastModifiedBy\": \"someone@mail.com\", \"lastModifiedByType\": \"User\" }, \"tags\": {}, \"type\": \"Microsoft.ContainerRegistry/registries\" } Take note of loginServer: djangoaksregistrybb.azurecr.io . Pushing the application to ACR: Login: $ az acr login --name djangoaksregistrybb Tag image: $ docker tag bbearce/django-aks:v1.0.0 djangoaksregistrybb.azurecr.io/django-aks:v1.0.0 Push image: $ docker push djangoaksregistrybb.azurecr.io/django-aks:v1.0.0 Step 3: Setting up a Kubernetes cluster with AKS Creating the AKS cluster We specify a few parameters to let Azure know how we want to configure the Kubernetes cluster. We make sure that the AKS cluster is put in the same resource group as the ACR registry. $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --generate-ssh-keys \\ --kubernetes-version 1.16.8 In western Europe (my resource location) kubernetes version 1.16.8 wasn't available. Using az aks get-versions --location westeurope , you can see what versions are available. So instead I used: $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --generate-ssh-keys \\ --kubernetes-version 1.19.0 Also there is a section and or flag related to supplying your own ssh key. Brief example which doesn't apply to this tutorial as the --generate-ssh-keys worked for me... $ ssh-keygen -f ~/.ssh/django-aks-ssh Then create the AKS cluster with almost the same command as we tried before but now pointing towards the manually created SSH key. $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --ssh-key-value ~\\.ssh\\django-aks-ssh.pub \\ --kubernetes-version 1.16.8 moving on... Connecting to the AKS cluster $ az aks install-cli Now configure kubectl to connect to your AKS cluster, the credentials will be downloaded on the background and the context of the Kubernetes command-line tool will be set to your cluster. $ az aks get-credentials --resource-group djangoAKS --name djangoaks-cluster Verify that the connection from your development machine to the AKS cluster is working by checking the status of the nodes you created previously. $ kubectl get nodes Namespace Now as your first act as cluster-manager, create a Kubernetes namespace for the resources we are going to create in the next steps. $ kubectl create namespace djaks Step 4: Set up Helm 3 Install Helm 3 Helm 3 is a package manager for Kubernetes and helps you manage applications on your cluster. Helm 3 works with so called \u2018charts\u2019. A chart describes the application resources and provides easy accessible and repeatable application installations, as well as very clever in-place application upgrades. We will be needing Helm 3 to install Jetstack/Cert-Manager and the NGINX Ingress controller. Follow the official installation guide. I highly recommend Windows users to install Helm 3 with Chocolatey, as to prevent some first time installation problems. We will be needing Helm 3 to install Jetstack/Cert-Manager and the NGINX Ingress controller. Follow the official installation guide . I highly recommend Windows users to install Helm 3 with Chocolatey , as to prevent some first time installation problems. Manage Helm 3 Helm 3 has a variety of pre-created charts available in the official stable charts repository. Add the stable repository to your Helm 3 installation. $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Also adding: $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Now update the list of charts you just added to your Helm 3 installation. $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ingress-nginx\" chart repository ...Successfully got an update from the \"jetstack\" chart repository ...Successfully got an update from the \"stable\" chart repository Update Complete. \u2388Happy Helming!\u2388 Step 5: Configure additional Azure resources When we created the AKS cluster, Azure automatically created an additional resource group where it attaches specific infrastructural resources like virtual machine scale sets, virtual networks and managed disks. This extra resource group is also known as the node resource group. When you delete the AKS cluster, this node resource group is deleted as well. The resource group follows the following name convention: MC_<resourceGroupName>_<clusterName>_<Region>. Create a static IP address Let\u2019s find out what the exact name of the node resource group is. $ az aks show --resource-group djangoAKS --name djangoaks-cluster --query nodeResourceGroup -o tsv MC_djangoAKS_djangoaks-cluster_westeurope Now create the public IP: $ az network public-ip create \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --name djangoAKSPublicIP \\ --sku Standard \\ --allocation-method static \\ --query publicIp.ipAdress \\ -o tsv Now let\u2019s find out what the actual IP address is. The static IPv4 address should be visible under ipAddress. $ az network public-ip show \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --name djangoAKSPublicIP ... \"ipAddress\": \"51.105.159.17\", ... \"name\": \"djangoAKSPublicIP\", ... Connect AKS with ACR with Azure AD As we host some of our application container images in ACR, it is essential that the AKS cluster can communicate with the container registry. When we configure the communication between the two services, Active Directory will handle the authentication on the background. $ az aks update -n djangoaks-cluster -g djangoAKS --attach-acr djangoaksregistrybb Step 6: Create the NGINX Ingress controller It is time to start using the magic from Helm 3. We can instantly deploy an nginx-ingress chart that is already available in de stable Helm repository! Here I'm trying something I found in Patrick's deploy_codalab.sh and sort of here : $ helm install --namespace djaks nginx-ingress nginx-stable/nginx-ingress --set controller.replicaCount=2 --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" --set controller.service.loadBalancerIP=\"51.105.159.17\" another version from this tutorial : $ helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux optionally: $ helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=\"51.105.159.17\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" This avoids the WARNING I show below... Below is original instructions from post I'm copying: Make sure that you replace the IP address in the command below under --set controller.service.loadBalancerTP with the static IP address you created in the previous step. Specify your preferable DNS name label under the --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\" Azure will create a Fully Qualified Domain Name (FQDN) with the provided DNS prefix. Note that we install the Ingress controller in the earlier created namespace djaks . $ helm install nginx-ingress stable/nginx-ingress \\ --namespace djaks \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=\"51.105.159.17\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" Note if that above doesn't work, here it is flattened: $ helm install nginx-ingress stable/nginx-ingress --namespace djaks --set controller.replicaCount=2 --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set controller.service.loadBalancerIP=\"51.105.159.17\" --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" Temp: Output from my run of the above command...meant to be deleted at some point, but I'm doing the tutorial as we speak. WARNING: This chart is deprecated NAME: nginx-ingress LAST DEPLOYED: Mon Oct 12 09:27:49 2020 NAMESPACE: djaks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************************************************* * DEPRECATED, please use https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx * ******************************************************************************************************* The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace djaks get services -o wide -w nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls You can monitor the progress to see when the EXTERNAL-IP is bound to the nginx-ingress-controller . Use CTRL-C to stop the monitoring. update: $ kubectl --namespace djaks get services -o wide -w nginx-ingress-ingress-nginx-controller old: $ kubectl --namespace djaks get services -o wide -w nginx-ingress-nginx-ingress FDQN stands for fully qualified domain name. Azure has created a FQDN when we created the Ingress controller, we can access our service via this domain as well. Let\u2019s find out what the domain name is. $ az network public-ip list \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --query \"[?name=='djangoAKSPublicIP'].[dnsSettings.fqdn]\" -o tsv Awesome! Our service is available via django-aks-ingress.westeurope.cloudapp.azure.com as well! Step 7: Manage Jetstack/Cert-Manager This was not in original tutorial but I will add a cert-manager namespace for the cert-manager pods: $ kubectl create namespace cert-manager Install Jetstack/Cert-Manager very useful resource-HoussemDellai His video In step 4 we configured Helm 3 and added their stable chart repository, in this repository resides a cert-manager chart as well. But this version is depreciated, therefore use the official chart repository of jetstack.io. $ helm repo add jetstack https://charts.jetstack.io Update the local Helm chart repository cache to fetch any updates. I was told I was up to date so... $ helm repo update \"jetstack\" already exists with the same configuration, skipping We have to apply the Custom Resource Definitions (CRDs) to the cluster as part of the Helm 3 release. Note that it is important that you have the earlier defined Kubernetes version 1.16.8+ running on your cluster. Also don\u2019t apply the CRDs manually to your cluster, they can have issues with custom namespace names. Install Jetstack/Cert-Manager. $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace djaks \\ --version v1.0.2 \\ --set installCRDs=true $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.0.0 \\ --set installCRDs=true $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace djaks \\ --set installCRDs=true You can verify that the installation was successful by checking the namespace for running pods. You should see three cert-manager pods running! $ kubectl get pods --namespace djaks Create an ACME ClusterIssuer ACME stands for Automated Certificate Management Environment. As we want to issue the certificates for the scope of the cluster, let\u2019s use a ClusterIssuer resource. You can find more on Issuers here . Let\u2019s checkout the ClusterIssuer manifest file. # k8s/cluster-issuer.yaml apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: # name: letsencrypt-staging name: letsencrypt-prod spec: acme: email: user@example.com # server: https://acme-staging-v02.api.letsencrypt.org/directory server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # name: letsencrypt-staging name: letsencrypt-prod solvers: - http01: ingress: class: nginx Create the ClusterIssuer in your cluster: $ kubectl apply -f cluster-issuer.yaml --namespace djaks Experimental command: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.2/cert-manager.yaml kubectl delete secret {NAME OF THE SECRET NAMED ON THE CERTIFICATE HERE} ... Next I listed the ingresses with, $ kubectl --namespace djaks get ing NAME HOSTS q ADDRESS PORTS AGE ingress-resource-rules django-aks-ingress.westeurope.cloudapp.azure.com 10.240.0.4,10.240.0.5 80, 443 9m48s and certificates with $ kubectl get all --namespace cert-manager $ kubectl --namespace djaks describe certificate tls-secret $ kubectl --namespace djaks describe certificaterequest tls-secret-skgpp kubectl --namespace djaks describe clusterissuer letsencrypt-prod helm --namespace ingress uninstall nginx-ingress .... Step 8: Deploy the Django application Apply the Deployment manifest Note, be sure to change image: in yours... # k8s/webapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: django-web-app labels: app: django-web spec: replicas: 2 selector: matchLabels: app: django-web template: metadata: labels: app: django-web spec: containers: - name: django-web-container imagePullPolicy: Always image: djangoaksregistrybb.azurecr.io/django-aks:v1.0.0 ports: - containerPort: 8010 env: - name: HOST_IP valueFrom: fieldRef: fieldPath: status.hostIP imagePullSecrets: - name: acr-secret Apply the Deployment manifest to the cluster. $ kubectl apply -f webapp-deployment.yaml --namespace djaks You can verify that the django-web-app Pods are running correctly by checking them out: kubectl get pods --namespace djaks If something went wrong you can look at the events that happened in the Pod by asking kubectl to describe the Pod. You need to specify a specific Pod name of the Pods you listed in the previous command. $ kubectl describe pod django-web-app-#########-##### --namespace djaks Apply the Service manifest # k8s/webapp-service.yaml apiVersion: v1 kind: Service metadata: name: webapp spec: type: ClusterIP ports: - port: 80 targetPort: 8010 selector: app: django-web $ kubectl apply -f webapp-service.yaml --namespace djaks The service is exposed on an internal IP with the ClusterIP ServiceSpec. This means that the service is only accessible from within the cluster, the Ingress controller will be publicly exposed and route the traffic to your application as we will see in a moment. You can verify now that the webapp service is available. $ kubectl get svc --namespace djaks Step 9: Set up the Ingress routing Define the Ingress rules Let\u2019s have a look at the Ingress routing file. # k8s/ingress-routing.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-resource-rules annotations: kubernetes.io/ingress.class: nginx # cert-manager.io/cluster-issuer: letsencrypt-staging cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: tls: - hosts: - django-aks-ingress.westeurope.cloudapp.azure.com secretName: tls-secret rules: - host: django-aks-ingress.westeurope.cloudapp.azure.com http: paths: - backend: serviceName: webapp servicePort: 80 path: /(.*) Apply the Ingress to your cluster. $ kubectl apply -f ingress-routing.yaml --namespace djaks Configure the Django allowed host Go to the settings.py file and add your FQDN domain to the ALLOWED_HOSTS list. # app/app/settings.py ... ALLOWED_HOSTS = ['django-aks-ingress.westeurope.cloudapp.azure.com'] ... Now re-build the Docker container image of your new application version locally. $ docker build -f Dockerfile -t django-aks:v1.0.1 . Tag the image for your remote ACR registry. $ docker tag django-aks:v1.0.1 djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 Push the image to the ACR registry. $ docker push djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 If you get an authentication error, login to the ACR registry and try again. $ az acr login --name djangoaksregistrybb Now we have to let the cluster know that we want to update our django-web-app Deployment. $ kubectl set image deployment django-web-app django-web-container=djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 --namespace djaks Step 10: Clean up resources If you want to get rid of the created resources, you can clean them up. I will present two clean up options! Keep the Azure resources, wipe the cluster namespace When you want to keep the Azure resources, then you are good to go by deleting the namespace in which we created the cluster resources. $ kubectl delete namespace djaks Delete all resources When you want to get back to a clean slate and make sure the paid Azure resources are removed as well, delete the Azure resource group we created in the beginning. $ az group delete --name djangoAKS That\u2019s it, we are done!","title":"Sample Deployment (Django)"},{"location":"notes/kubernetes/sample-django-deployment/#sample-django-deployment","text":"Source medium","title":"Sample Django Deployment"},{"location":"notes/kubernetes/sample-django-deployment/#stack-scope","text":"As I already revealed we are going to deploy a Django web application on a managed Kubernetes cluster on Azure Kubernetes Service. The Django application is written in Python 3, and built locally with Docker. To manage some Azure resources, like Azure Container Registry (ACR), Load Balancer, static IP and the AKS cluster itself we will use the Azure CLI. In order to manage deployments and services within the cluster we use kubectl, the official Kubernetes command-line tool. For installing the NGINX Ingress controller and setting up Jetstack/Cert-Manager we utilize the power of Helm 3, a Kubernetes package manager. python3 docker Azure Container Registry (ACR) Load Balancer Azure CLI kubectl helm 3","title":"Stack-Scope"},{"location":"notes/kubernetes/sample-django-deployment/#kubernetes-concepts","text":"Before we roll up our sleeves, let\u2019s review some of the main Kubernetes concepts. Make sure to peek at the official documentation if you have some spare time, to obtain a better understanding of the various parts and abstractions of Kubernetes. Pod \u2014 A Pod represents a single instance of an application in Kubernetes and is the basic used unit of deployment. In common use cases the \u201cone-container-per-pod\u201d model is the way to go, but you can also run multiple containers in a single Pod. You can think of the Pod as a wrapper around your \u2014 Docker \u2014 container which describes it\u2019s resources, network configuration and other options. Service \u2014 A Service in Kubernetes is an abstract networking layer for multiple Pods. It has the ability to target multiple Pods by their label and defines the policy of how Pods can access each other, also known as microservice context management. The Service can be given an unique DNS name as a single Service internal access point. ReplicaSet \u2014 A ReplicaSet describes how many replica Pods of a specific Pod should be running at any given time. With the replica Pods of your application running you can make sure that your service will be available when one of the Pods crashes. Because a Kubernetes Deployment is a higher-level conceptual controller that manages ReplicaSets, you will most likely not use this controller directly in your projects. However, abstract knowledge about this concept helps understanding the basic concepts and utilities of Kubernetes. Deployment \u2014 A Deployment describes a desired state and can be applied as declarative update or release for Pods and ReplicaSets. Wait what? Imagine that your Django web application is running in a Pod on your Kubernetes cluster. When you decide to update your application with a newer version of your code, you can declare the new desired state \u2014 which most likely points to an updated container image \u2014 in a Deployment description which you can apply in your cluster. The native Kubernetes Deployment Controller will detect the changes and will handle the rollout by changing the current state of a Pod or ReplicaSet to the desired state. Ingress \u2014 An Ingress describes the routing rules for your cluster. It is an advanced routing layer between your cluster services and the internet. Next to the Ingress you need an Ingress controller to satisfy the rules that you described in the Ingress. Together they form a very powerful couple to expose your services. An Ingress also has many advantages relative to using other external traffic routing methods i.e. the primitive NodePort or costly LoadBalancer. Node \u2014 Last but not least, a Node represents a virtual or physical machine where Kubernetes will run your Pods\u2019 container. Most of the time the nodes will be Linux machines, however Azure offers the possibility now to use Windows machines as well! Excellent! We learned to walk, now we can start to run!","title":"Kubernetes Concepts"},{"location":"notes/kubernetes/sample-django-deployment/#step-1-build-locally","text":"Clone: $ git clone https://github.com/wmarcuse/django-azure-aks-ingress.git Build: $ docker build -t bbearce/django-aks:v1.0.0 . Run: $ docker run --rm -it -p 8010:8010 bbearce/django-aks:v1.0.0 Visit: $ Visit http://127.0.0.1:8010","title":"Step 1: Build Locally"},{"location":"notes/kubernetes/sample-django-deployment/#step-2-manage-acr","text":"","title":"Step 2: Manage ACR"},{"location":"notes/kubernetes/sample-django-deployment/#login-to-azure","text":"$ az login Opening in existing browser session. [1009/183807.113943:ERROR:nacl_helper_linux.cc(308)] NaCl helper process running without a sandbox! Most likely you need to configure your SUID sandbox correctly You have logged in. Now let us find all the subscriptions to which you have access... [ { \"cloudName\": \"AzureCloud\", \"homeTenantId\": \"####################################\", \"id\": \"####################################\", \"isDefault\": true, \"managedByTenants\": [], \"name\": \"Pay-As-You-Go\", \"state\": \"Enabled\", \"tenantId\": \"####################################\", \"user\": { \"name\": \"someone@mail.com\", \"type\": \"user\" } } ]","title":"Login to Azure:"},{"location":"notes/kubernetes/sample-django-deployment/#create-a-resource-group","text":"$ az group create --name djangoAKS --location westeurope djangoAKS --location westeurope { \"id\": \"/subscriptions/####################################/resourceGroups/djangoAKS\", \"location\": \"westeurope\", \"managedBy\": null, \"name\": \"djangoAKS\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" }","title":"Create a resource group:"},{"location":"notes/kubernetes/sample-django-deployment/#create-the-acr-resource","text":"Create the ACR resource in the newly created resource group specifying the basic service level : $ az acr create --resource-group djangoAKS --name djangoAksRegistryBB --sku Basic {- Finished .. \"adminUserEnabled\": false, \"creationDate\": \"2020-10-09T22:45:49.761838+00:00\", \"dataEndpointEnabled\": false, \"dataEndpointHostNames\": [], \"encryption\": { \"keyVaultProperties\": null, \"status\": \"disabled\" }, \"id\": \"/subscriptions/####################################/resourceGroups/djangoAKS/providers/Microsoft.ContainerRegistry/registries/djangoAksRegistryBB\", \"identity\": null, \"location\": \"westeurope\", \"loginServer\": \"djangoaksregistrybb.azurecr.io\", \"name\": \"djangoAksRegistryBB\", \"networkRuleSet\": null, \"policies\": { \"quarantinePolicy\": { \"status\": \"disabled\" }, \"retentionPolicy\": { \"days\": 7, \"lastUpdatedTime\": \"2020-10-09T22:45:54.821697+00:00\", \"status\": \"disabled\" }, \"trustPolicy\": { \"status\": \"disabled\", \"type\": \"Notary\" } }, \"privateEndpointConnections\": [], \"provisioningState\": \"Succeeded\", \"publicNetworkAccess\": \"Enabled\", \"resourceGroup\": \"djangoAKS\", \"sku\": { \"name\": \"Basic\", \"tier\": \"Basic\" }, \"status\": null, \"storageAccount\": null, \"systemData\": { \"createdAt\": \"2020-10-09T22:45:49.7618387+00:00\", \"createdBy\": \"someone@mail.com\", \"createdByType\": \"User\", \"lastModifiedAt\": \"2020-10-09T22:45:49.7618387+00:00\", \"lastModifiedBy\": \"someone@mail.com\", \"lastModifiedByType\": \"User\" }, \"tags\": {}, \"type\": \"Microsoft.ContainerRegistry/registries\" } Take note of loginServer: djangoaksregistrybb.azurecr.io .","title":"Create the ACR resource"},{"location":"notes/kubernetes/sample-django-deployment/#pushing-the-application-to-acr","text":"Login: $ az acr login --name djangoaksregistrybb Tag image: $ docker tag bbearce/django-aks:v1.0.0 djangoaksregistrybb.azurecr.io/django-aks:v1.0.0 Push image: $ docker push djangoaksregistrybb.azurecr.io/django-aks:v1.0.0","title":"Pushing the application to ACR:"},{"location":"notes/kubernetes/sample-django-deployment/#step-3-setting-up-a-kubernetes-cluster-with-aks","text":"","title":"Step 3: Setting up a Kubernetes cluster with AKS"},{"location":"notes/kubernetes/sample-django-deployment/#creating-the-aks-cluster","text":"We specify a few parameters to let Azure know how we want to configure the Kubernetes cluster. We make sure that the AKS cluster is put in the same resource group as the ACR registry. $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --generate-ssh-keys \\ --kubernetes-version 1.16.8 In western Europe (my resource location) kubernetes version 1.16.8 wasn't available. Using az aks get-versions --location westeurope , you can see what versions are available. So instead I used: $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --generate-ssh-keys \\ --kubernetes-version 1.19.0 Also there is a section and or flag related to supplying your own ssh key. Brief example which doesn't apply to this tutorial as the --generate-ssh-keys worked for me... $ ssh-keygen -f ~/.ssh/django-aks-ssh Then create the AKS cluster with almost the same command as we tried before but now pointing towards the manually created SSH key. $ az aks create \\ --resource-group djangoAKS \\ --name djangoaks-cluster \\ --node-count 2 \\ --node-vm-size Standard_B2s \\ --ssh-key-value ~\\.ssh\\django-aks-ssh.pub \\ --kubernetes-version 1.16.8 moving on...","title":"Creating the AKS cluster"},{"location":"notes/kubernetes/sample-django-deployment/#connecting-to-the-aks-cluster","text":"$ az aks install-cli Now configure kubectl to connect to your AKS cluster, the credentials will be downloaded on the background and the context of the Kubernetes command-line tool will be set to your cluster. $ az aks get-credentials --resource-group djangoAKS --name djangoaks-cluster Verify that the connection from your development machine to the AKS cluster is working by checking the status of the nodes you created previously. $ kubectl get nodes","title":"Connecting to the AKS cluster"},{"location":"notes/kubernetes/sample-django-deployment/#namespace","text":"Now as your first act as cluster-manager, create a Kubernetes namespace for the resources we are going to create in the next steps. $ kubectl create namespace djaks","title":"Namespace"},{"location":"notes/kubernetes/sample-django-deployment/#step-4-set-up-helm-3","text":"","title":"Step 4: Set up Helm 3"},{"location":"notes/kubernetes/sample-django-deployment/#install-helm-3","text":"Helm 3 is a package manager for Kubernetes and helps you manage applications on your cluster. Helm 3 works with so called \u2018charts\u2019. A chart describes the application resources and provides easy accessible and repeatable application installations, as well as very clever in-place application upgrades. We will be needing Helm 3 to install Jetstack/Cert-Manager and the NGINX Ingress controller. Follow the official installation guide. I highly recommend Windows users to install Helm 3 with Chocolatey, as to prevent some first time installation problems. We will be needing Helm 3 to install Jetstack/Cert-Manager and the NGINX Ingress controller. Follow the official installation guide . I highly recommend Windows users to install Helm 3 with Chocolatey , as to prevent some first time installation problems.","title":"Install Helm 3"},{"location":"notes/kubernetes/sample-django-deployment/#manage-helm-3","text":"Helm 3 has a variety of pre-created charts available in the official stable charts repository. Add the stable repository to your Helm 3 installation. $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Also adding: $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Now update the list of charts you just added to your Helm 3 installation. $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ingress-nginx\" chart repository ...Successfully got an update from the \"jetstack\" chart repository ...Successfully got an update from the \"stable\" chart repository Update Complete. \u2388Happy Helming!\u2388","title":"Manage Helm 3"},{"location":"notes/kubernetes/sample-django-deployment/#step-5-configure-additional-azure-resources","text":"When we created the AKS cluster, Azure automatically created an additional resource group where it attaches specific infrastructural resources like virtual machine scale sets, virtual networks and managed disks. This extra resource group is also known as the node resource group. When you delete the AKS cluster, this node resource group is deleted as well. The resource group follows the following name convention: MC_<resourceGroupName>_<clusterName>_<Region>.","title":"Step 5: Configure additional Azure resources"},{"location":"notes/kubernetes/sample-django-deployment/#create-a-static-ip-address","text":"Let\u2019s find out what the exact name of the node resource group is. $ az aks show --resource-group djangoAKS --name djangoaks-cluster --query nodeResourceGroup -o tsv MC_djangoAKS_djangoaks-cluster_westeurope Now create the public IP: $ az network public-ip create \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --name djangoAKSPublicIP \\ --sku Standard \\ --allocation-method static \\ --query publicIp.ipAdress \\ -o tsv Now let\u2019s find out what the actual IP address is. The static IPv4 address should be visible under ipAddress. $ az network public-ip show \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --name djangoAKSPublicIP ... \"ipAddress\": \"51.105.159.17\", ... \"name\": \"djangoAKSPublicIP\", ...","title":"Create a static IP address"},{"location":"notes/kubernetes/sample-django-deployment/#connect-aks-with-acr-with-azure-ad","text":"As we host some of our application container images in ACR, it is essential that the AKS cluster can communicate with the container registry. When we configure the communication between the two services, Active Directory will handle the authentication on the background. $ az aks update -n djangoaks-cluster -g djangoAKS --attach-acr djangoaksregistrybb","title":"Connect AKS with ACR with Azure AD"},{"location":"notes/kubernetes/sample-django-deployment/#step-6-create-the-nginx-ingress-controller","text":"It is time to start using the magic from Helm 3. We can instantly deploy an nginx-ingress chart that is already available in de stable Helm repository! Here I'm trying something I found in Patrick's deploy_codalab.sh and sort of here : $ helm install --namespace djaks nginx-ingress nginx-stable/nginx-ingress --set controller.replicaCount=2 --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" --set controller.service.loadBalancerIP=\"51.105.159.17\" another version from this tutorial : $ helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux optionally: $ helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=\"51.105.159.17\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" This avoids the WARNING I show below... Below is original instructions from post I'm copying: Make sure that you replace the IP address in the command below under --set controller.service.loadBalancerTP with the static IP address you created in the previous step. Specify your preferable DNS name label under the --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\" Azure will create a Fully Qualified Domain Name (FQDN) with the provided DNS prefix. Note that we install the Ingress controller in the earlier created namespace djaks . $ helm install nginx-ingress stable/nginx-ingress \\ --namespace djaks \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set controller.service.loadBalancerIP=\"51.105.159.17\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" Note if that above doesn't work, here it is flattened: $ helm install nginx-ingress stable/nginx-ingress --namespace djaks --set controller.replicaCount=2 --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set controller.service.loadBalancerIP=\"51.105.159.17\" --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"django-aks-ingress\" Temp: Output from my run of the above command...meant to be deleted at some point, but I'm doing the tutorial as we speak. WARNING: This chart is deprecated NAME: nginx-ingress LAST DEPLOYED: Mon Oct 12 09:27:49 2020 NAMESPACE: djaks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************************************************* * DEPRECATED, please use https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx * ******************************************************************************************************* The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace djaks get services -o wide -w nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls You can monitor the progress to see when the EXTERNAL-IP is bound to the nginx-ingress-controller . Use CTRL-C to stop the monitoring. update: $ kubectl --namespace djaks get services -o wide -w nginx-ingress-ingress-nginx-controller old: $ kubectl --namespace djaks get services -o wide -w nginx-ingress-nginx-ingress FDQN stands for fully qualified domain name. Azure has created a FQDN when we created the Ingress controller, we can access our service via this domain as well. Let\u2019s find out what the domain name is. $ az network public-ip list \\ --resource-group MC_djangoAKS_djangoaks-cluster_westeurope \\ --query \"[?name=='djangoAKSPublicIP'].[dnsSettings.fqdn]\" -o tsv Awesome! Our service is available via django-aks-ingress.westeurope.cloudapp.azure.com as well!","title":"Step 6: Create the NGINX Ingress controller"},{"location":"notes/kubernetes/sample-django-deployment/#step-7-manage-jetstackcert-manager","text":"This was not in original tutorial but I will add a cert-manager namespace for the cert-manager pods: $ kubectl create namespace cert-manager","title":"Step 7: Manage Jetstack/Cert-Manager"},{"location":"notes/kubernetes/sample-django-deployment/#install-jetstackcert-manager","text":"very useful resource-HoussemDellai His video In step 4 we configured Helm 3 and added their stable chart repository, in this repository resides a cert-manager chart as well. But this version is depreciated, therefore use the official chart repository of jetstack.io. $ helm repo add jetstack https://charts.jetstack.io Update the local Helm chart repository cache to fetch any updates. I was told I was up to date so... $ helm repo update \"jetstack\" already exists with the same configuration, skipping We have to apply the Custom Resource Definitions (CRDs) to the cluster as part of the Helm 3 release. Note that it is important that you have the earlier defined Kubernetes version 1.16.8+ running on your cluster. Also don\u2019t apply the CRDs manually to your cluster, they can have issues with custom namespace names. Install Jetstack/Cert-Manager. $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace djaks \\ --version v1.0.2 \\ --set installCRDs=true $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.0.0 \\ --set installCRDs=true $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace djaks \\ --set installCRDs=true You can verify that the installation was successful by checking the namespace for running pods. You should see three cert-manager pods running! $ kubectl get pods --namespace djaks","title":"Install Jetstack/Cert-Manager"},{"location":"notes/kubernetes/sample-django-deployment/#create-an-acme-clusterissuer","text":"ACME stands for Automated Certificate Management Environment. As we want to issue the certificates for the scope of the cluster, let\u2019s use a ClusterIssuer resource. You can find more on Issuers here . Let\u2019s checkout the ClusterIssuer manifest file. # k8s/cluster-issuer.yaml apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: # name: letsencrypt-staging name: letsencrypt-prod spec: acme: email: user@example.com # server: https://acme-staging-v02.api.letsencrypt.org/directory server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # name: letsencrypt-staging name: letsencrypt-prod solvers: - http01: ingress: class: nginx Create the ClusterIssuer in your cluster: $ kubectl apply -f cluster-issuer.yaml --namespace djaks Experimental command: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.2/cert-manager.yaml kubectl delete secret {NAME OF THE SECRET NAMED ON THE CERTIFICATE HERE} ... Next I listed the ingresses with, $ kubectl --namespace djaks get ing NAME HOSTS q ADDRESS PORTS AGE ingress-resource-rules django-aks-ingress.westeurope.cloudapp.azure.com 10.240.0.4,10.240.0.5 80, 443 9m48s and certificates with $ kubectl get all --namespace cert-manager $ kubectl --namespace djaks describe certificate tls-secret $ kubectl --namespace djaks describe certificaterequest tls-secret-skgpp kubectl --namespace djaks describe clusterissuer letsencrypt-prod helm --namespace ingress uninstall nginx-ingress ....","title":"Create an ACME ClusterIssuer"},{"location":"notes/kubernetes/sample-django-deployment/#step-8-deploy-the-django-application","text":"","title":"Step 8: Deploy the Django application"},{"location":"notes/kubernetes/sample-django-deployment/#apply-the-deployment-manifest","text":"Note, be sure to change image: in yours... # k8s/webapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: django-web-app labels: app: django-web spec: replicas: 2 selector: matchLabels: app: django-web template: metadata: labels: app: django-web spec: containers: - name: django-web-container imagePullPolicy: Always image: djangoaksregistrybb.azurecr.io/django-aks:v1.0.0 ports: - containerPort: 8010 env: - name: HOST_IP valueFrom: fieldRef: fieldPath: status.hostIP imagePullSecrets: - name: acr-secret Apply the Deployment manifest to the cluster. $ kubectl apply -f webapp-deployment.yaml --namespace djaks You can verify that the django-web-app Pods are running correctly by checking them out: kubectl get pods --namespace djaks If something went wrong you can look at the events that happened in the Pod by asking kubectl to describe the Pod. You need to specify a specific Pod name of the Pods you listed in the previous command. $ kubectl describe pod django-web-app-#########-##### --namespace djaks","title":"Apply the Deployment manifest"},{"location":"notes/kubernetes/sample-django-deployment/#apply-the-service-manifest","text":"# k8s/webapp-service.yaml apiVersion: v1 kind: Service metadata: name: webapp spec: type: ClusterIP ports: - port: 80 targetPort: 8010 selector: app: django-web $ kubectl apply -f webapp-service.yaml --namespace djaks The service is exposed on an internal IP with the ClusterIP ServiceSpec. This means that the service is only accessible from within the cluster, the Ingress controller will be publicly exposed and route the traffic to your application as we will see in a moment. You can verify now that the webapp service is available. $ kubectl get svc --namespace djaks","title":"Apply the Service manifest"},{"location":"notes/kubernetes/sample-django-deployment/#step-9-set-up-the-ingress-routing","text":"","title":"Step 9: Set up the Ingress routing"},{"location":"notes/kubernetes/sample-django-deployment/#define-the-ingress-rules","text":"Let\u2019s have a look at the Ingress routing file. # k8s/ingress-routing.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-resource-rules annotations: kubernetes.io/ingress.class: nginx # cert-manager.io/cluster-issuer: letsencrypt-staging cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: tls: - hosts: - django-aks-ingress.westeurope.cloudapp.azure.com secretName: tls-secret rules: - host: django-aks-ingress.westeurope.cloudapp.azure.com http: paths: - backend: serviceName: webapp servicePort: 80 path: /(.*) Apply the Ingress to your cluster. $ kubectl apply -f ingress-routing.yaml --namespace djaks","title":"Define the Ingress rules"},{"location":"notes/kubernetes/sample-django-deployment/#configure-the-django-allowed-host","text":"Go to the settings.py file and add your FQDN domain to the ALLOWED_HOSTS list. # app/app/settings.py ... ALLOWED_HOSTS = ['django-aks-ingress.westeurope.cloudapp.azure.com'] ... Now re-build the Docker container image of your new application version locally. $ docker build -f Dockerfile -t django-aks:v1.0.1 . Tag the image for your remote ACR registry. $ docker tag django-aks:v1.0.1 djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 Push the image to the ACR registry. $ docker push djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 If you get an authentication error, login to the ACR registry and try again. $ az acr login --name djangoaksregistrybb Now we have to let the cluster know that we want to update our django-web-app Deployment. $ kubectl set image deployment django-web-app django-web-container=djangoaksregistrybb.azurecr.io/django-aks:v1.0.1 --namespace djaks","title":"Configure the Django allowed host"},{"location":"notes/kubernetes/sample-django-deployment/#step-10-clean-up-resources","text":"If you want to get rid of the created resources, you can clean them up. I will present two clean up options!","title":"Step 10: Clean up resources"},{"location":"notes/kubernetes/sample-django-deployment/#keep-the-azure-resources-wipe-the-cluster-namespace","text":"When you want to keep the Azure resources, then you are good to go by deleting the namespace in which we created the cluster resources. $ kubectl delete namespace djaks","title":"Keep the Azure resources, wipe the cluster namespace"},{"location":"notes/kubernetes/sample-django-deployment/#delete-all-resources","text":"When you want to get back to a clean slate and make sure the paid Azure resources are removed as well, delete the Azure resource group we created in the beginning. $ az group delete --name djangoAKS That\u2019s it, we are done!","title":"Delete all resources"},{"location":"notes/kubernetes/sample-flask-deployment/","text":"Flask_Deploy We're going to document this demo . First we need to create an aks cluster. Source Be sure to login before beginning: $ az login Create resource group myResourceGroup=\"flask_demo\" location=\"eastus\" az group create --name $myResourceGroup --location $location Create azure container registry az acr create --resource-group $myResourceGroup \\ --name bbearcecontainerregistry --sku Basic Create azure kubernetes cluster By default, the Azure CLI automatically enables RBAC (Role Based Access Control) when you create an AKS cluster myAKSCluster=\"flaskAKSCluster\" acrName=\"bbearcecontainerregistry\" az aks create \\ --resource-group $myResourceGroup \\ --name $myAKSCluster \\ --node-count 1 \\ --generate-ssh-keys \\ --attach-acr $acrName Install kubectl If not already installed, install with: az aks install-cli Connect to cluster using kubectl az aks get-credentials --resource-group $myResourceGroup --name $myAKSCluster Test connection: kubectl get nodes Now we are all setup to begin the real tutorial: Kubernetes Ingress with TLS/SSL git clone this project . You should have these files: app-namespace.yaml app1-deploy-svc.yaml app1-deploy-svc.yaml app-ingress.yaml ssl-tls-cluster-issuer.yaml ssl-tls-ingress.yaml Dockerfile Build an image for deployment: Dockerfile: FROM python:latest WORKDIR /app COPY app.py . COPY requirements.txt . RUN pip install -r requirements.txt ENV FLASK_APP=app.py ENTRYPOINT [\"flask\",\"run\"] CMD [\"--host\",\"0.0.0.0\", \"--port\", \"5000\"] Build Build Step: docker build -t bbearcecontainerregistry.azurecr.io/flaskapp . Push Login: az acr login -n bbearcecontainerregistry Push: docker push bbearcecontainerregistry.azurecr.io/flaskapp Deploy App Set these files up to get ready for deployment: I editied the app-deploy to this from the two that are default: It's important to run this file with LoadBalancer as the type in the Service section. app-deploy-svc.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: webapp-deploy namespace: app spec: selector: matchLabels: app: webapp replicas: 1 template: metadata: labels: app: webapp spec: containers: - name: webapp image: bbearcecontainerregistry.azurecr.io/flaskapp ports: - containerPort: 5000 --- kind: Service apiVersion: v1 metadata: name: webapp-svc namespace: app spec: selector: app: webapp ports: - port: 80 type: LoadBalancer # ClusterIP # NodePort # Ingress Then I edited the app-ingress.yaml to this app-ingress-modified.yaml : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: webapp-ingress namespace: app annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: / - host: front.end.20.185.73.238.nip.io # change the IP address here Now deploy everything. Note that we delete and re-apply the app-deploy-svc.yaml with type: ClusterIP . app-namespace.yaml: apiVersion: v1 kind: Namespace metadata: name: app # Create a namespace for the apps kubectl apply -f app-namespace.yaml # Deploy flask app kubectl apply -f app-deploy-svc.yaml # Deploy the 2 sample apps into Kubernetes to follow original demo #kubectl apply -f app1-deploy-svc.yaml #kubectl apply -f app2-deploy-svc.yaml # Get the 2 public IP addresses kubectl get services --namespace app # Add the Helm chart for Nginx Ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Helm (v3) chart for nginx ingress controller # (If using Bash instead of Powershell, replace ` with \\) helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux # Get the Ingress Controller public IP address kubectl get services --namespace ingress # Update the service type to ClusterIP instead of LoadBalancer, ignore if it was always ClusterIP # in app-deploy-svc.yaml file # Delete and redeploy the service for the update to take effect kubectl delete -f app-deploy-svc.yaml kubectl apply -f app-deploy-svc.yaml # This is the original tutorial: # kubectl delete -f app1-deploy-svc.yaml # kubectl delete -f app2-deploy-svc.yaml # kubectl apply -f app1-dekubectl apply -f app-ingress-modified.yaml ploy-svc.yaml # kubectl apply -f app2-deploy-svc.yaml # Deploy the Ingress resource into Kubernetes kubectl apply -f app-ingress-modified.yaml # Cleanup resources kubectl delete -f app-deploy-svc.yaml kubectl delete -f app-deploy-svc.yaml kubectl delete -f app-namespace.yaml helm delete app-ingress --namespace ingress kubectl delete namespace ingress Cert Manager In this second part of the lab, we will enable HTTPS in Kubernetes using Cert Manager and Lets Encrypt. The Cert Manager is used to automatically generate and configure Let's Encrypt certificates. Create a namespace for Cert Manager kubectl create namespace cert-manager Get the Helm Chart for Cert Manager helm repo add jetstack https://charts.jetstack.io helm repo update Install Cert Manager using Helm charts Use option [3], [1] and [2] didn't work for me (10/27/2020) [1] Warning Install CRDs manually for v0.14.0 kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.crds.yaml Then install cert-manager helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --set installCRDs=true \\ --version v0.14.0 or maybe: [2] Warning Label the cert-manager namespace to disable resource validation kubectl label namespace ingress cert-manager.io/disable-validation=true helm install \\ cert-manager \\ --namespace ingress \\ --version v0.16.1 \\ --set installCRDs=true \\ --set nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ jetstack/cert-manager or maybe [3] Installing with regular manifests (tried at 8:15pm) source Note: If you\u2019re using a kubectl version below v1.19.0-rc.1 you will have issues updating the CRDs. For more info see the v0.16 upgrade notes Kubernetes 1.15+: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.yaml Check the created Pods kubectl get pods --namespace cert-manager Install the Cluster Issuer WARNING, you must change the email address to yours. The one in there might work if it is live, but only real email addresses are allowed. Also: Must install CRDs (disregard if you used step [3] during cert-manager install) What worked best for me was using the staging cert to practice and then the real one. I believe there is a limit for real certs so you don't want to make them make you wait for another real one to issue while you work out the kinks. Make sure ssl-tls-cluster-issuer.yaml has -staging appended to the letsencrypt authority. sst-tls-cluster-issuer.yaml: apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: bbearce@gmail.com privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: nginx Apply cluster issuer: kubectl apply --namespace app -f ssl-tls-cluster-issuer.yaml Install the Ingress resource configured with TLS/SSL [1] remove old non-https ingress app-ingress-modified.yaml kubectl delete -f app-ingress-modified.yaml Then apply new ssl-tls for https certs. This is the same ingress yaml but we changed the parameters to make a more secure ingress. kubectl apply --namespace app -f ssl-tls-ingress.yaml [2] sync this deploy with the cert-manager by using -staging : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ssl-tls-ingress annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-staging spec: tls: - hosts: - front.end.52.190.30.229.nip.io # update IP address here secretName: app-web-cert rules: - host: front.end.52.190.30.229.nip.io # update IP address here http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: / Verify that the certificate was issued kubectl get certificate --namespace app kubectl describe cert app-web-cert --namespace app If all this works go back and edit ssl-tls-ingress.yaml and ssl-tls-cluster-issuer.yaml and remove -staging from letsencrypt and re-apply them. ssl-tls-ingress.yaml: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ssl-tls-ingress annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt spec: tls: - hosts: - front.end.52.190.30.229.nip.io # update IP address here secretName: app-web-cert rules: - host: front.end.52.190.30.229.nip.io # update IP address here http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: / ssl-tls-cluster-issuer.yaml: apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: bbearce@gmail.com privateKeySecretRef: name: letsencrypt solvers: - http01: ingress: class: nginx Re-apply new ingress and cluser issuer: kubectl apply --namespace app -f ssl-tls-ingress.yaml kubectl apply --namespace app -f ssl-tls-cluster-issuer.yaml Check the services kubectl get services -n app Now test the app with HTTPS: https://frontend.<ip-address>.nip.io","title":"Sample Deployment (Flask)"},{"location":"notes/kubernetes/sample-flask-deployment/#flask_deploy","text":"We're going to document this demo . First we need to create an aks cluster. Source Be sure to login before beginning: $ az login","title":"Flask_Deploy"},{"location":"notes/kubernetes/sample-flask-deployment/#create-resource-group","text":"myResourceGroup=\"flask_demo\" location=\"eastus\" az group create --name $myResourceGroup --location $location","title":"Create resource group"},{"location":"notes/kubernetes/sample-flask-deployment/#create-azure-container-registry","text":"az acr create --resource-group $myResourceGroup \\ --name bbearcecontainerregistry --sku Basic","title":"Create azure container registry"},{"location":"notes/kubernetes/sample-flask-deployment/#create-azure-kubernetes-cluster","text":"By default, the Azure CLI automatically enables RBAC (Role Based Access Control) when you create an AKS cluster myAKSCluster=\"flaskAKSCluster\" acrName=\"bbearcecontainerregistry\" az aks create \\ --resource-group $myResourceGroup \\ --name $myAKSCluster \\ --node-count 1 \\ --generate-ssh-keys \\ --attach-acr $acrName","title":"Create azure kubernetes cluster"},{"location":"notes/kubernetes/sample-flask-deployment/#install-kubectl","text":"If not already installed, install with: az aks install-cli","title":"Install kubectl"},{"location":"notes/kubernetes/sample-flask-deployment/#connect-to-cluster-using-kubectl","text":"az aks get-credentials --resource-group $myResourceGroup --name $myAKSCluster Test connection: kubectl get nodes Now we are all setup to begin the real tutorial:","title":"Connect to cluster using kubectl"},{"location":"notes/kubernetes/sample-flask-deployment/#kubernetes-ingress-with-tlsssl","text":"git clone this project . You should have these files: app-namespace.yaml app1-deploy-svc.yaml app1-deploy-svc.yaml app-ingress.yaml ssl-tls-cluster-issuer.yaml ssl-tls-ingress.yaml","title":"Kubernetes Ingress with TLS/SSL"},{"location":"notes/kubernetes/sample-flask-deployment/#dockerfile","text":"Build an image for deployment: Dockerfile: FROM python:latest WORKDIR /app COPY app.py . COPY requirements.txt . RUN pip install -r requirements.txt ENV FLASK_APP=app.py ENTRYPOINT [\"flask\",\"run\"] CMD [\"--host\",\"0.0.0.0\", \"--port\", \"5000\"]","title":"Dockerfile"},{"location":"notes/kubernetes/sample-flask-deployment/#build","text":"Build Step: docker build -t bbearcecontainerregistry.azurecr.io/flaskapp .","title":"Build"},{"location":"notes/kubernetes/sample-flask-deployment/#push","text":"Login: az acr login -n bbearcecontainerregistry Push: docker push bbearcecontainerregistry.azurecr.io/flaskapp","title":"Push"},{"location":"notes/kubernetes/sample-flask-deployment/#deploy","text":"","title":"Deploy"},{"location":"notes/kubernetes/sample-flask-deployment/#app","text":"Set these files up to get ready for deployment: I editied the app-deploy to this from the two that are default: It's important to run this file with LoadBalancer as the type in the Service section. app-deploy-svc.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: webapp-deploy namespace: app spec: selector: matchLabels: app: webapp replicas: 1 template: metadata: labels: app: webapp spec: containers: - name: webapp image: bbearcecontainerregistry.azurecr.io/flaskapp ports: - containerPort: 5000 --- kind: Service apiVersion: v1 metadata: name: webapp-svc namespace: app spec: selector: app: webapp ports: - port: 80 type: LoadBalancer # ClusterIP # NodePort #","title":"App"},{"location":"notes/kubernetes/sample-flask-deployment/#ingress","text":"Then I edited the app-ingress.yaml to this app-ingress-modified.yaml : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: webapp-ingress namespace: app annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: / - host: front.end.20.185.73.238.nip.io # change the IP address here Now deploy everything. Note that we delete and re-apply the app-deploy-svc.yaml with type: ClusterIP . app-namespace.yaml: apiVersion: v1 kind: Namespace metadata: name: app # Create a namespace for the apps kubectl apply -f app-namespace.yaml # Deploy flask app kubectl apply -f app-deploy-svc.yaml # Deploy the 2 sample apps into Kubernetes to follow original demo #kubectl apply -f app1-deploy-svc.yaml #kubectl apply -f app2-deploy-svc.yaml # Get the 2 public IP addresses kubectl get services --namespace app # Add the Helm chart for Nginx Ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Helm (v3) chart for nginx ingress controller # (If using Bash instead of Powershell, replace ` with \\) helm install app-ingress ingress-nginx/ingress-nginx \\ --namespace ingress \\ --create-namespace \\ --set controller.replicaCount=2 \\ --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux # Get the Ingress Controller public IP address kubectl get services --namespace ingress # Update the service type to ClusterIP instead of LoadBalancer, ignore if it was always ClusterIP # in app-deploy-svc.yaml file # Delete and redeploy the service for the update to take effect kubectl delete -f app-deploy-svc.yaml kubectl apply -f app-deploy-svc.yaml # This is the original tutorial: # kubectl delete -f app1-deploy-svc.yaml # kubectl delete -f app2-deploy-svc.yaml # kubectl apply -f app1-dekubectl apply -f app-ingress-modified.yaml ploy-svc.yaml # kubectl apply -f app2-deploy-svc.yaml # Deploy the Ingress resource into Kubernetes kubectl apply -f app-ingress-modified.yaml # Cleanup resources kubectl delete -f app-deploy-svc.yaml kubectl delete -f app-deploy-svc.yaml kubectl delete -f app-namespace.yaml helm delete app-ingress --namespace ingress kubectl delete namespace ingress","title":"Ingress"},{"location":"notes/kubernetes/sample-flask-deployment/#cert-manager","text":"In this second part of the lab, we will enable HTTPS in Kubernetes using Cert Manager and Lets Encrypt. The Cert Manager is used to automatically generate and configure Let's Encrypt certificates.","title":"Cert Manager"},{"location":"notes/kubernetes/sample-flask-deployment/#create-a-namespace-for-cert-manager","text":"kubectl create namespace cert-manager","title":"Create a namespace for Cert Manager"},{"location":"notes/kubernetes/sample-flask-deployment/#get-the-helm-chart-for-cert-manager","text":"helm repo add jetstack https://charts.jetstack.io helm repo update","title":"Get the Helm Chart for Cert Manager"},{"location":"notes/kubernetes/sample-flask-deployment/#install-cert-manager-using-helm-charts","text":"Use option [3], [1] and [2] didn't work for me (10/27/2020) [1] Warning Install CRDs manually for v0.14.0 kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.crds.yaml Then install cert-manager helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --set installCRDs=true \\ --version v0.14.0 or maybe: [2] Warning Label the cert-manager namespace to disable resource validation kubectl label namespace ingress cert-manager.io/disable-validation=true helm install \\ cert-manager \\ --namespace ingress \\ --version v0.16.1 \\ --set installCRDs=true \\ --set nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux \\ jetstack/cert-manager or maybe [3] Installing with regular manifests (tried at 8:15pm) source Note: If you\u2019re using a kubectl version below v1.19.0-rc.1 you will have issues updating the CRDs. For more info see the v0.16 upgrade notes Kubernetes 1.15+: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.yaml","title":"Install Cert Manager using Helm charts"},{"location":"notes/kubernetes/sample-flask-deployment/#check-the-created-pods","text":"kubectl get pods --namespace cert-manager","title":"Check the created Pods"},{"location":"notes/kubernetes/sample-flask-deployment/#install-the-cluster-issuer","text":"WARNING, you must change the email address to yours. The one in there might work if it is live, but only real email addresses are allowed. Also: Must install CRDs (disregard if you used step [3] during cert-manager install) What worked best for me was using the staging cert to practice and then the real one. I believe there is a limit for real certs so you don't want to make them make you wait for another real one to issue while you work out the kinks. Make sure ssl-tls-cluster-issuer.yaml has -staging appended to the letsencrypt authority. sst-tls-cluster-issuer.yaml: apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: bbearce@gmail.com privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: nginx Apply cluster issuer: kubectl apply --namespace app -f ssl-tls-cluster-issuer.yaml","title":"Install the Cluster Issuer"},{"location":"notes/kubernetes/sample-flask-deployment/#install-the-ingress-resource-configured-with-tlsssl","text":"[1] remove old non-https ingress app-ingress-modified.yaml kubectl delete -f app-ingress-modified.yaml Then apply new ssl-tls for https certs. This is the same ingress yaml but we changed the parameters to make a more secure ingress. kubectl apply --namespace app -f ssl-tls-ingress.yaml [2] sync this deploy with the cert-manager by using -staging : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ssl-tls-ingress annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-staging spec: tls: - hosts: - front.end.52.190.30.229.nip.io # update IP address here secretName: app-web-cert rules: - host: front.end.52.190.30.229.nip.io # update IP address here http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: /","title":"Install the Ingress resource configured with TLS/SSL"},{"location":"notes/kubernetes/sample-flask-deployment/#verify-that-the-certificate-was-issued","text":"kubectl get certificate --namespace app kubectl describe cert app-web-cert --namespace app If all this works go back and edit ssl-tls-ingress.yaml and ssl-tls-cluster-issuer.yaml and remove -staging from letsencrypt and re-apply them. ssl-tls-ingress.yaml: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ssl-tls-ingress annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt spec: tls: - hosts: - front.end.52.190.30.229.nip.io # update IP address here secretName: app-web-cert rules: - host: front.end.52.190.30.229.nip.io # update IP address here http: paths: - backend: serviceName: webapp-svc servicePort: 80 path: / ssl-tls-cluster-issuer.yaml: apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: bbearce@gmail.com privateKeySecretRef: name: letsencrypt solvers: - http01: ingress: class: nginx Re-apply new ingress and cluser issuer: kubectl apply --namespace app -f ssl-tls-ingress.yaml kubectl apply --namespace app -f ssl-tls-cluster-issuer.yaml","title":"Verify that the certificate was issued"},{"location":"notes/kubernetes/sample-flask-deployment/#check-the-services","text":"kubectl get services -n app Now test the app with HTTPS: https://frontend.<ip-address>.nip.io","title":"Check the services"},{"location":"notes/layouts/holy_grail/","text":"The Holy Grail This is apparently a hard style to make...or it was. Now that HTML5 and CSS3 have made lots of updates, styling is easier now. Below is my implementation of this layout. Preview Let's discuss what happened. First the HTML layout. HTML <!DOCTYPE html> <html> <head> <title>New Site</title> <link rel=\"stylesheet\" href=\"./style.css\"></link> <script src=\"https://kit.fontawesome.com/36fc456441.js\" crossorigin=\"anonymous\"></script> </head> <body> <header> <i class=\"fab fa-pied-piper-alt fa-5x\"></i> <ul class=contact-info> <ul>bbearce@somewhere.org</ul> <ul>(123)-456-7890</ul> <ul>Cool St., City, ST 01234</ul> </ul> <ul class=navbar> <li>Home</li> <li>About</li> <li>Details</li> <li>Jokes</li> </ul> </header> <div class=content> <div class=left> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> <div class=middle> <p>Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.</p> </div> <div class=right> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> </div> <footer> <div class=icons> <i class=\"fas fa-cloud\"></i> <i class=\"fas fa-heart\"></i> <i class=\"fas fa-car\"></i> <i class=\"fas fa-file\"></i> <i class=\"fas fa-bars\"></i> </div> </footer> <script type=\"text/javascript\" src=\"./javascript.js\"></script> </body> </html> scss html { height: 94vh; } body { height: 100%; background: rgba(100,100,100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; li { display: inline-block; width: 50px; border: 1px solid; } } .contact-info { float: right; } } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0,0,100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100,100,100, 0.1); display: inline-block; } .right { color: green; float:right; width: 20%; height: 100%; background: rgba(0,100,0, 0.1); display: inline-block; } footer { background: rgba(100,0,0, 0.1); .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } } CSS html { height: 94vh; } body { height: 100%; background: rgba(100, 100, 100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; } header .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; } header .navbar li { display: inline-block; width: 50px; border: 1px solid; } header .contact-info { float: right; } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0, 0, 100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100, 100, 100, 0.1); display: inline-block; } .right { color: green; float: right; width: 20%; height: 100%; background: rgba(0, 100, 0, 0.1); display: inline-block; } footer { background: rgba(100, 0, 0, 0.1); } footer .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } /*# sourceMappingURL=style.css.map */ JS // none requred for this example","title":"Holy Grail"},{"location":"notes/layouts/holy_grail/#the-holy-grail","text":"This is apparently a hard style to make...or it was. Now that HTML5 and CSS3 have made lots of updates, styling is easier now. Below is my implementation of this layout. Preview Let's discuss what happened. First the HTML layout.","title":"The Holy Grail"},{"location":"notes/layouts/holy_grail/#html","text":"<!DOCTYPE html> <html> <head> <title>New Site</title> <link rel=\"stylesheet\" href=\"./style.css\"></link> <script src=\"https://kit.fontawesome.com/36fc456441.js\" crossorigin=\"anonymous\"></script> </head> <body> <header> <i class=\"fab fa-pied-piper-alt fa-5x\"></i> <ul class=contact-info> <ul>bbearce@somewhere.org</ul> <ul>(123)-456-7890</ul> <ul>Cool St., City, ST 01234</ul> </ul> <ul class=navbar> <li>Home</li> <li>About</li> <li>Details</li> <li>Jokes</li> </ul> </header> <div class=content> <div class=left> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> <div class=middle> <p>Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.</p> </div> <div class=right> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> </div> <footer> <div class=icons> <i class=\"fas fa-cloud\"></i> <i class=\"fas fa-heart\"></i> <i class=\"fas fa-car\"></i> <i class=\"fas fa-file\"></i> <i class=\"fas fa-bars\"></i> </div> </footer> <script type=\"text/javascript\" src=\"./javascript.js\"></script> </body> </html>","title":"HTML"},{"location":"notes/layouts/holy_grail/#scss","text":"html { height: 94vh; } body { height: 100%; background: rgba(100,100,100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; li { display: inline-block; width: 50px; border: 1px solid; } } .contact-info { float: right; } } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0,0,100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100,100,100, 0.1); display: inline-block; } .right { color: green; float:right; width: 20%; height: 100%; background: rgba(0,100,0, 0.1); display: inline-block; } footer { background: rgba(100,0,0, 0.1); .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } }","title":"scss"},{"location":"notes/layouts/holy_grail/#css","text":"html { height: 94vh; } body { height: 100%; background: rgba(100, 100, 100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; } header .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; } header .navbar li { display: inline-block; width: 50px; border: 1px solid; } header .contact-info { float: right; } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0, 0, 100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100, 100, 100, 0.1); display: inline-block; } .right { color: green; float: right; width: 20%; height: 100%; background: rgba(0, 100, 0, 0.1); display: inline-block; } footer { background: rgba(100, 0, 0, 0.1); } footer .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } /*# sourceMappingURL=style.css.map */","title":"CSS"},{"location":"notes/layouts/holy_grail/#js","text":"// none requred for this example","title":"JS"},{"location":"notes/nginx/nginx/","text":"Nginx","title":"Basics"},{"location":"notes/nginx/nginx/#nginx","text":"","title":"Nginx"},{"location":"notes/python/Environments/","text":"Environments You need to install python first. MGB's QTIM Standards courtesy of Chris Bridge give a good layout idea for us to follow. Venv (virtualenv in python2) packaging.python.org Install pip python3 -m pip install --user --upgrade pip python3 -m pip --version python3 -m pip install --user virtualenv Create Virtual Environment python3 -m venv env bbearce@pop-os:~/Desktop/venv_demo$ python3 -m venv env bbearce@pop-os:~/Desktop/venv_demo$ ls env bbearce@pop-os:~/Desktop/venv_demo$ source env/bin/activate (env) bbearce@pop-os:~/Desktop/venv_demo$ deactivate bbearce@pop-os:~/Desktop/venv_demo$ . env/bin/activate # abreviated (env) bbearce@pop-os:~/Desktop/venv_demo$ Install Packages python3 -m pip install pandas python3 -m pip uninstall -y pandas Short: pip install pandas pip uninstall -y pandas (env) bbearce@pop-os:~/Desktop/venv_demo$ python3 -m pip install pandas Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple Collecting pandas Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/d9/26/895a49ebddb4211f2d777150f38ef9e538deff6df7e179a3624c663efc98/pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB) Collecting numpy>=1.23.2 (from pandas) Obtaining dependency information for numpy>=1.23.2 from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.5/58.5 kB 1.2 MB/s eta 0:00:00 Collecting python-dateutil>=2.8.2 (from pandas) Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB) Collecting pytz>=2020.1 (from pandas) Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB) Collecting tzdata>=2022.1 (from pandas) Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 341.8/341.8 kB 8.4 MB/s eta 0:00:00 Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas) Using cached six-1.16.0-py2.py3-none-any.whl (11 kB) Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.6/12.6 MB 46.8 MB/s eta 0:00:00 Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.2/18.2 MB 34.0 MB/s eta 0:00:00 Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 502.5/502.5 kB 51.5 MB/s eta 0:00:00 Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas Successfully installed numpy-1.26.0 pandas-2.1.0 python-dateutil-2.8.2 pytz-2 (env) bbearce@pop-os:~/Desktop/venv_demo$ (env) bbearce@pop-os:~/Desktop/venv_demo$ python3 -m pip uninstall -y pandas Found existing installation: pandas 2.1.0 Uninstalling pandas-2.1.0: Successfully uninstalled pandas-2.1.0 It's best practive to put virtual environments in ~/.virtualenvs. VSCode and other IDE's will look here by default for python virtual environments. bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.virtualenvs total 16 drwxrwxr-x 4 bbearce bbearce 4096 Jul 22 2022 . drwxr-xr-x 77 bbearce bbearce 4096 Sep 17 15:40 .. drwxrwxr-x 6 bbearce bbearce 4096 Jul 22 2022 venv3.10.4 VSCode Example: Poetry Poetry Install curl -sSL https://install.python-poetry.org | python3 - Uninstall curl -sSL https://install.python-poetry.org | python3 - --uninstall Version $ poetry --version Poetry (version 1.6.1) Project Setup cd ~/Desktop poetry new poetry-demo bbearce@terry:~/Desktop$ ls poetry-demo bbearce@terry:~/Desktop$ ls poetry-demo poetry_demo poetry.lock pyproject.toml README.md tests Dependencies Store them under [tool.poetry.dependencies] in pyproject.toml: [tool.poetry.dependencies] python = \"^3.10\" pendulum = \"^2.1.2\" Install with poetry install : bbearce@terry:~/Desktop/poetry-demo$ poetry install Installing dependencies from lock file No dependencies to install or update Installing the current project: poetry-demo (0.1.0) Adding Dependencies: You can specify dependencies in pyproject.toml as above or use poetry's cli to add them: poetry add pendulum This makes a virtual environment automatically: bbearce@terry:~/Desktop/poetry-demo$ poetry add pendulum Creating virtualenv poetry-demo-yD2F6f32-py3.10 in /home/bbearce/.cache/pypoetry/virtualenvs Using version ^2.1.2 for pendulum Updating dependencies Resolving dependencies... Downloading https://files.pythonhosted.org/packages/e0/4f/4474bda990ee740a020cbc3eb271925ef7daa7c844424 Resolving dependencies... (0.6s) Package operations: 4 installs, 0 updates, 0 removals \u2022 Installing six (1.16.0) \u2022 Installing python-dateutil (2.8.2) \u2022 Installing pytzdata (2020.1) \u2022 Installing pendulum (2.1.2) Writing lock file By default, Poetry creates a virtual environment in {cache-dir}/virtualenvs . You can change the cache-dir value by editing the Poetry configuration. Additionally, you can use the virtualenvs.in-project configuration variable to create virtual environments within your project directory. Finally virtualenvs.create let's poetry know if you want it to create a virtualenv or not. If false , if it detects an already enabled virtual environment or an existing one in {cache-dir}/virtualenvs or {project-dir}/.venv it will install dependencies into them, otherwise it will install dependencies into the systems python environment. If a virtualenv isn't activated it will use system PATH and will use system python and or pyenv if that is what you are using. We explore this a litte deeper below Config As mentioned above we can set these config variables: cache-dir virtualenvs.in-project * virtualenvs.create cache-dir Type: string Environment Variable: POETRY_CACHE_DIR The path to the cache directory used by Poetry. Defaults to one of the following directories: macOS: ~/Library/Caches/pypoetry Windows: C:\\Users\\ \\AppData\\Local\\pypoetry\\Cache Unix: ~/.cache/pypoetry ls ~/.cache/pypoetry artifacts cache virtualenvs See project envs: bbearce@terry:~/Desktop/poetry-demo$ poetry env list poetry-demo-yD2F6f32-py3.10 (Activated) See project config: bbearce@terry:~/Desktop/poetry-demo$ poetry config --list cache-dir = \"/home/bbearce/.cache/pypoetry\" experimental.system-git-client = false installer.max-workers = null installer.modern-installation = true installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = null virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /home/bbearce/.cache/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" Set project config: poetry config cache-dir ./cache bbearce@terry:~/Desktop/poetry-demo$ cat ~/.config/pypoetry/config.toml cache-dir = \"cache\" # reset poetry config cache-dir /home/bbearce/.cache/pypoetry virtualenvs.in-project Type: boolean Default: None Environment Variable: POETRY_VIRTUALENVS_IN_PROJECT Create the virtualenv inside the project\u2019s root directory. If not set explicitly, poetry by default will create a virtual environment under {cache-dir}/virtualenvs or use the {project-dir}/.venv directory if one already exists. If set to true, the virtualenv will be created and expected in a folder named .venv within the root directory of the project. poetry config virtualenvs.in-project true poetry config virtualenvs.in-project false # to unset Now install: bbearce@pop-os:~/Desktop/poetry-demo$ poetry install Installing dependencies from lock file Package operations: 11 installs, 0 updates, 0 removals \u2022 Installing six (1.16.0) \u2022 Installing certifi (2023.7.22) \u2022 Installing charset-normalizer (3.3.0) \u2022 Installing idna (3.4) \u2022 Installing numpy (1.26.0) \u2022 Installing python-dateutil (2.8.2) \u2022 Installing pytz (2023.3.post1) \u2022 Installing tzdata (2023.3) \u2022 Installing urllib3 (2.0.6) \u2022 Installing pandas (2.1.1) \u2022 Installing requests (2.31.0) Installing the current project: poetry-demo (0.1.0) bbearce@pop-os:~/Desktop/poetry-demo$ ls -la .venv/lib/python3.10/site-packages/ | grep pandas drwxrwxr-x 15 bbearce bbearce 4096 Oct 7 18:16 pandas drwxrwxr-x 2 bbearce bbearce 4096 Oct 7 18:16 pandas-2.1.1.dist-info virtualenvs.create This config setting tells poetry to make a virtualenv in ./.venv or {cache-dir}/virtualenvs . We've seen how to create venvs but lets' see what happens if we don't. bearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.create false bbearce@pop-os:~/Desktop/poetry-demo$ poetry config --list cache-dir = \"/home/bbearce/.cache/pypoetry\" ... virtualenvs.create = false virtualenvs.in-project = false ... virtualenvs.path = \"{cache-dir}/virtualenvs\" # /home/bbearce/.cache/pypoetry/virtualenvs ... In this mode poetry will use the first python on the PATH . This could be: the system python an activated virtual environment * a local pyenv * a global pyenv * ... Activate Virtual Environments Source like normal: bash: .venv/bin/activatae: No such file or directory bbearce@pop-os:~/Desktop/poetry-demo$ source .venv/bin/activate (poetry-demo-py3.10) bbearce@pop-os:~/Desktop/poetry-demo$ Or use poetry shell to activate the virtual environment within a nested shell. bbearce@terry:~/Desktop/poetry-demo$ poetry shell Spawning shell within /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10 . /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10/bin/activate bbearce@terry:~/Desktop/poetry-demo$ . /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10/bin/activate # Deactivate with `exit` (poetry-demo-py3.10) bbearce@terry:~/Desktop/poetry-demo$ exit exit To deactivate the virtual environment and exit this new shell type exit . To deactivate the virtual environment without leaving the shell use deactivate . Git Always commit both the pyproject.toml and poetry.lock files to your project. Use Pyenv environments with Poetry If you want to use pyenv with its virtualenv extension to manage virtual environments, but still leverage Poetry's caching mechanism for PyPI packages, you can follow these steps: bbearce@pop-os:~/Desktop$ pyenv versions * system (set by /home/bbearce/.pyenv/version) 3.10.4 bbearce@pop-os:~/Desktop$ pyenv virtualenv 3.10.4 venv3.10.4 bbearce@pop-os:~/Desktop$ pyenv versions * system (set by /home/bbearce/.pyenv/version) 3.10.4 3.10.4/envs/venv3.10.4 venv3.10.4 --> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 bbearce@pop-os:~/Desktop$ poetry new poetry-demo Created package poetry_demo in poetry-demo bbearce@pop-os:~/Desktop$ cd poetry-demo/ bbearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.create false bbearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.in-project false bbearce@pop-os:~/Desktop/poetry-demo$ pyenv activate venv3.10.4 (venv3.10.4) bbearce@pop-os:~/Desktop/poetry-demo$ poetry add requests Using version ^2.31.0 for requests Updating dependencies Resolving dependencies... (0.2s) Package operations: 5 installs, 0 updates, 0 removals \u2022 Installing certifi (2023.7.22) \u2022 Installing charset-normalizer (3.3.0) \u2022 Installing idna (3.4) \u2022 Installing urllib3 (2.0.6) \u2022 Installing requests (2.31.0) Writing lock file (venv3.10.4) bbearce@pop-os:~/Desktop/poetry-demo$ python Python 3.10.4 (main, Oct 7 2023, 16:49:10) [GCC 11.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import requests >>> import sys >>> sys.executable '/home/bbearce/.pyenv/versions/venv3.10.4/bin/python' >>> Pyenv Official Docs realpython.com (they use the above to guide you) If venv was manual, pyenv is automatic and steamlined to create many different environments and virtual environments. When you use venv you are copying your main python installation into a virtual environment. You are locked into that version for virtual environements unless you manually install different python versions on your system. With pyenv , you do have to have these base python environments, but instead of installing where they normally go on your os, they are collected and managed by pyenv in ~/.pyenv/versions : bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv -rw-r--r-- 1 bbearce bbearce 7 Sep 16 16:02 version drwxr-xr-x 3 bbearce bbearce 4096 Sep 16 16:29 versions ... bbearce@pop-os:~/Desktop/venv_demo$ ls ~/.pyenv/versions 3.11.5 venv3.11.5 Via special pyenv bash scripts, not python installations as with venv above, we can install multiple python versions ( 3.11.5 ) and virtual environments( venv3.11.5 ). They live in the same directory, though the virtual environments are sylinks to ./envs/ under each actual python version. bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions/ drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions/3.11.5/envs/ drwxr-xr-x 5 bbearce bbearce 4096 Sep 16 16:29 venv3.11.5 So you can have any python version with any number of unique virtual environments. Create (install) Environment See environments availabe: pyenv install --list # all pyenv install --list | grep \" 3\\.[678]\" # get specific 3.6.0 3.6-dev 3.6.1 3.6.2 3.6.3 3.6.4 3.6.5 3.6.6 3.6.7 3.6.8 3.7.0 3.7-dev 3.7.1 3.7.2 3.8-dev pyenv install 3.10.4 # pyenv uninstall 3.10.4 # Uninstall # rm -rf ~/.pyenv/versions/3.10.4 # Equivalent bbearce@pop-os:~/Desktop/venv_demo$ pyenv install 3.10.4 Downloading Python-3.10.4.tar.xz... -> https://www.python.org/ftp/python/3.10.4/Python-3.10.4.tar.xz Installing Python-3.10.4... ... Installing Python-3.10.4... Installed Python-3.10.4 to /home/bbearce/.pyenv/versions/3.10.4 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 6 bbearce bbearce 4096 Sep 17 17:03 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 Create Virtual Environment You need a pyenv plugin called pyenv-virtualenv . # Ubuntu based install...check for your os git clone https://github.com/pyenv/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv Create: bbearce@pop-os:~/Desktop/venv_demo$ pyenv virtualenv 3.10.4 <venv-name> bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 6 bbearce bbearce 4096 Sep 17 17:03 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ pyenv virtualenv 3.10.4 venv3.10.4 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 7 bbearce bbearce 4096 Sep 17 17:19 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 17 17:19 venv3.10.4 -> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 Using Environments There are a variety of ways to specify when and which env\\venv you want to use. Here are the most basic. Global By default you are using a global pyenv version specified by a file in ~/.pyenv/version . bbearce@pop-os:~/Desktop/venv_demo$ cat ~/.pyenv/version 3.11.5 Set it with: pyenv global <version> bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by /home/bbearce/.pyenv/version) bbearce@pop-os:~/Desktop/venv_demo$ pyenv versions system 3.10.4 3.10.4/envs/venv3.10.4 * 3.11.5 (set by /home/bbearce/.pyenv/version) 3.11.5/envs/venv3.11.5 venv3.10.4 --> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 venv3.11.5 --> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ python Python 3.11.5 (main, Sep 16 2023, 15:49:34) [GCC 11.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> Shell Activates for quick use (essentially the same as venv ). pyenv shell <version> Regular env: bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell 3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by PYENV_VERSION environment variable) Virtualenv: pyenv activate <version> pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ pyenv activate venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by PYENV_VERSION environment variable) (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ You can use these with shell too. bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by PYENV_VERSION environment variable) However deactivate doesn't work: (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ deactivate pyenv-virtualenv: deactivate must be sourced. Run 'source deactivate' instead of 'deactivate' (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ source deactivate pyenv-virtualenv: deactivate 3.11.5/envs/venv3.11.5 # Need to use shell again to switch (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell 3.11.5 Local This is set for a direcory or project and creates a .python-version inside the directory pyenv local <version> bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by /home/bbearce/.pyenv/version) bbearce@pop-os:~/Desktop/venv_demo$ pyenv local venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ ls -la drwxrwxr-x 5 bbearce bbearce 4096 Sep 17 15:32 env -rw-rw-r-- 1 bbearce bbearce 11 Sep 17 17:29 .python-version -rw-rw-r-- 1 bbearce bbearce 22572 Sep 17 15:48 vscode-venv.jpg (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ cat .python-version venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by /home/bbearce/Desktop/venv_demo/.python-version) (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ rm .python-version # deactivates and removes .python-version bbearce@pop-os:~/Desktop/venv_demo$ PDM pdm.fming.dev/latest Warning: PEP 582 has been rejected PDM was created to mimic npm whereby you don't need virtualenvs. All you need are base pythons and __pypackages__ . Instead of bundling your packages with venvs, which are copies of base environmetns, why don't we use a single 3.11.5 and have projects with packages that all use the same base 3.11.5 version or any other version you'd like. PS: You can and unfortunately should use vitualenvs with pdm as PEP 582 has been rejected which was the whole point. Nonetheless this package manager is nice and organized. It uses .toml files like Poetry to manage depedencies so it is nice still. Install # Linux\\Mac curl -sSL https://pdm.fming.dev/install-pdm.py | python3 - New Project mkdir ~/Documents/pdm_setup_and_experimentation && cd ~/Documents/pdm_setup_and_experimentation pdm init Virtualenvs and not PEP 582 Source You can choose the backend used by PDM to create a virtualenv. Currently it supports three backends: virtualenv(default) venv conda Local Virtual Environment bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm init Creating a pyproject.toml for PDM... Please enter the Python interpreter to use 0. /home/bbearce/.pyenv/shims/python3 (3.11) 1. /home/bbearce/.pyenv/versions/3.11.5/bin/python (3.11) 2. /home/bbearce/.pyenv/versions/3.11.5/bin/python3.11 (3.11) 3. /home/bbearce/.pyenv/shims/python3.11 (3.11) 4. /home/bbearce/.pyenv/shims/python (3.11) 5. /home/bbearce/.pyenv/shims/python3.10 (3.10) 6. /usr/bin/python3.10 (3.10) 7. /home/bbearce/.pyenv/versions/3.10.4/bin/python3.10 (3.10) Please select (0): 6 Would you like to create a virtualenv with /usr/bin/python3.10? [y/n] (y): y Virtualenv is created successfully at /home/bbearce/Documents/pdm_setup_and_experimentation/.venv Is the project a library that is installable? If yes, we will need to ask a few more questions to include the project name and build backend [y/n] (n): n License(SPDX name) (MIT): Author name (Benjamin Bearce): Author email (bbearce@gmail.com): Python requires('*' to allow any) (>=3.10): Project is initialized successfully We can use any environment (regular or virtual) to start this project, including virtual ones we make upon running pdm init . We can see the .venv directory which was made from /usr/bin/python3.10 . bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la total 40 drwxrwxr-x 6 bbearce bbearce 4096 Sep 17 18:25 . drwxr-xr-x 36 bbearce bbearce 4096 Sep 17 14:17 .. -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 69 Sep 17 18:24 .pdm-python drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 18:25 __pycache__ -rw-rw-r-- 1 bbearce bbearce 217 Sep 17 18:25 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 tests drwxrwxr-x 4 bbearce bbearce 4096 Sep 17 18:24 .venv PDM Info: If Project Packages is None (as show below by it not being present), virtualenv mode is enabled. bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm info PDM version: 2.9.2 Python Interpreter: /home/bbearce/Documents/pdm_setup_and_experimentation/.venv/bin/python (3.10) Project Root: /home/bbearce/Documents/pdm_setup_and_experimentation Local Packages: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm info --env { \"implementation_name\": \"cpython\", \"implementation_version\": \"3.10.12\", \"os_name\": \"posix\", \"platform_machine\": \"x86_64\", \"platform_release\": \"6.4.6-76060406-generic\", \"platform_system\": \"Linux\", \"platform_version\": \"#202307241739~1694621917~22.04~ac5e1a8 SMP PREEMPT_DYNAMIC Wed S\", \"python_full_version\": \"3.10.12\", \"platform_python_implementation\": \"CPython\", \"python_version\": \"3.10\", \"sys_platform\": \"linux\" } Create Your Own Virtual Environment # Create a virtualenv based on 3.8 interpreter $ pdm venv create 3.8 # Assign a different name other than the version string $ pdm venv create --name for-test 3.8 # Use venv as the backend to create, support 3 backends: virtualenv(default), venv, conda $ pdm venv create --with venv 3.9 Notice you need an interpreter installed to use correctly: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create 3.8 [VirtualenvCreateError]: Can't resolve python interpreter 3.8 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --name for-test 3.8 [VirtualenvCreateError]: Can't resolve python interpreter 3.8 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --with venv 3.9 [VirtualenvCreateError]: Can't resolve python interpreter 3.9 Use 3.11.5 that we have: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create 3.11.5 Virtualenv /home/bbearce/Documents/pdm_setup_and_experimentation/.venv is created successfully bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la total 44 drwxrwxr-x 6 bbearce bbearce 4096 Sep 17 19:00 . drwxr-xr-x 36 bbearce bbearce 4096 Sep 17 14:17 .. -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 8119 Sep 17 18:39 pdm.lock drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 18:25 __pycache__ -rw-rw-r-- 1 bbearce bbearce 239 Sep 17 18:39 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 tests drwxrwxr-x 4 bbearce bbearce 4096 Sep 17 19:00 .venv bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --name for-test 3.11.5 Virtualenv /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test is created successfully bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --with venv 3.11.5 Virtualenv /home/bbearce/Documents/pdm_setup_and_experimentation/.venv is created successfully Reuse the virtualenv you created elsewhere: This will create file .pdm-python : pdm use -f /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat .pdm-python /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python3 List all virtualenvs created with this projects: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv list Virtualenvs created with this project: - in-project: /home/bbearce/Documents/pdm_setup_and_experimentation/.venv * for-test: /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test Remove a virtualenv: pdm venv remove for-test Activate a virtualenv: eval $(pdm venv activate) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ eval $(pdm venv activate) (pdm_setup_and_experimentation-3.11) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ which python /home/bbearce/Documents/pdm_setup_and_experimentation/.venv/bin/python bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ eval $(pdm venv activate for-test) (pdm_setup_and_experimentation-3.11) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ which python /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python Remember the python interpreter you are using in this project is set in .pdm-python PEP 582 Don't make a virtualenv: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm init Creating a pyproject.toml for PDM... Please enter the Python interpreter to use 0. /home/bbearce/.pyenv/shims/python3 (3.11) 1. /home/bbearce/.pyenv/versions/3.11.5/bin/python (3.11) 2. /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python (3.11) 3. /home/bbearce/.pyenv/versions/3.11.5/bin/python3.11 (3.11) 4. /home/bbearce/.pyenv/shims/python3.11 (3.11) 5. /home/bbearce/.pyenv/shims/python (3.11) 6. /home/bbearce/.pyenv/shims/python3.10 (3.10) 7. /usr/bin/python3.10 (3.10) 8. /home/bbearce/.pyenv/versions/3.10.4/bin/python3.10 (3.10) Please select (0): 7 Would you like to create a virtualenv with /usr/bin/python3.10? [y/n] (y): n You are using the PEP 582 mode, no virtualenv is created. For more info, please visit https://peps.python.org/pep-0582/ Is the project a library that is installable? If yes, we will need to ask a few more questions to include the project name and build backend [y/n] (n): n License(SPDX name) (MIT): Author name (Benjamin Bearce): Author email (bbearce@gmail.com): Python requires('*' to allow any) (>=3.10): Project is initialized successfully Notice there is no .venv directory: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat .pdm-python /usr/bin/python3.10 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 19 Sep 17 19:17 .pdm-python drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 19:17 __pycache__ -rw-rw-r-- 1 bbearce bbearce 217 Sep 17 19:17 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 19:17 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 19:17 tests Manage Depedencies Source pdm add <dependency> pdm add pandas Let's look at the files used to manage this project: pyproject.toml : Overall project management (check into git) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat pyproject.toml [project] name = \"\" version = \"\" description = \"\" authors = [ {name = \"Benjamin Bearce\", email = \"bbearce@gmail.com\"}, ] dependencies = [ \"pandas>=2.1.0\", ] requires-python = \">=3.10\" readme = \"README.md\" license = {text = \"MIT\"} pdm.lock : All pandas (and or others as needed) dependencies are recorded and are not to be touched (check into git) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat pdm.lock # This file is @generated by PDM. # It is not intended for manual editing. [metadata] groups = [\"default\"] cross_platform = true static_urls = false lock_version = \"4.3\" content_hash = \"sha256:dc9eafceda8a739aabf7f0d33d89566090337630b57dbc7a482740a5b8863523\" [[package]] name = \"numpy\" version = \"1.26.0\" requires_python = \"<3.13,>=3.9\" summary = \"Fundamental package for array computing in Python\" files = [ ... .pdm-python : Interpreter (do not check into git) cat .pdm-python /usr/local/bin/python3.11 pdm list bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm list \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 name \u2502 version \u2502 location \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 numpy \u2502 1.26.0 \u2502 \u2502 \u2502 pytz \u2502 2023.3.post1 \u2502 \u2502 \u2502 six \u2502 1.16.0 \u2502 \u2502 \u2502 tzdata \u2502 2023.3 \u2502 \u2502 \u2502 pandas \u2502 2.1.0 \u2502 \u2502 \u2502 python-dateutil \u2502 2.8.2 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm list --graph pandas 2.1.0 [ required: >=2.1.0 ] \u251c\u2500\u2500 numpy 1.26.0 [ required: >=1.22.4 ] \u251c\u2500\u2500 python-dateutil 2.8.2 [ required: >=2.8.2 ] \u2502 \u2514\u2500\u2500 six 1.16.0 [ required: >=1.5 ] \u251c\u2500\u2500 pytz 2023.3.post1 [ required: >=2020.1 ] \u2514\u2500\u2500 tzdata 2023.3 [ required: >=2022.1 ] Using Python: Packages with venv When in this mode packages are in .venv/site-pypackages : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ tree -L 3 .venv/lib .venv/lib \u2514\u2500\u2500 python3.10 \u2514\u2500\u2500 site-packages \u251c\u2500\u2500 dateutil \u251c\u2500\u2500 numpy \u251c\u2500\u2500 numpy-1.26.0.dist-info \u251c\u2500\u2500 numpy.libs \u251c\u2500\u2500 pandas \u251c\u2500\u2500 pandas-2.1.0.dist-info \u251c\u2500\u2500 __pycache__ \u251c\u2500\u2500 python_dateutil-2.8.2.dist-info \u251c\u2500\u2500 pytz \u251c\u2500\u2500 pytz-2023.3.post1.dist-info \u251c\u2500\u2500 six-1.16.0.dist-info \u251c\u2500\u2500 six.py \u251c\u2500\u2500 tzdata \u251c\u2500\u2500 tzdata-2023.3.dist-info \u251c\u2500\u2500 _virtualenv.pth \u2514\u2500\u2500 _virtualenv.py Packages with PEP 582 When in this mode packages are in __pypackages__ : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ tree -L 3 __pypackages__ __pypackages__ \u2514\u2500\u2500 3.10 \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 f2py \u251c\u2500\u2500 include \u2514\u2500\u2500 lib \u251c\u2500\u2500 dateutil \u251c\u2500\u2500 numpy \u251c\u2500\u2500 numpy-1.26.0.dist-info \u251c\u2500\u2500 numpy.libs \u251c\u2500\u2500 pandas \u251c\u2500\u2500 pandas-2.1.0.dist-info \u251c\u2500\u2500 python_dateutil-2.8.2.dist-info \u251c\u2500\u2500 pytz \u251c\u2500\u2500 pytz-2023.3.post1.dist-info \u251c\u2500\u2500 six-1.16.0.dist-info \u251c\u2500\u2500 six.py \u251c\u2500\u2500 tzdata \u2514\u2500\u2500 tzdata-2023.3.dist-info If using a pdm created venv with a name in ~/.local/share/pdm/venvs/ then packages would be under ~/.local/share/pdm/venvs/<venv name>/site-packages . When in this mode if you want a shell using this projects packages and interpreter you use pdm run python : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ pdm run python Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import pandas >>> pandas <module 'pandas' from '/home/bbearce/Documents/pdm_setup_and_experimentation/__pypackages__/3.10/lib/pandas/__init__.py'> Notice without pdm run , your system starts the same interpreter but doesn't have packages installed: bbearce@terry:~/Documents/pdm_setup_and_experimentation$ python3 Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import pandas Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ModuleNotFoundError: No module named 'pandas'","title":"Environments"},{"location":"notes/python/Environments/#environments","text":"You need to install python first. MGB's QTIM Standards courtesy of Chris Bridge give a good layout idea for us to follow.","title":"Environments"},{"location":"notes/python/Environments/#venv-virtualenv-in-python2","text":"packaging.python.org","title":"Venv (virtualenv in python2)"},{"location":"notes/python/Environments/#install-pip","text":"python3 -m pip install --user --upgrade pip python3 -m pip --version python3 -m pip install --user virtualenv","title":"Install pip"},{"location":"notes/python/Environments/#create-virtual-environment","text":"python3 -m venv env bbearce@pop-os:~/Desktop/venv_demo$ python3 -m venv env bbearce@pop-os:~/Desktop/venv_demo$ ls env bbearce@pop-os:~/Desktop/venv_demo$ source env/bin/activate (env) bbearce@pop-os:~/Desktop/venv_demo$ deactivate bbearce@pop-os:~/Desktop/venv_demo$ . env/bin/activate # abreviated (env) bbearce@pop-os:~/Desktop/venv_demo$","title":"Create Virtual Environment"},{"location":"notes/python/Environments/#install-packages","text":"python3 -m pip install pandas python3 -m pip uninstall -y pandas Short: pip install pandas pip uninstall -y pandas (env) bbearce@pop-os:~/Desktop/venv_demo$ python3 -m pip install pandas Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple Collecting pandas Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/d9/26/895a49ebddb4211f2d777150f38ef9e538deff6df7e179a3624c663efc98/pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB) Collecting numpy>=1.23.2 (from pandas) Obtaining dependency information for numpy>=1.23.2 from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.5/58.5 kB 1.2 MB/s eta 0:00:00 Collecting python-dateutil>=2.8.2 (from pandas) Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB) Collecting pytz>=2020.1 (from pandas) Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB) Collecting tzdata>=2022.1 (from pandas) Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 341.8/341.8 kB 8.4 MB/s eta 0:00:00 Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas) Using cached six-1.16.0-py2.py3-none-any.whl (11 kB) Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.6/12.6 MB 46.8 MB/s eta 0:00:00 Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.2/18.2 MB 34.0 MB/s eta 0:00:00 Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 502.5/502.5 kB 51.5 MB/s eta 0:00:00 Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas Successfully installed numpy-1.26.0 pandas-2.1.0 python-dateutil-2.8.2 pytz-2 (env) bbearce@pop-os:~/Desktop/venv_demo$ (env) bbearce@pop-os:~/Desktop/venv_demo$ python3 -m pip uninstall -y pandas Found existing installation: pandas 2.1.0 Uninstalling pandas-2.1.0: Successfully uninstalled pandas-2.1.0 It's best practive to put virtual environments in ~/.virtualenvs. VSCode and other IDE's will look here by default for python virtual environments. bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.virtualenvs total 16 drwxrwxr-x 4 bbearce bbearce 4096 Jul 22 2022 . drwxr-xr-x 77 bbearce bbearce 4096 Sep 17 15:40 .. drwxrwxr-x 6 bbearce bbearce 4096 Jul 22 2022 venv3.10.4 VSCode Example:","title":"Install Packages"},{"location":"notes/python/Environments/#poetry","text":"Poetry","title":"Poetry"},{"location":"notes/python/Environments/#install","text":"curl -sSL https://install.python-poetry.org | python3 -","title":"Install"},{"location":"notes/python/Environments/#uninstall","text":"curl -sSL https://install.python-poetry.org | python3 - --uninstall","title":"Uninstall"},{"location":"notes/python/Environments/#version","text":"$ poetry --version Poetry (version 1.6.1)","title":"Version"},{"location":"notes/python/Environments/#project-setup","text":"cd ~/Desktop poetry new poetry-demo bbearce@terry:~/Desktop$ ls poetry-demo bbearce@terry:~/Desktop$ ls poetry-demo poetry_demo poetry.lock pyproject.toml README.md tests","title":"Project Setup"},{"location":"notes/python/Environments/#dependencies","text":"Store them under [tool.poetry.dependencies] in pyproject.toml: [tool.poetry.dependencies] python = \"^3.10\" pendulum = \"^2.1.2\" Install with poetry install : bbearce@terry:~/Desktop/poetry-demo$ poetry install Installing dependencies from lock file No dependencies to install or update Installing the current project: poetry-demo (0.1.0)","title":"Dependencies"},{"location":"notes/python/Environments/#adding-dependencies","text":"You can specify dependencies in pyproject.toml as above or use poetry's cli to add them: poetry add pendulum This makes a virtual environment automatically: bbearce@terry:~/Desktop/poetry-demo$ poetry add pendulum Creating virtualenv poetry-demo-yD2F6f32-py3.10 in /home/bbearce/.cache/pypoetry/virtualenvs Using version ^2.1.2 for pendulum Updating dependencies Resolving dependencies... Downloading https://files.pythonhosted.org/packages/e0/4f/4474bda990ee740a020cbc3eb271925ef7daa7c844424 Resolving dependencies... (0.6s) Package operations: 4 installs, 0 updates, 0 removals \u2022 Installing six (1.16.0) \u2022 Installing python-dateutil (2.8.2) \u2022 Installing pytzdata (2020.1) \u2022 Installing pendulum (2.1.2) Writing lock file By default, Poetry creates a virtual environment in {cache-dir}/virtualenvs . You can change the cache-dir value by editing the Poetry configuration. Additionally, you can use the virtualenvs.in-project configuration variable to create virtual environments within your project directory. Finally virtualenvs.create let's poetry know if you want it to create a virtualenv or not. If false , if it detects an already enabled virtual environment or an existing one in {cache-dir}/virtualenvs or {project-dir}/.venv it will install dependencies into them, otherwise it will install dependencies into the systems python environment. If a virtualenv isn't activated it will use system PATH and will use system python and or pyenv if that is what you are using. We explore this a litte deeper below","title":"Adding Dependencies:"},{"location":"notes/python/Environments/#config","text":"As mentioned above we can set these config variables: cache-dir virtualenvs.in-project * virtualenvs.create","title":"Config"},{"location":"notes/python/Environments/#cache-dir","text":"Type: string Environment Variable: POETRY_CACHE_DIR The path to the cache directory used by Poetry. Defaults to one of the following directories: macOS: ~/Library/Caches/pypoetry Windows: C:\\Users\\ \\AppData\\Local\\pypoetry\\Cache Unix: ~/.cache/pypoetry ls ~/.cache/pypoetry artifacts cache virtualenvs See project envs: bbearce@terry:~/Desktop/poetry-demo$ poetry env list poetry-demo-yD2F6f32-py3.10 (Activated) See project config: bbearce@terry:~/Desktop/poetry-demo$ poetry config --list cache-dir = \"/home/bbearce/.cache/pypoetry\" experimental.system-git-client = false installer.max-workers = null installer.modern-installation = true installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = null virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /home/bbearce/.cache/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" Set project config: poetry config cache-dir ./cache bbearce@terry:~/Desktop/poetry-demo$ cat ~/.config/pypoetry/config.toml cache-dir = \"cache\" # reset poetry config cache-dir /home/bbearce/.cache/pypoetry","title":"cache-dir"},{"location":"notes/python/Environments/#virtualenvsin-project","text":"Type: boolean Default: None Environment Variable: POETRY_VIRTUALENVS_IN_PROJECT Create the virtualenv inside the project\u2019s root directory. If not set explicitly, poetry by default will create a virtual environment under {cache-dir}/virtualenvs or use the {project-dir}/.venv directory if one already exists. If set to true, the virtualenv will be created and expected in a folder named .venv within the root directory of the project. poetry config virtualenvs.in-project true poetry config virtualenvs.in-project false # to unset Now install: bbearce@pop-os:~/Desktop/poetry-demo$ poetry install Installing dependencies from lock file Package operations: 11 installs, 0 updates, 0 removals \u2022 Installing six (1.16.0) \u2022 Installing certifi (2023.7.22) \u2022 Installing charset-normalizer (3.3.0) \u2022 Installing idna (3.4) \u2022 Installing numpy (1.26.0) \u2022 Installing python-dateutil (2.8.2) \u2022 Installing pytz (2023.3.post1) \u2022 Installing tzdata (2023.3) \u2022 Installing urllib3 (2.0.6) \u2022 Installing pandas (2.1.1) \u2022 Installing requests (2.31.0) Installing the current project: poetry-demo (0.1.0) bbearce@pop-os:~/Desktop/poetry-demo$ ls -la .venv/lib/python3.10/site-packages/ | grep pandas drwxrwxr-x 15 bbearce bbearce 4096 Oct 7 18:16 pandas drwxrwxr-x 2 bbearce bbearce 4096 Oct 7 18:16 pandas-2.1.1.dist-info","title":"virtualenvs.in-project"},{"location":"notes/python/Environments/#virtualenvscreate","text":"This config setting tells poetry to make a virtualenv in ./.venv or {cache-dir}/virtualenvs . We've seen how to create venvs but lets' see what happens if we don't. bearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.create false bbearce@pop-os:~/Desktop/poetry-demo$ poetry config --list cache-dir = \"/home/bbearce/.cache/pypoetry\" ... virtualenvs.create = false virtualenvs.in-project = false ... virtualenvs.path = \"{cache-dir}/virtualenvs\" # /home/bbearce/.cache/pypoetry/virtualenvs ... In this mode poetry will use the first python on the PATH . This could be: the system python an activated virtual environment * a local pyenv * a global pyenv * ...","title":"virtualenvs.create"},{"location":"notes/python/Environments/#activate-virtual-environments","text":"Source like normal: bash: .venv/bin/activatae: No such file or directory bbearce@pop-os:~/Desktop/poetry-demo$ source .venv/bin/activate (poetry-demo-py3.10) bbearce@pop-os:~/Desktop/poetry-demo$ Or use poetry shell to activate the virtual environment within a nested shell. bbearce@terry:~/Desktop/poetry-demo$ poetry shell Spawning shell within /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10 . /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10/bin/activate bbearce@terry:~/Desktop/poetry-demo$ . /home/bbearce/.cache/pypoetry/virtualenvs/poetry-demo-yD2F6f32-py3.10/bin/activate # Deactivate with `exit` (poetry-demo-py3.10) bbearce@terry:~/Desktop/poetry-demo$ exit exit To deactivate the virtual environment and exit this new shell type exit . To deactivate the virtual environment without leaving the shell use deactivate .","title":"Activate Virtual Environments"},{"location":"notes/python/Environments/#git","text":"Always commit both the pyproject.toml and poetry.lock files to your project.","title":"Git"},{"location":"notes/python/Environments/#use-pyenv-environments-with-poetry","text":"If you want to use pyenv with its virtualenv extension to manage virtual environments, but still leverage Poetry's caching mechanism for PyPI packages, you can follow these steps: bbearce@pop-os:~/Desktop$ pyenv versions * system (set by /home/bbearce/.pyenv/version) 3.10.4 bbearce@pop-os:~/Desktop$ pyenv virtualenv 3.10.4 venv3.10.4 bbearce@pop-os:~/Desktop$ pyenv versions * system (set by /home/bbearce/.pyenv/version) 3.10.4 3.10.4/envs/venv3.10.4 venv3.10.4 --> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 bbearce@pop-os:~/Desktop$ poetry new poetry-demo Created package poetry_demo in poetry-demo bbearce@pop-os:~/Desktop$ cd poetry-demo/ bbearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.create false bbearce@pop-os:~/Desktop/poetry-demo$ poetry config virtualenvs.in-project false bbearce@pop-os:~/Desktop/poetry-demo$ pyenv activate venv3.10.4 (venv3.10.4) bbearce@pop-os:~/Desktop/poetry-demo$ poetry add requests Using version ^2.31.0 for requests Updating dependencies Resolving dependencies... (0.2s) Package operations: 5 installs, 0 updates, 0 removals \u2022 Installing certifi (2023.7.22) \u2022 Installing charset-normalizer (3.3.0) \u2022 Installing idna (3.4) \u2022 Installing urllib3 (2.0.6) \u2022 Installing requests (2.31.0) Writing lock file (venv3.10.4) bbearce@pop-os:~/Desktop/poetry-demo$ python Python 3.10.4 (main, Oct 7 2023, 16:49:10) [GCC 11.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import requests >>> import sys >>> sys.executable '/home/bbearce/.pyenv/versions/venv3.10.4/bin/python' >>>","title":"Use Pyenv environments with Poetry"},{"location":"notes/python/Environments/#pyenv","text":"Official Docs realpython.com (they use the above to guide you) If venv was manual, pyenv is automatic and steamlined to create many different environments and virtual environments. When you use venv you are copying your main python installation into a virtual environment. You are locked into that version for virtual environements unless you manually install different python versions on your system. With pyenv , you do have to have these base python environments, but instead of installing where they normally go on your os, they are collected and managed by pyenv in ~/.pyenv/versions : bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv -rw-r--r-- 1 bbearce bbearce 7 Sep 16 16:02 version drwxr-xr-x 3 bbearce bbearce 4096 Sep 16 16:29 versions ... bbearce@pop-os:~/Desktop/venv_demo$ ls ~/.pyenv/versions 3.11.5 venv3.11.5 Via special pyenv bash scripts, not python installations as with venv above, we can install multiple python versions ( 3.11.5 ) and virtual environments( venv3.11.5 ). They live in the same directory, though the virtual environments are sylinks to ./envs/ under each actual python version. bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions/ drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions/3.11.5/envs/ drwxr-xr-x 5 bbearce bbearce 4096 Sep 16 16:29 venv3.11.5 So you can have any python version with any number of unique virtual environments.","title":"Pyenv"},{"location":"notes/python/Environments/#create-install-environment","text":"See environments availabe: pyenv install --list # all pyenv install --list | grep \" 3\\.[678]\" # get specific 3.6.0 3.6-dev 3.6.1 3.6.2 3.6.3 3.6.4 3.6.5 3.6.6 3.6.7 3.6.8 3.7.0 3.7-dev 3.7.1 3.7.2 3.8-dev pyenv install 3.10.4 # pyenv uninstall 3.10.4 # Uninstall # rm -rf ~/.pyenv/versions/3.10.4 # Equivalent bbearce@pop-os:~/Desktop/venv_demo$ pyenv install 3.10.4 Downloading Python-3.10.4.tar.xz... -> https://www.python.org/ftp/python/3.10.4/Python-3.10.4.tar.xz Installing Python-3.10.4... ... Installing Python-3.10.4... Installed Python-3.10.4 to /home/bbearce/.pyenv/versions/3.10.4 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 6 bbearce bbearce 4096 Sep 17 17:03 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5","title":"Create (install) Environment"},{"location":"notes/python/Environments/#create-virtual-environment_1","text":"You need a pyenv plugin called pyenv-virtualenv . # Ubuntu based install...check for your os git clone https://github.com/pyenv/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv Create: bbearce@pop-os:~/Desktop/venv_demo$ pyenv virtualenv 3.10.4 <venv-name> bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 6 bbearce bbearce 4096 Sep 17 17:03 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ pyenv virtualenv 3.10.4 venv3.10.4 bbearce@pop-os:~/Desktop/venv_demo$ ls -la ~/.pyenv/versions drwxr-xr-x 7 bbearce bbearce 4096 Sep 17 17:19 3.10.4 drwxr-xr-x 7 bbearce bbearce 4096 Sep 16 16:29 3.11.5 lrwxrwxrwx 1 bbearce bbearce 52 Sep 17 17:19 venv3.10.4 -> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 lrwxrwxrwx 1 bbearce bbearce 52 Sep 16 16:29 venv3.11.5 -> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5","title":"Create Virtual Environment"},{"location":"notes/python/Environments/#using-environments","text":"There are a variety of ways to specify when and which env\\venv you want to use. Here are the most basic.","title":"Using Environments"},{"location":"notes/python/Environments/#global","text":"By default you are using a global pyenv version specified by a file in ~/.pyenv/version . bbearce@pop-os:~/Desktop/venv_demo$ cat ~/.pyenv/version 3.11.5 Set it with: pyenv global <version> bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by /home/bbearce/.pyenv/version) bbearce@pop-os:~/Desktop/venv_demo$ pyenv versions system 3.10.4 3.10.4/envs/venv3.10.4 * 3.11.5 (set by /home/bbearce/.pyenv/version) 3.11.5/envs/venv3.11.5 venv3.10.4 --> /home/bbearce/.pyenv/versions/3.10.4/envs/venv3.10.4 venv3.11.5 --> /home/bbearce/.pyenv/versions/3.11.5/envs/venv3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ python Python 3.11.5 (main, Sep 16 2023, 15:49:34) [GCC 11.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>>","title":"Global"},{"location":"notes/python/Environments/#shell","text":"Activates for quick use (essentially the same as venv ). pyenv shell <version> Regular env: bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell 3.11.5 bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by PYENV_VERSION environment variable) Virtualenv: pyenv activate <version> pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ pyenv activate venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by PYENV_VERSION environment variable) (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ You can use these with shell too. bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by PYENV_VERSION environment variable) However deactivate doesn't work: (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv deactivate (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ deactivate pyenv-virtualenv: deactivate must be sourced. Run 'source deactivate' instead of 'deactivate' (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ source deactivate pyenv-virtualenv: deactivate 3.11.5/envs/venv3.11.5 # Need to use shell again to switch (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv shell 3.11.5","title":"Shell"},{"location":"notes/python/Environments/#local","text":"This is set for a direcory or project and creates a .python-version inside the directory pyenv local <version> bbearce@pop-os:~/Desktop/venv_demo$ pyenv version 3.11.5 (set by /home/bbearce/.pyenv/version) bbearce@pop-os:~/Desktop/venv_demo$ pyenv local venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ ls -la drwxrwxr-x 5 bbearce bbearce 4096 Sep 17 15:32 env -rw-rw-r-- 1 bbearce bbearce 11 Sep 17 17:29 .python-version -rw-rw-r-- 1 bbearce bbearce 22572 Sep 17 15:48 vscode-venv.jpg (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ cat .python-version venv3.11.5 (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ pyenv version venv3.11.5 (set by /home/bbearce/Desktop/venv_demo/.python-version) (venv3.11.5) bbearce@pop-os:~/Desktop/venv_demo$ rm .python-version # deactivates and removes .python-version bbearce@pop-os:~/Desktop/venv_demo$","title":"Local"},{"location":"notes/python/Environments/#pdm","text":"pdm.fming.dev/latest Warning: PEP 582 has been rejected PDM was created to mimic npm whereby you don't need virtualenvs. All you need are base pythons and __pypackages__ . Instead of bundling your packages with venvs, which are copies of base environmetns, why don't we use a single 3.11.5 and have projects with packages that all use the same base 3.11.5 version or any other version you'd like. PS: You can and unfortunately should use vitualenvs with pdm as PEP 582 has been rejected which was the whole point. Nonetheless this package manager is nice and organized. It uses .toml files like Poetry to manage depedencies so it is nice still.","title":"PDM"},{"location":"notes/python/Environments/#install_1","text":"# Linux\\Mac curl -sSL https://pdm.fming.dev/install-pdm.py | python3 -","title":"Install"},{"location":"notes/python/Environments/#new-project","text":"mkdir ~/Documents/pdm_setup_and_experimentation && cd ~/Documents/pdm_setup_and_experimentation pdm init","title":"New Project"},{"location":"notes/python/Environments/#virtualenvs-and-not-pep-582","text":"Source You can choose the backend used by PDM to create a virtualenv. Currently it supports three backends: virtualenv(default) venv conda","title":"Virtualenvs and not PEP 582"},{"location":"notes/python/Environments/#local-virtual-environment","text":"bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm init Creating a pyproject.toml for PDM... Please enter the Python interpreter to use 0. /home/bbearce/.pyenv/shims/python3 (3.11) 1. /home/bbearce/.pyenv/versions/3.11.5/bin/python (3.11) 2. /home/bbearce/.pyenv/versions/3.11.5/bin/python3.11 (3.11) 3. /home/bbearce/.pyenv/shims/python3.11 (3.11) 4. /home/bbearce/.pyenv/shims/python (3.11) 5. /home/bbearce/.pyenv/shims/python3.10 (3.10) 6. /usr/bin/python3.10 (3.10) 7. /home/bbearce/.pyenv/versions/3.10.4/bin/python3.10 (3.10) Please select (0): 6 Would you like to create a virtualenv with /usr/bin/python3.10? [y/n] (y): y Virtualenv is created successfully at /home/bbearce/Documents/pdm_setup_and_experimentation/.venv Is the project a library that is installable? If yes, we will need to ask a few more questions to include the project name and build backend [y/n] (n): n License(SPDX name) (MIT): Author name (Benjamin Bearce): Author email (bbearce@gmail.com): Python requires('*' to allow any) (>=3.10): Project is initialized successfully We can use any environment (regular or virtual) to start this project, including virtual ones we make upon running pdm init . We can see the .venv directory which was made from /usr/bin/python3.10 . bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la total 40 drwxrwxr-x 6 bbearce bbearce 4096 Sep 17 18:25 . drwxr-xr-x 36 bbearce bbearce 4096 Sep 17 14:17 .. -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 69 Sep 17 18:24 .pdm-python drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 18:25 __pycache__ -rw-rw-r-- 1 bbearce bbearce 217 Sep 17 18:25 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 tests drwxrwxr-x 4 bbearce bbearce 4096 Sep 17 18:24 .venv PDM Info: If Project Packages is None (as show below by it not being present), virtualenv mode is enabled. bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm info PDM version: 2.9.2 Python Interpreter: /home/bbearce/Documents/pdm_setup_and_experimentation/.venv/bin/python (3.10) Project Root: /home/bbearce/Documents/pdm_setup_and_experimentation Local Packages: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm info --env { \"implementation_name\": \"cpython\", \"implementation_version\": \"3.10.12\", \"os_name\": \"posix\", \"platform_machine\": \"x86_64\", \"platform_release\": \"6.4.6-76060406-generic\", \"platform_system\": \"Linux\", \"platform_version\": \"#202307241739~1694621917~22.04~ac5e1a8 SMP PREEMPT_DYNAMIC Wed S\", \"python_full_version\": \"3.10.12\", \"platform_python_implementation\": \"CPython\", \"python_version\": \"3.10\", \"sys_platform\": \"linux\" }","title":"Local Virtual Environment"},{"location":"notes/python/Environments/#create-your-own-virtual-environment","text":"# Create a virtualenv based on 3.8 interpreter $ pdm venv create 3.8 # Assign a different name other than the version string $ pdm venv create --name for-test 3.8 # Use venv as the backend to create, support 3 backends: virtualenv(default), venv, conda $ pdm venv create --with venv 3.9 Notice you need an interpreter installed to use correctly: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create 3.8 [VirtualenvCreateError]: Can't resolve python interpreter 3.8 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --name for-test 3.8 [VirtualenvCreateError]: Can't resolve python interpreter 3.8 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --with venv 3.9 [VirtualenvCreateError]: Can't resolve python interpreter 3.9 Use 3.11.5 that we have: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create 3.11.5 Virtualenv /home/bbearce/Documents/pdm_setup_and_experimentation/.venv is created successfully bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la total 44 drwxrwxr-x 6 bbearce bbearce 4096 Sep 17 19:00 . drwxr-xr-x 36 bbearce bbearce 4096 Sep 17 14:17 .. -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 8119 Sep 17 18:39 pdm.lock drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 18:25 __pycache__ -rw-rw-r-- 1 bbearce bbearce 239 Sep 17 18:39 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 18:25 tests drwxrwxr-x 4 bbearce bbearce 4096 Sep 17 19:00 .venv bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --name for-test 3.11.5 Virtualenv /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test is created successfully bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv create --with venv 3.11.5 Virtualenv /home/bbearce/Documents/pdm_setup_and_experimentation/.venv is created successfully Reuse the virtualenv you created elsewhere: This will create file .pdm-python : pdm use -f /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat .pdm-python /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python3 List all virtualenvs created with this projects: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm venv list Virtualenvs created with this project: - in-project: /home/bbearce/Documents/pdm_setup_and_experimentation/.venv * for-test: /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test Remove a virtualenv: pdm venv remove for-test Activate a virtualenv: eval $(pdm venv activate) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ eval $(pdm venv activate) (pdm_setup_and_experimentation-3.11) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ which python /home/bbearce/Documents/pdm_setup_and_experimentation/.venv/bin/python bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ eval $(pdm venv activate for-test) (pdm_setup_and_experimentation-3.11) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ which python /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python Remember the python interpreter you are using in this project is set in .pdm-python","title":"Create Your Own Virtual Environment"},{"location":"notes/python/Environments/#pep-582","text":"Don't make a virtualenv: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm init Creating a pyproject.toml for PDM... Please enter the Python interpreter to use 0. /home/bbearce/.pyenv/shims/python3 (3.11) 1. /home/bbearce/.pyenv/versions/3.11.5/bin/python (3.11) 2. /home/bbearce/.local/share/pdm/venvs/pdm_setup_and_experimentation-wABLsGD0-for-test/bin/python (3.11) 3. /home/bbearce/.pyenv/versions/3.11.5/bin/python3.11 (3.11) 4. /home/bbearce/.pyenv/shims/python3.11 (3.11) 5. /home/bbearce/.pyenv/shims/python (3.11) 6. /home/bbearce/.pyenv/shims/python3.10 (3.10) 7. /usr/bin/python3.10 (3.10) 8. /home/bbearce/.pyenv/versions/3.10.4/bin/python3.10 (3.10) Please select (0): 7 Would you like to create a virtualenv with /usr/bin/python3.10? [y/n] (y): n You are using the PEP 582 mode, no virtualenv is created. For more info, please visit https://peps.python.org/pep-0582/ Is the project a library that is installable? If yes, we will need to ask a few more questions to include the project name and build backend [y/n] (n): n License(SPDX name) (MIT): Author name (Benjamin Bearce): Author email (bbearce@gmail.com): Python requires('*' to allow any) (>=3.10): Project is initialized successfully Notice there is no .venv directory: bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat .pdm-python /usr/bin/python3.10 bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ ls -la -rw-r--r-- 1 bbearce bbearce 3102 Sep 17 13:47 .gitignore -rw-rw-r-- 1 bbearce bbearce 19 Sep 17 19:17 .pdm-python drwxrwxr-x 2 bbearce bbearce 4096 Sep 17 19:17 __pycache__ -rw-rw-r-- 1 bbearce bbearce 217 Sep 17 19:17 pyproject.toml -rw-r--r-- 1 bbearce bbearce 18 Sep 17 13:47 README.md drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 19:17 src drwxrwxr-x 3 bbearce bbearce 4096 Sep 17 19:17 tests","title":"PEP 582"},{"location":"notes/python/Environments/#manage-depedencies","text":"Source pdm add <dependency> pdm add pandas Let's look at the files used to manage this project: pyproject.toml : Overall project management (check into git) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat pyproject.toml [project] name = \"\" version = \"\" description = \"\" authors = [ {name = \"Benjamin Bearce\", email = \"bbearce@gmail.com\"}, ] dependencies = [ \"pandas>=2.1.0\", ] requires-python = \">=3.10\" readme = \"README.md\" license = {text = \"MIT\"} pdm.lock : All pandas (and or others as needed) dependencies are recorded and are not to be touched (check into git) bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ cat pdm.lock # This file is @generated by PDM. # It is not intended for manual editing. [metadata] groups = [\"default\"] cross_platform = true static_urls = false lock_version = \"4.3\" content_hash = \"sha256:dc9eafceda8a739aabf7f0d33d89566090337630b57dbc7a482740a5b8863523\" [[package]] name = \"numpy\" version = \"1.26.0\" requires_python = \"<3.13,>=3.9\" summary = \"Fundamental package for array computing in Python\" files = [ ... .pdm-python : Interpreter (do not check into git) cat .pdm-python /usr/local/bin/python3.11 pdm list bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm list \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 name \u2502 version \u2502 location \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 numpy \u2502 1.26.0 \u2502 \u2502 \u2502 pytz \u2502 2023.3.post1 \u2502 \u2502 \u2502 six \u2502 1.16.0 \u2502 \u2502 \u2502 tzdata \u2502 2023.3 \u2502 \u2502 \u2502 pandas \u2502 2.1.0 \u2502 \u2502 \u2502 python-dateutil \u2502 2.8.2 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f bbearce@pop-os:~/Documents/pdm_setup_and_experimentation$ pdm list --graph pandas 2.1.0 [ required: >=2.1.0 ] \u251c\u2500\u2500 numpy 1.26.0 [ required: >=1.22.4 ] \u251c\u2500\u2500 python-dateutil 2.8.2 [ required: >=2.8.2 ] \u2502 \u2514\u2500\u2500 six 1.16.0 [ required: >=1.5 ] \u251c\u2500\u2500 pytz 2023.3.post1 [ required: >=2020.1 ] \u2514\u2500\u2500 tzdata 2023.3 [ required: >=2022.1 ]","title":"Manage Depedencies"},{"location":"notes/python/Environments/#using-python","text":"","title":"Using Python:"},{"location":"notes/python/Environments/#packages-with-venv","text":"When in this mode packages are in .venv/site-pypackages : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ tree -L 3 .venv/lib .venv/lib \u2514\u2500\u2500 python3.10 \u2514\u2500\u2500 site-packages \u251c\u2500\u2500 dateutil \u251c\u2500\u2500 numpy \u251c\u2500\u2500 numpy-1.26.0.dist-info \u251c\u2500\u2500 numpy.libs \u251c\u2500\u2500 pandas \u251c\u2500\u2500 pandas-2.1.0.dist-info \u251c\u2500\u2500 __pycache__ \u251c\u2500\u2500 python_dateutil-2.8.2.dist-info \u251c\u2500\u2500 pytz \u251c\u2500\u2500 pytz-2023.3.post1.dist-info \u251c\u2500\u2500 six-1.16.0.dist-info \u251c\u2500\u2500 six.py \u251c\u2500\u2500 tzdata \u251c\u2500\u2500 tzdata-2023.3.dist-info \u251c\u2500\u2500 _virtualenv.pth \u2514\u2500\u2500 _virtualenv.py","title":"Packages with venv"},{"location":"notes/python/Environments/#packages-with-pep-582","text":"When in this mode packages are in __pypackages__ : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ tree -L 3 __pypackages__ __pypackages__ \u2514\u2500\u2500 3.10 \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 f2py \u251c\u2500\u2500 include \u2514\u2500\u2500 lib \u251c\u2500\u2500 dateutil \u251c\u2500\u2500 numpy \u251c\u2500\u2500 numpy-1.26.0.dist-info \u251c\u2500\u2500 numpy.libs \u251c\u2500\u2500 pandas \u251c\u2500\u2500 pandas-2.1.0.dist-info \u251c\u2500\u2500 python_dateutil-2.8.2.dist-info \u251c\u2500\u2500 pytz \u251c\u2500\u2500 pytz-2023.3.post1.dist-info \u251c\u2500\u2500 six-1.16.0.dist-info \u251c\u2500\u2500 six.py \u251c\u2500\u2500 tzdata \u2514\u2500\u2500 tzdata-2023.3.dist-info If using a pdm created venv with a name in ~/.local/share/pdm/venvs/ then packages would be under ~/.local/share/pdm/venvs/<venv name>/site-packages . When in this mode if you want a shell using this projects packages and interpreter you use pdm run python : bbearce@terry:~/Documents/pdm_setup_and_experimentation$ pdm run python Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import pandas >>> pandas <module 'pandas' from '/home/bbearce/Documents/pdm_setup_and_experimentation/__pypackages__/3.10/lib/pandas/__init__.py'> Notice without pdm run , your system starts the same interpreter but doesn't have packages installed: bbearce@terry:~/Documents/pdm_setup_and_experimentation$ python3 Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import pandas Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ModuleNotFoundError: No module named 'pandas'","title":"Packages with PEP 582"},{"location":"notes/python/Installs/","text":"Installs 09/17/2023 - Might be a bit out of date. Installing python3.6 Source1 realpython Source2 stackoverflow Ubuntu 17.10, Ubuntu 18.04 (and above) come with Python 3.6 by default. You should be able to invoke it with the command python3 . Ubuntu 16.10 and 17.04 do not come with Python 3.6 by default, but it is in the Universe repository. You should be able to install it with the following commands: $ sudo apt-get update $ sudo apt-get install python3.6 You can then invoke it with the command python3.6 . If you are using Ubuntu 14.04 or 16.04, Python 3.6 is not in the Universe repository, and you need to get it from a Personal Package Archive (PPA). For example, to install Python from the \u201cdeadsnakes\u201d PPA, do the following: $ sudo add-apt-repository ppa:deadsnakes/ppa $ sudo apt-get update $ sudo apt-get install python3.6 As above, invoke with the command python3.6 . Installing python3.7+ Source linuxize Install with Apt Installing Python 3.7 on Ubuntu with apt is a relatively straightforward process and will only take a few minutes: Start by updating the packages list and installing the prerequisites: sudo apt update sudo apt install software-properties-common Next, add the deadsnakes PPA to your sources list: sudo add-apt-repository ppa:deadsnakes/ppa When prompted press Enter to continue: Press [ENTER] to continue or Ctrl-c to cancel adding it. Once the repository is enabled, install Python 3.7 with: sudo apt install python3.7 At this point, Python 3.7 is installed on your Ubuntu system and ready to be used. You can verify it by typing: python3.7 --version Python 3.7.3 For virtual environments you need to install venv separately: qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ sudo apt install python3.7-venv Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: grub-pc-bin linux-headers-4.15.0-91 Use 'sudo apt autoremove' to remove them. The following NEW packages will be installed: python3.7-venv 0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded. Need to get 1801 kB of archives. After this operation, 2019 kB of additional disk space will be used. Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.7-venv amd64 3.7.7-1+bionic1 [1801 kB] Fetched 1801 kB in 1s (2022 kB/s) Selecting previously unselected package python3.7-venv. (Reading database ... 75546 files and directories currently installed.) Preparing to unpack .../python3.7-venv_3.7.7-1+bionic1_amd64.deb ... Unpacking python3.7-venv (3.7.7-1+bionic1) ... Setting up python3.7-venv (3.7.7-1+bionic1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ python3.7 -m venv venv qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ ls app.py database.json requirements.txt venv Install from source Source linuxize This first step may or may not be necessary but is probably a good idea. First, update the packages list and install the packages necessary to build Python source: sudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev wget You an use wget to grab the latest like so: cd /usr/src sudo wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz You can also go to https://www.python.org/ and download it. This gave me a Python-3.7.4.tar.xz file. To unzip it use: $ tar -xf Python-3.7.4.tar.xz $ ls -la drwxr-xr-x 18 bbearce bbearce 4096 Jul 8 14:31 Python-3.7.4 -rw-rw-r-- 1 bbearce bbearce 17131432 Sep 4 11:09 Python-3.7.4.tar.xz To see other unzipping techniques for different file types try here Change into that new directory and use make to install Python: $ cd Python-3.7.4 Note: Notice the README.rst $ vi README.rst ... Build Instructions On Unix, Linux, BSD, macOS, and Cygwin:: ./configure make make test sudo make install This will install Python as python3 . ... Installing multiple versions On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix ( --prefix argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using make altinstall contain the major and minor version and can thus live side-by-side. make install also creates ${prefix}/bin/python3 which refers to ${prefix}/bin/pythonX.Y . If you intend to install multiple versions using the same prefix you must decide which version (if any) is your \"primary\" version. Install that version using make install . Install all other versions using make altinstall . To continue run make : Note that $ ./configure $ make $ make test $ sudo make altinstall Uninstall Source howtoinstall Uninstall python To remove just python package itself from Ubuntu 16.04 (Xenial Xerus) execute on terminal: sudo apt-get remove python Uninstall python and it's dependent packages To remove the python package and any other dependant package which are no longer needed from Ubuntu Xenial. sudo apt-get remove --auto-remove python Purging python If you also want to delete configuration and/or data files of python from Ubuntu Xenial then this will work: sudo apt-get purge python To delete configuration and/or data files of python and it's dependencies from Ubuntu Xenial then execute: sudo apt-get purge --auto-remove python","title":"Base"},{"location":"notes/python/Installs/#installs","text":"09/17/2023 - Might be a bit out of date.","title":"Installs"},{"location":"notes/python/Installs/#installing-python36","text":"Source1 realpython Source2 stackoverflow Ubuntu 17.10, Ubuntu 18.04 (and above) come with Python 3.6 by default. You should be able to invoke it with the command python3 . Ubuntu 16.10 and 17.04 do not come with Python 3.6 by default, but it is in the Universe repository. You should be able to install it with the following commands: $ sudo apt-get update $ sudo apt-get install python3.6 You can then invoke it with the command python3.6 . If you are using Ubuntu 14.04 or 16.04, Python 3.6 is not in the Universe repository, and you need to get it from a Personal Package Archive (PPA). For example, to install Python from the \u201cdeadsnakes\u201d PPA, do the following: $ sudo add-apt-repository ppa:deadsnakes/ppa $ sudo apt-get update $ sudo apt-get install python3.6 As above, invoke with the command python3.6 .","title":"Installing python3.6"},{"location":"notes/python/Installs/#installing-python37","text":"Source linuxize","title":"Installing python3.7+"},{"location":"notes/python/Installs/#install-with-apt","text":"Installing Python 3.7 on Ubuntu with apt is a relatively straightforward process and will only take a few minutes: Start by updating the packages list and installing the prerequisites: sudo apt update sudo apt install software-properties-common Next, add the deadsnakes PPA to your sources list: sudo add-apt-repository ppa:deadsnakes/ppa When prompted press Enter to continue: Press [ENTER] to continue or Ctrl-c to cancel adding it. Once the repository is enabled, install Python 3.7 with: sudo apt install python3.7 At this point, Python 3.7 is installed on your Ubuntu system and ready to be used. You can verify it by typing: python3.7 --version Python 3.7.3 For virtual environments you need to install venv separately: qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ sudo apt install python3.7-venv Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: grub-pc-bin linux-headers-4.15.0-91 Use 'sudo apt autoremove' to remove them. The following NEW packages will be installed: python3.7-venv 0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded. Need to get 1801 kB of archives. After this operation, 2019 kB of additional disk space will be used. Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.7-venv amd64 3.7.7-1+bionic1 [1801 kB] Fetched 1801 kB in 1s (2022 kB/s) Selecting previously unselected package python3.7-venv. (Reading database ... 75546 files and directories currently installed.) Preparing to unpack .../python3.7-venv_3.7.7-1+bionic1_amd64.deb ... Unpacking python3.7-venv (3.7.7-1+bionic1) ... Setting up python3.7-venv (3.7.7-1+bionic1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ python3.7 -m venv venv qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ ls app.py database.json requirements.txt venv","title":"Install with Apt"},{"location":"notes/python/Installs/#install-from-source","text":"Source linuxize This first step may or may not be necessary but is probably a good idea. First, update the packages list and install the packages necessary to build Python source: sudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev wget You an use wget to grab the latest like so: cd /usr/src sudo wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz You can also go to https://www.python.org/ and download it. This gave me a Python-3.7.4.tar.xz file. To unzip it use: $ tar -xf Python-3.7.4.tar.xz $ ls -la drwxr-xr-x 18 bbearce bbearce 4096 Jul 8 14:31 Python-3.7.4 -rw-rw-r-- 1 bbearce bbearce 17131432 Sep 4 11:09 Python-3.7.4.tar.xz To see other unzipping techniques for different file types try here Change into that new directory and use make to install Python: $ cd Python-3.7.4 Note: Notice the README.rst $ vi README.rst ...","title":"Install from source"},{"location":"notes/python/Installs/#build-instructions","text":"On Unix, Linux, BSD, macOS, and Cygwin:: ./configure make make test sudo make install This will install Python as python3 . ...","title":"Build Instructions"},{"location":"notes/python/Installs/#installing-multiple-versions","text":"On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix ( --prefix argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using make altinstall contain the major and minor version and can thus live side-by-side. make install also creates ${prefix}/bin/python3 which refers to ${prefix}/bin/pythonX.Y . If you intend to install multiple versions using the same prefix you must decide which version (if any) is your \"primary\" version. Install that version using make install . Install all other versions using make altinstall . To continue run make : Note that $ ./configure $ make $ make test $ sudo make altinstall","title":"Installing multiple versions"},{"location":"notes/python/Installs/#uninstall","text":"Source howtoinstall","title":"Uninstall"},{"location":"notes/python/Installs/#uninstall-python","text":"To remove just python package itself from Ubuntu 16.04 (Xenial Xerus) execute on terminal: sudo apt-get remove python","title":"Uninstall python"},{"location":"notes/python/Installs/#uninstall-python-and-its-dependent-packages","text":"To remove the python package and any other dependant package which are no longer needed from Ubuntu Xenial. sudo apt-get remove --auto-remove python","title":"Uninstall python and it's dependent packages"},{"location":"notes/python/Installs/#purging-python","text":"If you also want to delete configuration and/or data files of python from Ubuntu Xenial then this will work: sudo apt-get purge python To delete configuration and/or data files of python and it's dependencies from Ubuntu Xenial then execute: sudo apt-get purge --auto-remove python","title":"Purging python"},{"location":"notes/python/s3/","text":"Working with s3 Courtesy of these resources: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html https://aws.amazon.com/cli/ https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html https://devcenter.heroku.com/articles/s3-upload-python import boto3 # Let's use Amazon S3 s3 = boto3.resource('s3') for bucket in s3.buckets.all(): print(bucket.name) # Upload a new file data = open('favicon-32x32.png', 'rb') s3.Bucket('thegratefulbrauer/recipe_description_images').put_object(Key='favicon-32x32.png', Body=data)","title":"S3"},{"location":"notes/python/s3/#working-with-s3","text":"Courtesy of these resources: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html https://aws.amazon.com/cli/ https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html https://devcenter.heroku.com/articles/s3-upload-python import boto3 # Let's use Amazon S3 s3 = boto3.resource('s3') for bucket in s3.buckets.all(): print(bucket.name) # Upload a new file data = open('favicon-32x32.png', 'rb') s3.Bucket('thegratefulbrauer/recipe_description_images').put_object(Key='favicon-32x32.png', Body=data)","title":"Working with s3"},{"location":"notes/python/PIL/basics/","text":"PIL (Image mManipulation) Resize Image import os from PIL import Image from math import floor input_path = \".\" dest_path = \".\" image_name = \"image.jpg\" new_image_name = \"new_image.jpg\" def resize(path): i = Image.open(path) size = i.size new_size = int(floor(size[0]*.75)), int(floor(size[1]*.75)) i = i.resize(new_size, Image.ANTIALIAS) return i new_image = resize(os.path.join(input_path, image_name)) new_image.save(os.path.join(dest_path, new_image_name), optimize=True, quality=50)","title":"PIL"},{"location":"notes/python/PIL/basics/#pil-image-mmanipulation","text":"","title":"PIL (Image mManipulation)"},{"location":"notes/python/PIL/basics/#resize-image","text":"import os from PIL import Image from math import floor input_path = \".\" dest_path = \".\" image_name = \"image.jpg\" new_image_name = \"new_image.jpg\" def resize(path): i = Image.open(path) size = i.size new_size = int(floor(size[0]*.75)), int(floor(size[1]*.75)) i = i.resize(new_size, Image.ANTIALIAS) return i new_image = resize(os.path.join(input_path, image_name)) new_image.save(os.path.join(dest_path, new_image_name), optimize=True, quality=50)","title":"Resize Image"},{"location":"notes/python/advanced/Classes/","text":"Classes Basic Class Definition Intro: - Class: Blueprint - Object - Instance class Shark: # Basic Method definition def swim(self): print(\"The shark is swimming.\") def be_awesome(self): print(\"The shark is being awesome.\") Notice the use of self to reference an instance specifically...the one calling the method. Implementing sammy = Shark() sammy.swim() # >>> The shark is swimming. sammy.be_awesome() # >>> The shark is being awesome. Now let's dicuss init . class Shark: def __init__(self): print(\"This is the constructor method.\") >>> Shark() This is the constructor method. <__main__.Shark object at 0x10348d470> Finally, we can set the name of the Shark object sammy as equal to \"Sammy\" by passing it as a parameter of the Shark class: class Shark: def __init__(self, name): self.name = name def swim(self): print(self.name + \" is swimming.\") def be_awesome(self): print(self.name + \" is being awesome.\") def main(): # Set name of Shark object sammy = Shark(\"Sammy\") sammy.swim() sammy.be_awesome() if __name__ == \"__main__\": main() run... $ python shark.py Sammy is swimming. Sammy is being awesome. Inheritance super() and inheritance. In this tutorial, you\u2019ll learn about the following: The concept of inheritance in Python Multiple inheritance in Python How the super() function works How the super() function in single inheritance works How the super() function in multiple inheritance works Let's start with a simple example: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square: def __init__(self, length): self.length = length def area(self): return self.length * self.length def perimeter(self): return 4 * self.length >>> square = Square(4) >>> square.area() 16 >>> rectangle = Rectangle(2,4) >>> rectangle.area() 8 Here no references to inheritance are being made. super() in Single Inheritance super() gives you access to methods in a superclass from the subclass that inherits from it. super() alone returns a temporary object of the superclass that then allows you to call that superclass\u2019s methods. By using inheritance, you can reduce the amount of code you write while simultaneously reflecting the real-world relationship between rectangles and squares: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from the Rectangle class class Square(Rectangle): def __init__(self, length): super().__init__(length, length) >>> square = Square(4) >>> square.area() 16 What Can super() Do for You? Like in other object-oriented languages, it allows you to call methods of the superclass in your subclass. The primary use case of this is to extend the functionality of the inherited method. In the example below, you will create a class Cube that inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance: class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length >>> cube = Cube(3) >>> cube.surface_area() 54 >>> cube.volume() 27 Here you have implemented two methods for the Cube class: .surface_area() and .volume(). Both of these calculations rely on calculating the area of a single face, so rather than reimplementing the area calculation, you use super() to extend the area calculation. Also notice that the Cube class definition does not have an . init (). Because Cube inherits from Square and . init () doesn\u2019t really do anything differently for Cube than it already does for Square, you can skip defining it, and the . init () of the superclass (Square) will be called automatically. A super() Deep Dive While the examples above (and below) call super() without any parameters, super() can also take two parameters: the first is the subclass, and the second parameter is an object that is an instance of that subclass. First, let\u2019s see two examples showing what manipulating the first variable can do, using the classes already shown: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) In Python 3, the super(Square, self) call is equivalent to the parameterless super() call. The first parameter refers to the subclass Square, while the second parameter refers to a Square object which, in this case, is self. You can call super() with other classes as well: class Cube(Square): def surface_area(self): face_area = super(Square, self).area() return face_area * 6 def volume(self): face_area = super(Square, self).area() return face_area * self.length In this example, you are setting Square as the subclass argument to super(), instead of Cube. This causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle. In this specific example, the behavior doesn\u2019t change. But imagine that Square also implemented an .area() function that you wanted to make sure Cube did not use. Calling super() in this way allows you to do that. What about the second parameter? Remember, this is an object that is an instance of the class used as the first parameter. For an example, isinstance(Cube, Square) must return True. By including an instantiated object, super() returns a bound method: a method that is bound to the object, which gives the method the object\u2019s context such as any instance attributes. If this parameter is not included, the method returned is just a function, unassociated with an object\u2019s context. For more information about bound methods, unbound methods, and functions, read the Python documentation on its descriptor system . Multiple Inheritance and Composition super() in Multiple Inheritance Now that you\u2019ve worked through an overview and some examples of super() and single inheritance, you will be introduced to an overview and some examples that will demonstrate how multiple inheritance works and how super() enables that functionality. Multiple Inheritance Overview There is another use case in which super() really shines, and this one isn\u2019t as common as the single inheritance scenario. In addition to single inheritance, Python supports multiple inheritance, in which a subclass can inherit from multiple superclasses that don\u2019t necessarily inherit from each other (also known as sibling classes). Superclass 1 Superclass 2 | | | | | | | | | | ------> Subclass <------- Let's get reacquainted with our base code: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) Now let's proceed... To better illustrate multiple inheritance in action, here is some code for you to try out, showing how you can build a right pyramid (a pyramid with a square base) out of a Triangle and a Square: class Triangle: def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height class RightPyramid(Triangle, Square): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area This example declares a Triangle class and a RightPyramid class that inherits from both Square and Triangle. You\u2019ll see another .area() method that uses super() just like in single inheritance, with the aim of it reaching the .perimeter() and .area() methods defined all the way up in the Rectangle class. The problem, though, is that both superclasses (Triangle and Square) define a .area(). Take a second and think about what might happen when you call .area() on RightPyramid, and then try calling it like below: >> pyramid = RightPyramid(2, 4) >> pyramid.area() Traceback (most recent call last): File \"shapes.py\", line 63, in print(pyramid.area()) File \"shapes.py\", line 47, in area base_area = super().area() File \"shapes.py\", line 38, in area return 0.5 * self.base * self.height AttributeError: 'RightPyramid' object has no attribute 'height' Did you guess that Python will try to call Triangle.area()? This is because of something called the method resolution order. Method Resolution Order The method resolution order (or MRO) tells Python how to search for inherited methods. This comes in handy when you\u2019re using super() because the MRO tells you exactly where Python will look for a method you\u2019re calling with super() and in what order. Every class has an . mro attribute that allows us to inspect the order, so let\u2019s do that: >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Triangle'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class 'object'>) This tells us that methods will be searched first in Rightpyramid, then in Triangle, then in Square, then Rectangle, and then, if nothing is found, in object, from which all classes originate. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. Luckily, you have some control over how the MRO is constructed. Just by changing the signature of the RightPyramid class, you can search in the order you want, and the methods will resolve correctly: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area Notice that RightPyramid initializes partially with the . init () from the Square class. This allows .area() to use the .length on the object, as is designed. Now, you can build a pyramid, inspect the MRO, and calculate the surface area: >>> pyramid = RightPyramid(2, 4) >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) >>> pyramid.area() 20.0 You see that the MRO is now what you\u2019d expect, and you can inspect the area of the pyramid as well, thanks to .area() and .perimeter(). There\u2019s still a problem here, though. For the sake of simplicity, I did a few things wrong in this example: the first, and arguably most importantly, was that I had two separate classes with the same method name and signature. This causes issues with method resolution, because the first instance of .area() that is encountered in the MRO list will be called. When you\u2019re using super() with multiple inheritance, it\u2019s imperative to design your classes to cooperate. Part of this is ensuring that your methods are unique so that they get resolved in the MRO, by making sure method signatures are unique\u2014whether by using method names or method parameters. In this case, to avoid a complete overhaul of your code, you can rename the Triangle class\u2019s .area() method to .tri_area(). This way, the area methods can continue using class properties rather than taking external parameters: class Triangle: def __init__(self, base, height): self.base = base self.height = height super().__init__() def tri_area(self): return 0.5 * self.base * self.height Let\u2019s also go ahead and use this in the RightPyramid class: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area The next issue here is that the code doesn\u2019t have a delegated Triangle object like it does for a Square object, so calling .area_2() will give us an AttributeError since .base and .height don\u2019t have any values. You need to do two things to fix this: All methods that are called with super() need to have a call to their superclass\u2019s version of that method. This means that you will need to add super(). init () to the . init () methods of Triangle and Rectangle. Redesign all the . init () calls to take a keyword dictionary. See the complete code below. class Rectangle: def __init__(self, length, width, **kwargs): self.length = length self.width = width super().__init__(**kwargs) def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from # the Rectangle class class Square(Rectangle): def __init__(self, length, **kwargs): super().__init__(length=length, width=length, **kwargs) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length class Triangle: def __init__(self, base, height, **kwargs): self.base = base self.height = height super().__init__(**kwargs) def tri_area(self): return 0.5 * self.base * self.height class RightPyramid(Square, Triangle): def __init__(self, base, slant_height, **kwargs): self.base = base self.slant_height = slant_height kwargs[\"height\"] = slant_height kwargs[\"length\"] = base super().__init__(base=base, **kwargs) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area There are a number of important differences in this code: kwargs is modified in some places (such as RightPyramid. init ()): This will allow users of these objects to instantiate them only with the arguments that make sense for that particular object. Setting up named arguments before kwargs: You can see this in RightPyramid. init (). This has the neat effect of popping that key right out of the kwargs dictionary, so that by the time that it ends up at the end of the MRO in the object class, **kwargs is empty. Now, when you use these updated classes, you have this: >>> pyramid = RightPyramid(base=2, slant_height=4) >>> pyramid.area() 20.0 >>> pyramid.area_2() 20.0 It works! You\u2019ve used super() to successfully navigate a complicated class hierarchy while using both inheritance and composition to create new classes with minimal reimplementation. Quiz!!! When Rectangle calls super() what are the values of kwargs and what class gets it's init method called Answer: kwargs is still containing {'base': 2, 'height': 4} values as they haven't been removed from kwargs super().__init__(**kwargs) calls Triangle's init , but Rectangle doesn't inherit from Triangle so why? Remember the MRO? Let's look at RightPyramid's MRO >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) Even though Rectangle doesn't inherit from Triangle, Triangle is next in the list of classes to search for an init method. You'll notice object is at the end of the list. super().__init__(**kwargs) Multiple Inheritance Alternatives As you can see, multiple inheritance can be useful but also lead to very complicated situations and code that is hard to read. It\u2019s also rare to have objects that neatly inherit everything from more than multiple other objects. If you see yourself beginning to use multiple inheritance and a complicated class hierarchy, it\u2019s worth asking yourself if you can achieve code that is cleaner and easier to understand by using composition instead of inheritance. With composition, you can add very specific functionality to your classes from a specialized, simple class called a mixin. Since this article is focused on inheritance, I won\u2019t go into too much detail on composition and how to wield it in Python, but here\u2019s a short example using VolumeMixin to give specific functionality to our 3D objects\u2014in this case, a volume calculation: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class VolumeMixin: def volume(self): return self.area() * self.height class Cube(VolumeMixin, Square): def __init__(self, length): super().__init__(length) self.height = length def face_area(self): return super().area() def surface_area(self): return super().area() * 6 In this example, the code was reworked to include a mixin called VolumeMixin. The mixin is then used by Cube and gives Cube the ability to calculate its volume, which is shown below: >>> cube = Cube(2) >>> cube.surface_area() 24 >>> cube.volume() 8 A super() Recap In this tutorial, you learned how to supercharge your classes with super(). Your journey started with a review of single inheritance and then showed how to call superclass methods easily with super(). You then learned how multiple inheritance works in Python, and techniques to combine super() with multiple inheritance. You also learned about how Python resolves method calls using the method resolution order (MRO), as well as how to inspect and modify the MRO to ensure appropriate methods are called at appropriate times. For more information about object-oriented programming in Python and using super(), check out these resources: Official super() documentation Python\u2019s super() Considered Super by Raymond Hettinger Object-Oriented Programming in Python 3","title":"Classes"},{"location":"notes/python/advanced/Classes/#classes","text":"","title":"Classes"},{"location":"notes/python/advanced/Classes/#basic-class-definition","text":"Intro: - Class: Blueprint - Object - Instance class Shark: # Basic Method definition def swim(self): print(\"The shark is swimming.\") def be_awesome(self): print(\"The shark is being awesome.\") Notice the use of self to reference an instance specifically...the one calling the method. Implementing sammy = Shark() sammy.swim() # >>> The shark is swimming. sammy.be_awesome() # >>> The shark is being awesome. Now let's dicuss init . class Shark: def __init__(self): print(\"This is the constructor method.\") >>> Shark() This is the constructor method. <__main__.Shark object at 0x10348d470> Finally, we can set the name of the Shark object sammy as equal to \"Sammy\" by passing it as a parameter of the Shark class: class Shark: def __init__(self, name): self.name = name def swim(self): print(self.name + \" is swimming.\") def be_awesome(self): print(self.name + \" is being awesome.\") def main(): # Set name of Shark object sammy = Shark(\"Sammy\") sammy.swim() sammy.be_awesome() if __name__ == \"__main__\": main() run... $ python shark.py Sammy is swimming. Sammy is being awesome.","title":"Basic Class Definition"},{"location":"notes/python/advanced/Classes/#inheritance","text":"super() and inheritance. In this tutorial, you\u2019ll learn about the following: The concept of inheritance in Python Multiple inheritance in Python How the super() function works How the super() function in single inheritance works How the super() function in multiple inheritance works Let's start with a simple example: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square: def __init__(self, length): self.length = length def area(self): return self.length * self.length def perimeter(self): return 4 * self.length >>> square = Square(4) >>> square.area() 16 >>> rectangle = Rectangle(2,4) >>> rectangle.area() 8 Here no references to inheritance are being made. super() in Single Inheritance super() gives you access to methods in a superclass from the subclass that inherits from it. super() alone returns a temporary object of the superclass that then allows you to call that superclass\u2019s methods. By using inheritance, you can reduce the amount of code you write while simultaneously reflecting the real-world relationship between rectangles and squares: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from the Rectangle class class Square(Rectangle): def __init__(self, length): super().__init__(length, length) >>> square = Square(4) >>> square.area() 16 What Can super() Do for You? Like in other object-oriented languages, it allows you to call methods of the superclass in your subclass. The primary use case of this is to extend the functionality of the inherited method. In the example below, you will create a class Cube that inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance: class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length >>> cube = Cube(3) >>> cube.surface_area() 54 >>> cube.volume() 27 Here you have implemented two methods for the Cube class: .surface_area() and .volume(). Both of these calculations rely on calculating the area of a single face, so rather than reimplementing the area calculation, you use super() to extend the area calculation. Also notice that the Cube class definition does not have an . init (). Because Cube inherits from Square and . init () doesn\u2019t really do anything differently for Cube than it already does for Square, you can skip defining it, and the . init () of the superclass (Square) will be called automatically. A super() Deep Dive While the examples above (and below) call super() without any parameters, super() can also take two parameters: the first is the subclass, and the second parameter is an object that is an instance of that subclass. First, let\u2019s see two examples showing what manipulating the first variable can do, using the classes already shown: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) In Python 3, the super(Square, self) call is equivalent to the parameterless super() call. The first parameter refers to the subclass Square, while the second parameter refers to a Square object which, in this case, is self. You can call super() with other classes as well: class Cube(Square): def surface_area(self): face_area = super(Square, self).area() return face_area * 6 def volume(self): face_area = super(Square, self).area() return face_area * self.length In this example, you are setting Square as the subclass argument to super(), instead of Cube. This causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle. In this specific example, the behavior doesn\u2019t change. But imagine that Square also implemented an .area() function that you wanted to make sure Cube did not use. Calling super() in this way allows you to do that. What about the second parameter? Remember, this is an object that is an instance of the class used as the first parameter. For an example, isinstance(Cube, Square) must return True. By including an instantiated object, super() returns a bound method: a method that is bound to the object, which gives the method the object\u2019s context such as any instance attributes. If this parameter is not included, the method returned is just a function, unassociated with an object\u2019s context. For more information about bound methods, unbound methods, and functions, read the Python documentation on its descriptor system .","title":"Inheritance"},{"location":"notes/python/advanced/Classes/#multiple-inheritance-and-composition","text":"super() in Multiple Inheritance Now that you\u2019ve worked through an overview and some examples of super() and single inheritance, you will be introduced to an overview and some examples that will demonstrate how multiple inheritance works and how super() enables that functionality. Multiple Inheritance Overview There is another use case in which super() really shines, and this one isn\u2019t as common as the single inheritance scenario. In addition to single inheritance, Python supports multiple inheritance, in which a subclass can inherit from multiple superclasses that don\u2019t necessarily inherit from each other (also known as sibling classes). Superclass 1 Superclass 2 | | | | | | | | | | ------> Subclass <------- Let's get reacquainted with our base code: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) Now let's proceed... To better illustrate multiple inheritance in action, here is some code for you to try out, showing how you can build a right pyramid (a pyramid with a square base) out of a Triangle and a Square: class Triangle: def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height class RightPyramid(Triangle, Square): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area This example declares a Triangle class and a RightPyramid class that inherits from both Square and Triangle. You\u2019ll see another .area() method that uses super() just like in single inheritance, with the aim of it reaching the .perimeter() and .area() methods defined all the way up in the Rectangle class. The problem, though, is that both superclasses (Triangle and Square) define a .area(). Take a second and think about what might happen when you call .area() on RightPyramid, and then try calling it like below: >> pyramid = RightPyramid(2, 4) >> pyramid.area() Traceback (most recent call last): File \"shapes.py\", line 63, in print(pyramid.area()) File \"shapes.py\", line 47, in area base_area = super().area() File \"shapes.py\", line 38, in area return 0.5 * self.base * self.height AttributeError: 'RightPyramid' object has no attribute 'height' Did you guess that Python will try to call Triangle.area()? This is because of something called the method resolution order. Method Resolution Order The method resolution order (or MRO) tells Python how to search for inherited methods. This comes in handy when you\u2019re using super() because the MRO tells you exactly where Python will look for a method you\u2019re calling with super() and in what order. Every class has an . mro attribute that allows us to inspect the order, so let\u2019s do that: >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Triangle'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class 'object'>) This tells us that methods will be searched first in Rightpyramid, then in Triangle, then in Square, then Rectangle, and then, if nothing is found, in object, from which all classes originate. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. Luckily, you have some control over how the MRO is constructed. Just by changing the signature of the RightPyramid class, you can search in the order you want, and the methods will resolve correctly: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area Notice that RightPyramid initializes partially with the . init () from the Square class. This allows .area() to use the .length on the object, as is designed. Now, you can build a pyramid, inspect the MRO, and calculate the surface area: >>> pyramid = RightPyramid(2, 4) >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) >>> pyramid.area() 20.0 You see that the MRO is now what you\u2019d expect, and you can inspect the area of the pyramid as well, thanks to .area() and .perimeter(). There\u2019s still a problem here, though. For the sake of simplicity, I did a few things wrong in this example: the first, and arguably most importantly, was that I had two separate classes with the same method name and signature. This causes issues with method resolution, because the first instance of .area() that is encountered in the MRO list will be called. When you\u2019re using super() with multiple inheritance, it\u2019s imperative to design your classes to cooperate. Part of this is ensuring that your methods are unique so that they get resolved in the MRO, by making sure method signatures are unique\u2014whether by using method names or method parameters. In this case, to avoid a complete overhaul of your code, you can rename the Triangle class\u2019s .area() method to .tri_area(). This way, the area methods can continue using class properties rather than taking external parameters: class Triangle: def __init__(self, base, height): self.base = base self.height = height super().__init__() def tri_area(self): return 0.5 * self.base * self.height Let\u2019s also go ahead and use this in the RightPyramid class: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area The next issue here is that the code doesn\u2019t have a delegated Triangle object like it does for a Square object, so calling .area_2() will give us an AttributeError since .base and .height don\u2019t have any values. You need to do two things to fix this: All methods that are called with super() need to have a call to their superclass\u2019s version of that method. This means that you will need to add super(). init () to the . init () methods of Triangle and Rectangle. Redesign all the . init () calls to take a keyword dictionary. See the complete code below. class Rectangle: def __init__(self, length, width, **kwargs): self.length = length self.width = width super().__init__(**kwargs) def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from # the Rectangle class class Square(Rectangle): def __init__(self, length, **kwargs): super().__init__(length=length, width=length, **kwargs) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length class Triangle: def __init__(self, base, height, **kwargs): self.base = base self.height = height super().__init__(**kwargs) def tri_area(self): return 0.5 * self.base * self.height class RightPyramid(Square, Triangle): def __init__(self, base, slant_height, **kwargs): self.base = base self.slant_height = slant_height kwargs[\"height\"] = slant_height kwargs[\"length\"] = base super().__init__(base=base, **kwargs) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area There are a number of important differences in this code: kwargs is modified in some places (such as RightPyramid. init ()): This will allow users of these objects to instantiate them only with the arguments that make sense for that particular object. Setting up named arguments before kwargs: You can see this in RightPyramid. init (). This has the neat effect of popping that key right out of the kwargs dictionary, so that by the time that it ends up at the end of the MRO in the object class, **kwargs is empty. Now, when you use these updated classes, you have this: >>> pyramid = RightPyramid(base=2, slant_height=4) >>> pyramid.area() 20.0 >>> pyramid.area_2() 20.0 It works! You\u2019ve used super() to successfully navigate a complicated class hierarchy while using both inheritance and composition to create new classes with minimal reimplementation. Quiz!!! When Rectangle calls super() what are the values of kwargs and what class gets it's init method called Answer: kwargs is still containing {'base': 2, 'height': 4} values as they haven't been removed from kwargs super().__init__(**kwargs) calls Triangle's init , but Rectangle doesn't inherit from Triangle so why? Remember the MRO? Let's look at RightPyramid's MRO >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) Even though Rectangle doesn't inherit from Triangle, Triangle is next in the list of classes to search for an init method. You'll notice object is at the end of the list. super().__init__(**kwargs) Multiple Inheritance Alternatives As you can see, multiple inheritance can be useful but also lead to very complicated situations and code that is hard to read. It\u2019s also rare to have objects that neatly inherit everything from more than multiple other objects. If you see yourself beginning to use multiple inheritance and a complicated class hierarchy, it\u2019s worth asking yourself if you can achieve code that is cleaner and easier to understand by using composition instead of inheritance. With composition, you can add very specific functionality to your classes from a specialized, simple class called a mixin. Since this article is focused on inheritance, I won\u2019t go into too much detail on composition and how to wield it in Python, but here\u2019s a short example using VolumeMixin to give specific functionality to our 3D objects\u2014in this case, a volume calculation: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class VolumeMixin: def volume(self): return self.area() * self.height class Cube(VolumeMixin, Square): def __init__(self, length): super().__init__(length) self.height = length def face_area(self): return super().area() def surface_area(self): return super().area() * 6 In this example, the code was reworked to include a mixin called VolumeMixin. The mixin is then used by Cube and gives Cube the ability to calculate its volume, which is shown below: >>> cube = Cube(2) >>> cube.surface_area() 24 >>> cube.volume() 8 A super() Recap In this tutorial, you learned how to supercharge your classes with super(). Your journey started with a review of single inheritance and then showed how to call superclass methods easily with super(). You then learned how multiple inheritance works in Python, and techniques to combine super() with multiple inheritance. You also learned about how Python resolves method calls using the method resolution order (MRO), as well as how to inspect and modify the MRO to ensure appropriate methods are called at appropriate times. For more information about object-oriented programming in Python and using super(), check out these resources: Official super() documentation Python\u2019s super() Considered Super by Raymond Hettinger Object-Oriented Programming in Python 3","title":"Multiple Inheritance and Composition"},{"location":"notes/python/advanced/Closures/","text":"Closures pyenv activate venv3.10.4 Notice below when pdb.set_trace() is called, var will not be available. import pdb def create_closure(var): def closure(): pdb.set_trace() print(f\"test\") return closure # Create a closure my_closure = create_closure(\"Hello, World!\") # Print the source code print(my_closure()) Notice below when pdb.set_trace() is called, var will be available. import pdb def create_closure(var): def closure(): pdb.set_trace() print(f\"test {var}\") return closure # Create a closure my_closure = create_closure(\"Hello, World!\") # Print the source code print(my_closure()) The reason is that the variable var is not defined in the local scope of the closure. When pdb.set_trace() is called, it will not be able to find the variable var .","title":"Closures"},{"location":"notes/python/advanced/Closures/#closures","text":"pyenv activate venv3.10.4 Notice below when pdb.set_trace() is called, var will not be available. import pdb def create_closure(var): def closure(): pdb.set_trace() print(f\"test\") return closure # Create a closure my_closure = create_closure(\"Hello, World!\") # Print the source code print(my_closure()) Notice below when pdb.set_trace() is called, var will be available. import pdb def create_closure(var): def closure(): pdb.set_trace() print(f\"test {var}\") return closure # Create a closure my_closure = create_closure(\"Hello, World!\") # Print the source code print(my_closure()) The reason is that the variable var is not defined in the local scope of the closure. When pdb.set_trace() is called, it will not be able to find the variable var .","title":"Closures"},{"location":"notes/python/advanced/Decorators/","text":"Decorators @Add_This_Functionality_To_Any_Function! Simple Example: def my_decorator(original_function): def new_function(*args,**kwargs): print(\"you did it!\") of = original_function(*args,**kwargs) return of return new_function @my_decorator def my_func1(stuff): print(\"do things\") @my_decorator def my_func2(stuff): print(\"do things\") @my_decorator def my_func3(stuff): print(\"do things\") my_func1(1);my_func2(1);my_func3(1); Tutorial: We need to discuss global variables and function closure [1] functions: def foo(): return 1; [2] scope a_string = 'this is a string' def foo(): print(locals()) foo() # {} print(globals()) # {..., 'a_string': 'this is a string'} [3] varible resolution rules Local variables with the same name as global ones don't modify the global one. [4] variable lifetime def foo(): x = 1; foo() # NameError: name 'x' is not defined [5] function arguments and parameters def foo(x): print(locals()) foo(1) # {'x': 1} def foo(x, y=0): # remember if no default it's mandatory return x - y foo() # Traceback (most recent call last): # File \"\", line 1, in # TypeError: foo() missing 1 required positional argument: 'x' [6] Nested Functions def outer(): x = 1 def inner(): print(x) inner() outer() # 1 [7] Functions are first class objectsin python # all objects in Python inherit from a common baseclass issubclass(int, object) # True def foo(): pass foo.__class__ # <type 'function'> issubclass(foo.__class__, object) # True #..so what? def add(x,y): return x + y def sub(x,y): return x - y def apply(func, x, y): return func(x, y) apply(add, 2, 1) # 3 apply(sub, 2, 1) # 1 #.. closure lead in def outer(): def inner(): print('this is inner') return inner # the function not what it returns foo = outer() foo # .inner at 0x10be011e0> foo() # 'this is inner' [8] Closures def outer(): x = 1 def inner(): print(x) return inner # the function not what it returns foo = outer() foo.__closure__ # (,) # aside: cell objects are used to store the free variables of a closure # without closures x would have not existed as after the call to outer x is gone based on variable life time. # inner functions defined in non-global scope remember what their enclosing namespaces looked like at definition time. foo() # 1 # let's tweak it def outer(x): def inner(): print(x) return inner print1 = outer(1) print2 = outer(2) print1() # 1 print2() # 2 [9] Decorators def outer(some_func): def inner(): print('before some_func') ret = some_func() # 1 return ret + 1 return inner def foo(): return 1 decorated = outer(foo) decorated() # we've added functionality to foo()! # what if we did this foo = outer(foo) foo() # before some_func # 2 # Let's write a more useful decorator class Coordinate(object): def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \"Coord: \" + str(self.__dict__) def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) one = Coordinate(100, 200) two = Coordinate(300, 200) add(one, two) # Coord: {'y': 400, 'x': 400} # add this instance three = Coordinate(-100, -100) def wrapper(func): def checker(a, b): # 1 if a.x < 0 or a.y < 0: a = Coordinate(a.x if a.x > 0 else 0, a.y if a.y > 0 else 0) if b.x < 0 or b.y < 0: b = Coordinate(b.x if b.x > 0 else 0, b.y if b.y > 0 else 0) ret = func(a, b) if ret.x < 0 or ret.y < 0: ret = Coordinate(ret.x if ret.x > 0 else 0, ret.y if ret.y > 0 else 0) return ret return checker add = wrapper(add) sub = wrapper(sub) sub(one, two) add(one, three) [10] the @ symbol # so instead of wrapper(add)\\wrapper(sub), use @wrapper @wrapper def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) @wrapper def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) [11] *args and **kwargs def one(*args): print(args) one() # () one(1,2,3) # (1, 2, 3) def two(x, y, *args): print(x, y, args) two('a','b','c') # Reminder # l = (1,2,3) # one(l[0], l[1], l[2]) # (1, 2, 3) # one(*l) # (1, 2, 3) def foo(**kwargs): print(kwargs) foo() foo(x=1, y=2) [12] More generic decorators def logger(func): def inner(*args, **kwargs): print('Arguments were: {}, {}'.format(args,kwargs)) return func(*args, **kwargs) return inner @logger def foo1(x,y=1): return x * y @logger def foo2(): return 2 foo1(5,4) # Arguments were: (5, 4), {} # 20 foo2() # Arguments were: (), {} # 2","title":"Decorators"},{"location":"notes/python/advanced/Decorators/#decorators","text":"@Add_This_Functionality_To_Any_Function!","title":"Decorators"},{"location":"notes/python/advanced/Decorators/#simple-example","text":"def my_decorator(original_function): def new_function(*args,**kwargs): print(\"you did it!\") of = original_function(*args,**kwargs) return of return new_function @my_decorator def my_func1(stuff): print(\"do things\") @my_decorator def my_func2(stuff): print(\"do things\") @my_decorator def my_func3(stuff): print(\"do things\") my_func1(1);my_func2(1);my_func3(1);","title":"Simple Example:"},{"location":"notes/python/advanced/Decorators/#tutorial","text":"We need to discuss global variables and function closure","title":"Tutorial:"},{"location":"notes/python/advanced/Decorators/#1-functions","text":"def foo(): return 1;","title":"[1] functions:"},{"location":"notes/python/advanced/Decorators/#2-scope","text":"a_string = 'this is a string' def foo(): print(locals()) foo() # {} print(globals()) # {..., 'a_string': 'this is a string'}","title":"[2] scope"},{"location":"notes/python/advanced/Decorators/#3-varible-resolution-rules","text":"Local variables with the same name as global ones don't modify the global one.","title":"[3] varible resolution rules"},{"location":"notes/python/advanced/Decorators/#4-variable-lifetime","text":"def foo(): x = 1; foo() # NameError: name 'x' is not defined","title":"[4] variable lifetime"},{"location":"notes/python/advanced/Decorators/#5-function-arguments-and-parameters","text":"def foo(x): print(locals()) foo(1) # {'x': 1} def foo(x, y=0): # remember if no default it's mandatory return x - y foo() # Traceback (most recent call last): # File \"\", line 1, in # TypeError: foo() missing 1 required positional argument: 'x'","title":"[5] function arguments and parameters"},{"location":"notes/python/advanced/Decorators/#6-nested-functions","text":"def outer(): x = 1 def inner(): print(x) inner() outer() # 1","title":"[6] Nested Functions"},{"location":"notes/python/advanced/Decorators/#7-functions-are-first-class-objectsin-python","text":"# all objects in Python inherit from a common baseclass issubclass(int, object) # True def foo(): pass foo.__class__ # <type 'function'> issubclass(foo.__class__, object) # True #..so what? def add(x,y): return x + y def sub(x,y): return x - y def apply(func, x, y): return func(x, y) apply(add, 2, 1) # 3 apply(sub, 2, 1) # 1 #.. closure lead in def outer(): def inner(): print('this is inner') return inner # the function not what it returns foo = outer() foo # .inner at 0x10be011e0> foo() # 'this is inner'","title":"[7] Functions are first class objectsin python"},{"location":"notes/python/advanced/Decorators/#8-closures","text":"def outer(): x = 1 def inner(): print(x) return inner # the function not what it returns foo = outer() foo.__closure__ # (,) # aside: cell objects are used to store the free variables of a closure # without closures x would have not existed as after the call to outer x is gone based on variable life time. # inner functions defined in non-global scope remember what their enclosing namespaces looked like at definition time. foo() # 1 # let's tweak it def outer(x): def inner(): print(x) return inner print1 = outer(1) print2 = outer(2) print1() # 1 print2() # 2","title":"[8] Closures"},{"location":"notes/python/advanced/Decorators/#9-decorators","text":"def outer(some_func): def inner(): print('before some_func') ret = some_func() # 1 return ret + 1 return inner def foo(): return 1 decorated = outer(foo) decorated() # we've added functionality to foo()! # what if we did this foo = outer(foo) foo() # before some_func # 2 # Let's write a more useful decorator class Coordinate(object): def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \"Coord: \" + str(self.__dict__) def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) one = Coordinate(100, 200) two = Coordinate(300, 200) add(one, two) # Coord: {'y': 400, 'x': 400} # add this instance three = Coordinate(-100, -100) def wrapper(func): def checker(a, b): # 1 if a.x < 0 or a.y < 0: a = Coordinate(a.x if a.x > 0 else 0, a.y if a.y > 0 else 0) if b.x < 0 or b.y < 0: b = Coordinate(b.x if b.x > 0 else 0, b.y if b.y > 0 else 0) ret = func(a, b) if ret.x < 0 or ret.y < 0: ret = Coordinate(ret.x if ret.x > 0 else 0, ret.y if ret.y > 0 else 0) return ret return checker add = wrapper(add) sub = wrapper(sub) sub(one, two) add(one, three)","title":"[9] Decorators"},{"location":"notes/python/advanced/Decorators/#10-the-symbol","text":"# so instead of wrapper(add)\\wrapper(sub), use @wrapper @wrapper def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) @wrapper def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y)","title":"[10] the @ symbol"},{"location":"notes/python/advanced/Decorators/#11-args-and-kwargs","text":"def one(*args): print(args) one() # () one(1,2,3) # (1, 2, 3) def two(x, y, *args): print(x, y, args) two('a','b','c') # Reminder # l = (1,2,3) # one(l[0], l[1], l[2]) # (1, 2, 3) # one(*l) # (1, 2, 3) def foo(**kwargs): print(kwargs) foo() foo(x=1, y=2)","title":"[11] *args and **kwargs"},{"location":"notes/python/advanced/Decorators/#12-more-generic-decorators","text":"def logger(func): def inner(*args, **kwargs): print('Arguments were: {}, {}'.format(args,kwargs)) return func(*args, **kwargs) return inner @logger def foo1(x,y=1): return x * y @logger def foo2(): return 2 foo1(5,4) # Arguments were: (5, 4), {} # 20 foo2() # Arguments were: (), {} # 2","title":"[12] More generic decorators"},{"location":"notes/python/advanced/Generators/","text":"Generators A simplified Iterator Quick Example: def all_even(): n = 0 while True: yield n n += 2 x = all_even() print(next(x)) # 0 print(next(x)) # 2 print(next(x)) # 4 #.... forever What are generators in Python? There is a lot of overhead in building an iterator in Python; we have to implement a class with iter () and next () method, keep track of internal states, raise StopIteration when there was no values to be returned etc. This is both lengthy and counter intuitive. Generator comes into rescue in such situations. Python generators are a simple way of creating iterators. All the overhead we mentioned above are automatically handled by generators in Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time). How to create a generator in Python? It is fairly simple to create a generator in Python. It is as easy as defining a normal function with yield statement instead of a return statement. If a function contains at least one yield statement (it may contain other yield or return statements), it becomes a generator function. Both yield and return will return some value from a function. The difference is that, while a return statement terminates a function entirely, yield statement pauses the function saving all its states and later continues from there on successive calls. Differences between Generator function and a Normal function Generator function contains one or more yield statement. When called, it returns an object (iterator) but does not start execution immediately. Methods like iter () and next () are implemented automatically. So we can iterate through the items using next(). Once the function yields, the function is paused and the control is transferred to the caller. Local variables and their states are remembered between successive calls. Finally, when the function terminates, StopIteration is raised automatically on further calls. Simple Example: # A simple generator function def my_gen(): n = 1 print('This is printed first') # Generator function contains yield statements yield n n += 1 print('This is printed second') yield n n += 1 print('This is printed at last') yield n mg = my_gen() next(mg) # This is printed first next(mg) # This is printed second next(mg) # This is printed third next(mg) # Raises error... One interesting thing to note in the above example is that, the value of variable n is remembered between each call. Unlike normal functions, the local variables are not destroyed when the function yields. Furthermore, the generator object can be iterated only once. To restart the process we need to create another generator object using something like a = my_gen(). Note: One final thing to note is that we can use generators with for loops directly. This is because, a for loop takes an iterator and iterates over it using next() function. It automatically ends when StopIteration is raised. Check here to know how a for loop is actually implemented in Python . Python Generators with a Loop The above example is of less use and we studied it just to get an idea of what was happening in the background. Normally, generator functions are implemented with a loop having a suitable terminating condition. Let's take an example of a generator that reverses a string. def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] # For loop to reverse the string # Output: # o # l # l # e # h for char in rev_str(\"hello\"): print(char) In this example, we use range() function to get the index in reverse order using the for loop. It turns out that this generator function not only works with string, but also with other kind of iterables like list, tuple etc. Python Generator Expression Simple generators can be easily created on the fly using generator expressions. It makes building generators easy. Same as lambda function creates an anonymous function, generator expression creates an anonymous generator function. The syntax for generator expression is similar to that of a list comprehension in Python. But the square brackets are replaced with round parentheses. The major difference between a list comprehension and a generator expression is that while list comprehension produces the entire list, generator expression produces one item at a time. They are kind of lazy, producing items only when asked for. For this reason, a generator expression is much more memory efficient than an equivalent list comprehension. # Initialize the list my_list = [1, 3, 6, 10] # square each term using list comprehension # Output: [1, 9, 36, 100] [x**2 for x in my_list] # same thing can be done using generator expression # Output: at 0x0000000002EBDAF8> y = (x**2 for x in my_list) print(next(y)) # 1 print(next(y)) # 9 print(next(y)) # 36 print(next(y)) # 100 print(next(y)) # error Generator expression can be used inside functions. When used in such a way, the round parentheses can be dropped. >>> sum(x**2 for x in my_list) 146 >>> max(x**2 for x in my_list) 100 Why generators are used in Python? Easy to Implement: Generators can be implemented in a clear and concise way as compared to their iterator class counterpart. Following is an example to implement a sequence of power of 2's using iterator class. class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n > self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result This was lengthy. Now lets do the same using a generator function. def PowTwoGen(max = 0): n = 0 while n < max: yield 2 ** n n += 1 Memory Efficient: A normal function to return a sequence will create the entire sequence in memory before returning the result. This is an overkill if the number of items in the sequence is very large. Generator implementation of such sequence is memory friendly and is preferred since it only produces one item at a time. Represent Infinite Stream: Generators are excellent medium to represent an infinite stream of data. Infinite streams cannot be stored in memory and since generators produce only one item at a time, it can represent infinite stream of data. The following example can generate all the even numbers (at least in theory). def all_even(): n = 0 while True: yield n n += 2 Pipelining Generators: Generators can be used to pipeline a series of operations. This is best illustrated using an example. Suppose we have a log file from a famous fast food chain. The log file has a column (4th column) that keeps track of the number of pizza sold every hour and we want to sum it to find the total pizzas sold in 5 years. Assume everything is in string and numbers that are not available are marked as 'N/A'. A generator implementation of this could be as follows. # - sells.log - # # 1 2 3 1 # 1 2 3 2 # 1 2 3 3 # 1 2 3 4 # 1 2 3 5 # 1 2 3 6 # 1 2 3 7 # 1 2 3 8 # 1 2 3 9 # 1 2 3 10 # - - # with open('sells.log') as file: pizza_col = (line.split()[3] for line in file) per_hour = (int(x) for x in pizza_col if x != 'N/A') print(\"Total pizzas sold = \",sum(per_hour)) This pipelining is efficient and easy to read (and yes, a lot cooler!).","title":"Generators"},{"location":"notes/python/advanced/Generators/#generators","text":"A simplified Iterator","title":"Generators"},{"location":"notes/python/advanced/Generators/#quick-example","text":"def all_even(): n = 0 while True: yield n n += 2 x = all_even() print(next(x)) # 0 print(next(x)) # 2 print(next(x)) # 4 #.... forever","title":"Quick Example:"},{"location":"notes/python/advanced/Generators/#what-are-generators-in-python","text":"There is a lot of overhead in building an iterator in Python; we have to implement a class with iter () and next () method, keep track of internal states, raise StopIteration when there was no values to be returned etc. This is both lengthy and counter intuitive. Generator comes into rescue in such situations. Python generators are a simple way of creating iterators. All the overhead we mentioned above are automatically handled by generators in Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time).","title":"What are generators in Python?"},{"location":"notes/python/advanced/Generators/#how-to-create-a-generator-in-python","text":"It is fairly simple to create a generator in Python. It is as easy as defining a normal function with yield statement instead of a return statement. If a function contains at least one yield statement (it may contain other yield or return statements), it becomes a generator function. Both yield and return will return some value from a function. The difference is that, while a return statement terminates a function entirely, yield statement pauses the function saving all its states and later continues from there on successive calls.","title":"How to create a generator in Python?"},{"location":"notes/python/advanced/Generators/#differences-between-generator-function-and-a-normal-function","text":"Generator function contains one or more yield statement. When called, it returns an object (iterator) but does not start execution immediately. Methods like iter () and next () are implemented automatically. So we can iterate through the items using next(). Once the function yields, the function is paused and the control is transferred to the caller. Local variables and their states are remembered between successive calls. Finally, when the function terminates, StopIteration is raised automatically on further calls. Simple Example: # A simple generator function def my_gen(): n = 1 print('This is printed first') # Generator function contains yield statements yield n n += 1 print('This is printed second') yield n n += 1 print('This is printed at last') yield n mg = my_gen() next(mg) # This is printed first next(mg) # This is printed second next(mg) # This is printed third next(mg) # Raises error... One interesting thing to note in the above example is that, the value of variable n is remembered between each call. Unlike normal functions, the local variables are not destroyed when the function yields. Furthermore, the generator object can be iterated only once. To restart the process we need to create another generator object using something like a = my_gen(). Note: One final thing to note is that we can use generators with for loops directly. This is because, a for loop takes an iterator and iterates over it using next() function. It automatically ends when StopIteration is raised. Check here to know how a for loop is actually implemented in Python .","title":"Differences between Generator function and a Normal function"},{"location":"notes/python/advanced/Generators/#python-generators-with-a-loop","text":"The above example is of less use and we studied it just to get an idea of what was happening in the background. Normally, generator functions are implemented with a loop having a suitable terminating condition. Let's take an example of a generator that reverses a string. def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] # For loop to reverse the string # Output: # o # l # l # e # h for char in rev_str(\"hello\"): print(char) In this example, we use range() function to get the index in reverse order using the for loop. It turns out that this generator function not only works with string, but also with other kind of iterables like list, tuple etc.","title":"Python Generators with a Loop"},{"location":"notes/python/advanced/Generators/#python-generator-expression","text":"Simple generators can be easily created on the fly using generator expressions. It makes building generators easy. Same as lambda function creates an anonymous function, generator expression creates an anonymous generator function. The syntax for generator expression is similar to that of a list comprehension in Python. But the square brackets are replaced with round parentheses. The major difference between a list comprehension and a generator expression is that while list comprehension produces the entire list, generator expression produces one item at a time. They are kind of lazy, producing items only when asked for. For this reason, a generator expression is much more memory efficient than an equivalent list comprehension. # Initialize the list my_list = [1, 3, 6, 10] # square each term using list comprehension # Output: [1, 9, 36, 100] [x**2 for x in my_list] # same thing can be done using generator expression # Output: at 0x0000000002EBDAF8> y = (x**2 for x in my_list) print(next(y)) # 1 print(next(y)) # 9 print(next(y)) # 36 print(next(y)) # 100 print(next(y)) # error Generator expression can be used inside functions. When used in such a way, the round parentheses can be dropped. >>> sum(x**2 for x in my_list) 146 >>> max(x**2 for x in my_list) 100","title":"Python Generator Expression"},{"location":"notes/python/advanced/Generators/#why-generators-are-used-in-python","text":"Easy to Implement: Generators can be implemented in a clear and concise way as compared to their iterator class counterpart. Following is an example to implement a sequence of power of 2's using iterator class. class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n > self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result This was lengthy. Now lets do the same using a generator function. def PowTwoGen(max = 0): n = 0 while n < max: yield 2 ** n n += 1 Memory Efficient: A normal function to return a sequence will create the entire sequence in memory before returning the result. This is an overkill if the number of items in the sequence is very large. Generator implementation of such sequence is memory friendly and is preferred since it only produces one item at a time. Represent Infinite Stream: Generators are excellent medium to represent an infinite stream of data. Infinite streams cannot be stored in memory and since generators produce only one item at a time, it can represent infinite stream of data. The following example can generate all the even numbers (at least in theory). def all_even(): n = 0 while True: yield n n += 2 Pipelining Generators: Generators can be used to pipeline a series of operations. This is best illustrated using an example. Suppose we have a log file from a famous fast food chain. The log file has a column (4th column) that keeps track of the number of pizza sold every hour and we want to sum it to find the total pizzas sold in 5 years. Assume everything is in string and numbers that are not available are marked as 'N/A'. A generator implementation of this could be as follows. # - sells.log - # # 1 2 3 1 # 1 2 3 2 # 1 2 3 3 # 1 2 3 4 # 1 2 3 5 # 1 2 3 6 # 1 2 3 7 # 1 2 3 8 # 1 2 3 9 # 1 2 3 10 # - - # with open('sells.log') as file: pizza_col = (line.split()[3] for line in file) per_hour = (int(x) for x in pizza_col if x != 'N/A') print(\"Total pizzas sold = \",sum(per_hour)) This pipelining is efficient and easy to read (and yes, a lot cooler!).","title":"Why generators are used in Python?"},{"location":"notes/python/base_modules/Args%20and%20Kwargs/","text":"Args and Kwargs Passing unknown amounts of inputs to a function *args - any number of inputs of any data type. They will be referenced in order with indicies being of the form args[0]...etc. **kwargs - The same as above but with key work arguments, so you would be able to access elements by key name. kwargs The * is important as it signifies if args or kwargs are being used. Otherwise this happens: >>> def prac(*args): ... for i in args: ... print(\"This arg is :{}\".format(i)) ... >>> prac([1,2,3,4,5]) This arg is :[1, 2, 3, 4, 5] # ooooops! >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> def prac(**kwargs): ... for k in kwargs.keys(): ... print(kwargs[k]) ... >>> prac(**{'A':1,'B':2}) 1 2 Also note iterables are acceptible inputs: >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*(1,2,3,4,5)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*{1,2,3,4,5}) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*range(1,6)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 Below are additional examples def f(*args,**kwargs): print(args, kwargs) l = [1,2,3] t = (4,5,6) d = {'a':7,'b':8,'c':9} f() # () {} f(1,2,3) # (1, 2, 3) {} f(1,2,3,\"groovy\") # (1, 2, 3, 'groovy') {} f(a=1,b=2,c=3) # () {'a': 1, 'c': 3, 'b': 2} f(a=1,b=2,c=3,zzz=\"hi\") # () {'a': 1, 'c': 3, 'b': 2, 'zzz': 'hi'} f(1,2,3,a=1,b=2,c=3) # (1, 2, 3) {'a': 1, 'c': 3, 'b': 2} f(*l,**d) # (1, 2, 3) {'a': 7, 'c': 9, 'b': 8} f(*t,**d) # (4, 5, 6) {'a': 7, 'c': 9, 'b': 8} f(1,2,*t) # (1, 2, 4, 5, 6) {} f(q=\"winning\",**d) # () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f(1,2,*t,q=\"winning\",**d) # (1, 2, 4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} def f2(arg1,arg2,*args,**kwargs): print(arg1,arg2, args, kwargs) f2(1,2,3) # 1 2 (3,) {} f2(1,2,3,\"groovy\") # 1 2 (3, 'groovy') {} f2(arg1=1,arg2=2,c=3) # 1 2 () {'c': 3} f2(arg1=1,arg2=2,c=3,zzz=\"hi\") # 1 2 () {'c': 3, 'zzz': 'hi'} f2(1,2,3,a=1,b=2,c=3) # 1 2 (3,) {'a': 1, 'c': 3, 'b': 2} f2(*l,**d) # 1 2 (3,) {'a': 7, 'c': 9, 'b': 8} f2(*t,**d) # 4 5 (6,) {'a': 7, 'c': 9, 'b': 8} f2(1,2,*t) # 1 2 (4, 5, 6) {} f2(1,1,q=\"winning\",**d) # 1 1 () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f2(1,2,*t,q=\"winning\",**d) # 1 2 (4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8}","title":"args, kwargs"},{"location":"notes/python/base_modules/Args%20and%20Kwargs/#args-and-kwargs","text":"Passing unknown amounts of inputs to a function *args - any number of inputs of any data type. They will be referenced in order with indicies being of the form args[0]...etc. **kwargs - The same as above but with key work arguments, so you would be able to access elements by key name. kwargs The * is important as it signifies if args or kwargs are being used. Otherwise this happens: >>> def prac(*args): ... for i in args: ... print(\"This arg is :{}\".format(i)) ... >>> prac([1,2,3,4,5]) This arg is :[1, 2, 3, 4, 5] # ooooops! >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> def prac(**kwargs): ... for k in kwargs.keys(): ... print(kwargs[k]) ... >>> prac(**{'A':1,'B':2}) 1 2 Also note iterables are acceptible inputs: >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*(1,2,3,4,5)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*{1,2,3,4,5}) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*range(1,6)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 Below are additional examples def f(*args,**kwargs): print(args, kwargs) l = [1,2,3] t = (4,5,6) d = {'a':7,'b':8,'c':9} f() # () {} f(1,2,3) # (1, 2, 3) {} f(1,2,3,\"groovy\") # (1, 2, 3, 'groovy') {} f(a=1,b=2,c=3) # () {'a': 1, 'c': 3, 'b': 2} f(a=1,b=2,c=3,zzz=\"hi\") # () {'a': 1, 'c': 3, 'b': 2, 'zzz': 'hi'} f(1,2,3,a=1,b=2,c=3) # (1, 2, 3) {'a': 1, 'c': 3, 'b': 2} f(*l,**d) # (1, 2, 3) {'a': 7, 'c': 9, 'b': 8} f(*t,**d) # (4, 5, 6) {'a': 7, 'c': 9, 'b': 8} f(1,2,*t) # (1, 2, 4, 5, 6) {} f(q=\"winning\",**d) # () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f(1,2,*t,q=\"winning\",**d) # (1, 2, 4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} def f2(arg1,arg2,*args,**kwargs): print(arg1,arg2, args, kwargs) f2(1,2,3) # 1 2 (3,) {} f2(1,2,3,\"groovy\") # 1 2 (3, 'groovy') {} f2(arg1=1,arg2=2,c=3) # 1 2 () {'c': 3} f2(arg1=1,arg2=2,c=3,zzz=\"hi\") # 1 2 () {'c': 3, 'zzz': 'hi'} f2(1,2,3,a=1,b=2,c=3) # 1 2 (3,) {'a': 1, 'c': 3, 'b': 2} f2(*l,**d) # 1 2 (3,) {'a': 7, 'c': 9, 'b': 8} f2(*t,**d) # 4 5 (6,) {'a': 7, 'c': 9, 'b': 8} f2(1,2,*t) # 1 2 (4, 5, 6) {} f2(1,1,q=\"winning\",**d) # 1 1 () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f2(1,2,*t,q=\"winning\",**d) # 1 2 (4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8}","title":"Args and Kwargs"},{"location":"notes/python/base_modules/Input/","text":"Input Here we show how to accept user input print('Enter your name:') x = input() print('Hello, ' + x) >>> Enter your name: Ben Hello, Ben","title":"input()"},{"location":"notes/python/base_modules/Input/#input","text":"Here we show how to accept user input print('Enter your name:') x = input() print('Hello, ' + x) >>> Enter your name: Ben Hello, Ben","title":"Input"},{"location":"notes/python/base_modules/Map/","text":"Map map() allows us to apply a function to a list of items # Simple Example >>> list(map(float, ['1.0', '2.0'])) [1.0, 2.0] # Turn this: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) # Into this with a lambda function: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items))","title":"map()"},{"location":"notes/python/base_modules/Map/#map","text":"map() allows us to apply a function to a list of items # Simple Example >>> list(map(float, ['1.0', '2.0'])) [1.0, 2.0] # Turn this: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) # Into this with a lambda function: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items))","title":"Map"},{"location":"notes/python/base_modules/Pass%20By%20Reference/","text":"Pass By Reference Python will pass values between objects as references to their memory position. Addresses not values are passed...except for primitives.. An example: def f(x,l=[]): for i in range(x): l.append(i*i) print(l) f(2) # [0, 1] f(3,[3,2,1]) # [3, 2, 1, 0, 1, 4] Here python treats the variable l as a fresh list because a fresh list or more accurately, it's location in memory was passed. See python created [3, 2, 1] separately and passed it's location to the function. Watch what happends when we call the function again with no list. f(3) # [0, 1, 0, 1, 4] why not [0,1,4]?. When the function was first called we created the default list \"l=[]\", and according to the function's directions, by default this function refers to that list's memory position. Check out the following examples of pythons memory usage for this example: l_mem = [] l = l_mem # the first call for i in range(2): l.append(i*i) print(l) # [0, 1] l = [3,2,1] # the second call for i in range(3): l.append(i*i) print(l) # [3, 2, 1, 0, 1, 4] l = l_mem # the third call for i in range(3): l.append(i*i) print(l) # [0, 1, 0, 1, 4]","title":"P.B. Reference"},{"location":"notes/python/base_modules/Pass%20By%20Reference/#pass-by-reference","text":"Python will pass values between objects as references to their memory position. Addresses not values are passed...except for primitives.. An example: def f(x,l=[]): for i in range(x): l.append(i*i) print(l) f(2) # [0, 1] f(3,[3,2,1]) # [3, 2, 1, 0, 1, 4] Here python treats the variable l as a fresh list because a fresh list or more accurately, it's location in memory was passed. See python created [3, 2, 1] separately and passed it's location to the function. Watch what happends when we call the function again with no list. f(3) # [0, 1, 0, 1, 4] why not [0,1,4]?. When the function was first called we created the default list \"l=[]\", and according to the function's directions, by default this function refers to that list's memory position. Check out the following examples of pythons memory usage for this example: l_mem = [] l = l_mem # the first call for i in range(2): l.append(i*i) print(l) # [0, 1] l = [3,2,1] # the second call for i in range(3): l.append(i*i) print(l) # [3, 2, 1, 0, 1, 4] l = l_mem # the third call for i in range(3): l.append(i*i) print(l) # [0, 1, 0, 1, 4]","title":"Pass By Reference"},{"location":"notes/python/base_modules/Pdb/","text":"Pdb Useage - Docs You can use import pdb pdb.set_trace() or New: as of 3.7 breakpoint() breakpoint() to enter debug mode. Hidden gems Use ! to escape default pdb commands like n(ext), c(ont(inue)) and h(elp) >>> c = 1 >>> breakpoint() --Return-- > <stdin>(1)<module>()->None (Pdb) !c 1 (Pdb)","title":"Pdb"},{"location":"notes/python/base_modules/Pdb/#pdb","text":"","title":"Pdb"},{"location":"notes/python/base_modules/Pdb/#useage-docs","text":"You can use import pdb pdb.set_trace() or New: as of 3.7 breakpoint() breakpoint() to enter debug mode.","title":"Useage - Docs"},{"location":"notes/python/base_modules/Pdb/#hidden-gems","text":"Use ! to escape default pdb commands like n(ext), c(ont(inue)) and h(elp) >>> c = 1 >>> breakpoint() --Return-- > <stdin>(1)<module>()->None (Pdb) !c 1 (Pdb)","title":"Hidden gems"},{"location":"notes/python/base_modules/Sets/","text":"Sets Sets In Python A Set is an unordered collection data type that is iterable, mutable, and has no duplicate elements. Python\u2019s set class represents the mathematical notion of a set. The major advantage of using a set, as opposed to a list, is that it has a highly optimized method for checking whether a specific element is contained in the set. This is based on a data structure known as a hash table. Methods: add() add(x) Method: Adds the item x to set if it is not already present in the set. >>> s3.add(9) >>> s3 {9, 3} 2. union(s) Method: Returns a union of two set.Using the \u2018|\u2019 operator between 2 sets is the same as writing set1.union(set2) union() >>> s1.union(s2) {1, 2, 3, 4, 5} >>> s1 | s2 {1, 2, 3, 4, 5} 3. intersect(s) Method: Returns an intersection of two sets.The \u2018&\u2019 operator comes can also be used in this case. intersection() >>> s1.intersection(s2) {3} >>> s1 & s2 {3} difference difference(s) Method: Returns a set containing all the elements of invoking set but not of the second set. We can use \u2018-\u2018 operator here. >>> s1.difference(s2) {1, 2} >>> s1 - s2 {1, 2} 5. clear() Method: Empties the whole set. >>> s1.clear() >>> s2.clear() >>> s1 set() >>> s2 set() discard() discard() Method: The discard() method takes a single element x and removes it from the set (if present). >>> s1 {1, 2, 3} >>> s1.discard(1) >>> s1 {2, 3} issubset() issubset() Method: The issubset() method returns True if all elements of a set are present in another set (passed as an argument). If not, it returns False. >>> s1 {1, 2, 3} >>> s1.issubset({1,2,3,4}) True Operators for Sets Sets and frozen sets support the following operators: >>> k = 1 >>> key in s1 # containment check True >>> key not in s1 # non-containment check False >>> s1 == s2 # s1 is equivalent to s2 False >>> s1 != s2 # s1 is not equivalent to s2 True >>> s1 <= s2 # s1 is subset of s2 False >>> s1 < s2 # s1 is proper subset of s2 False >>> s1 >= s2 # s1 is superset of s2 False >>> s1 > s2 # s1 is proper superset of s2 False >>> s1 | s2 # the union of s1 and s2 {1, 2, 3, 4, 5} >>> s1 & s2 # the intersection of s1 and s2 {3} >>> s1 - s2 # the set of elements in s1 but not s2 {1, 2} >>> s1 ^ s2 # the set of elements in precisely one of s1 or s2 {1, 2, 4, 5}","title":"Sets"},{"location":"notes/python/base_modules/Sets/#sets","text":"Sets In Python A Set is an unordered collection data type that is iterable, mutable, and has no duplicate elements. Python\u2019s set class represents the mathematical notion of a set. The major advantage of using a set, as opposed to a list, is that it has a highly optimized method for checking whether a specific element is contained in the set. This is based on a data structure known as a hash table. Methods:","title":"Sets"},{"location":"notes/python/base_modules/Sets/#add","text":"add(x) Method: Adds the item x to set if it is not already present in the set. >>> s3.add(9) >>> s3 {9, 3} 2. union(s) Method: Returns a union of two set.Using the \u2018|\u2019 operator between 2 sets is the same as writing set1.union(set2)","title":"add()"},{"location":"notes/python/base_modules/Sets/#union","text":">>> s1.union(s2) {1, 2, 3, 4, 5} >>> s1 | s2 {1, 2, 3, 4, 5} 3. intersect(s) Method: Returns an intersection of two sets.The \u2018&\u2019 operator comes can also be used in this case.","title":"union()"},{"location":"notes/python/base_modules/Sets/#intersection","text":">>> s1.intersection(s2) {3} >>> s1 & s2 {3}","title":"intersection()"},{"location":"notes/python/base_modules/Sets/#difference","text":"difference(s) Method: Returns a set containing all the elements of invoking set but not of the second set. We can use \u2018-\u2018 operator here. >>> s1.difference(s2) {1, 2} >>> s1 - s2 {1, 2} 5. clear() Method: Empties the whole set. >>> s1.clear() >>> s2.clear() >>> s1 set() >>> s2 set()","title":"difference"},{"location":"notes/python/base_modules/Sets/#discard","text":"discard() Method: The discard() method takes a single element x and removes it from the set (if present). >>> s1 {1, 2, 3} >>> s1.discard(1) >>> s1 {2, 3}","title":"discard()"},{"location":"notes/python/base_modules/Sets/#issubset","text":"issubset() Method: The issubset() method returns True if all elements of a set are present in another set (passed as an argument). If not, it returns False. >>> s1 {1, 2, 3} >>> s1.issubset({1,2,3,4}) True Operators for Sets Sets and frozen sets support the following operators: >>> k = 1 >>> key in s1 # containment check True >>> key not in s1 # non-containment check False >>> s1 == s2 # s1 is equivalent to s2 False >>> s1 != s2 # s1 is not equivalent to s2 True >>> s1 <= s2 # s1 is subset of s2 False >>> s1 < s2 # s1 is proper subset of s2 False >>> s1 >= s2 # s1 is superset of s2 False >>> s1 > s2 # s1 is proper superset of s2 False >>> s1 | s2 # the union of s1 and s2 {1, 2, 3, 4, 5} >>> s1 & s2 # the intersection of s1 and s2 {3} >>> s1 - s2 # the set of elements in s1 but not s2 {1, 2} >>> s1 ^ s2 # the set of elements in precisely one of s1 or s2 {1, 2, 4, 5}","title":"issubset()"},{"location":"notes/python/base_modules/Sorted/","text":"Sorted Sort Arrays Want to sort something, sorted() is a great start. Syntax: sorted(iterable, key, reverse) Parameters: sorted takes three parameters from which two are optional. Iterable: sequence (list, tuple, string) or collection (dictionary, set, frozenset) or any other iterator that needs to be sorted Key(optional) : A function that would server as a key or a basis of sort comparison Reverse(optional) : If set true, then the iterable would be sorted in reverse (descending) order, by default it is set as false Note: A list also has sort() method which performs the same way as sorted(). Only difference being, sort() method doesn't return any value and changes the original list itself. x = [2,44,3,87,5] print(x) # [2, 44, 3, 87, 5] print(sorted(x)) #[2, 3, 5, 44, 87] print(sorted(x, reverse=True)) # [87, 44, 5, 3, 2] Custom Sorting using the key parameter: sorted() function has an optional parameter called \u2018key\u2019 which takes a function as its value. This key function transforms each element before sorting, it takes the value and returns 1 value which is then used within sort instead of the original value. For example, if we pass a list of strings in sorted(), it gets sorted alphabetically . But if we specify key = len, i.e. give len function as key, then the strings would be passed to len, and the value it returns, i.e. the length of strings will be sorted. Which means that the strings would be sorted based on their lengths instead # sort by your own criteria L = [\"cccc\", \"b\", \"dd\", \"aaa\"] print(\"Normal sort :\", sorted(L)) print(\"Sort with len :\", sorted(L, key = len))","title":"sorted()"},{"location":"notes/python/base_modules/Sorted/#sorted","text":"Sort Arrays Want to sort something, sorted() is a great start. Syntax: sorted(iterable, key, reverse) Parameters: sorted takes three parameters from which two are optional. Iterable: sequence (list, tuple, string) or collection (dictionary, set, frozenset) or any other iterator that needs to be sorted Key(optional) : A function that would server as a key or a basis of sort comparison Reverse(optional) : If set true, then the iterable would be sorted in reverse (descending) order, by default it is set as false Note: A list also has sort() method which performs the same way as sorted(). Only difference being, sort() method doesn't return any value and changes the original list itself. x = [2,44,3,87,5] print(x) # [2, 44, 3, 87, 5] print(sorted(x)) #[2, 3, 5, 44, 87] print(sorted(x, reverse=True)) # [87, 44, 5, 3, 2] Custom Sorting using the key parameter: sorted() function has an optional parameter called \u2018key\u2019 which takes a function as its value. This key function transforms each element before sorting, it takes the value and returns 1 value which is then used within sort instead of the original value. For example, if we pass a list of strings in sorted(), it gets sorted alphabetically . But if we specify key = len, i.e. give len function as key, then the strings would be passed to len, and the value it returns, i.e. the length of strings will be sorted. Which means that the strings would be sorted based on their lengths instead # sort by your own criteria L = [\"cccc\", \"b\", \"dd\", \"aaa\"] print(\"Normal sort :\", sorted(L)) print(\"Sort with len :\", sorted(L, key = len))","title":"Sorted"},{"location":"notes/python/base_modules/Zip/","text":"Zip Combine iterables and return them as tuple sets. The zip() function returns an iterator of tuples based on the iterable object. If no parameters are passed, zip() returns an empty iterator If a single iterable is passed, zip() returns an iterator of 1-tuples. Meaning, the number of elements in each tuple is 1 If multiple iterables are passed, ith tuple contains ith iterable values from all iterables. Suppose, two iterables are passed; one iterable containing 3 and other containing 5 elements, then the returned iterator will have 3 tuples >>> zip() <zip object at 0x102c9be48> # length 0 >>> list(zip()) [] # length 1 >>> list(zip([1,2,3])) [(1,), (2,), (3,)] # same length iterables >>> x, y = [1,2,3], [4,5,6] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # different length iterables >>> x, y = [1,2,3], [4,5,6,7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] iterables - can be built-in iterables (like: list, set, tuple, string, dict...), or user-defined iterables (object that has iter method). x, y, z = [1,2,3], [4,5,6], {'a':4,'b':5,'c':6} # dictionaries use keys by default results_default = set(zip(x,y,z)) results_2 = set(zip(x,y,z.keys())) results_3 = set(zip(x,y,z.values())) print(results_default) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_2) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_3) # {(1, 4, 4), (3, 6, 6), (2, 5, 5)} View the zip contents with a list or set or tuple data type: >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] >>> set(zip(x,y)) {(2, 5), (3, 6), (1, 4)} >>> tuple(zip(x,y)) ((1, 4), (2, 5), (3, 6)) Unzipping is possible too: > x [1, 2, 3] >>> y [4, 5, 6, 7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # Use * to unzip a zip object. You have to call zip() as a wrapper for *zip(). >>> list(zip(*zip(x,y))) [(1, 2, 3), (4, 5, 6)] >>> zip(*zip(x,y)) <zip object at 0x102ccb888> # The zip object is automatically unpacked into a nd b. >>> a,b = zip(*zip(x,y)) >>> a (1, 2, 3) >>> b (4, 5, 6) >>> Watch what happens when you try to view a zip object with a dictionary: >>> dict(zip([1,2],['a','b'])) {1: 'a', 2: 'b'} We can create dictionaries from individual unassociated key, values lists!","title":"zip()"},{"location":"notes/python/base_modules/Zip/#zip","text":"Combine iterables and return them as tuple sets. The zip() function returns an iterator of tuples based on the iterable object. If no parameters are passed, zip() returns an empty iterator If a single iterable is passed, zip() returns an iterator of 1-tuples. Meaning, the number of elements in each tuple is 1 If multiple iterables are passed, ith tuple contains ith iterable values from all iterables. Suppose, two iterables are passed; one iterable containing 3 and other containing 5 elements, then the returned iterator will have 3 tuples >>> zip() <zip object at 0x102c9be48> # length 0 >>> list(zip()) [] # length 1 >>> list(zip([1,2,3])) [(1,), (2,), (3,)] # same length iterables >>> x, y = [1,2,3], [4,5,6] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # different length iterables >>> x, y = [1,2,3], [4,5,6,7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] iterables - can be built-in iterables (like: list, set, tuple, string, dict...), or user-defined iterables (object that has iter method). x, y, z = [1,2,3], [4,5,6], {'a':4,'b':5,'c':6} # dictionaries use keys by default results_default = set(zip(x,y,z)) results_2 = set(zip(x,y,z.keys())) results_3 = set(zip(x,y,z.values())) print(results_default) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_2) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_3) # {(1, 4, 4), (3, 6, 6), (2, 5, 5)} View the zip contents with a list or set or tuple data type: >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] >>> set(zip(x,y)) {(2, 5), (3, 6), (1, 4)} >>> tuple(zip(x,y)) ((1, 4), (2, 5), (3, 6)) Unzipping is possible too: > x [1, 2, 3] >>> y [4, 5, 6, 7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # Use * to unzip a zip object. You have to call zip() as a wrapper for *zip(). >>> list(zip(*zip(x,y))) [(1, 2, 3), (4, 5, 6)] >>> zip(*zip(x,y)) <zip object at 0x102ccb888> # The zip object is automatically unpacked into a nd b. >>> a,b = zip(*zip(x,y)) >>> a (1, 2, 3) >>> b (4, 5, 6) >>> Watch what happens when you try to view a zip object with a dictionary: >>> dict(zip([1,2],['a','b'])) {1: 'a', 2: 'b'} We can create dictionaries from individual unassociated key, values lists!","title":"Zip"},{"location":"notes/python/base_modules/basic_servers/","text":"Some Quick Python Based Servers Update Anvileight has a better article covering more topics Courtesy of 2ality Below notes are from second link Python2 SimpleHTTPServer: a quick way to serve a directory Using SimpleHTTPServer SimpleHTTPServer is invoked like this (the parameter is optional): python -m SimpleHTTPServer <port> (On OS X, Python is pre-installed and this command works out of the box.) Let\u2019s look at an example of using SimpleHTTPServer: During the following Unix shell interaction, I first list the files in the current directory and then start SimpleHTTPServer to serve it. $ ls . foo.html $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... Afterwards, I can access the following URLs: http://localhost:8000/ lists the files in the current directory (namely, just foo.html). If there were a file index.html, it would be displayed, instead. http://localhost:8000/foo.html displays the file foo.html in the current directory. Customizing SimpleHTTPServer The following Unix shell script demonstrates how to customize SimpleHTTPServer so that it serves files that have a given file name extension with a given media type. One case where that matters is Firefox being picky about the media type of the webapp.manifest. #!/usr/bin/python import SimpleHTTPServer import SocketServer PORT = 8000 Handler = SimpleHTTPServer.SimpleHTTPRequestHandler Handler.extensions_map.update({ '.webapp': 'application/x-web-app-manifest+json', }); httpd = SocketServer.TCPServer((\"\", PORT), Handler) print \"Serving at port\", PORT httpd.serve_forever() Python3 In python 3 you can run: python3 -m http.server 8080 to create a server that will serve to the folder you are currntly in.","title":"servers"},{"location":"notes/python/base_modules/basic_servers/#some-quick-python-based-servers","text":"Update Anvileight has a better article covering more topics Courtesy of 2ality Below notes are from second link","title":"Some Quick Python Based Servers"},{"location":"notes/python/base_modules/basic_servers/#python2","text":"SimpleHTTPServer: a quick way to serve a directory","title":"Python2"},{"location":"notes/python/base_modules/basic_servers/#using-simplehttpserver","text":"SimpleHTTPServer is invoked like this (the parameter is optional): python -m SimpleHTTPServer <port> (On OS X, Python is pre-installed and this command works out of the box.) Let\u2019s look at an example of using SimpleHTTPServer: During the following Unix shell interaction, I first list the files in the current directory and then start SimpleHTTPServer to serve it. $ ls . foo.html $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... Afterwards, I can access the following URLs: http://localhost:8000/ lists the files in the current directory (namely, just foo.html). If there were a file index.html, it would be displayed, instead. http://localhost:8000/foo.html displays the file foo.html in the current directory.","title":"Using SimpleHTTPServer"},{"location":"notes/python/base_modules/basic_servers/#customizing-simplehttpserver","text":"The following Unix shell script demonstrates how to customize SimpleHTTPServer so that it serves files that have a given file name extension with a given media type. One case where that matters is Firefox being picky about the media type of the webapp.manifest. #!/usr/bin/python import SimpleHTTPServer import SocketServer PORT = 8000 Handler = SimpleHTTPServer.SimpleHTTPRequestHandler Handler.extensions_map.update({ '.webapp': 'application/x-web-app-manifest+json', }); httpd = SocketServer.TCPServer((\"\", PORT), Handler) print \"Serving at port\", PORT httpd.serve_forever()","title":"Customizing SimpleHTTPServer"},{"location":"notes/python/base_modules/basic_servers/#python3","text":"In python 3 you can run: python3 -m http.server 8080 to create a server that will serve to the folder you are currntly in.","title":"Python3"},{"location":"notes/python/base_modules/io/","text":"IO Reading \\ Writing with open('test.txt', \"w\") as file: file.write(\"Bam\") Source: w3schools f = open(\"demofile2.txt\", \"a\") f.write(\"Now the file has more content!\") f.close() #open and read the file after the appending: f = open(\"demofile2.txt\", \"r\") print(f.read()) f = open(\"demofile3.txt\", \"w\") f.write(\"Woops! I have deleted the content!\") f.close() #open and read the file after the appending: f = open(\"demofile3.txt\", \"r\") print(f.read()) Source: geeksforgeeks Create folder import os os.mkdir(\"./to_here\") Move File Source: w3schools import shutil with open('from_here.txt', \"w\") as file: file.write(\"from here\") src=\"./from_here.txt\" dest=\"./to_here/\" shutil.move(src, dest) Copy File import shutil src=\"from_here.txt\" dest=\"./to_here/\" shutil.copyfile(src, os.path.join(dest,src)) Delete A File import os os.remove(\"test.txt\") Delete A Folder import shutil, os #if empty os.rmdir(\"from_here\") #if not empty os.rmdir(\"from_here\") # Error if it has from_here.txt inside # So use shutil.rmtree(\"to_here\") > [shutil.rmtree](https://docs.python.org/3/library/shutil.html#shutil.rmtree)","title":"IO"},{"location":"notes/python/base_modules/io/#io","text":"","title":"IO"},{"location":"notes/python/base_modules/io/#reading-writing","text":"with open('test.txt', \"w\") as file: file.write(\"Bam\") Source: w3schools f = open(\"demofile2.txt\", \"a\") f.write(\"Now the file has more content!\") f.close() #open and read the file after the appending: f = open(\"demofile2.txt\", \"r\") print(f.read()) f = open(\"demofile3.txt\", \"w\") f.write(\"Woops! I have deleted the content!\") f.close() #open and read the file after the appending: f = open(\"demofile3.txt\", \"r\") print(f.read()) Source: geeksforgeeks","title":"Reading \\ Writing"},{"location":"notes/python/base_modules/io/#create-folder","text":"import os os.mkdir(\"./to_here\")","title":"Create folder"},{"location":"notes/python/base_modules/io/#move-file","text":"Source: w3schools import shutil with open('from_here.txt', \"w\") as file: file.write(\"from here\") src=\"./from_here.txt\" dest=\"./to_here/\" shutil.move(src, dest)","title":"Move File"},{"location":"notes/python/base_modules/io/#copy-file","text":"import shutil src=\"from_here.txt\" dest=\"./to_here/\" shutil.copyfile(src, os.path.join(dest,src))","title":"Copy File"},{"location":"notes/python/base_modules/io/#delete-a-file","text":"import os os.remove(\"test.txt\")","title":"Delete A File"},{"location":"notes/python/base_modules/io/#delete-a-folder","text":"import shutil, os #if empty os.rmdir(\"from_here\") #if not empty os.rmdir(\"from_here\") # Error if it has from_here.txt inside # So use shutil.rmtree(\"to_here\") > [shutil.rmtree](https://docs.python.org/3/library/shutil.html#shutil.rmtree)","title":"Delete A Folder"},{"location":"notes/python/base_modules/zip_files/","text":"Working With Zip Files Source: medium.com A file recognized by Python can store three types of data: Text (string) Binary (bytes) Raw data Python considers an object falling in the above three categories as a \u201cfile-like object.\u201d They are also called streams from where data can be read from or written. The data stored in streams are called buffers of that stream. The first two, i.e. Text and Binary streams, are buffered I/O streams, and raw type is unbuffered. In this article, we are only interested in buffered streams. Python\u2019s io package provides two classes: StringIO: for storing UTF-8 string buffers BytesIO: for storing binary buffers Text Streams from io import StringIO text_stream = StringIO() text_stream.write(\"I am a text buffer\") # prints 18 print(text_stream.getvalue()) # Prints 'I am a text buffer' to console text_stream.close() One should be aware that, in Python, a file-like object can be used in any I/O operation. The classic example is the print statement. Python\u2019s print statement takes a keyword argument called file that decides which stream to write the given message/objects. Its value is a \u201cfile-like object.\u201d See the definition of print, print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) Let us modify our program to change the destination of print to our custom text stream. from io import StringIO text_stream = StringIO() text_stream.write(\"I am a text buffer\") print(\" in python\", file=text_stream) # Doesn't print to console, instead writes to stream print(text_stream.getvalue()) # Prints 'I am a text buffer in python' to console text_stream.close() Text streams are only useful in operating on UTF-8 buffers(XML, JSON, CSV). There are many other cases where we have to represent binary buffers(ZIP, PDF, Custom Extensions) in program memory. Binary streams come to the rescue. Binary Streams A binary stream stores and operates on binary data(bytes). It has the same methods as StringIO like getvalue, read, write. from io import BytesIO, StringIO binary_stream = BytesIO(b'I am a byte string \\x01') print(binary_stream.getvalue()) # Prints b'I am a byte string \\x01' try: text_stream = StringIO(binary_stream.getvalue()) except TypeError: print('Sorry, text stream cannot store bytes') # Prints 'Sorry, text stream cannot store bytes' One can store any binary data coming from a PDF or ZIP file into a custom binary stream like the preceding one. Understanding the Python zipfile API A zip file is a binary file. Contents of a zip file are compressed using an algorithm and paths are preserved. All open-source zip tools do the same thing, understand the binary representation, process it. It is a no-brainer that one should use BytesIO while working with zip files. Python provides a package to work with zip archives called zipfile . The zipfile package has two main classes: ZipFile : to represent a zip archive in memory. ZipInfo : to represent a member of a zip file. A ZipFile is an exact representation of a zip archive. It means you can load a .zip file directly into that class object or dump a ZipFile object to a new archive. Every ZipFile has a list of members. Those members are ZipInfo objects. mkdir -p config/app mkdir -p config/docker touch config/app/app-config.json touch config/docker/docker-compose.yaml touch config/root-config.json zip -r config.zip config (venv) bbearce@pop-os:~$ python3 -m zipfile -l config.zip File Name Modified Size config/ 2020-12-11 15:32:24 0 config/root-config.json 2020-12-11 15:32:24 0 config/docker/ 2020-12-11 15:32:16 0 config/docker/docker-compose.yaml 2020-12-11 15:32:16 0 config/app/ 2020-12-11 15:32:06 0 config/app/app-config.json using python interactively: from zipfile import ZipFile with ZipFile('config.zip') as zip_archive: for item in zip_archive.filelist: print(item) print(f'\\nThere are {len(zip_archive.filelist)} ZipInfo objects present in archive') out: <ZipInfo filename='config/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/root-config.json' filemode='-rw-rw-r--' file_size=0> <ZipInfo filename='config/docker/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/docker/docker-compose.yaml' filemode='-rw-rw-r--' file_size=0> <ZipInfo filename='config/app/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/app/app-config.json' filemode='-rw-rw-r--' file_size=0> There are 6 ZipInfo objects present in archive Use case #1: Create zip archive with files We can create a zip file with the given name by opening a new ZipFile object with write mode \u2018w\u2019 or exclusive create mode \u2018x\u2019. Next, we can add files/paths to the zip archive. There are two approaches to do that: v1: Add a file as a file-like object from zipfile import ZipFile from io import BytesIO def create_zip_v1(): \"\"\" returns: zip archive \"\"\" archive = BytesIO() with ZipFile(archive, 'w') as zip_archive: # Create three files on zip archive with zip_archive.open('docker/docker-compose.yaml', 'w') as file1: file1.write(b'compose-file-content...') with zip_archive.open('app/app-config.json', 'w') as file2: file2.write(b'app-config-content...') with zip_archive.open('root-config.json', 'w') as file3: file3.write(b'root-config-content...') return archive archive = create_zip_v1() # Flush archive stream to a file on disk with open('config.zip', 'wb') as f: f.write(archive.getbuffer()) archive.close() v2: Add a file as a ZipInfo object This approach composes files as objects and gives more flexibility to add meta information on file. from zipfile import ZipFile, ZipInfo from io import BytesIO def create_zip_v2(): \"\"\" returns: zip archive \"\"\" archive = BytesIO() with ZipFile(archive, 'w') as zip_archive: # Create three files on zip archive file1 = ZipInfo('docker/docker-compose.yaml') zip_archive.writestr(file1, b'compose-file-content...') file2 = ZipInfo('app/app-config.json') zip_archive.writestr(file2, b'app-config-content...') file3 = ZipInfo('root-config.json') zip_archive.writestr(file3, b'root-config-content...') return archive archive = create_zip_v2() # Flush archive stream to a file on disk with open('config.zip', 'wb') as f: f.write(archive.getbuffer()) archive.close() Use case #2: Read a file from zip archive from zipfile import ZipFile docker_compose_config = None with ZipFile('config.zip') as zip_archive: docker_compose_config = zip_archive.read('docker/docker-compose.yaml') print(docker_compose_config) Use case #3: Update or Insert a file in zip arch This use case is the most tricky part of the zipping business in Python. On the first look, it might look simple. Let us try attempting a few solutions. Attempt #1: Don't use as it erases everything but update from zipfile import ZipFile with ZipFile('config.zip', 'w') as zip_archive: zip_archive.writestr( 'docker/docker-compose.yaml', # File to replace b'docker-compose-file-content-new' # Data ) If we run the preceding script, it replaces the file in archive config.zip, but, as zipfile is opened in write mode \u2018w,\u2019 the other files/paths in archive can vanish. They do, I checked. Don\u2019t use \u2018w\u2019 mode, when you update/replace a single file in a zip archive, or your data is gone for good. Attempt #2: Doesn't erase old data upon update so there is extra data from zipfile import ZipFile with ZipFile('config.zip', 'a') as zip_archive: zip_archive.writestr( 'docker/docker-compose.yaml', # File to replace b'docker-compose-file-content-new' # Data ) $ python -m zipfile -l config.zip File Name Modified Size docker/docker-compose.yaml 1980-01-01 00:00:00 23 app/app-config.json 1980-01-01 00:00:00 21 root-config.json 1980-01-01 00:00:00 22 docker/docker-compose.yaml 2020-12-11 16:41:14 31 So docker/docker-compose.yaml had appeared twice in the ZipInfo list but only once in extraction. For every update, the zip archive size grows and grows in the magnitude of the updated file size. If you ignore the Python warning, at some point the junk in the archive may occupy more space than actual files. The two attempts until now couldn\u2019t achieve an acceptable solution. Now comes third, which is a clean and elegant way. Attempt #3: Works! You have to loop through all objects and edit the one you want and if it's not the file you are looking for then copy old contents. Old contents are not untouched and in fact are deleted if you edit any part of the zip! from zipfile import ZipFile, ZipInfo from io import BytesIO def update_or_insert(path, data): \"\"\" Param: path -> file in archive Param: data -> data to be updated Returns a new zip file with the updated content for the given path \"\"\" new_zip = BytesIO() print('here?') with ZipFile('config.zip', 'r') as old_archive: with ZipFile(new_zip, 'w') as new_archive: for item in old_archive.filelist: # If you spot an existing file, create a new object if item.filename == path: zip_inf = ZipInfo(path) new_archive.writestr(zip_inf, data) else: # Copy other contents as it is new_archive.writestr(item, old_archive.read(item.filename)) return new_zip new_zip = update_or_insert( 'docker/docker-compose.yaml', b'docker-compose-file-content-new' ) # Flush new zip to disk with open('config.zip', 'wb') as f: f.write(new_zip.getbuffer()) new_zip.close() Use case #4: Remove an existing file from zip archive The cleanest way again is to copy contents from the old archive to the new archive and skip the ZipInfo objects that match the given path. from zipfile import ZipFile, ZipInfo from io import BytesIO def delete(path): \"\"\" Param: path -> file in archive Returns a new zip file after deleting path \"\"\" new_zip = BytesIO() with ZipFile('config.zip', 'r') as old_archive: with ZipFile(new_zip, 'w') as new_archive: for item in old_archive.filelist: if item.filename != path: # Copy everything other than path to be inserted new_archive.writestr(item, old_archive.read(item.filename)) return new_zip new_zip = delete('docker/docker-compose.yaml') # Flush new zip to disk with open('config.zip', 'wb') as f: f.write(new_zip.getbuffer()) new_zip.close()","title":"Zip Files"},{"location":"notes/python/base_modules/zip_files/#working-with-zip-files","text":"Source: medium.com A file recognized by Python can store three types of data: Text (string) Binary (bytes) Raw data Python considers an object falling in the above three categories as a \u201cfile-like object.\u201d They are also called streams from where data can be read from or written. The data stored in streams are called buffers of that stream. The first two, i.e. Text and Binary streams, are buffered I/O streams, and raw type is unbuffered. In this article, we are only interested in buffered streams. Python\u2019s io package provides two classes: StringIO: for storing UTF-8 string buffers BytesIO: for storing binary buffers","title":"Working With Zip Files"},{"location":"notes/python/base_modules/zip_files/#text-streams","text":"from io import StringIO text_stream = StringIO() text_stream.write(\"I am a text buffer\") # prints 18 print(text_stream.getvalue()) # Prints 'I am a text buffer' to console text_stream.close() One should be aware that, in Python, a file-like object can be used in any I/O operation. The classic example is the print statement. Python\u2019s print statement takes a keyword argument called file that decides which stream to write the given message/objects. Its value is a \u201cfile-like object.\u201d See the definition of print, print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) Let us modify our program to change the destination of print to our custom text stream. from io import StringIO text_stream = StringIO() text_stream.write(\"I am a text buffer\") print(\" in python\", file=text_stream) # Doesn't print to console, instead writes to stream print(text_stream.getvalue()) # Prints 'I am a text buffer in python' to console text_stream.close() Text streams are only useful in operating on UTF-8 buffers(XML, JSON, CSV). There are many other cases where we have to represent binary buffers(ZIP, PDF, Custom Extensions) in program memory. Binary streams come to the rescue.","title":"Text Streams"},{"location":"notes/python/base_modules/zip_files/#binary-streams","text":"A binary stream stores and operates on binary data(bytes). It has the same methods as StringIO like getvalue, read, write. from io import BytesIO, StringIO binary_stream = BytesIO(b'I am a byte string \\x01') print(binary_stream.getvalue()) # Prints b'I am a byte string \\x01' try: text_stream = StringIO(binary_stream.getvalue()) except TypeError: print('Sorry, text stream cannot store bytes') # Prints 'Sorry, text stream cannot store bytes' One can store any binary data coming from a PDF or ZIP file into a custom binary stream like the preceding one.","title":"Binary Streams"},{"location":"notes/python/base_modules/zip_files/#understanding-the-python-zipfile-api","text":"A zip file is a binary file. Contents of a zip file are compressed using an algorithm and paths are preserved. All open-source zip tools do the same thing, understand the binary representation, process it. It is a no-brainer that one should use BytesIO while working with zip files. Python provides a package to work with zip archives called zipfile . The zipfile package has two main classes: ZipFile : to represent a zip archive in memory. ZipInfo : to represent a member of a zip file. A ZipFile is an exact representation of a zip archive. It means you can load a .zip file directly into that class object or dump a ZipFile object to a new archive. Every ZipFile has a list of members. Those members are ZipInfo objects. mkdir -p config/app mkdir -p config/docker touch config/app/app-config.json touch config/docker/docker-compose.yaml touch config/root-config.json zip -r config.zip config (venv) bbearce@pop-os:~$ python3 -m zipfile -l config.zip File Name Modified Size config/ 2020-12-11 15:32:24 0 config/root-config.json 2020-12-11 15:32:24 0 config/docker/ 2020-12-11 15:32:16 0 config/docker/docker-compose.yaml 2020-12-11 15:32:16 0 config/app/ 2020-12-11 15:32:06 0 config/app/app-config.json using python interactively: from zipfile import ZipFile with ZipFile('config.zip') as zip_archive: for item in zip_archive.filelist: print(item) print(f'\\nThere are {len(zip_archive.filelist)} ZipInfo objects present in archive') out: <ZipInfo filename='config/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/root-config.json' filemode='-rw-rw-r--' file_size=0> <ZipInfo filename='config/docker/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/docker/docker-compose.yaml' filemode='-rw-rw-r--' file_size=0> <ZipInfo filename='config/app/' filemode='drwxrwxr-x' external_attr=0x10> <ZipInfo filename='config/app/app-config.json' filemode='-rw-rw-r--' file_size=0> There are 6 ZipInfo objects present in archive","title":"Understanding the Python zipfile API"},{"location":"notes/python/base_modules/zip_files/#use-case-1-create-zip-archive-with-files","text":"We can create a zip file with the given name by opening a new ZipFile object with write mode \u2018w\u2019 or exclusive create mode \u2018x\u2019. Next, we can add files/paths to the zip archive. There are two approaches to do that:","title":"Use case #1: Create zip archive with files"},{"location":"notes/python/base_modules/zip_files/#v1-add-a-file-as-a-file-like-object","text":"from zipfile import ZipFile from io import BytesIO def create_zip_v1(): \"\"\" returns: zip archive \"\"\" archive = BytesIO() with ZipFile(archive, 'w') as zip_archive: # Create three files on zip archive with zip_archive.open('docker/docker-compose.yaml', 'w') as file1: file1.write(b'compose-file-content...') with zip_archive.open('app/app-config.json', 'w') as file2: file2.write(b'app-config-content...') with zip_archive.open('root-config.json', 'w') as file3: file3.write(b'root-config-content...') return archive archive = create_zip_v1() # Flush archive stream to a file on disk with open('config.zip', 'wb') as f: f.write(archive.getbuffer()) archive.close()","title":"v1: Add a file as a file-like object"},{"location":"notes/python/base_modules/zip_files/#v2-add-a-file-as-a-zipinfo-object","text":"This approach composes files as objects and gives more flexibility to add meta information on file. from zipfile import ZipFile, ZipInfo from io import BytesIO def create_zip_v2(): \"\"\" returns: zip archive \"\"\" archive = BytesIO() with ZipFile(archive, 'w') as zip_archive: # Create three files on zip archive file1 = ZipInfo('docker/docker-compose.yaml') zip_archive.writestr(file1, b'compose-file-content...') file2 = ZipInfo('app/app-config.json') zip_archive.writestr(file2, b'app-config-content...') file3 = ZipInfo('root-config.json') zip_archive.writestr(file3, b'root-config-content...') return archive archive = create_zip_v2() # Flush archive stream to a file on disk with open('config.zip', 'wb') as f: f.write(archive.getbuffer()) archive.close()","title":"v2: Add a file as a ZipInfo object"},{"location":"notes/python/base_modules/zip_files/#use-case-2-read-a-file-from-zip-archive","text":"from zipfile import ZipFile docker_compose_config = None with ZipFile('config.zip') as zip_archive: docker_compose_config = zip_archive.read('docker/docker-compose.yaml') print(docker_compose_config)","title":"Use case #2: Read a file from zip archive"},{"location":"notes/python/base_modules/zip_files/#use-case-3-update-or-insert-a-file-in-zip-arch","text":"This use case is the most tricky part of the zipping business in Python. On the first look, it might look simple. Let us try attempting a few solutions.","title":"Use case #3: Update or Insert a file in zip arch"},{"location":"notes/python/base_modules/zip_files/#attempt-1-dont-use-as-it-erases-everything-but-update","text":"from zipfile import ZipFile with ZipFile('config.zip', 'w') as zip_archive: zip_archive.writestr( 'docker/docker-compose.yaml', # File to replace b'docker-compose-file-content-new' # Data ) If we run the preceding script, it replaces the file in archive config.zip, but, as zipfile is opened in write mode \u2018w,\u2019 the other files/paths in archive can vanish. They do, I checked. Don\u2019t use \u2018w\u2019 mode, when you update/replace a single file in a zip archive, or your data is gone for good.","title":"Attempt #1: Don't use as it erases everything but update"},{"location":"notes/python/base_modules/zip_files/#attempt-2-doesnt-erase-old-data-upon-update-so-there-is-extra-data","text":"from zipfile import ZipFile with ZipFile('config.zip', 'a') as zip_archive: zip_archive.writestr( 'docker/docker-compose.yaml', # File to replace b'docker-compose-file-content-new' # Data ) $ python -m zipfile -l config.zip File Name Modified Size docker/docker-compose.yaml 1980-01-01 00:00:00 23 app/app-config.json 1980-01-01 00:00:00 21 root-config.json 1980-01-01 00:00:00 22 docker/docker-compose.yaml 2020-12-11 16:41:14 31 So docker/docker-compose.yaml had appeared twice in the ZipInfo list but only once in extraction. For every update, the zip archive size grows and grows in the magnitude of the updated file size. If you ignore the Python warning, at some point the junk in the archive may occupy more space than actual files. The two attempts until now couldn\u2019t achieve an acceptable solution. Now comes third, which is a clean and elegant way.","title":"Attempt #2: Doesn't erase old data upon update so there is extra data"},{"location":"notes/python/base_modules/zip_files/#attempt-3-works","text":"You have to loop through all objects and edit the one you want and if it's not the file you are looking for then copy old contents. Old contents are not untouched and in fact are deleted if you edit any part of the zip! from zipfile import ZipFile, ZipInfo from io import BytesIO def update_or_insert(path, data): \"\"\" Param: path -> file in archive Param: data -> data to be updated Returns a new zip file with the updated content for the given path \"\"\" new_zip = BytesIO() print('here?') with ZipFile('config.zip', 'r') as old_archive: with ZipFile(new_zip, 'w') as new_archive: for item in old_archive.filelist: # If you spot an existing file, create a new object if item.filename == path: zip_inf = ZipInfo(path) new_archive.writestr(zip_inf, data) else: # Copy other contents as it is new_archive.writestr(item, old_archive.read(item.filename)) return new_zip new_zip = update_or_insert( 'docker/docker-compose.yaml', b'docker-compose-file-content-new' ) # Flush new zip to disk with open('config.zip', 'wb') as f: f.write(new_zip.getbuffer()) new_zip.close()","title":"Attempt #3: Works!"},{"location":"notes/python/base_modules/zip_files/#use-case-4-remove-an-existing-file-from-zip-archive","text":"The cleanest way again is to copy contents from the old archive to the new archive and skip the ZipInfo objects that match the given path. from zipfile import ZipFile, ZipInfo from io import BytesIO def delete(path): \"\"\" Param: path -> file in archive Returns a new zip file after deleting path \"\"\" new_zip = BytesIO() with ZipFile('config.zip', 'r') as old_archive: with ZipFile(new_zip, 'w') as new_archive: for item in old_archive.filelist: if item.filename != path: # Copy everything other than path to be inserted new_archive.writestr(item, old_archive.read(item.filename)) return new_zip new_zip = delete('docker/docker-compose.yaml') # Flush new zip to disk with open('config.zip', 'wb') as f: f.write(new_zip.getbuffer()) new_zip.close()","title":"Use case #4: Remove an existing file from zip archive"},{"location":"notes/python/base_modules/collections/Counter/","text":"Counter Counter is a dict subclass for counting hashable objects: >>> c = Counter() # a new, empty counter >>> c Counter() >>> c = Counter('gallahad') # a new counter from an iterable >>> c Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1}) # a new counter from a mapping >>> c = Counter({'red': 4, 'blue': 2}) # a new counter from a mapping >>> c Counter({'red': 4, 'blue': 2}) >>> c = Counter(cats=4, dogs=8) # a new counter from keyword args >>> c Counter({'dogs': 8, 'cats': 4}) >>> Counter([1,2,2,3,3,3,4,4,4,4]) Counter({4: 4, 3: 3, 2: 2, 1: 1}) Delete objects as shown below: # Delete records as shown below: >>> c = Counter(['eggs', 'ham']) >>> c Counter({'eggs': 1, 'ham': 1}) >>> del c['ham'] >>> c Counter({'eggs': 1}) Counter objects support three methods beyond those available for all dictionaries: elements() Return an iterator over elements repeating each as many times as its count. Elements are returned in arbitrary order. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> list(c.elements()) ['a', 'a', 'a', 'a', 'b', 'b'] most_common([n]) Return a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered arbitrarily: >>> Counter('abracadabra').most_common(3) [('a', 5), ('r', 2), ('b', 2)] subtract([iterable-or-mapping]) Elements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> d = Counter(a=1, b=2, c=3, d=4) >>> c.subtract(d) >>> c Counter({'a': 3, 'b': 0, 'c': -3, 'd': -6})","title":"Counter"},{"location":"notes/python/base_modules/collections/Counter/#counter","text":"Counter is a dict subclass for counting hashable objects: >>> c = Counter() # a new, empty counter >>> c Counter() >>> c = Counter('gallahad') # a new counter from an iterable >>> c Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1}) # a new counter from a mapping >>> c = Counter({'red': 4, 'blue': 2}) # a new counter from a mapping >>> c Counter({'red': 4, 'blue': 2}) >>> c = Counter(cats=4, dogs=8) # a new counter from keyword args >>> c Counter({'dogs': 8, 'cats': 4}) >>> Counter([1,2,2,3,3,3,4,4,4,4]) Counter({4: 4, 3: 3, 2: 2, 1: 1})","title":"Counter"},{"location":"notes/python/base_modules/collections/Counter/#delete-objects-as-shown-below","text":"# Delete records as shown below: >>> c = Counter(['eggs', 'ham']) >>> c Counter({'eggs': 1, 'ham': 1}) >>> del c['ham'] >>> c Counter({'eggs': 1}) Counter objects support three methods beyond those available for all dictionaries:","title":"Delete objects as shown below:"},{"location":"notes/python/base_modules/collections/Counter/#elements","text":"Return an iterator over elements repeating each as many times as its count. Elements are returned in arbitrary order. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> list(c.elements()) ['a', 'a', 'a', 'a', 'b', 'b']","title":"elements()"},{"location":"notes/python/base_modules/collections/Counter/#most_commonn","text":"Return a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered arbitrarily: >>> Counter('abracadabra').most_common(3) [('a', 5), ('r', 2), ('b', 2)]","title":"most_common([n])"},{"location":"notes/python/base_modules/collections/Counter/#subtractiterable-or-mapping","text":"Elements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> d = Counter(a=1, b=2, c=3, d=4) >>> c.subtract(d) >>> c Counter({'a': 3, 'b': 0, 'c': -3, 'd': -6})","title":"subtract([iterable-or-mapping])"},{"location":"notes/python/base_modules/collections/Default%20Dict/","text":"Default Dict Initialize dictionary values with a data type. Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. The first argument provides the initial value for the default_factory attribute; it defaults to None . All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. from collections import defaultdict # Try to append all numbers in a list to their descriptions s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d[k].append(v) # You can't because there is no default data type for the values of the keys being created. >>> Traceback (most recent call last): File \"\", line 1, in File \"\", line 6, in KeyError: 'yellow' # Use a default dict to accomplish this: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = defaultdict(list) for k, v in s: d[k].append(v) >>> d defaultdict(, {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}) # You can accomplish this with the base dictionary like this: >>> d = {} >>> d.setdefault('yellow', []) [] >>> d {'yellow': []} or >>> d = {} >>> d.setdefault('yellow', list) >>> d {'yellow': } # ...and our previous example: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d.setdefault(k, []).append(v) >>> d {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}","title":"default_dict()"},{"location":"notes/python/base_modules/collections/Default%20Dict/#default-dict","text":"Initialize dictionary values with a data type. Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. The first argument provides the initial value for the default_factory attribute; it defaults to None . All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. from collections import defaultdict # Try to append all numbers in a list to their descriptions s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d[k].append(v) # You can't because there is no default data type for the values of the keys being created. >>> Traceback (most recent call last): File \"\", line 1, in File \"\", line 6, in KeyError: 'yellow' # Use a default dict to accomplish this: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = defaultdict(list) for k, v in s: d[k].append(v) >>> d defaultdict(, {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}) # You can accomplish this with the base dictionary like this: >>> d = {} >>> d.setdefault('yellow', []) [] >>> d {'yellow': []} or >>> d = {} >>> d.setdefault('yellow', list) >>> d {'yellow': } # ...and our previous example: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d.setdefault(k, []).append(v) >>> d {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}","title":"Default Dict"},{"location":"notes/python/base_modules/strings/Strings/","text":"Strings Padding: center(), rjust(), ljust() The center() method returns a string which is padded with the specified character. The rjust()\\ljust() methods return a string which is padded on either side. >>> 'A'.center(1, '*') 'A' >>> 'A'.center(2, '*') 'A*' >>> 'A'.center(3, '*') '*A*' >>> 'A'.rjust(1, '*') 'A' >>> 'A'.rjust(2, '*') '*A' >>> 'A'.rjust(3, '*') '**A' >>> 'A'.ljust(1, '*') 'A' >>> 'A'.ljust(2, '*') 'A*' >>> 'A'.ljust(3, '*') 'A**' str.format() Different common uses of the str.format() function format(value[, format_spec]) First example: >>> \"{}\".format('value') 'value' Multiple values: >>> \"{} {}\".format('value1','value2') 'value1 value2' Reverse order: >>> \"{1} {0}\".format('value1','value2') 'value2 value1' Key Word Arguments >>> print(\"{kwarg} is {0} used for {1}\" .format(\"being\", \"string formatting\", kwarg =\"Some Key Word Argument\")) ... Some Key Word Argument is being used for string formatting We can also use types to further format values: Syntax: {field_name:conversion}.format(value) Note: field_name can be the index (0) or name of key word argument conversion values: s \u2013 strings d \u2013 decimal integers (base-10) f \u2013 floating point display c \u2013 character b \u2013 binary o \u2013 octal x \u2013 hexadecimal with lowercase letters after 9 X \u2013 hexadecimal with uppercase letters after 9 e \u2013 exponent notation Use like this: s - string >>> '{kwarg:s}'.format(kwarg='5') '5' >>> '{kwarg:s}'.format(kwarg=5) Traceback (most recent call last): File \"\", line 1, in ValueError: Unknown format code 's' for object of type 'int' Notice how we got an error for trying to convert an int to a string d - decimal integers >>> # This works print(\"Convert {0} to decimal integer: {0:d}.\".format(100)) # Notice the Error print(\"Convert {0} to decimal integer: {0:d}.\".format(100.0)) ... Convert 100 to decimal integer: 100. >>> ... Traceback (most recent call last): File \"\", line 2, in ValueError: Unknown format code 'd' for object of type 'float' f - floats >>> # Default decimal precision to 0.000001 print(\"Convert {0} to float: {0:f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123457. >>> # Change decimal precision to 0.01 print(\"Convert {0} to float: {0:.2f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.12. >>> # Change decimal precision to 0.1 print(\"Convert {0} to float: {0:.1f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.1. >>> # Change decimal precision to 0.000000001 print(\"Convert {0} to float: {0:.9f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123456789. c - single character (accepts integer or single character string). Use this link for a unicode character lookup table Brief sample... NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ... #000 >>> ' '.join(['{0:c}'.format(_) for _ in range(16)]) '\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f' #001 >>> ' '.join(['{0:c}'.format(_) for _ in range(16,32)]) '\\x10 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16 \\x17 \\x18 \\x19 \\x1a \\x1b \\x1c \\x1d \\x1e \\x1f' #002 >>> ' '.join(['{0:c}'.format(_) for _ in range(32,48)]) ' ! \" # $ % & \\' ( ) * + , - . /' #003 >>> ' '.join(['{0:c}'.format(_) for _ in range(48,64)]) '0 1 2 3 4 5 6 7 8 9 : ; < = > ?' #004 >>> ' '.join(['{0:c}'.format(_) for _ in range(64,80)]) '@ A B C D E F G H I J K L M N O' #005 >>> ' '.join(['{0:c}'.format(_) for _ in range(80,96)]) 'P Q R S T U V W X Y Z [ \\\\ ] ^ _' b - binary >>> print(\"{0:b}\".format(0)) 0 >>> print(\"{0:b}\".format(1)) 1 >>> print(\"{0:b}\".format(2)) 10 >>> print(\"{0:b}\".format(3)) 11 >>> print(\"{0:b}\".format(100)) 1100100 o - octal >>> print(\"{0:o}\".format(0)) 0 >>> print(\"{0:o}\".format(8)) 10 >>> print(\"{0:o}\".format(8*2)) 20 >>> print(\"{0:o}\".format(8*3)) 30 >>> print(\"{0:o}\".format(8*4)) 40 >>> print(\"{0:o}\".format(8 ** 1)) 10 >>> print(\"{0:o}\".format(8 ** 2)) 100 >>> print(\"{0:o}\".format(8 ** 3)) 1000 x\\H - hex\\HEX print(\"{0:x}\".format(0*16)) # 0 print(\"{0:x}\".format(1*16)) # 10 print(\"{0:x}\".format(2*16)) # 20 print(\"{0:x}\".format(16 ** 1)) # 10 print(\"{0:x}\".format(16 ** 2)) # 100 print(\"{0:x}\".format(16 ** 3)) # 1000 print(\"{0:x}\".format(10)) # a print(\"{0:X}\".format(10)) # A print(\"{0:x}\".format(15)) # f print(\"{0:X}\".format(15)) # F e - exponent print(\"{0:e}\".format(10)) # 1.000000e+01 print(\"{0:e}\".format(100)) # 1.000000e+02 print(\"{0:e}\".format(1000)) # 1.000000e+03 str.find() Use a substring and start-end range to identify where a substring resides in a larger string str.find(sub[, start[, end]]) s = 'ABCBCBCBCBC' # str.find() will return the index found or -1. # with only a start value the rest of the string is searched print(s.find('BC', 0)) # >>> 1 print(s.find('BC', 1)) # >>> 1 print(s.find('BC', 2)) # >>> 3 print(s.find('BC', 3)) # >>> 3 print(s.find('BC', 4)) # >>> 5 print(s.find('BC', 5)) # >>> 5 print(s.find('BC', 6)) # >>> 7 print(s.find('BC', 7)) # >>> 7 print(s.find('BC', 8)) # >>> 9 print(s.find('BC', 9)) # >>> 9 print(s.find('BC', 10)) # >>> -1 # with start and end, the substring must be within the range print(s.find('BC', 0,1)) # >>> -1 print(s.find('BC', 0,2)) # >>> -1 print(s.find('BC', 0,3)) # >>> 1","title":"Strings"},{"location":"notes/python/base_modules/strings/Strings/#strings","text":"","title":"Strings"},{"location":"notes/python/base_modules/strings/Strings/#padding-center-rjust-ljust","text":"The center() method returns a string which is padded with the specified character. The rjust()\\ljust() methods return a string which is padded on either side. >>> 'A'.center(1, '*') 'A' >>> 'A'.center(2, '*') 'A*' >>> 'A'.center(3, '*') '*A*' >>> 'A'.rjust(1, '*') 'A' >>> 'A'.rjust(2, '*') '*A' >>> 'A'.rjust(3, '*') '**A' >>> 'A'.ljust(1, '*') 'A' >>> 'A'.ljust(2, '*') 'A*' >>> 'A'.ljust(3, '*') 'A**'","title":"Padding: center(), rjust(), ljust()"},{"location":"notes/python/base_modules/strings/Strings/#strformat","text":"Different common uses of the str.format() function format(value[, format_spec]) First example: >>> \"{}\".format('value') 'value' Multiple values: >>> \"{} {}\".format('value1','value2') 'value1 value2' Reverse order: >>> \"{1} {0}\".format('value1','value2') 'value2 value1' Key Word Arguments >>> print(\"{kwarg} is {0} used for {1}\" .format(\"being\", \"string formatting\", kwarg =\"Some Key Word Argument\")) ... Some Key Word Argument is being used for string formatting We can also use types to further format values: Syntax: {field_name:conversion}.format(value) Note: field_name can be the index (0) or name of key word argument conversion values: s \u2013 strings d \u2013 decimal integers (base-10) f \u2013 floating point display c \u2013 character b \u2013 binary o \u2013 octal x \u2013 hexadecimal with lowercase letters after 9 X \u2013 hexadecimal with uppercase letters after 9 e \u2013 exponent notation Use like this: s - string >>> '{kwarg:s}'.format(kwarg='5') '5' >>> '{kwarg:s}'.format(kwarg=5) Traceback (most recent call last): File \"\", line 1, in ValueError: Unknown format code 's' for object of type 'int' Notice how we got an error for trying to convert an int to a string d - decimal integers >>> # This works print(\"Convert {0} to decimal integer: {0:d}.\".format(100)) # Notice the Error print(\"Convert {0} to decimal integer: {0:d}.\".format(100.0)) ... Convert 100 to decimal integer: 100. >>> ... Traceback (most recent call last): File \"\", line 2, in ValueError: Unknown format code 'd' for object of type 'float' f - floats >>> # Default decimal precision to 0.000001 print(\"Convert {0} to float: {0:f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123457. >>> # Change decimal precision to 0.01 print(\"Convert {0} to float: {0:.2f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.12. >>> # Change decimal precision to 0.1 print(\"Convert {0} to float: {0:.1f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.1. >>> # Change decimal precision to 0.000000001 print(\"Convert {0} to float: {0:.9f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123456789. c - single character (accepts integer or single character string). Use this link for a unicode character lookup table Brief sample... NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ... #000 >>> ' '.join(['{0:c}'.format(_) for _ in range(16)]) '\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f' #001 >>> ' '.join(['{0:c}'.format(_) for _ in range(16,32)]) '\\x10 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16 \\x17 \\x18 \\x19 \\x1a \\x1b \\x1c \\x1d \\x1e \\x1f' #002 >>> ' '.join(['{0:c}'.format(_) for _ in range(32,48)]) ' ! \" # $ % & \\' ( ) * + , - . /' #003 >>> ' '.join(['{0:c}'.format(_) for _ in range(48,64)]) '0 1 2 3 4 5 6 7 8 9 : ; < = > ?' #004 >>> ' '.join(['{0:c}'.format(_) for _ in range(64,80)]) '@ A B C D E F G H I J K L M N O' #005 >>> ' '.join(['{0:c}'.format(_) for _ in range(80,96)]) 'P Q R S T U V W X Y Z [ \\\\ ] ^ _' b - binary >>> print(\"{0:b}\".format(0)) 0 >>> print(\"{0:b}\".format(1)) 1 >>> print(\"{0:b}\".format(2)) 10 >>> print(\"{0:b}\".format(3)) 11 >>> print(\"{0:b}\".format(100)) 1100100 o - octal >>> print(\"{0:o}\".format(0)) 0 >>> print(\"{0:o}\".format(8)) 10 >>> print(\"{0:o}\".format(8*2)) 20 >>> print(\"{0:o}\".format(8*3)) 30 >>> print(\"{0:o}\".format(8*4)) 40 >>> print(\"{0:o}\".format(8 ** 1)) 10 >>> print(\"{0:o}\".format(8 ** 2)) 100 >>> print(\"{0:o}\".format(8 ** 3)) 1000 x\\H - hex\\HEX print(\"{0:x}\".format(0*16)) # 0 print(\"{0:x}\".format(1*16)) # 10 print(\"{0:x}\".format(2*16)) # 20 print(\"{0:x}\".format(16 ** 1)) # 10 print(\"{0:x}\".format(16 ** 2)) # 100 print(\"{0:x}\".format(16 ** 3)) # 1000 print(\"{0:x}\".format(10)) # a print(\"{0:X}\".format(10)) # A print(\"{0:x}\".format(15)) # f print(\"{0:X}\".format(15)) # F e - exponent print(\"{0:e}\".format(10)) # 1.000000e+01 print(\"{0:e}\".format(100)) # 1.000000e+02 print(\"{0:e}\".format(1000)) # 1.000000e+03","title":"str.format()"},{"location":"notes/python/base_modules/strings/Strings/#strfind","text":"Use a substring and start-end range to identify where a substring resides in a larger string str.find(sub[, start[, end]]) s = 'ABCBCBCBCBC' # str.find() will return the index found or -1. # with only a start value the rest of the string is searched print(s.find('BC', 0)) # >>> 1 print(s.find('BC', 1)) # >>> 1 print(s.find('BC', 2)) # >>> 3 print(s.find('BC', 3)) # >>> 3 print(s.find('BC', 4)) # >>> 5 print(s.find('BC', 5)) # >>> 5 print(s.find('BC', 6)) # >>> 7 print(s.find('BC', 7)) # >>> 7 print(s.find('BC', 8)) # >>> 9 print(s.find('BC', 9)) # >>> 9 print(s.find('BC', 10)) # >>> -1 # with start and end, the substring must be within the range print(s.find('BC', 0,1)) # >>> -1 print(s.find('BC', 0,2)) # >>> -1 print(s.find('BC', 0,3)) # >>> 1","title":"str.find()"},{"location":"notes/python/dicom/dicom_to_viewable_img_file/","text":"DICOM Conversion to Viewable Image Much simpler than previous example and rescales appropriately. # Code to save 2D grayscale image as bmp import os import numpy as np import torch import pydicom from PIL import Image import pdb #path_to_dicoms = './kmotion_data/Ikbeoms_List' path_to_dicoms = './kmotion_data/ikbeomSimilarSliceNew' #path_to_pngs = './JPG_test/' path_to_pngs = './ikbeomSimilarSliceNew_JPG_contrast/' dir_list = os.listdir(path_to_dicoms) def saveimage(adjust_for_contrast=False): for n, image_file_dicom in enumerate(dir_list): if image_file_dicom != \"images.csv\": print(image_file_dicom) image = os.path.join(path_to_dicoms,image_file_dicom) try: ds = pydicom.dcmread(image) except: print('yup') pdb.set_trace() visual = ds.pixel_array #pdb.set_trace() # cut off top 99th percentile pixel intensities if adjust_for_contrast == True: imgvec = visual.flatten() n_pixel = len(imgvec) max_intensity = np.sort(imgvec)[-int(n_pixel*(1-0.995))] # 99.5th percentile img_brighter = np.where(visual<max_intensity, visual, max_intensity) visual = img_brighter visual = (visual - visual.min()) / (visual.max() - visual.min()) filename_new = image_file_dicom[:image_file_dicom.find(\".dcm\")] + \".png\" im = Image.fromarray((visual * 255).astype(np.uint8)) im.save(os.path.join(path_to_pngs, filename_new)) saveimage(adjust_for_contrast=True) Source: medium This example doesn't scale the DICOM pixel array between values of 0-255 and therefore clips all data between 255 and whatever max value the pixel array has. Likewise for negative numbers, which are clipped by 0. import os import pydicom import cv2 # make it True if you want in PNG format PNG = False # Specify the .dcm folder path folder_path = \"./\" # Specify the output jpg/png folder path jpg_folder_path = \"JPG_of_PNG_folder\" if __name__ == \"__main__\": images_path = os.listdir(folder_path) for n, image in enumerate(images_path): ds = pydicom.dcmread(os.path.join(folder_path, image)) pixel_array_numpy = ds.pixel_array if PNG == False: image = image.replace('.dcm', '.jpg') else: image = image.replace('.dcm', '.png') cv2.imwrite(os.path.join(jpg_folder_path, image), pixel_array_numpy) if n % 50 == 0: print('{} image converted'.format(n))","title":"DICOM to img file"},{"location":"notes/python/dicom/dicom_to_viewable_img_file/#dicom-conversion-to-viewable-image","text":"Much simpler than previous example and rescales appropriately. # Code to save 2D grayscale image as bmp import os import numpy as np import torch import pydicom from PIL import Image import pdb #path_to_dicoms = './kmotion_data/Ikbeoms_List' path_to_dicoms = './kmotion_data/ikbeomSimilarSliceNew' #path_to_pngs = './JPG_test/' path_to_pngs = './ikbeomSimilarSliceNew_JPG_contrast/' dir_list = os.listdir(path_to_dicoms) def saveimage(adjust_for_contrast=False): for n, image_file_dicom in enumerate(dir_list): if image_file_dicom != \"images.csv\": print(image_file_dicom) image = os.path.join(path_to_dicoms,image_file_dicom) try: ds = pydicom.dcmread(image) except: print('yup') pdb.set_trace() visual = ds.pixel_array #pdb.set_trace() # cut off top 99th percentile pixel intensities if adjust_for_contrast == True: imgvec = visual.flatten() n_pixel = len(imgvec) max_intensity = np.sort(imgvec)[-int(n_pixel*(1-0.995))] # 99.5th percentile img_brighter = np.where(visual<max_intensity, visual, max_intensity) visual = img_brighter visual = (visual - visual.min()) / (visual.max() - visual.min()) filename_new = image_file_dicom[:image_file_dicom.find(\".dcm\")] + \".png\" im = Image.fromarray((visual * 255).astype(np.uint8)) im.save(os.path.join(path_to_pngs, filename_new)) saveimage(adjust_for_contrast=True) Source: medium This example doesn't scale the DICOM pixel array between values of 0-255 and therefore clips all data between 255 and whatever max value the pixel array has. Likewise for negative numbers, which are clipped by 0. import os import pydicom import cv2 # make it True if you want in PNG format PNG = False # Specify the .dcm folder path folder_path = \"./\" # Specify the output jpg/png folder path jpg_folder_path = \"JPG_of_PNG_folder\" if __name__ == \"__main__\": images_path = os.listdir(folder_path) for n, image in enumerate(images_path): ds = pydicom.dcmread(os.path.join(folder_path, image)) pixel_array_numpy = ds.pixel_array if PNG == False: image = image.replace('.dcm', '.jpg') else: image = image.replace('.dcm', '.png') cv2.imwrite(os.path.join(jpg_folder_path, image), pixel_array_numpy) if n % 50 == 0: print('{} image converted'.format(n))","title":"DICOM Conversion to Viewable Image"},{"location":"notes/python/flask/basics/","text":"Flask Setup Source Miguel Grinberg and Real Python This is a great login tutorial flask_cors_and_redesign.py Here all we need to do is import the app from app import app app/__init__.py from flask import Flask from flask_cors import CORS app = Flask(__name__) CORS(app) from app import routes app/routes.py from app import app from flask import render_template, jsonify import math @app.route('/', methods=['GET']) def index(): return render_template('index.html', variable='test_variable') @app.route('/api', methods=['GET']) def api(): return jsonify({'data' : 'Hello World!'}) @app.route('/map', methods=['GET']) def map(): return render_template('maps.html') # return jsonify({'data' : 'Hello World!'}) @app.route('/get_latlngs', methods=['GET']) def get_latlngs(): zip_latlng = { 'lat': 42.3601, 'lng': -71.0589, } post_latlngs = [ {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, ] num_of_posts = len(post_latlngs) rad_spacing = 2*math.pi/num_of_posts radius = 1 for i,v in enumerate(post_latlngs): post_latlngs[i]['lat'] = zip_latlng['lat'] + radius*math.cos(i*rad_spacing) post_latlngs[i]['lng'] = zip_latlng['lng'] + radius*math.sin(i*rad_spacing) return jsonify(post_latlngs) app/models.py # Not shown, but this layout is for directory structure mainly. Before running it, though, Flask needs to be told how to import it, by setting the FLASK_APP environment variable: (venv) $ export FLASK_APP=microblog.py If you are using Microsoft Windows, use set instead of export in the command above. Are you ready to be blown away? You can run your first web application, with the following command: bash (venv) $ flask run * Serving Flask app \"microblog\" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)","title":"Basics"},{"location":"notes/python/flask/basics/#flask-setup","text":"Source Miguel Grinberg and Real Python This is a great login tutorial flask_cors_and_redesign.py Here all we need to do is import the app from app import app app/__init__.py from flask import Flask from flask_cors import CORS app = Flask(__name__) CORS(app) from app import routes app/routes.py from app import app from flask import render_template, jsonify import math @app.route('/', methods=['GET']) def index(): return render_template('index.html', variable='test_variable') @app.route('/api', methods=['GET']) def api(): return jsonify({'data' : 'Hello World!'}) @app.route('/map', methods=['GET']) def map(): return render_template('maps.html') # return jsonify({'data' : 'Hello World!'}) @app.route('/get_latlngs', methods=['GET']) def get_latlngs(): zip_latlng = { 'lat': 42.3601, 'lng': -71.0589, } post_latlngs = [ {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, ] num_of_posts = len(post_latlngs) rad_spacing = 2*math.pi/num_of_posts radius = 1 for i,v in enumerate(post_latlngs): post_latlngs[i]['lat'] = zip_latlng['lat'] + radius*math.cos(i*rad_spacing) post_latlngs[i]['lng'] = zip_latlng['lng'] + radius*math.sin(i*rad_spacing) return jsonify(post_latlngs) app/models.py # Not shown, but this layout is for directory structure mainly. Before running it, though, Flask needs to be told how to import it, by setting the FLASK_APP environment variable: (venv) $ export FLASK_APP=microblog.py If you are using Microsoft Windows, use set instead of export in the command above. Are you ready to be blown away? You can run your first web application, with the following command: bash (venv) $ flask run * Serving Flask app \"microblog\" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)","title":"Flask Setup"},{"location":"notes/python/flask/links_to_checkout/","text":"Some Good Links For Additional Tutorials Getting Started with Flask. It has the new flask directory structure Continuing the tutorial: Part 1 - MySQL, Flask-SQLAlchemy, Flask-Migrate, Flask-Login, Blueprints, Flask-WTF Part 2 - doesn't exist yet","title":"Links to Check Out"},{"location":"notes/python/flask/links_to_checkout/#some-good-links-for-additional-tutorials","text":"Getting Started with Flask. It has the new flask directory structure Continuing the tutorial: Part 1 - MySQL, Flask-SQLAlchemy, Flask-Migrate, Flask-Login, Blueprints, Flask-WTF Part 2 - doesn't exist yet","title":"Some Good Links For Additional Tutorials"},{"location":"notes/python/jupyter/basics/","text":"Jupyter Notebook ...Reference Lab for essentially the same behavior Source: ljvmiranda921 This one is very simple but I know I got the below one to work too... Lab Run jupyter remotely from container Source: benjlindsay To run Jupyter Lab on a login node, you need to open 2 terminal windows. In the first window: Note to self, when doing this for containers, run this remembering we are mapping the container port 5678 to server port 5678 with -p 5678:5678 . Example command: docker run -it --ipc=host --name=ben_pytorch_tutorial -p 5678:5678 --runtime=nvidia -v /home/bb927/Documents/pytorch:/workspace pytorch/pytorch:latest bash Note, additional step to install jupyter could be required $ pip install jupyterlab . (You'll need a container with python in it) Then inside the container start jupyter: $ jupyter lab --ip 0.0.0.0 --port 5678 --no-browser --allow-root and see it running... [I 22:05:13.843 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [I 22:05:14.047 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [I 22:05:14.047 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 22:05:14.050 LabApp] Serving notebooks from local directory: /workspace [I 22:05:14.050 LabApp] The Jupyter Notebook is running at: [I 22:05:14.050 LabApp] http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:05:14.053 LabApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-45-open.html Or copy and paste one of these URLs: http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second terminal window run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname Finally copy the link from the jupyter lab step into your browser (http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX). Run jupyter remotely from server Log into server and start jupyter: $ ssh username@hostname $ jupyter lab --no-browser --port=5678 ... [I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 10:17:14.160 LabApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second window (local machine) run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname You should be able to go to http://localhost:5678 to see jupyter lab now.","title":"Run Remotely"},{"location":"notes/python/jupyter/basics/#jupyter","text":"","title":"Jupyter"},{"location":"notes/python/jupyter/basics/#notebook","text":"...Reference Lab for essentially the same behavior Source: ljvmiranda921 This one is very simple but I know I got the below one to work too...","title":"Notebook"},{"location":"notes/python/jupyter/basics/#lab","text":"","title":"Lab"},{"location":"notes/python/jupyter/basics/#run-jupyter-remotely-from-container","text":"Source: benjlindsay To run Jupyter Lab on a login node, you need to open 2 terminal windows. In the first window: Note to self, when doing this for containers, run this remembering we are mapping the container port 5678 to server port 5678 with -p 5678:5678 . Example command: docker run -it --ipc=host --name=ben_pytorch_tutorial -p 5678:5678 --runtime=nvidia -v /home/bb927/Documents/pytorch:/workspace pytorch/pytorch:latest bash Note, additional step to install jupyter could be required $ pip install jupyterlab . (You'll need a container with python in it) Then inside the container start jupyter: $ jupyter lab --ip 0.0.0.0 --port 5678 --no-browser --allow-root and see it running... [I 22:05:13.843 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [I 22:05:14.047 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [I 22:05:14.047 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 22:05:14.050 LabApp] Serving notebooks from local directory: /workspace [I 22:05:14.050 LabApp] The Jupyter Notebook is running at: [I 22:05:14.050 LabApp] http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:05:14.053 LabApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-45-open.html Or copy and paste one of these URLs: http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second terminal window run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname Finally copy the link from the jupyter lab step into your browser (http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX).","title":"Run jupyter remotely from container"},{"location":"notes/python/jupyter/basics/#run-jupyter-remotely-from-server","text":"Log into server and start jupyter: $ ssh username@hostname $ jupyter lab --no-browser --port=5678 ... [I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 10:17:14.160 LabApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second window (local machine) run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname You should be able to go to http://localhost:5678 to see jupyter lab now.","title":"Run jupyter remotely from server"},{"location":"notes/python/pandas/basics/","text":"Pandas Some Notes pandas.pydata... Display pd.set_option('display.max_columns', 50) # How many to show pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show pd.set_option('display.width', 1000) # How far across the screen pd.set_option('display.max_colwidth', 1) # Column width in px pd.set_option('display.max_colwidth', 40) # Column width in px pd.set_option('expand_frame_repr', True) # allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise Making DataFrames From Dictionaries # Method 1: each key is a column with array datatype for values >>> data = {'a':[1,4], 'b':[2,5], 'c':[3,6]} >>> pandas.DataFrame(data) a b c 0 1 2 3 1 4 5 6 # Use 'columns' to specify order >>> pandas.DataFrame(data, columns=['b','c','a']) b c a 0 2 3 1 1 5 6 4 # Method 2: Creating Dataframe from list of dicts >>> data = [{'a': 1, 'b': 2, 'c': 3}, {'a': 10, 'b': 20, 'c': 30}] >>> pandas.DataFrame(data) a b c 0 1 2 3 1 10 20 30 From lists >>> rows = [[1,2,3],[4,5,6]] >>> pandas.DataFrame(rows, columns=['a','b','c']) a b c 0 1 2 3 1 4 5 6 From excel import pandas as pd cars = pd.read_excel(r'C:\\Users\\Ron\\Desktop\\Cars.xlsx') df = pd.DataFrame(cars, columns = ['Brand', 'Price']) Apply Function x = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[0,0,0,0,0,1,0,1], 'C':['a' for i in range(8)]}) y = pd.DataFrame({'A':[1,1,1,1,1,2,2,2,2,2], 'B':[1,0,1,0,1,0,1,0,1,1], 'C':['a' for i in range(10)]}) z = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[2,3,6,9,1,1,6,7], 'C':['a' for i in range(8)]}) def some_agg_function(df_group): # It's important for the values to be sorted to make this easier df_group = df_group.sort_values('B') nrows = len(df_group) if nrows % 2 == 0: middle_index = nrows / 2 - 1; # for 0 based inded purposes; chooses 2 out of (1,[2],3,4) else: middle_index = math.floor(nrows / 2) # pdb.set_trace() return df_group.iloc[int(middle_index),:] print(x); print(x[['A','B']].groupby('A').apply(some_agg_function)) print(y);print(y[['A','B']].groupby('A').apply(some_agg_function)) print(z);print(z[['A','B']].groupby('A').apply(some_agg_function)) sqlite3 Intgration Source: dataquest Just sqlite3 library In order to work with a SQLite database from Python, we first have to connect to it. We can do that using the connect function, which returns a Connection object: import sqlite3 conn = sqlite3.connect(\"flights.db\") Once we have a Connection object, we can then create a Cursor object. Cursors allow us to execute SQL queries against a database: cur = conn.cursor() Once we have a Cursor object, we can use it to execute a query against the database with the aptly named execute method. The below code will fetch the first 5 rows from the airlines table: cur.execute(\"select * from airlines limit 5;\") You may have noticed that we didn\u2019t assign the result of the above query to a variable. This is because we need to run another command to actually fetch the results. We can use the fetchall method to fetch all of the results of a query: results = cur.fetchall() print(results) [(0, '1', 'Private flight', '\\\\N', '-', None, None, None, 'Y'), (1, '2', '135 Airways', '\\\\N', None, 'GNL', 'GENERAL', 'United States', 'N'), (2, '3', '1Time Airline', '\\\\N', '1T', 'RNX', 'NEXTIME', 'South Africa', 'Y'), (3, '4', '2 Sqn No 1 Elementary Flying Training School', '\\\\N', None, 'WYT', None, 'United Kingdom', 'N'), (4, '5', '213 Flight Unit', '\\\\N', None, 'TFU', None, 'Russia', 'N')] As you can see, the results are formatted as a list of tuples. Each tuple corresponds to a row in the database that we accessed. Dealing with data this way is fairly painful. We\u2019d need to manually add column heads, and manually parse the data. Luckily, the pandas library has an easier way, which we\u2019ll look at in the next section. Before we move on, it\u2019s good practice to close Connection objects and Cursor objects that are open. This prevents the SQLite database from being locked. When a SQLite database is locked, you may be unable to update the database, and may get errors. We can close the Cursor and the Connection like this: cur.close() conn.close() Add Pandas import pandas as pd import sqlite3 conn = sqlite3.connect(\"flights.db\") df = pd.read_sql_query(\"select * from airlines limit 5;\", conn) Inserting rows with Python To insert a row, we need to write an INSERT query. The below code will add a new row to the airlines table. We specify 9 values to insert, one for each column in airlines. This will add a new row to the table. cur = conn.cursor() cur.execute(\"insert into airlines values (6048, 19846, 'Test flight', '', '', null, null, null, 'Y')\") conn.commit() You want to avoid doing this! Inserting values with Python string formatting makes your program vulnerable to SQL Injection attacks. Luckily, sqlite3 has a straightforward way to inject dynamic values without relying on string formatting: cur = conn.cursor() values = ('Test Flight', 'Y') cur.execute(\"insert into airlines values (6049, 19847, ?, '', '', null, null, null, ?)\", values) conn.commit() Updating rows We can modify rows in a SQLite table using the execute method: cur = conn.cursor() values = ('USA', 19847) cur.execute(\"update airlines set country=? where id=?\", values) conn.commit() Deleting rows Finally, we can delete the rows in a database using the execute method: cur = conn.cursor()values = (19847, ) cur.execute(\"delete from airlines where id=?\", values)conn.commit() Creating tables with pandas The pandas package gives us a much faster way to create tables. We just have to create a DataFrame first, then export it to a SQL table. First, we\u2019ll create a DataFrame: from datetime import datetime df = pd.DataFrame( [[1, datetime(2016, 9, 29, 0, 0) , datetime(2016, 9, 29, 12, 0), 'T1', 1]], columns=[\"id\", \"departure\", \"arrival\", \"number\", \"route_id\"]) Then, we\u2019ll be able to call the to_sql method to convert df to a table in a database. We set the keep_exists parameter to replace to delete and replace any existing tables named daily_flights: df.to_sql(\"daily_flights\", conn, if_exists=\"replace\")","title":"Pandas"},{"location":"notes/python/pandas/basics/#pandas","text":"Some Notes pandas.pydata...","title":"Pandas"},{"location":"notes/python/pandas/basics/#display","text":"pd.set_option('display.max_columns', 50) # How many to show pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show pd.set_option('display.width', 1000) # How far across the screen pd.set_option('display.max_colwidth', 1) # Column width in px pd.set_option('display.max_colwidth', 40) # Column width in px pd.set_option('expand_frame_repr', True) # allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise","title":"Display"},{"location":"notes/python/pandas/basics/#making-dataframes","text":"","title":"Making DataFrames"},{"location":"notes/python/pandas/basics/#from-dictionaries","text":"# Method 1: each key is a column with array datatype for values >>> data = {'a':[1,4], 'b':[2,5], 'c':[3,6]} >>> pandas.DataFrame(data) a b c 0 1 2 3 1 4 5 6 # Use 'columns' to specify order >>> pandas.DataFrame(data, columns=['b','c','a']) b c a 0 2 3 1 1 5 6 4 # Method 2: Creating Dataframe from list of dicts >>> data = [{'a': 1, 'b': 2, 'c': 3}, {'a': 10, 'b': 20, 'c': 30}] >>> pandas.DataFrame(data) a b c 0 1 2 3 1 10 20 30","title":"From Dictionaries"},{"location":"notes/python/pandas/basics/#from-lists","text":">>> rows = [[1,2,3],[4,5,6]] >>> pandas.DataFrame(rows, columns=['a','b','c']) a b c 0 1 2 3 1 4 5 6","title":"From lists"},{"location":"notes/python/pandas/basics/#from-excel","text":"import pandas as pd cars = pd.read_excel(r'C:\\Users\\Ron\\Desktop\\Cars.xlsx') df = pd.DataFrame(cars, columns = ['Brand', 'Price'])","title":"From excel"},{"location":"notes/python/pandas/basics/#apply-function","text":"x = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[0,0,0,0,0,1,0,1], 'C':['a' for i in range(8)]}) y = pd.DataFrame({'A':[1,1,1,1,1,2,2,2,2,2], 'B':[1,0,1,0,1,0,1,0,1,1], 'C':['a' for i in range(10)]}) z = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[2,3,6,9,1,1,6,7], 'C':['a' for i in range(8)]}) def some_agg_function(df_group): # It's important for the values to be sorted to make this easier df_group = df_group.sort_values('B') nrows = len(df_group) if nrows % 2 == 0: middle_index = nrows / 2 - 1; # for 0 based inded purposes; chooses 2 out of (1,[2],3,4) else: middle_index = math.floor(nrows / 2) # pdb.set_trace() return df_group.iloc[int(middle_index),:] print(x); print(x[['A','B']].groupby('A').apply(some_agg_function)) print(y);print(y[['A','B']].groupby('A').apply(some_agg_function)) print(z);print(z[['A','B']].groupby('A').apply(some_agg_function))","title":"Apply Function"},{"location":"notes/python/pandas/basics/#sqlite3-intgration","text":"Source: dataquest","title":"sqlite3 Intgration"},{"location":"notes/python/pandas/basics/#just-sqlite3-library","text":"In order to work with a SQLite database from Python, we first have to connect to it. We can do that using the connect function, which returns a Connection object: import sqlite3 conn = sqlite3.connect(\"flights.db\") Once we have a Connection object, we can then create a Cursor object. Cursors allow us to execute SQL queries against a database: cur = conn.cursor() Once we have a Cursor object, we can use it to execute a query against the database with the aptly named execute method. The below code will fetch the first 5 rows from the airlines table: cur.execute(\"select * from airlines limit 5;\") You may have noticed that we didn\u2019t assign the result of the above query to a variable. This is because we need to run another command to actually fetch the results. We can use the fetchall method to fetch all of the results of a query: results = cur.fetchall() print(results) [(0, '1', 'Private flight', '\\\\N', '-', None, None, None, 'Y'), (1, '2', '135 Airways', '\\\\N', None, 'GNL', 'GENERAL', 'United States', 'N'), (2, '3', '1Time Airline', '\\\\N', '1T', 'RNX', 'NEXTIME', 'South Africa', 'Y'), (3, '4', '2 Sqn No 1 Elementary Flying Training School', '\\\\N', None, 'WYT', None, 'United Kingdom', 'N'), (4, '5', '213 Flight Unit', '\\\\N', None, 'TFU', None, 'Russia', 'N')] As you can see, the results are formatted as a list of tuples. Each tuple corresponds to a row in the database that we accessed. Dealing with data this way is fairly painful. We\u2019d need to manually add column heads, and manually parse the data. Luckily, the pandas library has an easier way, which we\u2019ll look at in the next section. Before we move on, it\u2019s good practice to close Connection objects and Cursor objects that are open. This prevents the SQLite database from being locked. When a SQLite database is locked, you may be unable to update the database, and may get errors. We can close the Cursor and the Connection like this: cur.close() conn.close()","title":"Just sqlite3 library"},{"location":"notes/python/pandas/basics/#add-pandas","text":"import pandas as pd import sqlite3 conn = sqlite3.connect(\"flights.db\") df = pd.read_sql_query(\"select * from airlines limit 5;\", conn)","title":"Add Pandas"},{"location":"notes/python/pandas/basics/#inserting-rows-with-python","text":"To insert a row, we need to write an INSERT query. The below code will add a new row to the airlines table. We specify 9 values to insert, one for each column in airlines. This will add a new row to the table. cur = conn.cursor() cur.execute(\"insert into airlines values (6048, 19846, 'Test flight', '', '', null, null, null, 'Y')\") conn.commit() You want to avoid doing this! Inserting values with Python string formatting makes your program vulnerable to SQL Injection attacks. Luckily, sqlite3 has a straightforward way to inject dynamic values without relying on string formatting: cur = conn.cursor() values = ('Test Flight', 'Y') cur.execute(\"insert into airlines values (6049, 19847, ?, '', '', null, null, null, ?)\", values) conn.commit()","title":"Inserting rows with Python"},{"location":"notes/python/pandas/basics/#updating-rows","text":"We can modify rows in a SQLite table using the execute method: cur = conn.cursor() values = ('USA', 19847) cur.execute(\"update airlines set country=? where id=?\", values) conn.commit()","title":"Updating rows"},{"location":"notes/python/pandas/basics/#deleting-rows","text":"Finally, we can delete the rows in a database using the execute method: cur = conn.cursor()values = (19847, ) cur.execute(\"delete from airlines where id=?\", values)conn.commit()","title":"Deleting rows"},{"location":"notes/python/pandas/basics/#creating-tables-with-pandas","text":"The pandas package gives us a much faster way to create tables. We just have to create a DataFrame first, then export it to a SQL table. First, we\u2019ll create a DataFrame: from datetime import datetime df = pd.DataFrame( [[1, datetime(2016, 9, 29, 0, 0) , datetime(2016, 9, 29, 12, 0), 'T1', 1]], columns=[\"id\", \"departure\", \"arrival\", \"number\", \"route_id\"]) Then, we\u2019ll be able to call the to_sql method to convert df to a table in a database. We set the keep_exists parameter to replace to delete and replace any existing tables named daily_flights: df.to_sql(\"daily_flights\", conn, if_exists=\"replace\")","title":"Creating tables with pandas"},{"location":"notes/python/pysaml2/saml2/","text":"Saml2 in Python Source readthedocs","title":"Saml2 in Python"},{"location":"notes/python/pysaml2/saml2/#saml2-in-python","text":"Source readthedocs","title":"Saml2 in Python"},{"location":"notes/python/rabbitmq/hello_world/","text":"Hello World Source: rabbitmq.com RabbitMQ libraries RabbitMQ speaks multiple protocols. This tutorial uses AMQP 0-9-1, which is an open, general-purpose protocol for messaging. There are a number of clients for RabbitMQ in many different languages. In this tutorial series we're going to use Pika 1.0.0, which is the Python client recommended by the RabbitMQ team. To install it you can use the pip package management tool: python -m pip install pika --upgrade Our first program send.py will send a single message to the queue. The first thing we need to do is to establish a connection with RabbitMQ server. send.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # Next, before sending we need to make sure the recipient queue exists channel.queue_declare(queue='hello') # In RabbitMQ a message can never be sent directly to the queue, it always needs to go through an exchange. # The queue name needs to be specified in the routing_key parameter: channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') print(\" [x] Sent 'Hello World!'\") # Before exiting the program we need to make sure the # network buffers were flushed and our message was actually delivered to RabbitMQ. # We can do it by gently closing the connection. connection.close() receive.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # The next step, just like before, is to make sure that the queue exists. # Creating a queue using queue_declare is idempotent \u2012 # - we can run the command as many times as we like, and only one will be created. channel.queue_declare(queue='hello') # Receiving messages from the queue is more complex. # It works by subscribing a callback function to a queue. # Whenever we receive a message, this callback function is called by # the Pika library. In our case this function will print on the screen the contents of the message. def callback(ch, method, properties, body): print(\" [x] Received %r\" % body) channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback) # And finally, we enter a never-ending loop that waits for data and runs callbacks whenever necessary. print(' [*] Waiting for messages. To exit press CTRL+C') channel.start_consuming()","title":"Hellow World"},{"location":"notes/python/rabbitmq/hello_world/#hello-world","text":"Source: rabbitmq.com RabbitMQ libraries RabbitMQ speaks multiple protocols. This tutorial uses AMQP 0-9-1, which is an open, general-purpose protocol for messaging. There are a number of clients for RabbitMQ in many different languages. In this tutorial series we're going to use Pika 1.0.0, which is the Python client recommended by the RabbitMQ team. To install it you can use the pip package management tool: python -m pip install pika --upgrade Our first program send.py will send a single message to the queue. The first thing we need to do is to establish a connection with RabbitMQ server. send.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # Next, before sending we need to make sure the recipient queue exists channel.queue_declare(queue='hello') # In RabbitMQ a message can never be sent directly to the queue, it always needs to go through an exchange. # The queue name needs to be specified in the routing_key parameter: channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') print(\" [x] Sent 'Hello World!'\") # Before exiting the program we need to make sure the # network buffers were flushed and our message was actually delivered to RabbitMQ. # We can do it by gently closing the connection. connection.close() receive.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # The next step, just like before, is to make sure that the queue exists. # Creating a queue using queue_declare is idempotent \u2012 # - we can run the command as many times as we like, and only one will be created. channel.queue_declare(queue='hello') # Receiving messages from the queue is more complex. # It works by subscribing a callback function to a queue. # Whenever we receive a message, this callback function is called by # the Pika library. In our case this function will print on the screen the contents of the message. def callback(ch, method, properties, body): print(\" [x] Received %r\" % body) channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback) # And finally, we enter a never-ending loop that waits for data and runs callbacks whenever necessary. print(' [*] Waiting for messages. To exit press CTRL+C') channel.start_consuming()","title":"Hello World"},{"location":"notes/python/seaborn/basics/","text":"Seaborn Source import seaborn as sns Bar Plot Source seaborn sns.set(style=\"whitegrid\") tips = sns.load_dataset(\"tips\") ax = sns.barplot(x=\"day\", y=\"total_bill\", data=tips) ax = sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips) ax = sns.barplot(x=\"day\", y=\"tip\", data=tips, capsize=.2) ax = sns.barplot(\"size\", y=\"total_bill\", data=tips, palette=\"Blues_d\") Heatmap ax = sns.heatmap(A_V_A) Axes and Plot Options Source drawingfromdata Rotate Axes import seaborn as sns planets = sns.load_dataset(\"planets\") g = sns.factorplot(\"year\", data=planets, aspect=1.5, kind=\"count\", color=\"b\") g.set_xticklabels(rotation=30)","title":"Seaborn"},{"location":"notes/python/seaborn/basics/#seaborn","text":"Source import seaborn as sns","title":"Seaborn"},{"location":"notes/python/seaborn/basics/#bar-plot","text":"Source seaborn sns.set(style=\"whitegrid\") tips = sns.load_dataset(\"tips\") ax = sns.barplot(x=\"day\", y=\"total_bill\", data=tips) ax = sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips) ax = sns.barplot(x=\"day\", y=\"tip\", data=tips, capsize=.2) ax = sns.barplot(\"size\", y=\"total_bill\", data=tips, palette=\"Blues_d\")","title":"Bar Plot"},{"location":"notes/python/seaborn/basics/#heatmap","text":"ax = sns.heatmap(A_V_A)","title":"Heatmap"},{"location":"notes/python/seaborn/basics/#axes-and-plot-options","text":"Source drawingfromdata","title":"Axes and Plot Options"},{"location":"notes/python/seaborn/basics/#rotate-axes","text":"import seaborn as sns planets = sns.load_dataset(\"planets\") g = sns.factorplot(\"year\", data=planets, aspect=1.5, kind=\"count\", color=\"b\") g.set_xticklabels(rotation=30)","title":"Rotate Axes"},{"location":"notes/python/tinydb/basics/","text":"TinyDB This is great small json db, an amazing python package, for light json databases. There is an equivalent nodejs one too. python - jsondb tinydb Installing TinyDB To install TinyDB from PyPI, run: $ pip install tinydb You can also grab the latest development version from GitHub . After downloading and unpacking it, you can install it using: $ python setup.py install Basic Usage Let\u2019s cover the basics before going more into detail. We\u2019ll start by setting up a TinyDB database: >>> from tinydb import TinyDB, Query >>> db = TinyDB('db.json') You now have a TinyDB database that stores its data in db.json. What about inserting some data? TinyDB expects the data to be Python dicts: >>> db.insert({'type': 'apple', 'count': 7}) >>> db.insert({'type': 'peach', 'count': 3}) Note The insert method returns the inserted document\u2019s ID. Read more about it here: Using Document IDs . Now you can get all documents stored in the database by running: >>> db.all() [{'count': 7, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] You can also iter over stored documents: >>> for item in db: >>> print(item) {'count': 7, 'type': 'apple'} {'count': 3, 'type': 'peach'} Of course you\u2019ll also want to search for specific documents. Let\u2019s try: >>> Fruit = Query() >>> db.search(Fruit.type == 'peach') [{'count': 3, 'type': 'peach'}] >>> db.search(Fruit.count > 5) [{'count': 7, 'type': 'apple'}] Next we\u2019ll update the count field of the apples: >>> db.update({'count': 10}, Fruit.type == 'apple') >>> db.all() [{'count': 10, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] In the same manner you can also remove documents: >>> db.remove(Fruit.count < 5) >>> db.all() [{'count': 10, 'type': 'apple'}] And of course you can throw away all data to start with an empty database: >>> db.purge() Advanced Usage Source readthedocs - advanced tinydb nodejs - jsondb simple-json-db A simple, no-frills, JSON storage engine for Node.JS with full test coverage. Installation (bash) npm install --save simple-json-db Usage (javascript) Instantiation const JSONdb = require('simple-json-db'); const db = new JSONdb('/path/to/your/database.json'); The prototype of the constructor is new JSONdb(string, [object]) , and you can supply the optional options object by giving it as second parameter: const db = new JSONdb('/path/to/your/database.json', { ... }); See the Options section for more details. Options Key Value Description Default Value asyncWrite Boolean Enables the storage to be asynchronously written to disk. false (synchronous behaviour) syncOnWrite Boolean Makes the storage be written to disk after every modification. true Set a key db.set('key', 'value'); The key parameter must be a string, value can be whatever kind of object can be stored in JSON format. JSON.stringify() is your friend! Get a key db.get('key'); The key parameter must be a string. If the key exists its value is returned, if it doesn't the function returns undefined . Check a key db.has('key'); The key parameter must be a string. If the key exhists true is returned, if it doesn't the function returns false . Delete a key db.delete('key'); The key parameter must be a string. The function returns as per the delete operator if the key exhists, else it returns undefined . Sync to disk db.sync(); This function writes the JSON storage object to the file path specified as the parameter of the main constructor. Consult the Options section for usage details; on default options there is no need to manually invoke it. Access JSON storage db.JSON(); This will return a copy of the internal JSON storage object, for you to tinker with and loop over. Replace JSON storage db.JSON({ data }); Giving a parameter to the JSON function makes the object passed replace the internal one. Be careful, as there's no way to recover the old object if the changes have already been written to disk .","title":"TinyDB"},{"location":"notes/python/tinydb/basics/#tinydb","text":"This is great small json db, an amazing python package, for light json databases. There is an equivalent nodejs one too.","title":"TinyDB"},{"location":"notes/python/tinydb/basics/#python-jsondb","text":"tinydb","title":"python - jsondb"},{"location":"notes/python/tinydb/basics/#installing-tinydb","text":"To install TinyDB from PyPI, run: $ pip install tinydb You can also grab the latest development version from GitHub . After downloading and unpacking it, you can install it using: $ python setup.py install","title":"Installing TinyDB"},{"location":"notes/python/tinydb/basics/#basic-usage","text":"Let\u2019s cover the basics before going more into detail. We\u2019ll start by setting up a TinyDB database: >>> from tinydb import TinyDB, Query >>> db = TinyDB('db.json') You now have a TinyDB database that stores its data in db.json. What about inserting some data? TinyDB expects the data to be Python dicts: >>> db.insert({'type': 'apple', 'count': 7}) >>> db.insert({'type': 'peach', 'count': 3}) Note The insert method returns the inserted document\u2019s ID. Read more about it here: Using Document IDs . Now you can get all documents stored in the database by running: >>> db.all() [{'count': 7, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] You can also iter over stored documents: >>> for item in db: >>> print(item) {'count': 7, 'type': 'apple'} {'count': 3, 'type': 'peach'} Of course you\u2019ll also want to search for specific documents. Let\u2019s try: >>> Fruit = Query() >>> db.search(Fruit.type == 'peach') [{'count': 3, 'type': 'peach'}] >>> db.search(Fruit.count > 5) [{'count': 7, 'type': 'apple'}] Next we\u2019ll update the count field of the apples: >>> db.update({'count': 10}, Fruit.type == 'apple') >>> db.all() [{'count': 10, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] In the same manner you can also remove documents: >>> db.remove(Fruit.count < 5) >>> db.all() [{'count': 10, 'type': 'apple'}] And of course you can throw away all data to start with an empty database: >>> db.purge()","title":"Basic Usage"},{"location":"notes/python/tinydb/basics/#advanced-usage","text":"Source readthedocs - advanced tinydb","title":"Advanced Usage"},{"location":"notes/python/tinydb/basics/#nodejs-jsondb","text":"simple-json-db A simple, no-frills, JSON storage engine for Node.JS with full test coverage.","title":"nodejs - jsondb"},{"location":"notes/python/tinydb/basics/#installation-bash","text":"npm install --save simple-json-db","title":"Installation (bash)"},{"location":"notes/python/tinydb/basics/#usage-javascript","text":"Instantiation const JSONdb = require('simple-json-db'); const db = new JSONdb('/path/to/your/database.json'); The prototype of the constructor is new JSONdb(string, [object]) , and you can supply the optional options object by giving it as second parameter: const db = new JSONdb('/path/to/your/database.json', { ... }); See the Options section for more details. Options Key Value Description Default Value asyncWrite Boolean Enables the storage to be asynchronously written to disk. false (synchronous behaviour) syncOnWrite Boolean Makes the storage be written to disk after every modification. true Set a key db.set('key', 'value'); The key parameter must be a string, value can be whatever kind of object can be stored in JSON format. JSON.stringify() is your friend! Get a key db.get('key'); The key parameter must be a string. If the key exists its value is returned, if it doesn't the function returns undefined . Check a key db.has('key'); The key parameter must be a string. If the key exhists true is returned, if it doesn't the function returns false . Delete a key db.delete('key'); The key parameter must be a string. The function returns as per the delete operator if the key exhists, else it returns undefined . Sync to disk db.sync(); This function writes the JSON storage object to the file path specified as the parameter of the main constructor. Consult the Options section for usage details; on default options there is no need to manually invoke it. Access JSON storage db.JSON(); This will return a copy of the internal JSON storage object, for you to tinker with and loop over. Replace JSON storage db.JSON({ data }); Giving a parameter to the JSON function makes the object passed replace the internal one. Be careful, as there's no way to recover the old object if the changes have already been written to disk .","title":"Usage (javascript)"},{"location":"notes/sass/sass/","text":"Sass Courtesy of sass-lang.com Install Instructions SCSS or SASS? There are two syntaxes available for Sass. The first, known as SCSS (Sassy CSS) and used throughout this reference, is an extension of the syntax of CSS. This means that every valid CSS stylesheet is a valid SCSS file with the same meaning. This syntax is enhanced with the Sass features described below. Files using this syntax have the .scss extension. The second and older syntax , known as the indented syntax (or sometimes just \u201c Sass \u201d), provides a more concise way of writing CSS. It uses indentation rather than brackets to indicate nesting of selectors, and newlines rather than semicolons to separate properties. Files using this syntax have the .sass extension. Preprocessing CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again. Once you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your website. The most direct way to make this happen is in your terminal. Once Sass is installed, you can compile your Sass to CSS using the sass command. You'll need to tell Sass which file to build from, and where to output CSS to. For example, running sass input.scss output.css from your terminal would take a single Sass file, input.scss , and compile that file to output.css . You can also watch individual files or directories with the --watch flag. The watch flag tells Sass to watch your source files for changes, and re-compile CSS each time you save your Sass. If you wanted to watch (instead of manually build) your input.scss file, you'd just add the watch flag to your command, like so: sass --watch input.scss output.css You can watch and output to directories by using folder paths as your input and output, and separating them with a colon. In this example: sass --watch app/sass:public/stylesheets Sass would watch all files in the app/sass folder for changes, and compile CSS to the public/stylesheets folder. More... There is more, but it is so well documented on sass-lang, that it would be redundant to put it here.","title":"SASS"},{"location":"notes/sass/sass/#sass","text":"Courtesy of sass-lang.com Install Instructions","title":"Sass"},{"location":"notes/sass/sass/#scss-or-sass","text":"There are two syntaxes available for Sass. The first, known as SCSS (Sassy CSS) and used throughout this reference, is an extension of the syntax of CSS. This means that every valid CSS stylesheet is a valid SCSS file with the same meaning. This syntax is enhanced with the Sass features described below. Files using this syntax have the .scss extension. The second and older syntax , known as the indented syntax (or sometimes just \u201c Sass \u201d), provides a more concise way of writing CSS. It uses indentation rather than brackets to indicate nesting of selectors, and newlines rather than semicolons to separate properties. Files using this syntax have the .sass extension.","title":"SCSS or SASS?"},{"location":"notes/sass/sass/#preprocessing","text":"CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again. Once you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your website. The most direct way to make this happen is in your terminal. Once Sass is installed, you can compile your Sass to CSS using the sass command. You'll need to tell Sass which file to build from, and where to output CSS to. For example, running sass input.scss output.css from your terminal would take a single Sass file, input.scss , and compile that file to output.css . You can also watch individual files or directories with the --watch flag. The watch flag tells Sass to watch your source files for changes, and re-compile CSS each time you save your Sass. If you wanted to watch (instead of manually build) your input.scss file, you'd just add the watch flag to your command, like so: sass --watch input.scss output.css You can watch and output to directories by using folder paths as your input and output, and separating them with a colon. In this example: sass --watch app/sass:public/stylesheets Sass would watch all files in the app/sass folder for changes, and compile CSS to the public/stylesheets folder.","title":"Preprocessing"},{"location":"notes/sass/sass/#more","text":"There is more, but it is so well documented on sass-lang, that it would be redundant to put it here.","title":"More..."},{"location":"notes/singularity/basics/","text":"Singularity Source Sylabs Inc. Why the name \u201cSingularity\u201d? A \u201cSingularity\u201d is an astrophysics phenomenon in which a single point becomes infinitely dense. This type of a singularity can thus contain massive quantities of universe within it and thus encapsulating an infinite amount of data within it. Build from Recipe Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image). The Recipe File Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default Build Images from Scratch Use this command to build the recipe file into a *.simg sudo singularity build ubuntu.simg Singularity.recipe Here the \"ubuntu.simg singularity.recipe\" part of the command is \"<image-name>.simg <recipe-file-name>.recipe\" You can build from multiple sources and even create interactive containers that you can add to. We will cover these soon, but here is a picture illustrating the many sources for singularity containers: Red arrows represent operations that must be carried out with root privileges. Also I believe looking at the documentation that --writable and --sandbox containers are now 1 type singularity v3+. The difference is that you can either be in a --writable or non writable sandbox. Interactive Builds Singularity v3.0 and above produces immutable images in the Singularity Image File (SIF) format. This ensures reproducible and verifiable images and allows for many extra benefits such as the ability to sign and verify your containers. However, during testing and debugging you may want an image format that is writable. This way you can shell into the image and install software and dependencies until you are satisfied that your container will fulfill your needs. For these scenarios, Singularity also supports the sandbox format (which is really just a directory). Let's say I start with ubuntu: singularity build ubuntu.simg docker://ubuntu:latest bbearce@pop-os:~/$ ls ubuntu.simg Now this is read only and can just be run or you can shell into it: bbearce@pop-os:~/$ singularity shell ubuntu.simg Singularity> ls ubuntu.simg Notice that since the user's home directory is automatically mounted it shows the *.simg with ls . Watch as writing anything triggers an error: Singularity> touch anything.txt Singularity> ls anything.txt ubuntu.simg How come that worked? Well the mounted home directories are considered outside the container and you can read and write to those. Let's try again in /mnt : Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system ahhh, there we go. So how do we write inside a singularity container? Enter the --sandbox First we have to create a sandbox out of the original container: bbearce@pop-os:~/$ singularity build --sandbox s_ubuntu ubuntu.simg INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: bbearce@pop-os:~/$ ls s_ubuntu/ bin etc lib64 opt sbin tmp boot home libx32 proc singularity usr dev lib media root srv var environment lib32 mnt run sys An entire ubuntu os! Cool so now let's change stuff in it: bbearce@pop-os:~/$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag. bbearce@pop-os:~/$ singularity shell --writable s_ubuntu Singularity> touch /mnt/anything.txt Singularity> ls /mnt anything.txt We did it! Ok now how do we make this change into a new container: bbearce@pop-os:~/$ singularity build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified bbearce@pop-os:~/$ ls s_ubuntu_modified ubuntu.simg s_ubuntu Now let's see if our change made it: bbearce@pop-os:~/$ singularity shell s_ubuntu_modified Singularity> ls /mnt anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable . Run Continue from the Build section above with files test.py and Singularity.recipe. Run the image with run : bbearce@pop-os:~/$ singularity run ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ or treat as an executable: bbearce@pop-os:~/$ ./ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ run also works with shub:// and docker:// URIs. This creates an ephemeral container that runs and then disappears. $ singularity run shub://GodloveD/lolcow Files on the host are reachable from within the container. $ echo \"Hello World\" > $HOME/hello-kitty.txt $ singularity exec vsoch-hello-world-master.simg cat $HOME/hello-kitty.txt Hello World This example works because hello-kitty.txt exists in the user\u2019s home directory. By default singularity bind mounts /home/$USER , /tmp , and $PWD into your container at runtime. Mounting You can specify additional directories to bind mount into your container with the --bind option. In this example, the /data directory on the host system is bind mounted to the /mnt directory inside the container. $ echo \"I am your father\" >/data/vader.sez $ ~/sing-dev/bin/singularity exec --bind /data:/mnt hello-world.simg cat /mnt/vader.sez I am your father Inspect Look at meta-data with inspect . Notice the %labels sections details shows up under \"CREATOR\": \"Ben\" bbearce@bryce:~/singularity$ singularity inspect ubuntu.simg { \"org.label-schema.usage.singularity.deffile.bootstrap\": \"docker\", \"org.label-schema.usage.singularity.deffile\": \"singularity.recipe\", \"org.label-schema.usage\": \"/.singularity.d/runscript.help\", \"org.label-schema.schema-version\": \"1.0\", \"CREATOR\": \"Ben\", \"org.label-schema.usage.singularity.deffile.from\": \"ubuntu:18.04\", \"org.label-schema.build-date\": \"Tue,_04_Feb_2020_15:36:00_-0500\", \"org.label-schema.usage.singularity.runscript.help\": \"/.singularity.d/runscript.help\", \"org.label-schema.usage.singularity.version\": \"2.5.2-dist\", \"org.label-schema.build-size\": \"135MB\" } Useful flags Here are some of the flags we will need routinely: -B or --bind Note mounts can be specified as read only ro or read write rw (default). This is equivalent to dockers -v flag. Use this to mount directories: dotter[0]:bb927$ singularity shell -B ./in:/mnt/in -B ./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in in.txt new_file.txt out.txt Singularity> ls /mnt/out out.txt Singularity> you can comma delimit multiple mounts: dotter[0]:bb927$ singularity shell -B ./in:/mnt/in,./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in in.txt new_file.txt out.txt Singularity> ls /mnt/out out.txt Singularity> If you do not specify where in the container you mount to, the local mount will be mounted to the same directory inside the container: dotter[0]:bb927$ singularity shell -B /opt s_ubuntu/ Singularity> ls /opt containerd google microsoft rh Singularity> exit exit dotter[0]:bb927$ ls /opt containerd google microsoft rh dotter[0]:bb927$ This is the same as -B /opt:/opt --fakeroot This is important for interactive development because since you are not \"root\" normally you can't install stuff with apt-get. With this you can. --net Gives you internet access. --nv Give ccontainer access to GPUs --no-home When shelling into your container image, Singularity allows you to mount your current working directory ( CWD ) without mounting your host $HOME directory with the --no-home flag.","title":"Basics"},{"location":"notes/singularity/basics/#singularity","text":"Source Sylabs Inc. Why the name \u201cSingularity\u201d? A \u201cSingularity\u201d is an astrophysics phenomenon in which a single point becomes infinitely dense. This type of a singularity can thus contain massive quantities of universe within it and thus encapsulating an infinite amount of data within it.","title":"Singularity"},{"location":"notes/singularity/basics/#build-from-recipe","text":"Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image).","title":"Build from Recipe"},{"location":"notes/singularity/basics/#the-recipe-file","text":"Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default","title":"The Recipe File"},{"location":"notes/singularity/basics/#build-images-from-scratch","text":"Use this command to build the recipe file into a *.simg sudo singularity build ubuntu.simg Singularity.recipe Here the \"ubuntu.simg singularity.recipe\" part of the command is \"<image-name>.simg <recipe-file-name>.recipe\" You can build from multiple sources and even create interactive containers that you can add to. We will cover these soon, but here is a picture illustrating the many sources for singularity containers: Red arrows represent operations that must be carried out with root privileges. Also I believe looking at the documentation that --writable and --sandbox containers are now 1 type singularity v3+. The difference is that you can either be in a --writable or non writable sandbox.","title":"Build Images from Scratch"},{"location":"notes/singularity/basics/#interactive-builds","text":"Singularity v3.0 and above produces immutable images in the Singularity Image File (SIF) format. This ensures reproducible and verifiable images and allows for many extra benefits such as the ability to sign and verify your containers. However, during testing and debugging you may want an image format that is writable. This way you can shell into the image and install software and dependencies until you are satisfied that your container will fulfill your needs. For these scenarios, Singularity also supports the sandbox format (which is really just a directory). Let's say I start with ubuntu: singularity build ubuntu.simg docker://ubuntu:latest bbearce@pop-os:~/$ ls ubuntu.simg Now this is read only and can just be run or you can shell into it: bbearce@pop-os:~/$ singularity shell ubuntu.simg Singularity> ls ubuntu.simg Notice that since the user's home directory is automatically mounted it shows the *.simg with ls . Watch as writing anything triggers an error: Singularity> touch anything.txt Singularity> ls anything.txt ubuntu.simg How come that worked? Well the mounted home directories are considered outside the container and you can read and write to those. Let's try again in /mnt : Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system ahhh, there we go. So how do we write inside a singularity container?","title":"Interactive Builds"},{"location":"notes/singularity/basics/#enter-the-sandbox","text":"First we have to create a sandbox out of the original container: bbearce@pop-os:~/$ singularity build --sandbox s_ubuntu ubuntu.simg INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: bbearce@pop-os:~/$ ls s_ubuntu/ bin etc lib64 opt sbin tmp boot home libx32 proc singularity usr dev lib media root srv var environment lib32 mnt run sys An entire ubuntu os! Cool so now let's change stuff in it: bbearce@pop-os:~/$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag. bbearce@pop-os:~/$ singularity shell --writable s_ubuntu Singularity> touch /mnt/anything.txt Singularity> ls /mnt anything.txt We did it! Ok now how do we make this change into a new container: bbearce@pop-os:~/$ singularity build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified bbearce@pop-os:~/$ ls s_ubuntu_modified ubuntu.simg s_ubuntu Now let's see if our change made it: bbearce@pop-os:~/$ singularity shell s_ubuntu_modified Singularity> ls /mnt anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable .","title":"Enter the --sandbox"},{"location":"notes/singularity/basics/#run","text":"Continue from the Build section above with files test.py and Singularity.recipe. Run the image with run : bbearce@pop-os:~/$ singularity run ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ or treat as an executable: bbearce@pop-os:~/$ ./ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ run also works with shub:// and docker:// URIs. This creates an ephemeral container that runs and then disappears. $ singularity run shub://GodloveD/lolcow Files on the host are reachable from within the container. $ echo \"Hello World\" > $HOME/hello-kitty.txt $ singularity exec vsoch-hello-world-master.simg cat $HOME/hello-kitty.txt Hello World This example works because hello-kitty.txt exists in the user\u2019s home directory. By default singularity bind mounts /home/$USER , /tmp , and $PWD into your container at runtime.","title":"Run"},{"location":"notes/singularity/basics/#mounting","text":"You can specify additional directories to bind mount into your container with the --bind option. In this example, the /data directory on the host system is bind mounted to the /mnt directory inside the container. $ echo \"I am your father\" >/data/vader.sez $ ~/sing-dev/bin/singularity exec --bind /data:/mnt hello-world.simg cat /mnt/vader.sez I am your father","title":"Mounting"},{"location":"notes/singularity/basics/#inspect","text":"Look at meta-data with inspect . Notice the %labels sections details shows up under \"CREATOR\": \"Ben\" bbearce@bryce:~/singularity$ singularity inspect ubuntu.simg { \"org.label-schema.usage.singularity.deffile.bootstrap\": \"docker\", \"org.label-schema.usage.singularity.deffile\": \"singularity.recipe\", \"org.label-schema.usage\": \"/.singularity.d/runscript.help\", \"org.label-schema.schema-version\": \"1.0\", \"CREATOR\": \"Ben\", \"org.label-schema.usage.singularity.deffile.from\": \"ubuntu:18.04\", \"org.label-schema.build-date\": \"Tue,_04_Feb_2020_15:36:00_-0500\", \"org.label-schema.usage.singularity.runscript.help\": \"/.singularity.d/runscript.help\", \"org.label-schema.usage.singularity.version\": \"2.5.2-dist\", \"org.label-schema.build-size\": \"135MB\" }","title":"Inspect"},{"location":"notes/singularity/basics/#useful-flags","text":"Here are some of the flags we will need routinely:","title":"Useful flags"},{"location":"notes/singularity/basics/#-b-or-bind","text":"Note mounts can be specified as read only ro or read write rw (default). This is equivalent to dockers -v flag. Use this to mount directories: dotter[0]:bb927$ singularity shell -B ./in:/mnt/in -B ./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in in.txt new_file.txt out.txt Singularity> ls /mnt/out out.txt Singularity> you can comma delimit multiple mounts: dotter[0]:bb927$ singularity shell -B ./in:/mnt/in,./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in in.txt new_file.txt out.txt Singularity> ls /mnt/out out.txt Singularity> If you do not specify where in the container you mount to, the local mount will be mounted to the same directory inside the container: dotter[0]:bb927$ singularity shell -B /opt s_ubuntu/ Singularity> ls /opt containerd google microsoft rh Singularity> exit exit dotter[0]:bb927$ ls /opt containerd google microsoft rh dotter[0]:bb927$ This is the same as -B /opt:/opt","title":"-B or --bind"},{"location":"notes/singularity/basics/#-fakeroot","text":"This is important for interactive development because since you are not \"root\" normally you can't install stuff with apt-get. With this you can.","title":"--fakeroot"},{"location":"notes/singularity/basics/#-net","text":"Gives you internet access.","title":"--net"},{"location":"notes/singularity/basics/#-nv","text":"Give ccontainer access to GPUs","title":"--nv"},{"location":"notes/singularity/basics/#-no-home","text":"When shelling into your container image, Singularity allows you to mount your current working directory ( CWD ) without mounting your host $HOME directory with the --no-home flag.","title":"--no-home"},{"location":"notes/singularity/cu-singularity/","text":"Apptainer login: https://ondemand-rmacc.rc.colorado.edu/ Build Singularity is now apptainer . apptainer build ubuntu_latest.sif docker://ubuntu:latest apptainer build pytorch_w_cuda.sif pytorch_based.recipe Recipe File \"pytorch_based.recipe\" Bootstrap: docker From: pytorch/pytorch:latest %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / requirements.txt / %post apt-get -qq -y update pip install -r requirements.txt %runscript python /test.py Shell apptainer shell <image name> Notice we can write/see to ~/singularity and not /mnt . bearceb@ophlapps03:~/singularity$ ls pytorch_based.recipe pytorch_based.sif README.md requirements.txt test.py ubuntu_latest.sif bearceb@ophlapps03:~/singularity$ apptainer shell ubuntu_latest.sif Apptainer> ls README.md pytorch_based.recipe pytorch_based.sif requirements.txt test.py ubuntu_latest.sif Apptainer> pwd /home/bearceb/singularity Apptainer> touch test.txt Apptainer> touch /mnt/test.txt touch: cannot touch '/mnt/test.txt': Read-only file system Only mounted directories have write permissions ( $PWD is one automatically mounted). Shelling provides read-only access to images. By default most user-owned files and directories are available to any container that is run on Alpine and Blanca This includes files in: /home/$USER /projects/$USER /scratch/summit/$USER /rc_scratch/$USER) Sandbox apptainer build --sandbox <sandbox> <*.sif> bbearce@pop-os:~/Desktop/singularity$ apptainer build --sandbox s_ubuntu ubuntu_latest.sif INFO: Starting build... INFO: Verifying bootstrap image ubuntu_latest.sif INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu bbearce@pop-os:~/Desktop/singularity$ ls pytorch_based.recipe s_ubuntu ubuntu_latest.sif README.md test.py requirements.txt test.txt You need the --writable flag to write in the sandbox. Natively it too is read-only. bbearce@pop-os:~/Desktop/singularity$ apptainer shell s_ubuntu Apptainer> touch /mnt/test.txt touch: cannot touch '/mnt/test.txt': Read-only file system Apptainer> exit bbearce@pop-os:~/Desktop/singularity$ apptainer shell --writable s_ubuntu WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Apptainer> touch /mnt/test.txt Apptainer> pip install <package> To build back into an image: bbearce@pop-os:~/Desktop/singularity$ apptainer build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified Run singularity run ubuntu_latest.sif Apptainer> For this image brings you to bash Alpine Picking up where Scott left off we want to run on Alpine. login: https://ondemand-rmacc.rc.colorado.edu/ [bbearce@xsede.org@login-ci1 ~]$ [bbearce@xsede.org@login-ci1 ~]$ curc-quota ------------------------------------------------------------------------ Used Avail Quota Limit ------------------------------------------------------------------------ /home/bbearce@xsede.org 0 2.0G 2.0G /projects/bbearce@xsede.org 0 250G 250G /scratch/alpine1 0G 9537G 9537G [bbearce@xsede.org@login-ci1 ~]$ cd /projects/bbearce@xsede.org [bbearce@xsede.org@login-ci1 bbearce@xsede.org]$ ls ondemand README.mdwn acompile # start session Set important environment variables This is to now fill up your quota: module load singularity/3.6.4 export TMP=/scratch/alpine/$USER export TEMP=/scratch/alpine/$USER export TMPDIR=/scratch/alpine/$USER export TEMPDIR=/scratch/alpine/$USER export ALPINE_SCRATCH=/gpfs/alpine1/scratch/$USER export APPTAINER_TMPDIR=$ALPINE_SCRATCH/singularity/tmp export APPTAINER_CACHEDIR=$ALPINE_SCRATCH/singularity/cache mkdir -pv $APPTAINER_CACHEDIR $APPTAINER_TMPDIR Create From Recipe [bbearce@xsede.org@c3cpu-a5-u34-3 singularity]$ cd /projects/bbearce@xsede.org/singularity [bbearce@xsede.org@c3cpu-c15-u11-4 singularity]$ apptainer build pytorch_w_cuda.sif pytorch_based.recipe INFO: Environment variable SINGULARITY_TMPDIR is set, but APPTAINER_TMPDIR is preferred INFO: User not listed in /etc/subuid, trying root-mapped namespace INFO: Environment variable SINGULARITY_TMPDIR is set, but APPTAINER_TMPDIR is preferred INFO: The %post section will be run under fakeroot INFO: Environment variable SINGULARITY_CACHEDIR is set, but APPTAINER_CACHEDIR is preferred INFO: Starting build... Getting image source signatures Copying blob 99803d4b97f3 done Copying blob 4f4fb700ef54 done Copying blob 4ade0a4bc5d5 done Copying blob 2185b402c9ca done Copying blob 035a286326d6 [=========>----------------------------] 782.8MiB / 2.9GiB Sylabs Container Registry List Remotes: apptainer remote list Cloud Services Endpoints ======================== NAME URI ACTIVE GLOBAL EXCLUSIVE INSECURE DefaultRemote cloud.apptainer.org YES YES NO NO Keyservers ========== URI GLOBAL INSECURE ORDER https://keys.openpgp.org YES NO 1* * Active cloud services keyserver Sylabs was the default but we have to add retroactvely. apptainer remote add --no-login SylabsCloud cloud.sycloud.io Switch remote: apptainer remote use SylabsCloud Login: apptainer remote login Generate an access token at https://cloud.sycloud.io/auth/tokens, and paste it here. Token entered will be hidden for security. Access Token: INFO: Access Token Verified! INFO: Token stored in /home/bearceb/.apptainer/remote.yaml Push ( -U means unsigned. Sign for more security): apptainer push -U ubuntu_latest.sif library://bbearce/ubuntu/dockerhub:pushed WARNING: Skipping container verification 28.4MiB / 28.4MiB [================================================================] 100 % 6.0 MiB/s 0s Pull: apptainer pull library://bbearce/ubuntu/dockerhub:latest bearceb@ophlapps03:~/singularity$ ls dockerhub_latest.sif","title":"Demo for CU"},{"location":"notes/singularity/cu-singularity/#apptainer","text":"login: https://ondemand-rmacc.rc.colorado.edu/","title":"Apptainer"},{"location":"notes/singularity/cu-singularity/#build","text":"Singularity is now apptainer . apptainer build ubuntu_latest.sif docker://ubuntu:latest apptainer build pytorch_w_cuda.sif pytorch_based.recipe","title":"Build"},{"location":"notes/singularity/cu-singularity/#recipe-file","text":"\"pytorch_based.recipe\" Bootstrap: docker From: pytorch/pytorch:latest %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / requirements.txt / %post apt-get -qq -y update pip install -r requirements.txt %runscript python /test.py","title":"Recipe File"},{"location":"notes/singularity/cu-singularity/#shell","text":"apptainer shell <image name> Notice we can write/see to ~/singularity and not /mnt . bearceb@ophlapps03:~/singularity$ ls pytorch_based.recipe pytorch_based.sif README.md requirements.txt test.py ubuntu_latest.sif bearceb@ophlapps03:~/singularity$ apptainer shell ubuntu_latest.sif Apptainer> ls README.md pytorch_based.recipe pytorch_based.sif requirements.txt test.py ubuntu_latest.sif Apptainer> pwd /home/bearceb/singularity Apptainer> touch test.txt Apptainer> touch /mnt/test.txt touch: cannot touch '/mnt/test.txt': Read-only file system Only mounted directories have write permissions ( $PWD is one automatically mounted). Shelling provides read-only access to images. By default most user-owned files and directories are available to any container that is run on Alpine and Blanca This includes files in: /home/$USER /projects/$USER /scratch/summit/$USER /rc_scratch/$USER)","title":"Shell"},{"location":"notes/singularity/cu-singularity/#sandbox","text":"apptainer build --sandbox <sandbox> <*.sif> bbearce@pop-os:~/Desktop/singularity$ apptainer build --sandbox s_ubuntu ubuntu_latest.sif INFO: Starting build... INFO: Verifying bootstrap image ubuntu_latest.sif INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu bbearce@pop-os:~/Desktop/singularity$ ls pytorch_based.recipe s_ubuntu ubuntu_latest.sif README.md test.py requirements.txt test.txt You need the --writable flag to write in the sandbox. Natively it too is read-only. bbearce@pop-os:~/Desktop/singularity$ apptainer shell s_ubuntu Apptainer> touch /mnt/test.txt touch: cannot touch '/mnt/test.txt': Read-only file system Apptainer> exit bbearce@pop-os:~/Desktop/singularity$ apptainer shell --writable s_ubuntu WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Apptainer> touch /mnt/test.txt Apptainer> pip install <package> To build back into an image: bbearce@pop-os:~/Desktop/singularity$ apptainer build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified","title":"Sandbox"},{"location":"notes/singularity/cu-singularity/#run","text":"singularity run ubuntu_latest.sif Apptainer> For this image brings you to bash","title":"Run"},{"location":"notes/singularity/cu-singularity/#alpine","text":"Picking up where Scott left off we want to run on Alpine. login: https://ondemand-rmacc.rc.colorado.edu/ [bbearce@xsede.org@login-ci1 ~]$ [bbearce@xsede.org@login-ci1 ~]$ curc-quota ------------------------------------------------------------------------ Used Avail Quota Limit ------------------------------------------------------------------------ /home/bbearce@xsede.org 0 2.0G 2.0G /projects/bbearce@xsede.org 0 250G 250G /scratch/alpine1 0G 9537G 9537G [bbearce@xsede.org@login-ci1 ~]$ cd /projects/bbearce@xsede.org [bbearce@xsede.org@login-ci1 bbearce@xsede.org]$ ls ondemand README.mdwn acompile # start session","title":"Alpine"},{"location":"notes/singularity/cu-singularity/#set-important-environment-variables","text":"This is to now fill up your quota: module load singularity/3.6.4 export TMP=/scratch/alpine/$USER export TEMP=/scratch/alpine/$USER export TMPDIR=/scratch/alpine/$USER export TEMPDIR=/scratch/alpine/$USER export ALPINE_SCRATCH=/gpfs/alpine1/scratch/$USER export APPTAINER_TMPDIR=$ALPINE_SCRATCH/singularity/tmp export APPTAINER_CACHEDIR=$ALPINE_SCRATCH/singularity/cache mkdir -pv $APPTAINER_CACHEDIR $APPTAINER_TMPDIR","title":"Set important environment variables"},{"location":"notes/singularity/cu-singularity/#create-from-recipe","text":"[bbearce@xsede.org@c3cpu-a5-u34-3 singularity]$ cd /projects/bbearce@xsede.org/singularity [bbearce@xsede.org@c3cpu-c15-u11-4 singularity]$ apptainer build pytorch_w_cuda.sif pytorch_based.recipe INFO: Environment variable SINGULARITY_TMPDIR is set, but APPTAINER_TMPDIR is preferred INFO: User not listed in /etc/subuid, trying root-mapped namespace INFO: Environment variable SINGULARITY_TMPDIR is set, but APPTAINER_TMPDIR is preferred INFO: The %post section will be run under fakeroot INFO: Environment variable SINGULARITY_CACHEDIR is set, but APPTAINER_CACHEDIR is preferred INFO: Starting build... Getting image source signatures Copying blob 99803d4b97f3 done Copying blob 4f4fb700ef54 done Copying blob 4ade0a4bc5d5 done Copying blob 2185b402c9ca done Copying blob 035a286326d6 [=========>----------------------------] 782.8MiB / 2.9GiB","title":"Create From Recipe"},{"location":"notes/singularity/cu-singularity/#sylabs-container-registry","text":"List Remotes: apptainer remote list Cloud Services Endpoints ======================== NAME URI ACTIVE GLOBAL EXCLUSIVE INSECURE DefaultRemote cloud.apptainer.org YES YES NO NO Keyservers ========== URI GLOBAL INSECURE ORDER https://keys.openpgp.org YES NO 1* * Active cloud services keyserver Sylabs was the default but we have to add retroactvely. apptainer remote add --no-login SylabsCloud cloud.sycloud.io Switch remote: apptainer remote use SylabsCloud Login: apptainer remote login Generate an access token at https://cloud.sycloud.io/auth/tokens, and paste it here. Token entered will be hidden for security. Access Token: INFO: Access Token Verified! INFO: Token stored in /home/bearceb/.apptainer/remote.yaml Push ( -U means unsigned. Sign for more security): apptainer push -U ubuntu_latest.sif library://bbearce/ubuntu/dockerhub:pushed WARNING: Skipping container verification 28.4MiB / 28.4MiB [================================================================] 100 % 6.0 MiB/s 0s Pull: apptainer pull library://bbearce/ubuntu/dockerhub:latest bearceb@ophlapps03:~/singularity$ ls dockerhub_latest.sif","title":"Sylabs Container Registry"},{"location":"notes/singularity/demo_for_qtim/","text":"Demo Martinos Cluster Storage We have storage (20TB) on Martinos accessible at: /cluster/qtim (convenient path) /vault/cluster/qtim (real path) Here is what is there: (base) carlsbad[0]:qtim$ cd /cluster/qtim (base) carlsbad[0]:qtim$ ls -la total 2 drwxrws--- 4 root qtim 5 Nov 16 14:59 . drwxr-xr-x 4 root root 0 Nov 16 14:57 .. -rw-r--r-- 1 root root 0 Nov 12 23:24 ID-qtim drwxrwsr-x 4 bb927 qtim 4 Nov 16 14:59 machine_backups drwxrwsr-x 5 bb927 qtim 5 Nov 16 14:58 users (base) carlsbad[0]:qtim$ ls machine_backups/ bryce getafix (base) carlsbad[0]:qtim$ ls users bb927 kalpathy yifen For future use and organization I think we should keep users in a users folder. Other high level folders at the users or machine_backups level can be created as needed, but I'm proposing some best practices as of now. For general use we need to use this space when using machines managed by help desk (carlsbad, dotter, the new martinos GPU cluster). We have user accounts that are synced between all martinos machines and they all live on a drive managed by help desk. In order to keep space concerns down with 100s of users, they limit our space allowance to the point where it is unusable for more than configuratoin or tiny tests. It is because of this that the most important step is to symlink some important directories to change where the data stored for various programs (docker, singularity, python) will show up ultimately (not in ~/ ). Setup simlinks: Source Martinos Docs The two places that need symlinking are ~/.singularity and ~/.share/local/containers . Here is an example: (make sure ~/.singularity does not already exist first) ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /cluster/qtim/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /cluster/qtim/users/bb927/singularity ... I do have some and I will delete them to demonstrate the full workflow: (base) carlsbad[0]:bb927$ cd /cluster/qtim/users/bb927 (base) carlsbad[0]:bb927$ ls (base) carlsbad[0]:bb927$ mkdir share singularity (base) carlsbad[0]:bb927$ ls share singularity (base) carlsbad[0]:bb927$ ln -s /cluster/qtim/users/bb927/share ~/.share (base) carlsbad[0]:bb927$ ln -s /cluster/qtim/users/bb927/singularity ~/.singularity (base) carlsbad[0]:bb927$ mkdir ~/.singularity/tmp (base) carlsbad[0]:bb927$ export SINGULARITY_TMPDIR=~/.singularity/tmp (base) carlsbad[0]:bb927$ mkdir ~/.singularity/cache (base) carlsbad[0]:bb927$ export SINGULARITY_CACHEDIR=~/.singularity/cache Singularity Must know [1] Images are read-only by default...more on that later. [2] You are not root inside your image, but rather your username on linux (Ex: bb927 ). [3] Remember Singularity wants to auto --bind mount these 3 directories in the background whether you say to or not: /home/$USER /tmp $PWD [4] The concept of differentiating between image and container is no more. They are one in the same...more on that later. [5] For editable images (sandboxes) you must create your mount points ahead of time in the sandbox. Build from Recipe Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image). The Recipe File Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default Build Images from Scratch Use this command to build the recipe file into a *.simg (base) carlsbad[0]:bb927$ singularity build ubuntu.simg Singularity.recipe FATAL: You must be the root user, however you can use --remote or --fakeroot to build from a Singularity recipe file ...as I said we don't have permissions. I will need this demo for later so I will show how to get it running in the long run. Hint you can always build somewhere else first... Build from Docker Let's say I start with ubuntu from docker (btw you can grab images from shub:// as well): singularity build ubuntu.simg docker://ubuntu:latest Now let's see our image (unlike docker we won't use docker ps or docker images , rather all containers\\images live where you execute commands): (base) carlsbad[0]:bb927$ ls share singularity ubuntu.simg Now this is read only and can just be run or you can shell into it: (base) carlsbad[0]:bb927$ singularity shell ubuntu.simg Singularity> ls share singularity ubuntu.simg Notice that since the user's $PWD directory is automatically mounted it shows the *.simg with ls , so don't be alarmed if your singularity directory looks just like your current directory! However we know we are in the singularity ;) (couldn't resist) because / is not the same outside the container. Singularity> ls / autofs boot dev etc homes lib32 libx32 mnt proc run singularity sys usr bin cluster environment home lib lib64 media opt root sbin srv tmp var Singularity> exit exit (base) carlsbad[0]:bb927$ ls / autofs boot cmas dev etc homes lib64 lost+found misc nas opt proc run scratch srv tmp var bin cluster data eris home lib local_mount media mnt net panfs root sbin space sys usr vast Types of Containers Default You may be wondering while shelling into the container, was that the image or did we shell into an instance container?!? Neither. Singularity has one thing and is always an image, but may have writable copies created for development work. You could think of these writable versions as containers if you'd like, but that is the main paradigm shift when using this techinology. Let's learn more with examples: Singularity> ls share singularity ubuntu.simg Singularity> touch test_file.txt Singularity> touch /test_file.txt touch: cannot touch '/test_file.txt': Read-only file system Notice here that you can add files to the mount point $PWD , but not / . These images are read-only by default with the exception of mount points. So how do we make editable images? Sandbox --sandbox So we can't edit ubuntu.simg by default, but we can build an editable version with singularity build --sandbox : (base) carlsbad[0]:bb927$ singularity build --sandbox s_ubuntu ubuntu.simg WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu (base) carlsbad[0]:bb927$ ls share singularity s_ubuntu test_file.txt ubuntu.simg Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: (base) carlsbad[0]:bb927$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr An entire ubuntu os! Cool so now let's change stuff in it: (base) carlsbad[0]:bb927$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag or it will be a read only image. What is going on behind the scenes is really just the *.simg format being converted to a directory. This directory is still read only if shelled into. Note if you don't shell into the image, it is editable from outside! This will be necessary for some mounts later as we will see. (base) carlsbad[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container - !!!UGH!!! What?... - So remember that singularity wants to auto mount your home directory /home/$USER but this is a symlink in and of itself! So we need a little secret sauce for this to work. Singularity has a --no-home-- flag we can use to tell it not to mount the /home/$USER directory. (base) carlsbad[0]:bb927$ singularity shell --writable --no-home s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /cluster/qtim/users/bb927 [cwd]: /autofs/cluster/qtim/users/bb927 doesn't exist in container Singularity> touch /mnt/anything.txt Singularity> exit exit (base) carlsbad[0]:bb927$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr (base) carlsbad[0]:bb927$ ls s_ubuntu/mnt anything.txt Now how do we turn it into a new image when we make changes? Just as you can build from docker images and singularity images and even singularity files, you can also build from sandboxes. (base) carlsbad[0]:bb927$ singularity build s_ubuntu_modified s_ubuntu WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified (base) carlsbad[0]:bb927$ ls share singularity s_ubuntu s_ubuntu_modified test_file.txt ubuntu.simg Now let's see if our change made it: (base) carlsbad[0]:bb927$ singularity shell s_ubuntu_modified Singularity> ls /mnt/ anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable . Don't forget the --no-home flag. So we can just install software right? well...yes, but... Singularity> apt update -y Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease ... Singularity> apt-get install vim -y; Reading package lists... Done Building dependency tree Reading state information... Done ... ... dpkg: error: requested operation requires superuser privilege E: Sub-process /usr/bin/dpkg returned an error code (2) Singularity> What now? So remeber that singulariy doesn't let you run as root but rather as your normal user? However you can't install (some) software without being root. So we need to be root without being root... Last wonky flag I promise, but we need to be --fakeroot . Install as this user. (base) carlsbad[0]:bb927$ singularity shell --writable --no-home --fakeroot s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /cluster/qtim/users/bb927 [cwd]: /autofs/cluster/qtim/users/bb927 doesn't exist in container Singularity> apt update -y; apt-get install vim -y; Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease ... W: chown to _apt:root of directory /var/cache/apt/archives/partial failed - SetupAPTPartialDirectory (1: Operation not permitted) W: chown to _apt:root of directory /var/lib/apt/lists/auxfiles failed - SetupAPTPartialDirectory (1: Operation not permitted) W: Download is performed unsandboxed as root as file '/var/cache/apt/archives/partial/libmagic-mgc_1%3a5.38-4_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied) W: chown to root:adm of file /var/log/apt/term.log failed - OpenLog (1: Operation not permitted) Singularity> vim ~ ~ ~ ~ ~ VIM - Vi IMproved ~ ~ version 8.1.2269 ~ by Bram Moolenaar et al. ... So it works despite some rather concerning messages. Starting from ubuntu installing python worked and so did pip3, but using pip3 to install pandas failed because of a missing certificate. I tried a python image and stil lcouldn't install pandas, though I didn'tspend much time on it. Flask did work...We might have to practice a bit with this or only use fully formed images on this system. Run Use singularity run like docker: This image is the demo from earlier using the Singularity.recipe recipe file and test.py . (base) carlsbad[0]:bb927$ singularity run shub://bbearce/singularity:test INFO: Downloading shub image 58.24 MiB / 58.24 MiB [=========================================================================================] 100.00% 16.88 MiB/s 3s import os module and clearly python is installed! ~~~~~some environment variable~~~~~ So it ran and printed some stuff. Cool, but we really care about interactive mode. At the end I will discuss uploading images to shub and bulding from github repos. Useful flags Here are some of the flags we will need routinely: -B or --bind Note mounts can be specified as read only ro or read write rw (default). This is equivalent to dockers -v flag. Use this to mount directories: (base) carlsbad[0]:bb927$ pwd /cluster/qtim/users/bb927 (base) carlsbad[0]:bb927$ mkdir in (base) carlsbad[0]:bb927$ mkdir out (base) carlsbad[0]:bb927$ singularity shell -B ./in:/mnt/in -B ./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in Singularity> ls /mnt/out Singularity> touch /mnt/out/anything.txt Singularity> exit exit (base) carlsbad[0]:bb927$ ls out anything.txt Singularity> you can comma delimit multiple mounts: (base) carlsbad[0]:bb927$ singularity shell -B ./in:/mnt/in,./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/out anything.txt Singularity> If you do not specify where in the container you mount to, the local mount will be mounted to the same directory inside the container: (base) carlsbad[0]:bb927$ singularity shell -B /opt s_ubuntu/ Singularity> ls /opt containerd google microsoft rh Singularity> exit exit (base) carlsbad[0]:bb927$ ls /opt containerd google microsoft rh This is the same as -B /opt:/opt --fakeroot This is important for interactive development because since you are not \"root\" normally you can't install stuff with apt-get. With this you can. --net Gives you internet access. --nv Give container access to GPUs (base) carlsbad[0]:bb927$ singularity shell --nv pytorch.simg --no-home When shelling into your container image, Singularity allows you to mount your current working directory ( CWD ) without mounting your host $HOME directory with the --no-home flag. Jupyter Example Source ljvmiranda921 Also a note from Help Desk: Note from Martinos help Desk: If you run jupyter, you will not be able to access the port it is on ( 8888 ) remotely (such as your local machine at home over VPN) due to firewall on dotter. But port range 5900-5999 are open for VNC so if you force jupyter to use port 5999 instead of its default you can use that. [1] Forward via ssh in your local machine bbearce@pop-os:~$ ssh -N -f -L localhost:5999:localhost:5999 bb927@carlsbad.nmr.mgh.harvard.edu [2] Run jupyter notebook from remote machine (base) carlsbad[0]:bb927$ singularity shell --nv datascience-notebook_latest.sif Singularity> jupyter notebook --no-browser --port=5999 Fail to get yarn configuration. {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/jovyan/.cache/yarn\\\" because it is not writable.\"} {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/scratch/.yarn-cache-4643095\\\" because it is not writable.\"} {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/scratch/.yarn-cache\\\" because it is not writable.\"} {\"type\":\"error\",\"data\":\"Yarn hasn't been able to find a cache folder it can use. Please use the explicit --cache-folder option to tell it what location to use, or make one of the preferred locations writable.\"} {\"type\":\"info\",\"data\":\"Visit https://yarnpkg.com/en/docs/cli/config for documentation about this command.\"} [I 13:48:17.304 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [I 13:48:17.304 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 13:48:17.312 NotebookApp] Serving notebooks from local directory: /autofs/cluster/qtim/users/bb927 [I 13:48:17.312 NotebookApp] Jupyter Notebook 6.1.4 is running at: [I 13:48:17.312 NotebookApp] http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 [I 13:48:17.312 NotebookApp] or http://127.0.0.1:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 [I 13:48:17.312 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:48:17.323 NotebookApp] To access the notebook, open this file in a browser: file:///homes/3/bb927/.local/share/jupyter/runtime/nbserver-25344-open.html Or copy and paste one of these URLs: http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 or http://127.0.0.1:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 Go here: http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 and you should see this: shub Link to shub is: https://singularity-hub.org/ Recapping on our previous example, let's say you want to build from a recipe file: Singularity.test: Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Build: Had to do this on my local bbearce@pop-os:~/Documents/singularity$ ls README.md Singularity.test test.py bbearce@pop-os:~/Documents/singularity$ sudo singularity build test.simg Singularity.test [sudo] password for bbearce: INFO: Starting build... Getting image source signatures Copying blob 171857c49d0f skipped: already exists Copying blob 419640447d26 skipped: already exists Copying blob 61e52f862619 [--------------------------------------] 0.0b / 0.0b Copying config 33a51d0908 done Writing manifest to image destination Storing signatures 2020/11/17 13:40:36 info unpack layer: sha256:171857c49d0f5e2ebf623e6cb36a8bcad585ed0c2aa99c87a055df034c1e5848 2020/11/17 13:40:37 info unpack layer: sha256:419640447d267f068d2f84a093cb13a56ce77e130877f5b8bdb4294f4a90a84f 2020/11/17 13:40:37 info unpack layer: sha256:61e52f862619ab016d3bcfbd78e5c7aaaa1989b4c295e6dbcacddd2d7b93e1f5 INFO: Copying test.py to /tmp/build-temp-907729936/rootfs/ INFO: Running post scriptlet + apt-get -qq -y update + apt-get -qq -y install python debconf: delaying package configuration, since apt-utils is not installed INFO: Adding help info INFO: Adding labels INFO: Adding environment to container INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: test.simg bbearce@pop-os:~/Documents/singularity$ ls README.md Singularity.test test.py test.simg then run: bbearce@pop-os:~/Documents/singularity$ singularity run test.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ So shub allows you to link github repos and put code and recipe files there for auto build: which will become this on sub: Add images via github: [1] [2]","title":"Demo for QTIM"},{"location":"notes/singularity/demo_for_qtim/#demo-martinos-cluster","text":"","title":"Demo Martinos Cluster"},{"location":"notes/singularity/demo_for_qtim/#storage","text":"We have storage (20TB) on Martinos accessible at: /cluster/qtim (convenient path) /vault/cluster/qtim (real path) Here is what is there: (base) carlsbad[0]:qtim$ cd /cluster/qtim (base) carlsbad[0]:qtim$ ls -la total 2 drwxrws--- 4 root qtim 5 Nov 16 14:59 . drwxr-xr-x 4 root root 0 Nov 16 14:57 .. -rw-r--r-- 1 root root 0 Nov 12 23:24 ID-qtim drwxrwsr-x 4 bb927 qtim 4 Nov 16 14:59 machine_backups drwxrwsr-x 5 bb927 qtim 5 Nov 16 14:58 users (base) carlsbad[0]:qtim$ ls machine_backups/ bryce getafix (base) carlsbad[0]:qtim$ ls users bb927 kalpathy yifen For future use and organization I think we should keep users in a users folder. Other high level folders at the users or machine_backups level can be created as needed, but I'm proposing some best practices as of now. For general use we need to use this space when using machines managed by help desk (carlsbad, dotter, the new martinos GPU cluster). We have user accounts that are synced between all martinos machines and they all live on a drive managed by help desk. In order to keep space concerns down with 100s of users, they limit our space allowance to the point where it is unusable for more than configuratoin or tiny tests. It is because of this that the most important step is to symlink some important directories to change where the data stored for various programs (docker, singularity, python) will show up ultimately (not in ~/ ).","title":"Storage"},{"location":"notes/singularity/demo_for_qtim/#setup-simlinks","text":"Source Martinos Docs The two places that need symlinking are ~/.singularity and ~/.share/local/containers . Here is an example: (make sure ~/.singularity does not already exist first) ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /cluster/qtim/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /cluster/qtim/users/bb927/singularity ... I do have some and I will delete them to demonstrate the full workflow: (base) carlsbad[0]:bb927$ cd /cluster/qtim/users/bb927 (base) carlsbad[0]:bb927$ ls (base) carlsbad[0]:bb927$ mkdir share singularity (base) carlsbad[0]:bb927$ ls share singularity (base) carlsbad[0]:bb927$ ln -s /cluster/qtim/users/bb927/share ~/.share (base) carlsbad[0]:bb927$ ln -s /cluster/qtim/users/bb927/singularity ~/.singularity (base) carlsbad[0]:bb927$ mkdir ~/.singularity/tmp (base) carlsbad[0]:bb927$ export SINGULARITY_TMPDIR=~/.singularity/tmp (base) carlsbad[0]:bb927$ mkdir ~/.singularity/cache (base) carlsbad[0]:bb927$ export SINGULARITY_CACHEDIR=~/.singularity/cache","title":"Setup simlinks:"},{"location":"notes/singularity/demo_for_qtim/#singularity","text":"","title":"Singularity"},{"location":"notes/singularity/demo_for_qtim/#must-know","text":"[1] Images are read-only by default...more on that later. [2] You are not root inside your image, but rather your username on linux (Ex: bb927 ). [3] Remember Singularity wants to auto --bind mount these 3 directories in the background whether you say to or not: /home/$USER /tmp $PWD [4] The concept of differentiating between image and container is no more. They are one in the same...more on that later. [5] For editable images (sandboxes) you must create your mount points ahead of time in the sandbox.","title":"Must know"},{"location":"notes/singularity/demo_for_qtim/#build-from-recipe","text":"Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image).","title":"Build from Recipe"},{"location":"notes/singularity/demo_for_qtim/#the-recipe-file","text":"Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default Build Images from Scratch Use this command to build the recipe file into a *.simg (base) carlsbad[0]:bb927$ singularity build ubuntu.simg Singularity.recipe FATAL: You must be the root user, however you can use --remote or --fakeroot to build from a Singularity recipe file ...as I said we don't have permissions. I will need this demo for later so I will show how to get it running in the long run. Hint you can always build somewhere else first...","title":"The Recipe File"},{"location":"notes/singularity/demo_for_qtim/#build-from-docker","text":"Let's say I start with ubuntu from docker (btw you can grab images from shub:// as well): singularity build ubuntu.simg docker://ubuntu:latest Now let's see our image (unlike docker we won't use docker ps or docker images , rather all containers\\images live where you execute commands): (base) carlsbad[0]:bb927$ ls share singularity ubuntu.simg Now this is read only and can just be run or you can shell into it: (base) carlsbad[0]:bb927$ singularity shell ubuntu.simg Singularity> ls share singularity ubuntu.simg Notice that since the user's $PWD directory is automatically mounted it shows the *.simg with ls , so don't be alarmed if your singularity directory looks just like your current directory! However we know we are in the singularity ;) (couldn't resist) because / is not the same outside the container. Singularity> ls / autofs boot dev etc homes lib32 libx32 mnt proc run singularity sys usr bin cluster environment home lib lib64 media opt root sbin srv tmp var Singularity> exit exit (base) carlsbad[0]:bb927$ ls / autofs boot cmas dev etc homes lib64 lost+found misc nas opt proc run scratch srv tmp var bin cluster data eris home lib local_mount media mnt net panfs root sbin space sys usr vast","title":"Build from Docker"},{"location":"notes/singularity/demo_for_qtim/#types-of-containers","text":"","title":"Types of Containers"},{"location":"notes/singularity/demo_for_qtim/#default","text":"You may be wondering while shelling into the container, was that the image or did we shell into an instance container?!? Neither. Singularity has one thing and is always an image, but may have writable copies created for development work. You could think of these writable versions as containers if you'd like, but that is the main paradigm shift when using this techinology. Let's learn more with examples: Singularity> ls share singularity ubuntu.simg Singularity> touch test_file.txt Singularity> touch /test_file.txt touch: cannot touch '/test_file.txt': Read-only file system Notice here that you can add files to the mount point $PWD , but not / . These images are read-only by default with the exception of mount points. So how do we make editable images?","title":"Default"},{"location":"notes/singularity/demo_for_qtim/#sandbox-sandbox","text":"So we can't edit ubuntu.simg by default, but we can build an editable version with singularity build --sandbox : (base) carlsbad[0]:bb927$ singularity build --sandbox s_ubuntu ubuntu.simg WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu (base) carlsbad[0]:bb927$ ls share singularity s_ubuntu test_file.txt ubuntu.simg Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: (base) carlsbad[0]:bb927$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr An entire ubuntu os! Cool so now let's change stuff in it: (base) carlsbad[0]:bb927$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag or it will be a read only image. What is going on behind the scenes is really just the *.simg format being converted to a directory. This directory is still read only if shelled into. Note if you don't shell into the image, it is editable from outside! This will be necessary for some mounts later as we will see. (base) carlsbad[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container - !!!UGH!!! What?... - So remember that singularity wants to auto mount your home directory /home/$USER but this is a symlink in and of itself! So we need a little secret sauce for this to work. Singularity has a --no-home-- flag we can use to tell it not to mount the /home/$USER directory. (base) carlsbad[0]:bb927$ singularity shell --writable --no-home s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /cluster/qtim/users/bb927 [cwd]: /autofs/cluster/qtim/users/bb927 doesn't exist in container Singularity> touch /mnt/anything.txt Singularity> exit exit (base) carlsbad[0]:bb927$ ls s_ubuntu/ bin dev etc lib lib64 media opt root sbin srv tmp var boot environment home lib32 libx32 mnt proc run singularity sys usr (base) carlsbad[0]:bb927$ ls s_ubuntu/mnt anything.txt Now how do we turn it into a new image when we make changes? Just as you can build from docker images and singularity images and even singularity files, you can also build from sandboxes. (base) carlsbad[0]:bb927$ singularity build s_ubuntu_modified s_ubuntu WARNING: 'nodev' mount option set on /autofs/cluster/qtim, it could be a source of failure during build process INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified (base) carlsbad[0]:bb927$ ls share singularity s_ubuntu s_ubuntu_modified test_file.txt ubuntu.simg Now let's see if our change made it: (base) carlsbad[0]:bb927$ singularity shell s_ubuntu_modified Singularity> ls /mnt/ anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable . Don't forget the --no-home flag. So we can just install software right? well...yes, but... Singularity> apt update -y Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease ... Singularity> apt-get install vim -y; Reading package lists... Done Building dependency tree Reading state information... Done ... ... dpkg: error: requested operation requires superuser privilege E: Sub-process /usr/bin/dpkg returned an error code (2) Singularity> What now? So remeber that singulariy doesn't let you run as root but rather as your normal user? However you can't install (some) software without being root. So we need to be root without being root... Last wonky flag I promise, but we need to be --fakeroot . Install as this user. (base) carlsbad[0]:bb927$ singularity shell --writable --no-home --fakeroot s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /cluster/qtim/users/bb927 [cwd]: /autofs/cluster/qtim/users/bb927 doesn't exist in container Singularity> apt update -y; apt-get install vim -y; Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease ... W: chown to _apt:root of directory /var/cache/apt/archives/partial failed - SetupAPTPartialDirectory (1: Operation not permitted) W: chown to _apt:root of directory /var/lib/apt/lists/auxfiles failed - SetupAPTPartialDirectory (1: Operation not permitted) W: Download is performed unsandboxed as root as file '/var/cache/apt/archives/partial/libmagic-mgc_1%3a5.38-4_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied) W: chown to root:adm of file /var/log/apt/term.log failed - OpenLog (1: Operation not permitted) Singularity> vim ~ ~ ~ ~ ~ VIM - Vi IMproved ~ ~ version 8.1.2269 ~ by Bram Moolenaar et al. ... So it works despite some rather concerning messages. Starting from ubuntu installing python worked and so did pip3, but using pip3 to install pandas failed because of a missing certificate. I tried a python image and stil lcouldn't install pandas, though I didn'tspend much time on it. Flask did work...We might have to practice a bit with this or only use fully formed images on this system.","title":"Sandbox --sandbox"},{"location":"notes/singularity/demo_for_qtim/#run","text":"Use singularity run like docker: This image is the demo from earlier using the Singularity.recipe recipe file and test.py . (base) carlsbad[0]:bb927$ singularity run shub://bbearce/singularity:test INFO: Downloading shub image 58.24 MiB / 58.24 MiB [=========================================================================================] 100.00% 16.88 MiB/s 3s import os module and clearly python is installed! ~~~~~some environment variable~~~~~ So it ran and printed some stuff. Cool, but we really care about interactive mode. At the end I will discuss uploading images to shub and bulding from github repos.","title":"Run"},{"location":"notes/singularity/demo_for_qtim/#useful-flags","text":"Here are some of the flags we will need routinely:","title":"Useful flags"},{"location":"notes/singularity/demo_for_qtim/#-b-or-bind","text":"Note mounts can be specified as read only ro or read write rw (default). This is equivalent to dockers -v flag. Use this to mount directories: (base) carlsbad[0]:bb927$ pwd /cluster/qtim/users/bb927 (base) carlsbad[0]:bb927$ mkdir in (base) carlsbad[0]:bb927$ mkdir out (base) carlsbad[0]:bb927$ singularity shell -B ./in:/mnt/in -B ./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/in Singularity> ls /mnt/out Singularity> touch /mnt/out/anything.txt Singularity> exit exit (base) carlsbad[0]:bb927$ ls out anything.txt Singularity> you can comma delimit multiple mounts: (base) carlsbad[0]:bb927$ singularity shell -B ./in:/mnt/in,./out:/mnt/out s_ubuntu/ Singularity> ls /mnt/out anything.txt Singularity> If you do not specify where in the container you mount to, the local mount will be mounted to the same directory inside the container: (base) carlsbad[0]:bb927$ singularity shell -B /opt s_ubuntu/ Singularity> ls /opt containerd google microsoft rh Singularity> exit exit (base) carlsbad[0]:bb927$ ls /opt containerd google microsoft rh This is the same as -B /opt:/opt","title":"-B or --bind"},{"location":"notes/singularity/demo_for_qtim/#-fakeroot","text":"This is important for interactive development because since you are not \"root\" normally you can't install stuff with apt-get. With this you can.","title":"--fakeroot"},{"location":"notes/singularity/demo_for_qtim/#-net","text":"Gives you internet access.","title":"--net"},{"location":"notes/singularity/demo_for_qtim/#-nv","text":"Give container access to GPUs (base) carlsbad[0]:bb927$ singularity shell --nv pytorch.simg","title":"--nv"},{"location":"notes/singularity/demo_for_qtim/#-no-home","text":"When shelling into your container image, Singularity allows you to mount your current working directory ( CWD ) without mounting your host $HOME directory with the --no-home flag.","title":"--no-home"},{"location":"notes/singularity/demo_for_qtim/#jupyter-example","text":"Source ljvmiranda921 Also a note from Help Desk: Note from Martinos help Desk: If you run jupyter, you will not be able to access the port it is on ( 8888 ) remotely (such as your local machine at home over VPN) due to firewall on dotter. But port range 5900-5999 are open for VNC so if you force jupyter to use port 5999 instead of its default you can use that.","title":"Jupyter Example"},{"location":"notes/singularity/demo_for_qtim/#1-forward-via-ssh-in-your-local-machine","text":"bbearce@pop-os:~$ ssh -N -f -L localhost:5999:localhost:5999 bb927@carlsbad.nmr.mgh.harvard.edu","title":"[1] Forward via ssh in your local machine"},{"location":"notes/singularity/demo_for_qtim/#2-run-jupyter-notebook-from-remote-machine","text":"(base) carlsbad[0]:bb927$ singularity shell --nv datascience-notebook_latest.sif Singularity> jupyter notebook --no-browser --port=5999 Fail to get yarn configuration. {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/jovyan/.cache/yarn\\\" because it is not writable.\"} {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/scratch/.yarn-cache-4643095\\\" because it is not writable.\"} {\"type\":\"warning\",\"data\":\"Skipping preferred cache folder \\\"/home/scratch/.yarn-cache\\\" because it is not writable.\"} {\"type\":\"error\",\"data\":\"Yarn hasn't been able to find a cache folder it can use. Please use the explicit --cache-folder option to tell it what location to use, or make one of the preferred locations writable.\"} {\"type\":\"info\",\"data\":\"Visit https://yarnpkg.com/en/docs/cli/config for documentation about this command.\"} [I 13:48:17.304 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [I 13:48:17.304 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 13:48:17.312 NotebookApp] Serving notebooks from local directory: /autofs/cluster/qtim/users/bb927 [I 13:48:17.312 NotebookApp] Jupyter Notebook 6.1.4 is running at: [I 13:48:17.312 NotebookApp] http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 [I 13:48:17.312 NotebookApp] or http://127.0.0.1:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 [I 13:48:17.312 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:48:17.323 NotebookApp] To access the notebook, open this file in a browser: file:///homes/3/bb927/.local/share/jupyter/runtime/nbserver-25344-open.html Or copy and paste one of these URLs: http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 or http://127.0.0.1:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 Go here: http://carlsbad.nmr.mgh.harvard.edu:5999/?token=26d3e282bf9f134c9059eac068d10a76d624754182d36a55 and you should see this:","title":"[2] Run jupyter notebook from remote machine"},{"location":"notes/singularity/demo_for_qtim/#shub","text":"Link to shub is: https://singularity-hub.org/ Recapping on our previous example, let's say you want to build from a recipe file: Singularity.test: Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Build: Had to do this on my local bbearce@pop-os:~/Documents/singularity$ ls README.md Singularity.test test.py bbearce@pop-os:~/Documents/singularity$ sudo singularity build test.simg Singularity.test [sudo] password for bbearce: INFO: Starting build... Getting image source signatures Copying blob 171857c49d0f skipped: already exists Copying blob 419640447d26 skipped: already exists Copying blob 61e52f862619 [--------------------------------------] 0.0b / 0.0b Copying config 33a51d0908 done Writing manifest to image destination Storing signatures 2020/11/17 13:40:36 info unpack layer: sha256:171857c49d0f5e2ebf623e6cb36a8bcad585ed0c2aa99c87a055df034c1e5848 2020/11/17 13:40:37 info unpack layer: sha256:419640447d267f068d2f84a093cb13a56ce77e130877f5b8bdb4294f4a90a84f 2020/11/17 13:40:37 info unpack layer: sha256:61e52f862619ab016d3bcfbd78e5c7aaaa1989b4c295e6dbcacddd2d7b93e1f5 INFO: Copying test.py to /tmp/build-temp-907729936/rootfs/ INFO: Running post scriptlet + apt-get -qq -y update + apt-get -qq -y install python debconf: delaying package configuration, since apt-utils is not installed INFO: Adding help info INFO: Adding labels INFO: Adding environment to container INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: test.simg bbearce@pop-os:~/Documents/singularity$ ls README.md Singularity.test test.py test.simg then run: bbearce@pop-os:~/Documents/singularity$ singularity run test.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ So shub allows you to link github repos and put code and recipe files there for auto build: which will become this on sub:","title":"shub"},{"location":"notes/singularity/demo_for_qtim/#add-images-via-github","text":"[1] [2]","title":"Add images via github:"},{"location":"notes/singularity/install/","text":"Installing Singularity Source: sylabs.io Terminal Install: Pre-requisites: On Ubuntu or Debian install the following dependencies: sudo apt-get update && sudo apt-get install -y \\ build-essential \\ uuid-dev \\ libgpgme-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup-bin Install Go: export VERSION=1.5.12 OS=linux ARCH=amd64 && \\ wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \\ sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \\ rm go$VERSION.$OS-$ARCH.tar.gz echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc Install Singularity: export VERSION=3.6.3 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity then check out code from git: git clone https://github.com/sylabs/singularity.git && \\ cd singularity && \\ git checkout v3.6.3 Now compile Singularity: ./mconfig && \\ make -C ./builddir && \\ sudo make -C ./builddir install","title":"Install"},{"location":"notes/singularity/install/#installing-singularity","text":"Source: sylabs.io","title":"Installing Singularity"},{"location":"notes/singularity/install/#terminal-install","text":"","title":"Terminal Install:"},{"location":"notes/singularity/install/#pre-requisites","text":"On Ubuntu or Debian install the following dependencies: sudo apt-get update && sudo apt-get install -y \\ build-essential \\ uuid-dev \\ libgpgme-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup-bin Install Go: export VERSION=1.5.12 OS=linux ARCH=amd64 && \\ wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \\ sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \\ rm go$VERSION.$OS-$ARCH.tar.gz echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc","title":"Pre-requisites:"},{"location":"notes/singularity/install/#install-singularity","text":"export VERSION=3.6.3 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity then check out code from git: git clone https://github.com/sylabs/singularity.git && \\ cd singularity && \\ git checkout v3.6.3 Now compile Singularity: ./mconfig && \\ make -C ./builddir && \\ sudo make -C ./builddir install","title":"Install Singularity:"},{"location":"notes/singularity/use_with_docker/","text":"Use with docker Spoiler, while using docker is possible, I think it's going to be harder in the long run. I believe there is nothing wrong with starting on docker on bryce and in fact I recommend that. Once on Carlsbad or Dotter, I believe that just using singularity is easier (than using docker on these machines). #I_Love_Docker Use on Martinos machines: Setup simlinks: Source Martinos Docs ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /space/dotter/2/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /space/dotter/2/users/bb927/singularity ... Once you have this setup, you can proceed, otherwise you are in for a world of hurt if you accidentally fill up your real home dir. Martinos gotchas: Dotter for example: Pure Singularity: Your home directory and drive directories are actually simlinks and this will mess up singularity on dotter. dotter[0]:bb927$ ls -la /homes/3 lrwxrwxrwx 1 root root 17 Sep 17 2016 /homes/3 -> /autofs/homes/003 dotter[0]:bb927$ ls -la /space/dotter total 32 drwxr-xr-x 2 root root 4096 Jul 9 12:00 . drwx--x--x 984 root root 24576 Sep 19 18:12 .. lrwxrwxrwx 1 root root 24 Jul 9 12:00 1 -> /autofs/space/dotter_001 lrwxrwxrwx 1 root root 24 Jul 9 12:00 2 -> /autofs/space/dotter_002 lrwxrwxrwx 1 root root 24 Jul 9 12:00 3 -> /autofs/space/dotter_003 lrwxrwxrwx 1 root root 24 Jul 9 12:00 4 -> /autofs/space/dotter_004 lrwxrwxrwx 1 root root 24 Jul 9 12:00 5 -> /autofs/space/dotter_005 lrwxrwxrwx 1 root root 24 Jul 9 12:00 6 -> /autofs/space/dotter_006 Remember Singularity wants to auto --bind mount these 3 directories: /home/$USER /tmp $PWD Update, I think there are more but it depends on Help Desk. See docs I have a sanbox made from a regular ubuntu image (docker://ubuntu:latest) that was first pulled to ubuntu.simg and then the sandbox is called s_ubuntu . Let's try to shell into it. This is my current $PWD: dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg After shelling in let's look at the mounts: dotter[0]:bb927$ singularity shell s_ubuntu Singularity> ls ~ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM Singularity> pwd /space/dotter/2/users/bb927 Singularity> exit exit Now outside the container: dotter[0]:bb927$ ls ~ Desktop Downloads matlab_crash_dump.224407-1 Pictures Templates Documents matlab Music Public Videos dotter[0]:bb927$ ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ We see those three locations mounted perfectly! :) Now let's shell in with --writable dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container dotter[0]:bb927$ Ugh, what happend? Well the WARNINGS were sort of ignored but the FATAL error is because /homes doesn't exist in the container. We can work around this by creating it before hand: Note, see these notes dotter[0]:bb927$ mkdir s_ubuntu/homes dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /space/dotter/2/users/bb927 [cwd]: /autofs/space/dotter_002/users/bb927 doesn't exist in container Singularity> ls /homes 3 Singularity> ls /homes/3 bb927 Singularity> ls /homes/3/bb927/ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> Now even though the warning says we might not be able to write to our home directory, you can. This is probably because dotter has an environment variable HOME=/homes/3/bb927 and that is also set inside the singularity container. Even though that is a simlink you can access it inside the container. But we know that Help Desk has asked us not to use our home directories because they have limited storage, so let's create the drive mount location and get rid of another warning: dotter[0]:bb927$ mkdir -p s_ubuntu/autofs/space/dotter_002/users/bb927 dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> So we know we can ignore the first WARNING because there isn't much we can do, and as far as the second one is concerned, you have the tools to make it disappear if you need it to. Mounting, does it work?!?!? In my $PWD I have in and out folders: dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg dotter[0]:bb927$ ls in in.txt dotter[0]:bb927$ ls out out.txt Let's see if we can read and write to them: Note: Just from playing around I've noticed in --writable mode you need to have created the mount points ahead of time. It seems that when running the container, you will not need to do this as the mounts will be made on the fly as shown below: dotter[0]:bb927$ singularity shell --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt watch in --writable mode: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /mnt/in destination automatically without overlay or underlay FATAL: container creation failed: mount /autofs/space/dotter_002/users/bb927/in->/mnt/in error: while mounting /autofs/space/dotter_002/users/bb927/in: destination /mnt/in doesn't exist in container So create them: dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/in dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/out and try again: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt Nice! Ok, now the 1st of two important questions; can we read/write? Singularity> cd /mnt/in Singularity> touch new_file.txt Singularity> ls in.txt new_file.txt out.txt Singularity> pwd /mnt/in Singularity> exit exit dotter[0]:bb927$ ls in in.txt new_file.txt out.txt dotter[0]:bb927$ ls -la in total 16 drwxrwsr-x 2 bb927 pimi 4096 Oct 3 20:58 . drwxrwsr-x 7 bb927 pimi 4096 Oct 3 19:55 .. -rw-rw-r-- 1 bb927 pimi 10 Oct 3 20:56 in.txt -rw-r--r-- 1 bb927 pimi 0 Oct 3 20:58 new_file.txt -rw-rw-r-- 1 bb927 pimi 11 Oct 3 20:55 out.txt Nice! Ok so now can we access GPUs? Let's pull pytorch: dotter[0]:bb927$ singularity pull docker://pytorch/pytorch:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 23884877105a done Copying blob bc38caa0f5b9 done Copying blob 2910811b6c42 done Copying blob 36505266dcc6 done Copying blob 3472d01858ba done Copying blob 4a98b57681ff done Copying blob f3b419d1e6d5 done Copying config c35e69071c done Writing manifest to image destination Storing signatures 2020/10/03 21:02:21 info unpack layer: sha256:23884877105a7ff84a910895cd044061a4561385ff6c36480ee080b76ec0e771 2020/10/03 21:02:23 info unpack layer: sha256:bc38caa0f5b94141276220daaf428892096e4afd24b05668cd188311e00a635f 2020/10/03 21:02:23 info unpack layer: sha256:2910811b6c4227c2f42aaea9a3dd5f53b1d469f67e2cf7e601f631b119b61ff7 2020/10/03 21:02:23 info unpack layer: sha256:36505266dcc64eeb1010bd2112e6f73981e1a8246e4f6d4e287763b57f101b0b 2020/10/03 21:02:23 info unpack layer: sha256:3472d01858ba9ce89d2a493bb3964eaf26a63500a3b47ad0a4c72d5e5fe40e11 2020/10/03 21:02:23 info unpack layer: sha256:4a98b57681ffdd1fcba82e9aeb95171dbe0837980deb82464e4893f4449c4459 2020/10/03 21:03:08 info unpack layer: sha256:f3b419d1e6d54391719bc37b641b2c133915f207103d8dd28c776607e5d477d8 INFO: Creating SIF file... INFO: Build complete: pytorch_latest.sif dotter[0]:bb927$ Alright deep breath: dotter[0]:bb927$ singularity shell pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() False >>> NOOOOOooooooooooo...Ok there must be a flag: dotter[0]:bb927$ singularity shell --nv pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() True Swish Should we use Docker??? Actually I think this could be a problem. See as root inside docker if you write to mounts it shows up ownded by either root or 10000..:1000..., some weird user of numbers...Anyways you won't be able to manipulate that data once you log out of the container as you are not them! It seems singularity is the best way to go mainly because you are the same user inside and outside the singularity... Note I haven't seen this per se, but Kevin has and DeepNeuro made files with a cron job that had those user characteristics... Update: tried it and actually you can't write even though you are root: dotter[0]:bb927$ docker run -it --rm -v /space/dotter/2/users/bb927/in:/mnt/in pytorch/pytorch:latest bash root@7b3a11b77784:/workspace# ls /mnt/in in.txt new_file.txt out.txt root@7b3a11b77784:/workspace# touch /mnt/in/example_of_what_not_to_do.txt touch: cannot touch '/mnt/in/example_of_what_not_to_do.txt': Permission denied So if you know how to add users in your Dockerfile, you could in theory make the user inside the image your Carlsbad or Dotter user, but that's a lot of work. I hope that using singularity just works smoothly...Stay tuned for how to use SHUB, Singularities Dockerhub...","title":"Use With Docker"},{"location":"notes/singularity/use_with_docker/#use-with-docker","text":"Spoiler, while using docker is possible, I think it's going to be harder in the long run. I believe there is nothing wrong with starting on docker on bryce and in fact I recommend that. Once on Carlsbad or Dotter, I believe that just using singularity is easier (than using docker on these machines). #I_Love_Docker","title":"Use with docker"},{"location":"notes/singularity/use_with_docker/#use-on-martinos-machines","text":"","title":"Use on Martinos machines:"},{"location":"notes/singularity/use_with_docker/#setup-simlinks","text":"Source Martinos Docs ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /space/dotter/2/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /space/dotter/2/users/bb927/singularity ... Once you have this setup, you can proceed, otherwise you are in for a world of hurt if you accidentally fill up your real home dir.","title":"Setup simlinks:"},{"location":"notes/singularity/use_with_docker/#martinos-gotchas","text":"","title":"Martinos gotchas:"},{"location":"notes/singularity/use_with_docker/#dotter-for-example","text":"","title":"Dotter for example:"},{"location":"notes/singularity/use_with_docker/#pure-singularity","text":"Your home directory and drive directories are actually simlinks and this will mess up singularity on dotter. dotter[0]:bb927$ ls -la /homes/3 lrwxrwxrwx 1 root root 17 Sep 17 2016 /homes/3 -> /autofs/homes/003 dotter[0]:bb927$ ls -la /space/dotter total 32 drwxr-xr-x 2 root root 4096 Jul 9 12:00 . drwx--x--x 984 root root 24576 Sep 19 18:12 .. lrwxrwxrwx 1 root root 24 Jul 9 12:00 1 -> /autofs/space/dotter_001 lrwxrwxrwx 1 root root 24 Jul 9 12:00 2 -> /autofs/space/dotter_002 lrwxrwxrwx 1 root root 24 Jul 9 12:00 3 -> /autofs/space/dotter_003 lrwxrwxrwx 1 root root 24 Jul 9 12:00 4 -> /autofs/space/dotter_004 lrwxrwxrwx 1 root root 24 Jul 9 12:00 5 -> /autofs/space/dotter_005 lrwxrwxrwx 1 root root 24 Jul 9 12:00 6 -> /autofs/space/dotter_006 Remember Singularity wants to auto --bind mount these 3 directories: /home/$USER /tmp $PWD Update, I think there are more but it depends on Help Desk. See docs I have a sanbox made from a regular ubuntu image (docker://ubuntu:latest) that was first pulled to ubuntu.simg and then the sandbox is called s_ubuntu . Let's try to shell into it. This is my current $PWD: dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg After shelling in let's look at the mounts: dotter[0]:bb927$ singularity shell s_ubuntu Singularity> ls ~ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM Singularity> pwd /space/dotter/2/users/bb927 Singularity> exit exit Now outside the container: dotter[0]:bb927$ ls ~ Desktop Downloads matlab_crash_dump.224407-1 Pictures Templates Documents matlab Music Public Videos dotter[0]:bb927$ ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ We see those three locations mounted perfectly! :) Now let's shell in with --writable dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container dotter[0]:bb927$ Ugh, what happend? Well the WARNINGS were sort of ignored but the FATAL error is because /homes doesn't exist in the container. We can work around this by creating it before hand: Note, see these notes dotter[0]:bb927$ mkdir s_ubuntu/homes dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /space/dotter/2/users/bb927 [cwd]: /autofs/space/dotter_002/users/bb927 doesn't exist in container Singularity> ls /homes 3 Singularity> ls /homes/3 bb927 Singularity> ls /homes/3/bb927/ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> Now even though the warning says we might not be able to write to our home directory, you can. This is probably because dotter has an environment variable HOME=/homes/3/bb927 and that is also set inside the singularity container. Even though that is a simlink you can access it inside the container. But we know that Help Desk has asked us not to use our home directories because they have limited storage, so let's create the drive mount location and get rid of another warning: dotter[0]:bb927$ mkdir -p s_ubuntu/autofs/space/dotter_002/users/bb927 dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> So we know we can ignore the first WARNING because there isn't much we can do, and as far as the second one is concerned, you have the tools to make it disappear if you need it to. Mounting, does it work?!?!? In my $PWD I have in and out folders: dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg dotter[0]:bb927$ ls in in.txt dotter[0]:bb927$ ls out out.txt Let's see if we can read and write to them: Note: Just from playing around I've noticed in --writable mode you need to have created the mount points ahead of time. It seems that when running the container, you will not need to do this as the mounts will be made on the fly as shown below: dotter[0]:bb927$ singularity shell --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt watch in --writable mode: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /mnt/in destination automatically without overlay or underlay FATAL: container creation failed: mount /autofs/space/dotter_002/users/bb927/in->/mnt/in error: while mounting /autofs/space/dotter_002/users/bb927/in: destination /mnt/in doesn't exist in container So create them: dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/in dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/out and try again: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt Nice! Ok, now the 1st of two important questions; can we read/write? Singularity> cd /mnt/in Singularity> touch new_file.txt Singularity> ls in.txt new_file.txt out.txt Singularity> pwd /mnt/in Singularity> exit exit dotter[0]:bb927$ ls in in.txt new_file.txt out.txt dotter[0]:bb927$ ls -la in total 16 drwxrwsr-x 2 bb927 pimi 4096 Oct 3 20:58 . drwxrwsr-x 7 bb927 pimi 4096 Oct 3 19:55 .. -rw-rw-r-- 1 bb927 pimi 10 Oct 3 20:56 in.txt -rw-r--r-- 1 bb927 pimi 0 Oct 3 20:58 new_file.txt -rw-rw-r-- 1 bb927 pimi 11 Oct 3 20:55 out.txt Nice! Ok so now can we access GPUs? Let's pull pytorch: dotter[0]:bb927$ singularity pull docker://pytorch/pytorch:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 23884877105a done Copying blob bc38caa0f5b9 done Copying blob 2910811b6c42 done Copying blob 36505266dcc6 done Copying blob 3472d01858ba done Copying blob 4a98b57681ff done Copying blob f3b419d1e6d5 done Copying config c35e69071c done Writing manifest to image destination Storing signatures 2020/10/03 21:02:21 info unpack layer: sha256:23884877105a7ff84a910895cd044061a4561385ff6c36480ee080b76ec0e771 2020/10/03 21:02:23 info unpack layer: sha256:bc38caa0f5b94141276220daaf428892096e4afd24b05668cd188311e00a635f 2020/10/03 21:02:23 info unpack layer: sha256:2910811b6c4227c2f42aaea9a3dd5f53b1d469f67e2cf7e601f631b119b61ff7 2020/10/03 21:02:23 info unpack layer: sha256:36505266dcc64eeb1010bd2112e6f73981e1a8246e4f6d4e287763b57f101b0b 2020/10/03 21:02:23 info unpack layer: sha256:3472d01858ba9ce89d2a493bb3964eaf26a63500a3b47ad0a4c72d5e5fe40e11 2020/10/03 21:02:23 info unpack layer: sha256:4a98b57681ffdd1fcba82e9aeb95171dbe0837980deb82464e4893f4449c4459 2020/10/03 21:03:08 info unpack layer: sha256:f3b419d1e6d54391719bc37b641b2c133915f207103d8dd28c776607e5d477d8 INFO: Creating SIF file... INFO: Build complete: pytorch_latest.sif dotter[0]:bb927$ Alright deep breath: dotter[0]:bb927$ singularity shell pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() False >>> NOOOOOooooooooooo...Ok there must be a flag: dotter[0]:bb927$ singularity shell --nv pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() True Swish","title":"Pure Singularity:"},{"location":"notes/singularity/use_with_docker/#should-we-use-docker","text":"Actually I think this could be a problem. See as root inside docker if you write to mounts it shows up ownded by either root or 10000..:1000..., some weird user of numbers...Anyways you won't be able to manipulate that data once you log out of the container as you are not them! It seems singularity is the best way to go mainly because you are the same user inside and outside the singularity... Note I haven't seen this per se, but Kevin has and DeepNeuro made files with a cron job that had those user characteristics... Update: tried it and actually you can't write even though you are root: dotter[0]:bb927$ docker run -it --rm -v /space/dotter/2/users/bb927/in:/mnt/in pytorch/pytorch:latest bash root@7b3a11b77784:/workspace# ls /mnt/in in.txt new_file.txt out.txt root@7b3a11b77784:/workspace# touch /mnt/in/example_of_what_not_to_do.txt touch: cannot touch '/mnt/in/example_of_what_not_to_do.txt': Permission denied So if you know how to add users in your Dockerfile, you could in theory make the user inside the image your Carlsbad or Dotter user, but that's a lot of work. I hope that using singularity just works smoothly...Stay tuned for how to use SHUB, Singularities Dockerhub...","title":"Should we use Docker???"},{"location":"notes/sql/mysql/mySQL/","text":"mySQL Basics First, connect to your MySQL database using your MySQL client from your operating system command line: $ mysql -u root -p Next, after you're logged into your MySQL database, tell MySQL which database you want to use: mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | MySQL_DevDB | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.01 sec) mysql> use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +---------------------------+ | Tables_in_mysql | +---------------------------+ | columns_priv | | db | | event | | func | | general_log | | help_category | | help_keyword | | help_relation | | help_topic | | innodb_index_stats | | innodb_table_stats | | ndb_binlog_index | | plugin | | proc | | procs_priv | | proxies_priv | | servers | | slave_master_info | | slave_relay_log_info | | slave_worker_info | | slow_log | | tables_priv | | time_zone | | time_zone_leap_second | | time_zone_name | | time_zone_transition | | time_zone_transition_type | | user | +---------------------------+ 28 rows in set (0.00 sec) Create Database You can now work with the database. For example, the following commands demonstrate how to create a basic table named example , and how to insert some data into it: CREATE TABLE example ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO example ( id, name ) VALUES ( null, 'Sample data' ); Drop Database\\Table DROP DATABASE dbname; DROP TABLE tablename; Type \\q to exit the mysql program. New User Courtesty of a2hosting.com To create a database user, type the following command. Replace username with the user you want to create, and replace password with the user's password: GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password'; This command grants the user all permissions. However, you can grant specific permissions to maintain precise control over database access. For example, to explicitly grant the SELECT permission, you would use the following command: GRANT SELECT ON *.* TO 'username'@'localhost' To log in to MySQL as the user you just created, type the following command. Replace username with the name of the user you created in step 3: mysql -u username -p Delete User To view a list of all users, type the following command from the mysql> prompt: SELECT user FROM mysql.user GROUP BY user; To delete a specific user, type the following command from the mysql> prompt. Replace username with the name of the user that you want to delete: DELETE FROM mysql.user WHERE user = 'username'; Using SQL script files Create a file named example.sql and open it in your preferred text edtior. Copy and paste the following text into the file: CREATE DATABASE dbname; USE dbname; CREATE TABLE tablename ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO tablename ( id, name ) VALUES ( null, 'Sample data' ); To process the SQL script, type the following command. Replace username with the name of the user you just created: mysql -u username -p < example.sql MySQL Root Password Guide Courtest of a2hosting To reset the root password for MySQL, follow these steps: Log in to your account using SSH. You must runUsing SQL script files the commands in the following steps as the root user. Therefore, you can either log in directly as the root user (which is not recommended for security reasons), or use the su or sudo commands to run the commands as the root user. Stop the MySQL server using the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql stop For CentOS and Fedora, type: service mysqld stop Restart the MySQL server with the \u2014skip-grant-tables option. To do this, type the following command: mysqld_safe --skip-grant-tables & Make sure you type the ampersand ( & ) at the end of the command. This runs the command in the background and allows you to type the commands in the following steps. Running MySQL with the \u2014skip-grant-tables option enabled is highly insecure, and should only be done for a brief period while you reset the password. The steps below show you how to stop the mysqld_safe server instance safely and start the MySQL server securely after you have reset the root password. Log into MySQL using the following command: mysql At the mysql> prompt, reset the password. To do this, type the following command, replacing NEW-PASSWORD with the new root password: UPDATE mysql.user SET Password=PASSWORD('NEW-PASSWORD') WHERE User='root'; At the mysql> prompt, type the following commands: FLUSH PRIVILEGES; exit; Stop the MySQL server using the following command. You will be prompted to enter the new MySQL root password before the MySQL server shuts down: mysqladmin -u root -p shutdown Start the MySQL server normally. To do this, type the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql start For CentOS and Fedora, type: service mysqld start","title":"mySQL"},{"location":"notes/sql/mysql/mySQL/#mysql","text":"","title":"mySQL"},{"location":"notes/sql/mysql/mySQL/#basics","text":"First, connect to your MySQL database using your MySQL client from your operating system command line: $ mysql -u root -p Next, after you're logged into your MySQL database, tell MySQL which database you want to use: mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | MySQL_DevDB | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.01 sec) mysql> use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +---------------------------+ | Tables_in_mysql | +---------------------------+ | columns_priv | | db | | event | | func | | general_log | | help_category | | help_keyword | | help_relation | | help_topic | | innodb_index_stats | | innodb_table_stats | | ndb_binlog_index | | plugin | | proc | | procs_priv | | proxies_priv | | servers | | slave_master_info | | slave_relay_log_info | | slave_worker_info | | slow_log | | tables_priv | | time_zone | | time_zone_leap_second | | time_zone_name | | time_zone_transition | | time_zone_transition_type | | user | +---------------------------+ 28 rows in set (0.00 sec)","title":"Basics"},{"location":"notes/sql/mysql/mySQL/#create-database","text":"You can now work with the database. For example, the following commands demonstrate how to create a basic table named example , and how to insert some data into it: CREATE TABLE example ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO example ( id, name ) VALUES ( null, 'Sample data' );","title":"Create Database"},{"location":"notes/sql/mysql/mySQL/#drop-databasetable","text":"DROP DATABASE dbname; DROP TABLE tablename; Type \\q to exit the mysql program.","title":"Drop Database\\Table"},{"location":"notes/sql/mysql/mySQL/#new-user","text":"Courtesty of a2hosting.com To create a database user, type the following command. Replace username with the user you want to create, and replace password with the user's password: GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password'; This command grants the user all permissions. However, you can grant specific permissions to maintain precise control over database access. For example, to explicitly grant the SELECT permission, you would use the following command: GRANT SELECT ON *.* TO 'username'@'localhost' To log in to MySQL as the user you just created, type the following command. Replace username with the name of the user you created in step 3: mysql -u username -p","title":"New User"},{"location":"notes/sql/mysql/mySQL/#delete-user","text":"To view a list of all users, type the following command from the mysql> prompt: SELECT user FROM mysql.user GROUP BY user; To delete a specific user, type the following command from the mysql> prompt. Replace username with the name of the user that you want to delete: DELETE FROM mysql.user WHERE user = 'username';","title":"Delete User"},{"location":"notes/sql/mysql/mySQL/#using-sql-script-files","text":"Create a file named example.sql and open it in your preferred text edtior. Copy and paste the following text into the file: CREATE DATABASE dbname; USE dbname; CREATE TABLE tablename ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO tablename ( id, name ) VALUES ( null, 'Sample data' ); To process the SQL script, type the following command. Replace username with the name of the user you just created: mysql -u username -p < example.sql","title":"Using SQL script files"},{"location":"notes/sql/mysql/mySQL/#mysql-root-password-guide","text":"Courtest of a2hosting To reset the root password for MySQL, follow these steps:","title":"MySQL Root Password Guide"},{"location":"notes/sql/mysql/mySQL/#log-in-to-your-account-using-ssh","text":"You must runUsing SQL script files the commands in the following steps as the root user. Therefore, you can either log in directly as the root user (which is not recommended for security reasons), or use the su or sudo commands to run the commands as the root user.","title":"Log in to your account using SSH."},{"location":"notes/sql/mysql/mySQL/#stop-the-mysql-server-using-the-appropriate-command-for-your-linux-distribution","text":"For Debian and Ubuntu, type: service mysql stop For CentOS and Fedora, type: service mysqld stop","title":"Stop the MySQL server using the appropriate command for your Linux distribution:"},{"location":"notes/sql/mysql/mySQL/#restart-the-mysql-server-with-the-skip-grant-tables-option-to-do-this-type-the-following-command","text":"mysqld_safe --skip-grant-tables & Make sure you type the ampersand ( & ) at the end of the command. This runs the command in the background and allows you to type the commands in the following steps. Running MySQL with the \u2014skip-grant-tables option enabled is highly insecure, and should only be done for a brief period while you reset the password. The steps below show you how to stop the mysqld_safe server instance safely and start the MySQL server securely after you have reset the root password.","title":"Restart the MySQL server with the \u2014skip-grant-tables option. To do this, type the following command:"},{"location":"notes/sql/mysql/mySQL/#log-into-mysql-using-the-following-command","text":"mysql At the mysql> prompt, reset the password. To do this, type the following command, replacing NEW-PASSWORD with the new root password: UPDATE mysql.user SET Password=PASSWORD('NEW-PASSWORD') WHERE User='root'; At the mysql> prompt, type the following commands: FLUSH PRIVILEGES; exit; Stop the MySQL server using the following command. You will be prompted to enter the new MySQL root password before the MySQL server shuts down: mysqladmin -u root -p shutdown Start the MySQL server normally. To do this, type the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql start For CentOS and Fedora, type: service mysqld start","title":"Log into MySQL using the following command:"},{"location":"notes/sql/postgres/Postgres/","text":"Postgres CentOS Stream 8 Notes Courtesy of Stackoverflow Installation Whoops...I'm sure this is easy...I'll get to it eventually. First Time Use and Setup Here's what worked for postgresql-9.1 on Xubuntu 12.04.1 LTS. Connect to the default database with user postgres : sudo -u postgres psql template1 Set the password for user postgres , then exit psql (Ctrl-D) : postgres=# ALTER USER postgres with encrypted password 'xxxxxxx'; Edit the pg_hba.conf file: sudo vim /etc/postgresql/9.1/main/pg_hba.conf And change peer to md5 on the line concerning postgres : local all postgres peer md5 Note: you need sudo or the file will appear blank Restart the database: postgres=# sudo /etc/init.d/postgresql restart\\ (Here you can check it worked with psql -U postgres .) Create a user having the same name as you (to find it, you can type whoami ) : postgres=# createuser -U postgres -d -e -E -l -P -r -s <my_name> The options tell postgresql to create a user that can login, create databases, create new roles, is a superuser, and will have an encrypted password. The really important ones are -P -E , so that you're asked to type the password that will be encrypted, and -d so that you can do a createdb . Beware of passwords : it will first ask you twice the new password (for the new user), repeated, and then once the postgres password (the one specified on step 2). Again, edit the pg_hba.conf file (see step 3 above), and change peer to md5 on the line concerning \"all\" other users : local all all peer md5 Restart (like in step 4), and check that you can login without -U postgres : psql template1 Note that if you do a mere psql, it will fail since it will try to connect you to a default database having the same name as you (ie. whoami ). template1 is the admin database that is here from the start. Now createdb <dbname> should work. Display all DBs: Use \\list or \\l to display databases. postgres=# \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------------+----------+----------+------------+------------+----------------------- codalab_website | postgres | UTF8 | en_US.utf8 | en_US.utf8 | postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows) Display Tables Use \\dt to list all public tables, and \\dt * for all tables in the current DB you are connected to. postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------------------------------+-------+------- public | account_emailaddress | table | root public | account_emailconfirmation | table | root public | auth_group | table | root public | auth_group_permissions | table | root public | auth_permission | table | root public | authenz_cluser | table | root public | authenz_cluser_groups | table | root public | authenz_cluser_user_permissions | table | root public | captcha_captchastore | table | root public | coopetitions_dislike | table | root public | coopetitions_downloadrecord | table | root public | coopetitions_like | table | root public | customizer_configuration | table | root public | django_admin_log | table | root ... codalab_website=# \\dt * List of relations Schema | Name | Type | Owner ------------+-------------------------------------+-------+---------- pg_catalog | pg_aggregate | table | postgres pg_catalog | pg_am | table | postgres pg_catalog | pg_amop | table | postgres pg_catalog | pg_amproc | table | postgres pg_catalog | pg_attrdef | table | postgres pg_catalog | pg_attribute | table | postgres pg_catalog | pg_auth_members | table | postgres pg_catalog | pg_authid | table | postgres pg_catalog | pg_cast | table | postgres pg_catalog | pg_class | table | postgres ... Display Schema Source objectrocket SELECT column_name FROM information_schema.columns WHERE TABLE_NAME = 'some_table'; Display Connection Information codalab_website-# \\conninfo You are connected to database \"codalab_website\" as user \"root\" via socket in \"/var/run/postgresql\" at port \"5432\". You will never see tables in other databases, these tables aren't visible. You have to connect to the correct database to see its tables (and other objects). Switch DBs: To switch databases: postgres=# \\connect database_name Selects all non-template DBs: postgres=# SELECT datname FROM pg_database WHERE datistemplate = false; Select all tables in current DB connection postgres=# SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name; Delete Database: postgres=# DROP DATABASE [ IF EXISTS ] name Create Database: postgres=# CREATE DATABASE testdb; Create User: See all current users: postgres=# SELECT usename FROM pg_user; Create New User: Bash: $ createuser name or Postgres: postgres=# CREATE USER testuser; Drop User: Bash: $ dropuser name or Postgres: postgres=# DROP USER testuser; Bulk Insert: https://stackoverflow.com/questions/12856377/the-correct-copy-command-to-load-postgresql-data-from-csv-file-that-has-single-q Double single quotes (if standard_conforming_strings is on, see the docs) COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE ''''; or use the non-standard PostgreSQL-specific escape string: COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE E'\\''; If you have a header: COPY my_table FROM 'c:\\downloads\\file.csv' WITH DELIMITER ',' CSV HEADER; use \\copy in bash","title":"Postgres"},{"location":"notes/sql/postgres/Postgres/#postgres","text":"CentOS Stream 8 Notes Courtesy of Stackoverflow","title":"Postgres"},{"location":"notes/sql/postgres/Postgres/#installation","text":"Whoops...I'm sure this is easy...I'll get to it eventually.","title":"Installation"},{"location":"notes/sql/postgres/Postgres/#first-time-use-and-setup","text":"Here's what worked for postgresql-9.1 on Xubuntu 12.04.1 LTS. Connect to the default database with user postgres : sudo -u postgres psql template1 Set the password for user postgres , then exit psql (Ctrl-D) : postgres=# ALTER USER postgres with encrypted password 'xxxxxxx'; Edit the pg_hba.conf file: sudo vim /etc/postgresql/9.1/main/pg_hba.conf And change peer to md5 on the line concerning postgres : local all postgres peer md5 Note: you need sudo or the file will appear blank Restart the database: postgres=# sudo /etc/init.d/postgresql restart\\ (Here you can check it worked with psql -U postgres .) Create a user having the same name as you (to find it, you can type whoami ) : postgres=# createuser -U postgres -d -e -E -l -P -r -s <my_name> The options tell postgresql to create a user that can login, create databases, create new roles, is a superuser, and will have an encrypted password. The really important ones are -P -E , so that you're asked to type the password that will be encrypted, and -d so that you can do a createdb . Beware of passwords : it will first ask you twice the new password (for the new user), repeated, and then once the postgres password (the one specified on step 2). Again, edit the pg_hba.conf file (see step 3 above), and change peer to md5 on the line concerning \"all\" other users : local all all peer md5 Restart (like in step 4), and check that you can login without -U postgres : psql template1 Note that if you do a mere psql, it will fail since it will try to connect you to a default database having the same name as you (ie. whoami ). template1 is the admin database that is here from the start. Now createdb <dbname> should work.","title":"First Time Use and Setup"},{"location":"notes/sql/postgres/Postgres/#display-all-dbs","text":"Use \\list or \\l to display databases. postgres=# \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------------+----------+----------+------------+------------+----------------------- codalab_website | postgres | UTF8 | en_US.utf8 | en_US.utf8 | postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows)","title":"Display all DBs:"},{"location":"notes/sql/postgres/Postgres/#display-tables","text":"Use \\dt to list all public tables, and \\dt * for all tables in the current DB you are connected to. postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------------------------------+-------+------- public | account_emailaddress | table | root public | account_emailconfirmation | table | root public | auth_group | table | root public | auth_group_permissions | table | root public | auth_permission | table | root public | authenz_cluser | table | root public | authenz_cluser_groups | table | root public | authenz_cluser_user_permissions | table | root public | captcha_captchastore | table | root public | coopetitions_dislike | table | root public | coopetitions_downloadrecord | table | root public | coopetitions_like | table | root public | customizer_configuration | table | root public | django_admin_log | table | root ... codalab_website=# \\dt * List of relations Schema | Name | Type | Owner ------------+-------------------------------------+-------+---------- pg_catalog | pg_aggregate | table | postgres pg_catalog | pg_am | table | postgres pg_catalog | pg_amop | table | postgres pg_catalog | pg_amproc | table | postgres pg_catalog | pg_attrdef | table | postgres pg_catalog | pg_attribute | table | postgres pg_catalog | pg_auth_members | table | postgres pg_catalog | pg_authid | table | postgres pg_catalog | pg_cast | table | postgres pg_catalog | pg_class | table | postgres ...","title":"Display Tables"},{"location":"notes/sql/postgres/Postgres/#display-schema","text":"Source objectrocket SELECT column_name FROM information_schema.columns WHERE TABLE_NAME = 'some_table';","title":"Display Schema"},{"location":"notes/sql/postgres/Postgres/#display-connection-information","text":"codalab_website-# \\conninfo You are connected to database \"codalab_website\" as user \"root\" via socket in \"/var/run/postgresql\" at port \"5432\". You will never see tables in other databases, these tables aren't visible. You have to connect to the correct database to see its tables (and other objects).","title":"Display Connection Information"},{"location":"notes/sql/postgres/Postgres/#switch-dbs","text":"To switch databases: postgres=# \\connect database_name Selects all non-template DBs: postgres=# SELECT datname FROM pg_database WHERE datistemplate = false; Select all tables in current DB connection postgres=# SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;","title":"Switch DBs:"},{"location":"notes/sql/postgres/Postgres/#delete-database","text":"postgres=# DROP DATABASE [ IF EXISTS ] name","title":"Delete Database:"},{"location":"notes/sql/postgres/Postgres/#create-database","text":"postgres=# CREATE DATABASE testdb;","title":"Create Database:"},{"location":"notes/sql/postgres/Postgres/#create-user","text":"","title":"Create User:"},{"location":"notes/sql/postgres/Postgres/#see-all-current-users","text":"postgres=# SELECT usename FROM pg_user;","title":"See all current users:"},{"location":"notes/sql/postgres/Postgres/#create-new-user","text":"Bash: $ createuser name or Postgres: postgres=# CREATE USER testuser;","title":"Create New User:"},{"location":"notes/sql/postgres/Postgres/#drop-user","text":"Bash: $ dropuser name or Postgres: postgres=# DROP USER testuser;","title":"Drop User:"},{"location":"notes/sql/postgres/Postgres/#bulk-insert","text":"https://stackoverflow.com/questions/12856377/the-correct-copy-command-to-load-postgresql-data-from-csv-file-that-has-single-q Double single quotes (if standard_conforming_strings is on, see the docs) COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE ''''; or use the non-standard PostgreSQL-specific escape string: COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE E'\\''; If you have a header: COPY my_table FROM 'c:\\downloads\\file.csv' WITH DELIMITER ',' CSV HEADER; use \\copy in bash","title":"Bulk Insert:"},{"location":"notes/sql/postgres/pgadmin/","text":"pgAdmin Running the program Navigate to: /home/bbearce/pgAdmin4/pgAdmin4 note: pgAdmin4/pgAdmin4 is a virtual environment(venv). Activate venv: $ . bin/activate Run pgAdmin: $ python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py","title":"pgAdmin"},{"location":"notes/sql/postgres/pgadmin/#pgadmin","text":"","title":"pgAdmin"},{"location":"notes/sql/postgres/pgadmin/#running-the-program","text":"Navigate to: /home/bbearce/pgAdmin4/pgAdmin4 note: pgAdmin4/pgAdmin4 is a virtual environment(venv). Activate venv: $ . bin/activate Run pgAdmin: $ python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py","title":"Running the program"},{"location":"notes/sql/sql_server/Sql%20Server/","text":"SQL Server SELECT \"serve dat sql...chyeah!\" Microsoft SQL Server can be hard to connect to. Here I'm compiling some notes that help me connect in various situations. Drivers ODBC - pyodbc This is the standard way and it works pretty well. It's pretty simple to use. pyodbc import pyodbc SQL_Server=\"\" Database=\"\" View=\"\" USER=\"\" PASS=\"\" import pyodbc server = f'{SQL_Server}' database = f'{Database}' username = f'{USER}' password = f'{PASS}' driver= '{ODBC Driver 17 for SQL Server}' with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn: with conn.cursor() as cursor: # cursor.execute(\"SELECT TOP 3 name, collation_name FROM sys.databases\") cursor.execute(\"select top 10 * from viewInferencePXS\") row = cursor.fetchone() while row: print (str(row[0]) + \" \" + str(row[1])) row = cursor.fetchone() Much easier than JTDS below... JTDS - Java Tabular Data Stream DataGrip I was finally able to connect with the jtds : jTDS is an open source 100% pure Java (type 4) JDBC 3.0 driver for Microsoft SQL Server (6.5, 7, 2000, 2005, 2008 and 2012) and Sybase Adaptive Server Enterprise (10, 11, 12 and 15). jTDS is based on FreeTDS and is currently the fastest production-ready JDBC driver for SQL Server and Sybase ASE. jTDS is 100% JDBC 3.0 compatible, supporting forward-only and scrollable/updateable ResultSets, concurrent (completely independent) Statements and implementing all the DatabaseMetaData and ResultSetMetaData methods. Check out the feature matrix for more details. In DataGrip they have a connection for this driver and it looks like this when setup correctly: python To use the jtds driver in python you need these things: * install jaydebeapi * Follow this stackoverflow article: https://stackoverflow.com/questions/55376100/create-a-jtds-connection-string-in-python * under jar_path = \"<path_to>\\\\jtds-1.3.1.jar\" we need goto http://jtds.sourceforge.net/ and click on the Download navbar link and download and extract the jtds-1.3.1-dist.zip and then inside the extracted folder the jar is at: jtds-1.3.1-dist/jtds-1.3.1.jar . * In that article you will find a script. Below is an example import sys import jaydebeapi SQL_Server=\"emitsql07\" Database=\"MLInferenceDeploy\" View=\"ViewInferencePXS\" USER=\"bb927\" PASS=\"\" # I shouldn't tell you this ;) def main(): try: # jTDS Driver. driver_name = \"net.sourceforge.jtds.jdbc.Driver\" #driver_name = \"org.hsqldb.jdbcDriver\" # not sure when to use this # jTDS Connection string. connection_url = f\"jdbc:jtds:sqlserver://{SQL_Server}/{Database};useLOBs=false;instance={SQL_Server};domain=PARTNERS\" # jTDS Connection properties. # Some additional connection properties you may want to use # \"domain\": \"<domain>\" # \"ssl\": \"require\" # \"useNTLMv2\": \"true\" # See the FAQ for details http://jtds.sourceforge.net/faq.html connection_properties = { \"user\":USER, \"password\": PASS, } # Path to jTDS Jar # BB- !!! IMPORTANT !!! -BB jar_path = \"/home/bbearce/Documents/jtds-1.3.1-dist/jtds-1.3.1.jar\" # BB- !!! IMPORTANT !!! -BB # Establish connection. connection = jaydebeapi.connect(driver_name, connection_url, connection_properties, jar_path) cursor = connection.cursor() # Execute test query. cursor.execute(\"select 1 as test_connection\") # Execute a real query # cursor.execute(f\"select top 1 * from {View};\") res = cursor.fetchall() if res: print(str(res)) # Should print [(1,)] for test query or have results from a real one except Exception as err: print(str(err)) if __name__ == \"__main__\": sys.exit(main())","title":"SQL server"},{"location":"notes/sql/sql_server/Sql%20Server/#sql-server","text":"SELECT \"serve dat sql...chyeah!\" Microsoft SQL Server can be hard to connect to. Here I'm compiling some notes that help me connect in various situations.","title":"SQL Server"},{"location":"notes/sql/sql_server/Sql%20Server/#drivers","text":"","title":"Drivers"},{"location":"notes/sql/sql_server/Sql%20Server/#odbc-pyodbc","text":"This is the standard way and it works pretty well. It's pretty simple to use. pyodbc import pyodbc SQL_Server=\"\" Database=\"\" View=\"\" USER=\"\" PASS=\"\" import pyodbc server = f'{SQL_Server}' database = f'{Database}' username = f'{USER}' password = f'{PASS}' driver= '{ODBC Driver 17 for SQL Server}' with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn: with conn.cursor() as cursor: # cursor.execute(\"SELECT TOP 3 name, collation_name FROM sys.databases\") cursor.execute(\"select top 10 * from viewInferencePXS\") row = cursor.fetchone() while row: print (str(row[0]) + \" \" + str(row[1])) row = cursor.fetchone() Much easier than JTDS below...","title":"ODBC - pyodbc"},{"location":"notes/sql/sql_server/Sql%20Server/#jtds-java-tabular-data-stream","text":"","title":"JTDS - Java Tabular Data Stream"},{"location":"notes/sql/sql_server/Sql%20Server/#datagrip","text":"I was finally able to connect with the jtds : jTDS is an open source 100% pure Java (type 4) JDBC 3.0 driver for Microsoft SQL Server (6.5, 7, 2000, 2005, 2008 and 2012) and Sybase Adaptive Server Enterprise (10, 11, 12 and 15). jTDS is based on FreeTDS and is currently the fastest production-ready JDBC driver for SQL Server and Sybase ASE. jTDS is 100% JDBC 3.0 compatible, supporting forward-only and scrollable/updateable ResultSets, concurrent (completely independent) Statements and implementing all the DatabaseMetaData and ResultSetMetaData methods. Check out the feature matrix for more details. In DataGrip they have a connection for this driver and it looks like this when setup correctly:","title":"DataGrip"},{"location":"notes/sql/sql_server/Sql%20Server/#python","text":"To use the jtds driver in python you need these things: * install jaydebeapi * Follow this stackoverflow article: https://stackoverflow.com/questions/55376100/create-a-jtds-connection-string-in-python * under jar_path = \"<path_to>\\\\jtds-1.3.1.jar\" we need goto http://jtds.sourceforge.net/ and click on the Download navbar link and download and extract the jtds-1.3.1-dist.zip and then inside the extracted folder the jar is at: jtds-1.3.1-dist/jtds-1.3.1.jar . * In that article you will find a script. Below is an example import sys import jaydebeapi SQL_Server=\"emitsql07\" Database=\"MLInferenceDeploy\" View=\"ViewInferencePXS\" USER=\"bb927\" PASS=\"\" # I shouldn't tell you this ;) def main(): try: # jTDS Driver. driver_name = \"net.sourceforge.jtds.jdbc.Driver\" #driver_name = \"org.hsqldb.jdbcDriver\" # not sure when to use this # jTDS Connection string. connection_url = f\"jdbc:jtds:sqlserver://{SQL_Server}/{Database};useLOBs=false;instance={SQL_Server};domain=PARTNERS\" # jTDS Connection properties. # Some additional connection properties you may want to use # \"domain\": \"<domain>\" # \"ssl\": \"require\" # \"useNTLMv2\": \"true\" # See the FAQ for details http://jtds.sourceforge.net/faq.html connection_properties = { \"user\":USER, \"password\": PASS, } # Path to jTDS Jar # BB- !!! IMPORTANT !!! -BB jar_path = \"/home/bbearce/Documents/jtds-1.3.1-dist/jtds-1.3.1.jar\" # BB- !!! IMPORTANT !!! -BB # Establish connection. connection = jaydebeapi.connect(driver_name, connection_url, connection_properties, jar_path) cursor = connection.cursor() # Execute test query. cursor.execute(\"select 1 as test_connection\") # Execute a real query # cursor.execute(f\"select top 1 * from {View};\") res = cursor.fetchall() if res: print(str(res)) # Should print [(1,)] for test query or have results from a real one except Exception as err: print(str(err)) if __name__ == \"__main__\": sys.exit(main())","title":"python"},{"location":"notes/sql/sqlite/Basics/","text":"Sqlite create database Syntax Following is the basic syntax of sqlite3 command to create a database: $sqlite3 DatabaseName.db Always, database name should be unique within the RDBMS. Example If you want to create a new database , then SQLITE3 statement would be as follows \u2212 $sqlite3 testDB.db SQLite version 3.7.15.2 2013-01-09 11:53:05 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> The above command will create a file testDB.db in the current directory. This file will be used as database by SQLite engine. If you have noticed while creating database, sqlite3 command will provide a sqlite> prompt after creating a database file successfully. Once a database is created, you can verify it in the list of databases using the following SQLite .databases command. sqlite>.databases seq name file --- --------------- ---------------------- 0 main /home/sqlite/testDB.db You will use SQLite .quit command to come out of the sqlite prompt as follows: sqlite>.quit $ The .dump Command You can use .dump dot command to export complete database in a text file using the following SQLite command at the command prompt. $sqlite3 testDB.db .dump > testDB.sql The above command will convert the entire contents of testDB.db database into SQLite statements and dump it into ASCII text file testDB.sql. You can perform restoration from the generated testDB.sql in a simple way as follows: $sqlite3 testDB.db < testDB.sql At this moment your database is empty, so you can try above two procedures once you have few tables and data in your database. For now, let's proceed to the next chapter. .tables The following notes are from sqlitetutorial.net List tables after connecting using .tables $ sqlite3 kaggle_rsna SQLite version 3.11.0 2016-02-15 17:29:24 Enter \".help\" for usage hints. sqlite> .tables all_images final_annotation_list all_images_corrected remove_exams annotations remove_series annotations_corrected study_and_instance_annotation_ids The .tables command also can be used to show temporary tables. See the following example: First, create a new temporary table named temp_table1: sqlite> CREATE TEMPORARY TABLE temp_table1( name TEXT ); Second, list all tables from the database: sqlite> .tables The following shows the output: albums employees invoices playlists artists genres media_types temp.temp_table1 customers invoice_items playlist_track tracks Because the schema of temporary tables is temp, the command showed the names of schema and table of the temporary table such as temp.temp_table1 . If you want to show tables with the specific name, you can add a matching pattern: .tables pattern The command works the same as LIKE operator. The pattern must be surrounded by single quotation marks ('). For example, to find tables whose names start with the letter \u2018a\u2019, you use the following command: sqlite> .table 'a%' Here is the output: albums artists To shows the tables whose name contains the string ck, you use the %ck% pattern as shown in the following command: sqlite> .tables '%ck%' The output is as follows: playlist_track tracks Showing tables using SQL statement: Another way to list all tables in a database is to query them from the sqlite_master table. SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'; Here is the output: name albums customers employees genres invoices invoice_items media_types playlists playlist_track tracks .schema / create table View the schema of a particular table: sqlite> .schema table sqlite> .schema all_images CREATE TABLE all_images( \"InstanceID\" TEXT, \"SeriesID\" TEXT, \"StudyID\" TEXT ); bulk insert If the table doesn't exist, sqlite will try to create it and it's scheme assuming a header. .mode csv assumes a ',' separator. sqlite> .mode csv <table_name> sqlite> .import <path_to_csv> <table_name> if you want to specify a different separator use this instead (I used a specific csv for this...): sqlite> .separator , sqlite> .import <path_to_csv> <table_name> sqlite> .schema CREATE TABLE measurements( \"seriesUID\" TEXT, \"instanceUID\" TEXT, \"length\" TEXT, \"start_x\" TEXT, \"start_y\" TEXT, \"end_x\" TEXT, \"end_y\" TEXT, \"annotator\" TEXT ); I noticed that it was lazy about assigning data types. csv export An example export from the rad table. Any query can be used as the basis for the export, including multi-line complex ones. sqlite> .headers on sqlite> .mode csv sqlite> .output rad_data.csv sqlite> select * from rad; sqlite>","title":"Basics"},{"location":"notes/sql/sqlite/Basics/#sqlite","text":"","title":"Sqlite"},{"location":"notes/sql/sqlite/Basics/#create-database","text":"Syntax Following is the basic syntax of sqlite3 command to create a database: $sqlite3 DatabaseName.db Always, database name should be unique within the RDBMS. Example If you want to create a new database , then SQLITE3 statement would be as follows \u2212 $sqlite3 testDB.db SQLite version 3.7.15.2 2013-01-09 11:53:05 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> The above command will create a file testDB.db in the current directory. This file will be used as database by SQLite engine. If you have noticed while creating database, sqlite3 command will provide a sqlite> prompt after creating a database file successfully. Once a database is created, you can verify it in the list of databases using the following SQLite .databases command. sqlite>.databases seq name file --- --------------- ---------------------- 0 main /home/sqlite/testDB.db You will use SQLite .quit command to come out of the sqlite prompt as follows: sqlite>.quit $","title":"create database"},{"location":"notes/sql/sqlite/Basics/#the-dump-command","text":"You can use .dump dot command to export complete database in a text file using the following SQLite command at the command prompt. $sqlite3 testDB.db .dump > testDB.sql The above command will convert the entire contents of testDB.db database into SQLite statements and dump it into ASCII text file testDB.sql. You can perform restoration from the generated testDB.sql in a simple way as follows: $sqlite3 testDB.db < testDB.sql At this moment your database is empty, so you can try above two procedures once you have few tables and data in your database. For now, let's proceed to the next chapter.","title":"The .dump Command"},{"location":"notes/sql/sqlite/Basics/#tables","text":"The following notes are from sqlitetutorial.net List tables after connecting using .tables $ sqlite3 kaggle_rsna SQLite version 3.11.0 2016-02-15 17:29:24 Enter \".help\" for usage hints. sqlite> .tables all_images final_annotation_list all_images_corrected remove_exams annotations remove_series annotations_corrected study_and_instance_annotation_ids The .tables command also can be used to show temporary tables. See the following example: First, create a new temporary table named temp_table1: sqlite> CREATE TEMPORARY TABLE temp_table1( name TEXT ); Second, list all tables from the database: sqlite> .tables The following shows the output: albums employees invoices playlists artists genres media_types temp.temp_table1 customers invoice_items playlist_track tracks Because the schema of temporary tables is temp, the command showed the names of schema and table of the temporary table such as temp.temp_table1 . If you want to show tables with the specific name, you can add a matching pattern: .tables pattern The command works the same as LIKE operator. The pattern must be surrounded by single quotation marks ('). For example, to find tables whose names start with the letter \u2018a\u2019, you use the following command: sqlite> .table 'a%' Here is the output: albums artists To shows the tables whose name contains the string ck, you use the %ck% pattern as shown in the following command: sqlite> .tables '%ck%' The output is as follows: playlist_track tracks Showing tables using SQL statement: Another way to list all tables in a database is to query them from the sqlite_master table. SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'; Here is the output: name albums customers employees genres invoices invoice_items media_types playlists playlist_track tracks","title":".tables"},{"location":"notes/sql/sqlite/Basics/#schema-create-table","text":"View the schema of a particular table: sqlite> .schema table sqlite> .schema all_images CREATE TABLE all_images( \"InstanceID\" TEXT, \"SeriesID\" TEXT, \"StudyID\" TEXT );","title":".schema / create table"},{"location":"notes/sql/sqlite/Basics/#bulk-insert","text":"If the table doesn't exist, sqlite will try to create it and it's scheme assuming a header. .mode csv assumes a ',' separator. sqlite> .mode csv <table_name> sqlite> .import <path_to_csv> <table_name> if you want to specify a different separator use this instead (I used a specific csv for this...): sqlite> .separator , sqlite> .import <path_to_csv> <table_name> sqlite> .schema CREATE TABLE measurements( \"seriesUID\" TEXT, \"instanceUID\" TEXT, \"length\" TEXT, \"start_x\" TEXT, \"start_y\" TEXT, \"end_x\" TEXT, \"end_y\" TEXT, \"annotator\" TEXT ); I noticed that it was lazy about assigning data types.","title":"bulk insert"},{"location":"notes/sql/sqlite/Basics/#csv-export","text":"An example export from the rad table. Any query can be used as the basis for the export, including multi-line complex ones. sqlite> .headers on sqlite> .mode csv sqlite> .output rad_data.csv sqlite> select * from rad; sqlite>","title":"csv export"},{"location":"notes/sql/sqlite/installing/","text":"Installing Courtest of linuxfromscratch Introduction to SQLite The SQLite package is a software library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. This package is known to build and work properly using an LFS-9.0 platform. Package Information Download (HTTP): https://sqlite.org/2019/sqlite-autoconf-3300100.tar.gz Download MD5 sum: 51252dc6bc9094ba11ab151ba650ff3c Download size: 2.7 MB Estimated disk space required: 73 MB Estimated build time: 0.4 SBU (Using parallelism=4) Additional Downloads Optional Documentation Download (HTTP): https://sqlite.org/2019/sqlite-doc-3300100.zip Download MD5 sum: 0a631f0f293167c82be0c10831642469 Download size: 9.1 MB Installation of SQLite If you downloaded the optional documentation, issue the following command to install the documentation into the source tree: unzip -q ../sqlite-doc-3300100.zip Install SQLite by running the following commands: before that unzip with tar zxvf sqlite-autoconf-3300100.tar.gz and cd sqlite-autoconf-3300100 ./configure --prefix=/usr \\ --disable-static \\ --enable-fts5 \\ CFLAGS=\"-g -O2 \\ -DSQLITE_ENABLE_FTS3=1 \\ -DSQLITE_ENABLE_FTS4=1 \\ -DSQLITE_ENABLE_COLUMN_METADATA=1 \\ -DSQLITE_ENABLE_UNLOCK_NOTIFY=1 \\ -DSQLITE_ENABLE_DBSTAT_VTAB=1 \\ -DSQLITE_SECURE_DELETE=1 \\ -DSQLITE_ENABLE_FTS3_TOKENIZER=1\" && make This package does not come with a test suite. Now, as the root user run sudo make install : bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ sudo make install make[1]: Entering directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' /bin/mkdir -p '/usr/lib' /bin/bash ./libtool --mode=install /usr/bin/install -c libsqlite3.la '/usr/lib' libtool: install: /usr/bin/install -c .libs/libsqlite3.so.0.8.6 /usr/lib/libsqlite3.so.0.8.6 libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so.0 || { rm -f libsqlite3.so.0 && ln -s libsqlite3.so.0.8.6 libsqlite3.so.0; }; }) libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so || { rm -f libsqlite3.so && ln -s libsqlite3.so.0.8.6 libsqlite3.so; }; }) libtool: install: /usr/bin/install -c .libs/libsqlite3.lai /usr/lib/libsqlite3.la libtool: install: /usr/bin/install -c .libs/libsqlite3.a /usr/lib/libsqlite3.a libtool: install: chmod 644 /usr/lib/libsqlite3.a libtool: install: ranlib /usr/lib/libsqlite3.a libtool: warning: remember to run 'libtool --finish /usr/local/lib' /bin/mkdir -p '/usr/bin' /bin/bash ./libtool --mode=install /usr/bin/install -c sqlite3 '/usr/bin' libtool: install: /usr/bin/install -c sqlite3 /usr/bin/sqlite3 /bin/mkdir -p '/usr/include' /usr/bin/install -c -m 644 sqlite3.h sqlite3ext.h '/usr/include' /bin/mkdir -p '/usr/share/man/man1' /usr/bin/install -c -m 644 sqlite3.1 '/usr/share/man/man1' /bin/mkdir -p '/usr/lib/pkgconfig' /usr/bin/install -c -m 644 sqlite3.pc '/usr/lib/pkgconfig' make[1]: Leaving directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' Not sure what warning: remember to run 'libtool --finish /usr/local/lib' is for but I ran it and it didn't break anything and output this message: bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ libtool --finish /usr/local/lib libtool: finish: PATH=\"/home/bbearce/gems/bin:/home/bbearce/bin:/home/bbearce/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin\" ldconfig -n /usr/local/lib ---------------------------------------------------------------------- Libraries have been installed in: /usr/local/lib If you ever happen to want to link against installed libraries in a given directory, LIBDIR, you must either use libtool, and specify the full pathname of the library, or use the '-LLIBDIR' flag during linking and do at least one of the following: - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable during execution - add LIBDIR to the 'LD_RUN_PATH' environment variable during linking - use the '-Wl,-rpath -Wl,LIBDIR' linker flag - have your system administrator add LIBDIR to '/etc/ld.so.conf' See any operating system documentation about shared libraries for more information, such as the ld(1) and ld.so(8) manual pages. ---------------------------------------------------------------------- ...a little background from gnu : GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. If you downloaded the optional documentation, issue the following commands as the root user to install it: install -v -m755 -d /usr/share/doc/sqlite-3.30.1 && cp -v -R sqlite-doc-3300100/* /usr/share/doc/sqlite-3.30.1","title":"Installing"},{"location":"notes/sql/sqlite/installing/#installing","text":"Courtest of linuxfromscratch","title":"Installing"},{"location":"notes/sql/sqlite/installing/#introduction-to-sqlite","text":"The SQLite package is a software library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. This package is known to build and work properly using an LFS-9.0 platform.","title":"Introduction to SQLite"},{"location":"notes/sql/sqlite/installing/#package-information","text":"Download (HTTP): https://sqlite.org/2019/sqlite-autoconf-3300100.tar.gz Download MD5 sum: 51252dc6bc9094ba11ab151ba650ff3c Download size: 2.7 MB Estimated disk space required: 73 MB Estimated build time: 0.4 SBU (Using parallelism=4)","title":"Package Information"},{"location":"notes/sql/sqlite/installing/#additional-downloads","text":"","title":"Additional Downloads"},{"location":"notes/sql/sqlite/installing/#optional-documentation","text":"Download (HTTP): https://sqlite.org/2019/sqlite-doc-3300100.zip Download MD5 sum: 0a631f0f293167c82be0c10831642469 Download size: 9.1 MB","title":"Optional Documentation"},{"location":"notes/sql/sqlite/installing/#installation-of-sqlite","text":"If you downloaded the optional documentation, issue the following command to install the documentation into the source tree: unzip -q ../sqlite-doc-3300100.zip Install SQLite by running the following commands: before that unzip with tar zxvf sqlite-autoconf-3300100.tar.gz and cd sqlite-autoconf-3300100 ./configure --prefix=/usr \\ --disable-static \\ --enable-fts5 \\ CFLAGS=\"-g -O2 \\ -DSQLITE_ENABLE_FTS3=1 \\ -DSQLITE_ENABLE_FTS4=1 \\ -DSQLITE_ENABLE_COLUMN_METADATA=1 \\ -DSQLITE_ENABLE_UNLOCK_NOTIFY=1 \\ -DSQLITE_ENABLE_DBSTAT_VTAB=1 \\ -DSQLITE_SECURE_DELETE=1 \\ -DSQLITE_ENABLE_FTS3_TOKENIZER=1\" && make This package does not come with a test suite. Now, as the root user run sudo make install : bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ sudo make install make[1]: Entering directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' /bin/mkdir -p '/usr/lib' /bin/bash ./libtool --mode=install /usr/bin/install -c libsqlite3.la '/usr/lib' libtool: install: /usr/bin/install -c .libs/libsqlite3.so.0.8.6 /usr/lib/libsqlite3.so.0.8.6 libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so.0 || { rm -f libsqlite3.so.0 && ln -s libsqlite3.so.0.8.6 libsqlite3.so.0; }; }) libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so || { rm -f libsqlite3.so && ln -s libsqlite3.so.0.8.6 libsqlite3.so; }; }) libtool: install: /usr/bin/install -c .libs/libsqlite3.lai /usr/lib/libsqlite3.la libtool: install: /usr/bin/install -c .libs/libsqlite3.a /usr/lib/libsqlite3.a libtool: install: chmod 644 /usr/lib/libsqlite3.a libtool: install: ranlib /usr/lib/libsqlite3.a libtool: warning: remember to run 'libtool --finish /usr/local/lib' /bin/mkdir -p '/usr/bin' /bin/bash ./libtool --mode=install /usr/bin/install -c sqlite3 '/usr/bin' libtool: install: /usr/bin/install -c sqlite3 /usr/bin/sqlite3 /bin/mkdir -p '/usr/include' /usr/bin/install -c -m 644 sqlite3.h sqlite3ext.h '/usr/include' /bin/mkdir -p '/usr/share/man/man1' /usr/bin/install -c -m 644 sqlite3.1 '/usr/share/man/man1' /bin/mkdir -p '/usr/lib/pkgconfig' /usr/bin/install -c -m 644 sqlite3.pc '/usr/lib/pkgconfig' make[1]: Leaving directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' Not sure what warning: remember to run 'libtool --finish /usr/local/lib' is for but I ran it and it didn't break anything and output this message: bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ libtool --finish /usr/local/lib libtool: finish: PATH=\"/home/bbearce/gems/bin:/home/bbearce/bin:/home/bbearce/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin\" ldconfig -n /usr/local/lib ---------------------------------------------------------------------- Libraries have been installed in: /usr/local/lib If you ever happen to want to link against installed libraries in a given directory, LIBDIR, you must either use libtool, and specify the full pathname of the library, or use the '-LLIBDIR' flag during linking and do at least one of the following: - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable during execution - add LIBDIR to the 'LD_RUN_PATH' environment variable during linking - use the '-Wl,-rpath -Wl,LIBDIR' linker flag - have your system administrator add LIBDIR to '/etc/ld.so.conf' See any operating system documentation about shared libraries for more information, such as the ld(1) and ld.so(8) manual pages. ---------------------------------------------------------------------- ...a little background from gnu : GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. If you downloaded the optional documentation, issue the following commands as the root user to install it: install -v -m755 -d /usr/share/doc/sqlite-3.30.1 && cp -v -R sqlite-doc-3300100/* /usr/share/doc/sqlite-3.30.1","title":"Installation of SQLite"},{"location":"notes/sql/sqlite/useful_everyday_useage/","text":"Useful Everyday Code Drop If Exists DROP TABLE [IF EXISTS] [schema_name.]table_name; Select Into source stackoverflow CREATE TABLE equipments_backup AS SELECT * FROM equipments Ranks / Row_Number Courtest of sqlitetutorial Window function support was first added to SQLite with release version 3.25.0 (2018-09-15). The SQLite developers used the PostgreSQL window function documentation as their primary reference for how window functions ought to behave. The RANK() function is a window function that assigns a rank to each row in a query\u2019s result set. The rank of a row is calculated by one plus the number of ranks that comes before it. The following shows the syntax of the RANK() function: RANK() OVER ( PARTITION BY <expression1>[{,<expression2>...}] ORDER BY <expression1> [ASC|DESC], [{,<expression1>...}] ) Courtesty of sqlite SQLite supports the following 11 built-in window functions: row_number() The number of the row within the current partition. Rows are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition, or in arbitrary order otherwise. rank() The row_number() of the first peer in each group - the rank of the current row with gaps. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. dense_rank() The number of the current row's peer group within its partition - the rank of the current row without gaps. Partitions are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. percent_rank() Despite the name, this function always returns a value between 0.0 and 1.0 equal to (rank - 1)/(partition-rows - 1), where rank is the value returned by built-in window function rank() and partition-rows is the total number of rows in the partition. If the partition contains only one row, this function returns 0.0. cume_dist() The cumulative distribution. Calculated as row-number/partition-rows, where row-number is the value returned by row_number() for the last peer in the group and partition-rows the number of rows in the partition. ntile(N) Argument N is handled as an integer. This function divides the partition into N groups as evenly as possible and assigns an integer between 1 and N to each group, in the order defined by the ORDER BY clause, or in arbitrary order otherwise. If necessary, larger groups occur first. This function returns the integer value assigned to the group that the current row is a part of. Add/Change Column Data Type Source techonthenet Add Column The syntax to ADD A COLUMN in a table in SQLite (using the ALTER TABLE statement) is: ALTER TABLE table_name ADD new_column_name column_definition; Change Column You can not use the ALTER TABLE statement to modify a column in SQLite. Instead you will need to rename the table, create a new table, and copy the data into the new table. Syntax The syntax to MODIFY A COLUMN in a table in SQLite is: PRAGMA foreign_keys=off; BEGIN TRANSACTION; ALTER TABLE table1 RENAME TO _table1_old; CREATE TABLE table1 ( ( column1 datatype [ NULL | NOT NULL ], column2 datatype [ NULL | NOT NULL ], ... ); INSERT INTO table1 (column1, column2, ... column_n) SELECT column1, column2, ... column_n FROM _table1_old; COMMIT; PRAGMA foreign_keys=on; Pivot Table Courtesty of modern-sql Essentially what you need to do is a subquery in which the inner query has the data you want to source and pivot. The outer query will group by a key and explicitly define the columns that should be created from values in a particular row. Use FILTER to filter for the rows that will belong to each column. Note: This should be replaced by CASE statements in the future. SELECT year , SUM(revenue) FILTER (WHERE month = 1) jan_revenue , SUM(revenue) FILTER (WHERE month = 2) feb_revenue ... , SUM(revenue) FILTER (WHERE month = 12) dec_revenue FROM (SELECT invoices.* , EXTRACT(YEAR FROM invoice_date) year , EXTRACT(MONTH FROM invoice_date) month FROM invoices ) invoices GROUP BY year Julian Day Source techonthenet Syntax: The syntax for the julianday function in SQLite is: julianday(timestring [, modifier1, modifier2, ... modifier_n ] ) More concrete example: select mrn, date, julianday(date) as j_day from mrns_dates; output: # mrn date j_day 1 ####### 2011-04-19 2455670.5 2 ####### 2011-09-26 2455830.5 3 ####### 2012-02-01 2455958.5 4 ####### 2012-03-19 2456005.5 5 ####### 2012-04-24 2456041.5 6 ####### 2014-07-30 2456868.5 Notes: A date value. It can be one of the following: timestring Explanation now now is a literal used to return the current date YYYY-MM-DD Date value formatted as 'YYYY-MM-DD' YYYY-MM-DD HH:MM Date value formatted as 'YYYY-MM-DD HH:MM' YYYY-MM-DD HH:MM:SS Date value formatted as 'YYYY-MM-DD HH:MM:SS' YYYY-MM-DD HH:MM:SS.SSS Date value formatted as 'YYYY-MM-DD HH:MM:SS.SSS' HH:MM Date value formatted as 'HH:MM' HH:MM:SS Date value formatted as 'HH:MM:SS' HH:MM:SS.SSS Date value formatted as 'HH:MM:SS.SSS' YYYY-MM-DDTHH:MM Date value formatted as 'YYYY-MM-DDTHH:MM' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS Date value formatted as 'YYYY-MM-DDTHH:MM:SS' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS.SSS Date value formatted as 'YYYY-MM-DDTHH:MM:SS.SSS' where T is a literal character separating the date and time portions DDDDDDDDDD Julian date number Many to Many Relationship /*display formatting*/ .mode column .headers on CREATE TABLE books (book_id int, title varchar, description varchar); CREATE TABLE bookAuthor (bookAuthor int, author_id, book_id); CREATE TABLE authors (author_id int, last_name varchar, first_name varchar); insert into books (book_id, title, description) values(1, 'book1', 'Ben''s book'); insert into books (book_id, title, description) values(2, 'book2', 'Yuki''s book'); insert into books (book_id, title, description) values(3, 'book3', 'Collaboration of Yuki and Ben'); insert into bookAuthor (bookAuthor, author_id, book_id) values(1, 1, 1); /*Ben wrote this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(2, 2, 2); /*Yuki wrote this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(3, 1, 3); /*Ben wrote some of this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(4, 2, 3); /*Yuki wrote some of this book*/ insert into authors (author_id, last_name, first_name) values(1, 'Bearce', 'Ben'); insert into authors (author_id, last_name, first_name) values(2, 'Davidoff', 'Yuki'); select * from books; select * from bookAuthor; select * from authors; Which shows: sqlite> select * from books; book_id title description ---------- ---------- ----------- 1 book1 Ben's book 2 book2 Yuki's book 3 book3 Collaborati sqlite> select * from bookAuthor; bookAuthor author_id book_id ---------- ---------- ---------- 1 1 1 2 2 2 3 1 3 4 2 3 sqlite> select * from authors; author_id last_name first_name ---------- ---------- ---------- 1 Bearce Ben 2 Davidoff Yuki sqlite> select * from books; book_id title description ---------- ---------- ----------- 1 book1 Ben's book 2 book2 Yuki's book 3 book3 Collaborati sqlite> select * from books where title = 'book3'; book_id title description ---------- ---------- ----------------------------- 3 book3 Collaboration of Yuki and Ben Now we can showcase the purpose of such a join: select b.title, a.first_name, b.description from books as b inner join bookAuthor as ba on b.book_id = ba.book_id inner join authors as a on a.author_id = ba.author_id; which gives: title first_name description ---------- ---------- ----------- book1 Ben Ben's book book2 Yuki Yuki's book book3 Ben Collaborati book3 Yuki Collaborati","title":"Useful Code"},{"location":"notes/sql/sqlite/useful_everyday_useage/#useful-everyday-code","text":"","title":"Useful Everyday Code"},{"location":"notes/sql/sqlite/useful_everyday_useage/#drop-if-exists","text":"DROP TABLE [IF EXISTS] [schema_name.]table_name;","title":"Drop If Exists"},{"location":"notes/sql/sqlite/useful_everyday_useage/#select-into","text":"source stackoverflow CREATE TABLE equipments_backup AS SELECT * FROM equipments","title":"Select Into"},{"location":"notes/sql/sqlite/useful_everyday_useage/#ranks-row_number","text":"Courtest of sqlitetutorial Window function support was first added to SQLite with release version 3.25.0 (2018-09-15). The SQLite developers used the PostgreSQL window function documentation as their primary reference for how window functions ought to behave. The RANK() function is a window function that assigns a rank to each row in a query\u2019s result set. The rank of a row is calculated by one plus the number of ranks that comes before it. The following shows the syntax of the RANK() function: RANK() OVER ( PARTITION BY <expression1>[{,<expression2>...}] ORDER BY <expression1> [ASC|DESC], [{,<expression1>...}] ) Courtesty of sqlite SQLite supports the following 11 built-in window functions: row_number() The number of the row within the current partition. Rows are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition, or in arbitrary order otherwise. rank() The row_number() of the first peer in each group - the rank of the current row with gaps. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. dense_rank() The number of the current row's peer group within its partition - the rank of the current row without gaps. Partitions are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. percent_rank() Despite the name, this function always returns a value between 0.0 and 1.0 equal to (rank - 1)/(partition-rows - 1), where rank is the value returned by built-in window function rank() and partition-rows is the total number of rows in the partition. If the partition contains only one row, this function returns 0.0. cume_dist() The cumulative distribution. Calculated as row-number/partition-rows, where row-number is the value returned by row_number() for the last peer in the group and partition-rows the number of rows in the partition. ntile(N) Argument N is handled as an integer. This function divides the partition into N groups as evenly as possible and assigns an integer between 1 and N to each group, in the order defined by the ORDER BY clause, or in arbitrary order otherwise. If necessary, larger groups occur first. This function returns the integer value assigned to the group that the current row is a part of.","title":"Ranks / Row_Number"},{"location":"notes/sql/sqlite/useful_everyday_useage/#addchange-column-data-type","text":"Source techonthenet","title":"Add/Change Column Data Type"},{"location":"notes/sql/sqlite/useful_everyday_useage/#add-column","text":"The syntax to ADD A COLUMN in a table in SQLite (using the ALTER TABLE statement) is: ALTER TABLE table_name ADD new_column_name column_definition;","title":"Add Column"},{"location":"notes/sql/sqlite/useful_everyday_useage/#change-column","text":"You can not use the ALTER TABLE statement to modify a column in SQLite. Instead you will need to rename the table, create a new table, and copy the data into the new table. Syntax The syntax to MODIFY A COLUMN in a table in SQLite is: PRAGMA foreign_keys=off; BEGIN TRANSACTION; ALTER TABLE table1 RENAME TO _table1_old; CREATE TABLE table1 ( ( column1 datatype [ NULL | NOT NULL ], column2 datatype [ NULL | NOT NULL ], ... ); INSERT INTO table1 (column1, column2, ... column_n) SELECT column1, column2, ... column_n FROM _table1_old; COMMIT; PRAGMA foreign_keys=on;","title":"Change Column"},{"location":"notes/sql/sqlite/useful_everyday_useage/#pivot-table","text":"Courtesty of modern-sql Essentially what you need to do is a subquery in which the inner query has the data you want to source and pivot. The outer query will group by a key and explicitly define the columns that should be created from values in a particular row. Use FILTER to filter for the rows that will belong to each column. Note: This should be replaced by CASE statements in the future. SELECT year , SUM(revenue) FILTER (WHERE month = 1) jan_revenue , SUM(revenue) FILTER (WHERE month = 2) feb_revenue ... , SUM(revenue) FILTER (WHERE month = 12) dec_revenue FROM (SELECT invoices.* , EXTRACT(YEAR FROM invoice_date) year , EXTRACT(MONTH FROM invoice_date) month FROM invoices ) invoices GROUP BY year","title":"Pivot Table"},{"location":"notes/sql/sqlite/useful_everyday_useage/#julian-day","text":"Source techonthenet Syntax: The syntax for the julianday function in SQLite is: julianday(timestring [, modifier1, modifier2, ... modifier_n ] ) More concrete example: select mrn, date, julianday(date) as j_day from mrns_dates; output: # mrn date j_day 1 ####### 2011-04-19 2455670.5 2 ####### 2011-09-26 2455830.5 3 ####### 2012-02-01 2455958.5 4 ####### 2012-03-19 2456005.5 5 ####### 2012-04-24 2456041.5 6 ####### 2014-07-30 2456868.5 Notes: A date value. It can be one of the following: timestring Explanation now now is a literal used to return the current date YYYY-MM-DD Date value formatted as 'YYYY-MM-DD' YYYY-MM-DD HH:MM Date value formatted as 'YYYY-MM-DD HH:MM' YYYY-MM-DD HH:MM:SS Date value formatted as 'YYYY-MM-DD HH:MM:SS' YYYY-MM-DD HH:MM:SS.SSS Date value formatted as 'YYYY-MM-DD HH:MM:SS.SSS' HH:MM Date value formatted as 'HH:MM' HH:MM:SS Date value formatted as 'HH:MM:SS' HH:MM:SS.SSS Date value formatted as 'HH:MM:SS.SSS' YYYY-MM-DDTHH:MM Date value formatted as 'YYYY-MM-DDTHH:MM' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS Date value formatted as 'YYYY-MM-DDTHH:MM:SS' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS.SSS Date value formatted as 'YYYY-MM-DDTHH:MM:SS.SSS' where T is a literal character separating the date and time portions DDDDDDDDDD Julian date number","title":"Julian Day"},{"location":"notes/sql/sqlite/useful_everyday_useage/#many-to-many-relationship","text":"/*display formatting*/ .mode column .headers on CREATE TABLE books (book_id int, title varchar, description varchar); CREATE TABLE bookAuthor (bookAuthor int, author_id, book_id); CREATE TABLE authors (author_id int, last_name varchar, first_name varchar); insert into books (book_id, title, description) values(1, 'book1', 'Ben''s book'); insert into books (book_id, title, description) values(2, 'book2', 'Yuki''s book'); insert into books (book_id, title, description) values(3, 'book3', 'Collaboration of Yuki and Ben'); insert into bookAuthor (bookAuthor, author_id, book_id) values(1, 1, 1); /*Ben wrote this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(2, 2, 2); /*Yuki wrote this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(3, 1, 3); /*Ben wrote some of this book*/ insert into bookAuthor (bookAuthor, author_id, book_id) values(4, 2, 3); /*Yuki wrote some of this book*/ insert into authors (author_id, last_name, first_name) values(1, 'Bearce', 'Ben'); insert into authors (author_id, last_name, first_name) values(2, 'Davidoff', 'Yuki'); select * from books; select * from bookAuthor; select * from authors; Which shows: sqlite> select * from books; book_id title description ---------- ---------- ----------- 1 book1 Ben's book 2 book2 Yuki's book 3 book3 Collaborati sqlite> select * from bookAuthor; bookAuthor author_id book_id ---------- ---------- ---------- 1 1 1 2 2 2 3 1 3 4 2 3 sqlite> select * from authors; author_id last_name first_name ---------- ---------- ---------- 1 Bearce Ben 2 Davidoff Yuki sqlite> select * from books; book_id title description ---------- ---------- ----------- 1 book1 Ben's book 2 book2 Yuki's book 3 book3 Collaborati sqlite> select * from books where title = 'book3'; book_id title description ---------- ---------- ----------------------------- 3 book3 Collaboration of Yuki and Ben Now we can showcase the purpose of such a join: select b.title, a.first_name, b.description from books as b inner join bookAuthor as ba on b.book_id = ba.book_id inner join authors as a on a.author_id = ba.author_id; which gives: title first_name description ---------- ---------- ----------- book1 Ben Ben's book book2 Yuki Yuki's book book3 Ben Collaborati book3 Yuki Collaborati","title":"Many to Many Relationship"}]}